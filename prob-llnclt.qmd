# Law of Large Numbers and Central Limit Theorem {#sec-prob-llnclt}

```{r}
#| echo: false
source("./_common.R")
```

```{python}
import numpy as np
import matplotlib.pyplot as plt
```

{{< include macros.qmd >}}


In sampling distribution @sec-prob-sampling, we learn that as the sample size $n$ grows, the sampling distribution of sample mean 

1. has the mean identical to the expected value of the random variable being sampled, or the population mean
2. is less variable with a smaller variance
3. looks more like a normal distribution 

You may be wondering if it is always true. The answer is *yes* if the sample mean is the average of independent and identically distributed random variables. These properties are proved by the two theorems: the **Law of Large Numbers** and the **Central Limit Theorem**.


## Law of Large Numbers

The Law of Large Numbers (LLN) says that 

*If we keep on taking larger and larger random or iid samples (larger $n$), the statistic sample mean $\overline{X}$ is guaranteed to get closer and closer to the population mean $\mu = E(X)$ if the mean exists.*

Therefore, with LLN, if $E(X) = \mu$, $\overline{X} \approx \mu$ when $n$ is sufficiently large. It is always true, and it does not matter whether we know the value of $\mu$ or not, as long as $\mu$ exists or it is finite.

```{r}
#| label: fig-lln
#| fig-cap: "Illustration of Law of Large Numbers. The population mean is assumed having value three. The simulation is run twice."
#| code-fold: true

set.seed(2024)
x <- rnorm(1000, 3, 5); s <- cumsum(x)
set.seed(2025)
x1 <- rnorm(1000, 3, 5); s1 <- cumsum(x1)

par(mar = c(4, 4, 2, 1))
plot(s / (1:1000), type = "l", 
     main = "Law of Large Numbers with population mean 3", 
     xlab = "sample size (n)", ylab = "sample mean", las = 1)
lines(1:1000, s1 / (1:1000), col = 3)
abline(h = 3, col = "red", lwd = 2, lty = 2)
```

@fig-lln illustrates LLN using a simulation. We consider the sample size $n = 1, 2, \dots, 1000$. For each $n$, we draw a random sample $x_1, \dots, x_n$ from a normal distribution $N(3, 5^2)$, and the corresponding sample mean is $\bar{x} = (x_1+ \cdots + x_n)/n$. When $n$ is small, $\bar{x}$ is quite from away from the population mean value $\mu = 3$ due to the randomness of the samples. For example, in this simulation, $x_1$, $x_2$, and $x_3$ are `r round(x[1], 2)`, `r round(x[2], 2)`, and `r round(x[3], 2)` respectively, and hence the sample mean $\bar{x}_3 =$ `r round(mean(x[1:3]), 2)`. Each $x_i$ comes from $N(3, 5^2)$. Although their expected value is 3, keep in mind that theoretically its realized value can be any real value. With larger variance, it is more likely to have a value away from its expected value. When the sample size is small, such randomness generally will not be washed out simply by averaging. Therefore, the sample mean is not that close to the population.

The marvelous part is that when the sample size is large, such randomness or the deviation from the mean can be washed out via averaging, and the sample mean will get closer to the population mean. In the figure, two simulations are run, and both show that when the sample size is over 200, the sample mean is quite close to the population mean, and it does not fluctuate much as when the sample size is small. This is probably one of the reasons why we love large sample size. Heard **Big Data** before? When we don't know but want to sort of guess or learn the value of $\mu$, we can draw a larger sample from the target population, calculate the sample mean, then use it as a *proxy* or *representative* of the population mean. The entire process of using sample statistics to learn unknown population properties or quantities is called **statistical inference** which will be discussed in detail starting next part.

:::{.callout-note}
- Remember that sine $\overline{X}$ is a transformation of random variables $X_1, \dots, X_n$, $\overline{X}$ itself is also a random variable with some randomness and variation. Therefore, when the simulation (random sampling) is run twice (independently), for each size of $n$, we will get two different sets of values of $x_1, \dots, x_n$, and two different $\bar{x}$s, resulting in two lines in @fig-lln. 

- Additionally, as shown in the figure, the black and green values of $\overline{X}$ are quite different when $n$ is small, showing a larger variation. Why? It comes from the fact that $\Var(\overline{X}) = \sigma^2/n$. It also tells us why $\overline{X}$ converges to $\mu$. When $n$ gets large, its variation gets small, and eventually this random variable behaves like a constant with no variation. That constant is $\mu$.
:::


::::: {.panel-tabset}

## Algorithm
::: {.instructions}

**Simulation of LLN (Normal distribution example)**  

1. Consider different sample size $n$. 

2. For each size of $n$, do:
    + **[1]** Draw values $x_1, x_2, \dots, x_{n}$ from $N(\mu, \sigma^2)$ for some values of $\mu$ and $\sigma^2$.
    + **[2]** Compute $\overline{x}_n = \frac{1}{n}\sum_{i=1}^n x_i$.

3. Plot $\overline{x}_n$ (y-axis) vs. $n$ (x-axis).
:::

## R Simulation Result

```{r ref.label = "lln"}
#| purl: false
#| echo: false
#| warning: false
#| cache: true
```


## R Code
```{r}
#| eval: false
#| label: lln
#| echo: !expr c(-1)
par(mar = c(4, 4, 2, 1))
mu <- 3
sd <- 5
nn <- 1000

## Algorithm
# [1] Generate normal distribution samples
set.seed(1000)
x <- rnorm(nn, mean = mu, sd = sd)
# [2] Calculate cumulative mean
x_bar <- cumsum(x) / seq_len(nn)

## Plotting
plot(x_bar, type = "l", 
     main = "Law of Large Numbers w/ mu = 3", 
     xlab = "sample size (n)", 
     ylab = "sample mean", las = 1)
abline(h = 3, col = "red", lwd = 2, lty = 2)
```

## Python Simulation Result

```{python ref.label = "lln-py"}
#| purl: false
#| echo: false
#| warning: false
#| cache: true
```


## Python Code
```{python}
#| eval: false
#| label: lln-py
#| echo: true

import numpy as np
import matplotlib.pyplot as plt

# Parameters
mu = 3
sd = 5
nn = 1000

## Algorithm
# [1] Generate normal distribution samples
np.random.seed(1000)
x = np.random.normal(mu, sd, nn)
# [2] Calculate cumulative mean
x_bar = np.cumsum(x) / np.arange(1, nn + 1)

# Plotting
plt.figure(figsize=(8, 6))
plt.plot(x_bar, label="Sample Mean")
plt.axhline(y=mu, color='red', linestyle='--', linewidth=2, label="True Mean (mu=3)")
plt.title("Law of Large Numbers w/ mu = 3")
plt.xlabel("Sample size (n)")
plt.ylabel("Sample mean")
plt.legend()
plt.show()
```

:::::

## Central Limit Theorem

It's time to talk about the most important theorem in probability and statistics, at least in my opinion, the **central limit theorem (CLT)**.

In sampling distribution @sec-prob-sampling, we learned that if $X_i \stackrel{iid}{\sim} N(\mu, \sigma^2)$, then $\overline{X} \sim N\left(\mu, \frac{\sigma^2}{n} \right)$. But the question is <span style="color:blue"> what if the population distribution is **NOT** normal? </span> What does the sampling distribution of the sample mean look like if the population distribution is multimodal? or skewed? or not bell-shaped? Well, the central limit theorem gives us the answer!

<!-- <center> -->
<!-- The **central limit theorem (CLT)** gives us the answer! -->
<!-- </center> -->

<!-- <br> -->

**Central Limit Theorem (CLT)**: 

*Suppose $\overline{X}$ is the sample mean from a random sample of size $n$ and from a population distribution having mean $\mu$ and standard deviation $\sigma < \infty$. As $n$ increases, the sampling distribution of $\overline{X}$ looks more and more like $N(\mu, \sigma^2/n)$ regardless of the distribution from which we are sampling!*

```{r}
#| label: fig-clt
#| fig-cap: "Illustration of Central Limit Theorem. Source: Wiki."
#| echo: false
knitr::include_graphics("./images/img-prob/clt.png")
```

@fig-clt illustrates the CLT. First, the random sample $(X_1, \dots, X_n)$ can be collected from any population distribution, whether it is normal or not. The magic part is that the sampling distribution of the sample mean $\overline{X}$ always looks like normal distribution $N(\mu, \sigma^2/n)$ as long as the sample size $n$ is sufficiently large. The larger $n$ is, the more normal-like the sampling distribution of $\overline{X}$ is. One question is how large is enough for $n$. Amazingly the normal approximation is quite well when $n \ge 30$. The variance of the sampling distribution which is $\sigma^2/n$ is decreasing with $n$ as well.

Please try the [app](https://www.statcrunch.com/applets/type3&samplingdist) and see how the shape of the sampling distribution changes with the sample size $n$ and with the shape of the population distribution. You will find that it requires larger $n$ to get a more normal-like sampling distribution if the population distribution is very skewed. You can also see how the CLT works in @fig-clt-right and @fig-clt-u. The population distribution can be discrete, like binomial or Poisson distribution. Their sampling distribution of $\overline{X}$ will still look like normal although the sampling distribution is not continuous.

<!-- ------------------------------------------------------------------ -->

<!-- <span style="color:blue"> **Illustration of the Central Limit Theorem** </span> -->


```{r}
#| label: fig-clt-right
#| fig-asp: 0.3
#| fig-cap: "CLT Illustration: A Right-Skewed Distribution."
#| echo: false
par(mfrow = c(1, 4))
par(mar = c(4, 1, 1, 0), mgp = c(2, 0.5, 0), las = 1)
shape <- 1
rate <- 1/2
# gamma_sample <- rgamma(1000, shape = shape, rate = rate)
# hist(gamma_sample, breaks = 30, col = "#FFCC00", border = "white", xlab = "x",
#      main = "Population distr. w/ mean 2 var 4", prob = TRUE)
x <- seq(0, 12, length.out = 1000)
plot(x, dgamma(x, shape = shape, rate = rate), type="n", xlab="x", ylab="", axes = TRUE, ylim = c(0, 0.6),
     main = "Population distr. w/ mean 2 var 4", cex.main = 0.8)
lines(x, dgamma(x, shape = shape, rate = rate), col = "#003366", lwd = 3)
polygon(c(x,rev(x)), c(rep(0, 1000), rev(dgamma(x, shape = shape, rate = rate))), col="#FFCC00")

# lines(density(gamma_sample), col = "red", lwd = 4) 
n_s <- 2
n_m <- 10
n_l <- 1000
sample_s <- replicate(10000, rgamma(n_s, shape = shape, rate = rate))
sample_m <- replicate(10000, rgamma(n_m, shape = shape, rate = rate))
sample_l <- replicate(10000, rgamma(n_l, shape= shape, rate = rate))
sample_mean_s <- apply(sample_s, 2, mean)
sample_mean_m <- apply(sample_m, 2, mean)
sample_mean_l <- apply(sample_l, 2, mean)
x_s <- seq(-6, 8, length.out = 1000)
x_m <- seq(-1, 5, length.out = 1000)
x_l <- seq(1.7, 2.3, length.out = 1000)
hist(sample_mean_s, breaks = 30, col = 4, border = "white", xlim = range(x_s),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_s, ")"),
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_s, dnorm(x_s, shape/rate, sqrt(shape/rate^2)/sqrt(n_s)), lwd = 3, col = "red")
# lines(density(sample_mean_s), col = "red", lwd = 4) 
hist(sample_mean_m, breaks = 30, col = 4, border = "white", xlim = range(x_m),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_m, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_m, dnorm(x_m, shape/rate, sqrt(shape/rate^2)/sqrt(n_m)), lwd = 3, col = "red")
# lines(density(sample_mean_m), col = "red", lwd = 4) 
hist(sample_mean_l, breaks = 30, col = 4, border = "white", xlim = range(x_l),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_l, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
lines(x_l, dnorm(x_l, shape/rate, sqrt(shape/rate^2)/sqrt(n_l)), lwd = 1, col = "red")
# lines(density(sample_mean_l), col = "red", lwd = 4) 
# paste("Mean of the sample mean of size 1000 =", mean(sample_mean_l))
# paste("Variance of the sample mean of size 1000 =", var(sample_mean_l))
# paste("True mean of the sample mean =", shape / rate)
# paste("True variance of the sample mean =", (shape / rate ^ 2) / n_l)
```

```{r}
#| label: fig-clt-u
#| fig-cap: "CLT Illustration: A U-shaped Distribution."
#| echo: false
#| fig-asp: 0.3
par(mfrow = c(1, 4))
par(mar = c(4, 1, 1, 0), mgp = c(2, 0.5, 0), las = 1)
shape1 <- 1/2
shape2 <- 1/2
x <- seq(0, 1, length.out = 1000)
plot(x, dbeta(x, shape1 = shape1, shape2 = shape2), type="n", xlab="x", ylab="", axes = TRUE, ylim = c(0, 3), cex.main = 0.8,
     main = "Popu. distr. w/ mu 0.5 var 0.125")
lines(x, dbeta(x, shape1 = shape1, shape2 = shape2), col = "#003366", lwd = 3)
# polygon(c(x, rev(x)), c(rev(rep(0, 1000)), (dbeta(x, shape1 = shape1, shape2 = shape2))), col="#FFCC00")

# lines(density(gamma_sample), col = "red", lwd = 4) 
n_s <- 2
n_m <- 10
n_l <- 1000
sample_s <- replicate(10000, rbeta(n_s, shape1 = shape1, shape2 = shape2))
sample_m <- replicate(10000, rbeta(n_m, shape1 = shape1, shape2 = shape2))
sample_l <- replicate(10000, rbeta(n_l, shape1 = shape1, shape2 = shape2))
sample_mean_s <- apply(sample_s, 2, mean)
sample_mean_m <- apply(sample_m, 2, mean)
sample_mean_l <- apply(sample_l, 2, mean)
x_s <- seq(-0.5, 1.5, length.out = 1000)
x_m <- seq(0, 1, length.out = 1000)
x_l <- seq(0.45, 0.55, length.out = 1000)
hist(sample_mean_s, breaks = 20, col = 4, border = "white", xlim = range(x_s),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_s, ")"),
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_s, dnorm(x_s, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_s)), lwd = 3, col = "red")
# lines(density(sample_mean_s), col = "red", lwd = 4) 
hist(sample_mean_m, breaks = 20, col = 4, border = "white", xlim = range(x_m),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_m, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_m, dnorm(x_m, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_m)), lwd = 3, col = "red")
# lines(density(sample_mean_m), col = "red", lwd = 4) 
hist(sample_mean_l, breaks = 20, col = 4, border = "white", xlim = range(x_l),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_l, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
lines(x_l, dnorm(x_l, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_l)), lwd = 1, col = "red")
# lines(density(sample_mean_l), col = "red", lwd = 4) 
# paste("Mean of the sample mean of size 1000 =", mean(sample_mean_l))
# paste("Variance of the sample mean of size 1000 =", var(sample_mean_l))
# paste("Theoretical mean of the sample mean =", shape1/(shape1+shape2))
# paste("Theoretical variance of the sample mean =", 0.125 / n_l)
```




In sum, for a random sample $(X_1, \dots, X_n)$, if the population distribution is normally distributed, then of course with no surprise the sampling distribution of the sample mean is also exactly normally distributed. If the population distribution is not normally distributed, as long as its mean and variance exist, its sampling distribution of the sample mean will still look like a normal distribution when the sample size $n$ is large enough.


- $X_i \stackrel{iid}{\sim} N(\mu, \sigma^2)$. $\overline{X} \sim N\left(\mu, \frac{\sigma^2}{n} \right)$

- $X_i \stackrel{iid}{\sim}$ *any* distribution ($\mu, \sigma^2$). $\overline{X}$ looks like $N\left(\mu, \frac{\sigma^2}{n} \right)$ (for $n$ sufficiently large)



<!-- ------------------------------------------------------------------ -->

<!-- <span style="color:blue"> **** </span> -->

Why is the central limit theorem Important? Many well-developed statistical methods are based on the normal distribution assumption. With the central limit theorem, we can use these methods even if we are sampling from a non-normal distribution or if we have no idea what the population distribution is, provided that the sample size is large enough.


::::: {.panel-tabset}

## Algorithm
To show the distribution of $\overline{X}$ looks like Gaussian when $n$ is large, we need lots of values of $\overline{X}$ and draw a histogram.

::: {.instructions}

**Simulation of CLT (Poisson distribution example)**  

1. Consider different sample size $n$. Set the sample size of $\overline{X}$ be $M$.

2. For each size of $n$, do:
    - **[1]** For each $m = 1, 2, \dots, M$, draw sample $(x_1^m, x_2^m, \dots, x_n^m)$ from some distribution with mean $\mu$ and standard deviation $\sigma$.
    - **[2]** For each $m = 1, 2, \dots, M$, compute $\overline{x}^m_n = \frac{1}{n}\sum_{i=1}^n x_i^m$.
    - **[3]** Draw a histogram of the sample $(\overline{x}^1_n, \overline{x}^2_n, \dots, \overline{x}^M_n)$

:::

## R Simulation Result
```{r ref.label = "clt"}
#| purl: false
#| echo: false
#| warning: false
#| cache: true
```

## R Code

```{r}
#| eval: false
#| label: clt
#| echo: !expr c(-1)

n_vec <- c(2, 8, 100)
M <- 10000
lambda_par <- 1

for (k in n_vec) {
    ## [1]
    x_sam <- matrix(0, nrow = k, ncol = M)
    for (m in seq_len(M)) {
        x_sam[, m] <- rpois(k, lambda = lambda_par)
    }
    ## [2]
    x_bar_sam <- apply(x_sam, 2, mean)
    
    ## [3]
    hist(x_bar_sam, main = paste("size =", k))
}
```

## Python Simulation Result
```{python ref.label = "clt-py"}
#| purl: false
#| echo: false
#| warning: false
#| cache: true
```

## Python Code

The `np.random.poisson` function is used to generate Poisson-distributed random samples. The shape `(k, M)` ensures that you generate k samples for each of the M simulations.

The `np.mean(x_sam, axis=0)` computes the mean of the k samples for each of the M simulations.

`f"Sample size = {k}"` uses a feature in Python called [f-strings (formatted string literals)](https://docs.python.org/3/tutorial/inputoutput.html#tut-f-strings). The `f` before the opening quote marks indicates that this is a formatted string. It tells Python that the string may contain placeholders (expressions) inside curly braces `{}` that should be evaluated. Anything inside the curly braces `{}` is treated as an expression, and its value will be evaluated and inserted into the string. In this case, k is a variable, so its value will be converted to a string and placed inside the output.

```{python}
#| eval: false
#| label: clt-py
#| echo: true

import numpy as np
import matplotlib.pyplot as plt

n_vec = [2, 8, 100]  # Vector of sample sizes
M = 10000            # Number of simulations
lambda_par = 1       # Poisson parameter

for k in n_vec:
    # [1] Generate Poisson samples of size k, repeated M times
    x_sam = np.random.poisson(lambda_par, (k, M))
    
    # [2] Compute the sample means for each column m = 1, ..., M
    x_bar_sam = np.mean(x_sam, axis=0)
    
    # [3] Plot histogram of sample means
    plt.hist(x_bar_sam)
    plt.title(f"Size = {k}")
    plt.show()

```


:::::



## Central Limit Theorem Example
::::{.columns}
:::{.column width="75%"}
Suppose that the selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000. 

In 100 randomly selected sales, what is the probability the *average* selling price is more than $400,000?
:::

:::{.column width="25%"}
```{r}
#| echo: false
knitr::include_graphics("./images/img-prob/house.jpeg")
```
:::
::::

Since the sample size is fairly large $(n = 100)$, by the central limit theorem, the sampling distribution of the average selling price is approximately normal with a mean of $382,000 and a standard deviation of $150,000 / \sqrt{100}$.

Then $P(\overline{X} > 400000) = P\left(\frac{\overline{X} - 382000}{150000/\sqrt{100}}  > \frac{400000 - 382000}{150000/\sqrt{100}}\right) \approx P(Z > 1.2)$ where $Z \sim N(0, 1)$.

<!-- - We need the sampling distribution of the average selling price in order to obtain the probability.  -->

Therefore using R/Python we get the probability

::: {.panel-tabset}

## R
```{r}
#| echo: true
pnorm(1.2, lower.tail = FALSE)
pnorm(400000, mean = 382000, 
      sd = 150000/sqrt(100), lower.tail = FALSE)
```

## Python

```{python}
#| echo: true
from scipy.stats import norm
norm.sf(1.2)
norm.sf(400000, loc=382000, scale=150000/np.sqrt(100))
```

:::


## Further Readings and References