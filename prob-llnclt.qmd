# Law of Large Numbers and Central Limit Theorem {#sec-prob-llnclt}

{{< include macros.qmd >}}

In sampling distribution @sec-prob-samdist, we learn that as the sample size $n$ grows, the sampling distribution of sample mean 

1. has the mean identical to the expected value of the random variable being sampled, or the population mean
2. is less variable with a smaller variance
3. looks more like a normal distribution 

You may be wondering if it is always true. The answer is *yes* if the sample mean is the average of independent and identically distributed random variables. These properties are proved by the two theorems: the **Law of Large Numbers** and the **Central Limit Theorem**.


## Law of Large Numbers

The Law of Large Numbers (LLN) says that 

*If we keep on taking larger and larger random or iid samples (larger $n$), the statistic sample mean $\overline{X}$ is guaranteed to get closer and closer to the population mean $\mu = E(X)$ if the mean exists.*

Therefore, with LLN, if $E(X) = \mu$, $\overline{X} \approx \mu$ when $n$ is sufficiently large. It is always true, and it does not matter whether we know the value of $\mu$ or not, as long as $\mu$ exists or it is finite.

```{r}
#| label: fig-lln
#| fig-cap: "Illustration of Law of Large Numbers. The population mean is assumed having value three. The simulation is run twice."
#| code-fold: true

set.seed(2024)
x <- rnorm(1000, 3, 5); s <- cumsum(x)
set.seed(2025)
x1 <- rnorm(1000, 3, 5); s1 <- cumsum(x1)

par(mar = c(4, 4, 2, 1))
plot(s / (1:1000), type = "l", 
     main = "Law of Large Numbers with population mean 3", 
     xlab = "sample size (n)", ylab = "sample mean", las = 1)
lines(1:1000, s1 / (1:1000), col = 3)
abline(h = 3, col = "red", lwd = 2, lty = 2)
```

@fig-lln illustrates LLN using a simulation. We consider the sample size $n = 1, 2, \dots, 1000$. For each $n$, we draw a random sample $x_1, \dots, x_n$ from a normal distribution $N(3, 5^2)$, and the corresponding sample mean is $\bar{x} = (x_1+ \cdots + x_n)/n$. When $n$ is small, $\bar{x}$ is quite from away from the population mean value $\mu = 3$ due to the randomness of the samples. For example, in this simulation, $x_1$, $x_2$, and $x_3$ are `r round(x[1], 2)`, `r round(x[2], 2)`, and `r round(x[3], 2)` respectively, and hence the sample mean $\bar{x}_3 =$ `r round(mean(x[1:3]), 2)`. Each $x_i$ comes from $N(3, 5^2)$. Although their expected value is 3, keep in mind that theoretically its realized value can be any real value. With larger variance, it is more likely to have a value away from its expected value. When the sample size is small, such randomness generally will not be washed out simply by averaging. Therefore, the sample mean is not that close to the population.

The marvelous part is that when the sample size is large, such randomness or the deviation from the mean can be washed out via averaging, and the sample mean will get closer to the population mean. In the figure, two simulations are run, and both show that when the sample size is over 200, the sample mean is quite close to the population mean, and it does not fluctuate much as when the sample size is small. This is probably one of the reasons why we love large sample size. Heard **Big Data** before? When we don't know but want to sort of guess or learn the value of $\mu$, we can draw a larger sample from the target population, calculate the sample mean, then use it as a *proxy* or *representative* of the population mean. The entire process of using sample statistics to learn unknown population properties or quantities is called **statistical inference** which will be discussed in detail starting next part.

:::{.callout-note}
- Remember that sine $\overline{X}$ is a transformation of random variables $X_1, \dots, X_n$, $\overline{X}$ itself is also a random variable with some randomness and variation. Therefore, when the simulation (random sampling) is run twice (independently), for each size of $n$, we will get two different sets of values of $x_1, \dots, x_n$, and two different $\bar{x}$s, resulting in two lines in @fig-lln. 

- Additionally, as shown in the figure, the black and green values of $\overline{X}$ are quite different when $n$ is small, showing a larger variation. Why? It comes from the fact that $\Var(\overline{X}) = \sigma^2/n$. It also tells us why $\overline{X}$ converges to $\mu$. When $n$ gets large, its variation gets small, and eventually this random variable behaves like a constant with no variation. That constant is $\mu$.
:::

## Central Limit Theorem

It's time to talk about the most important theorem in probability and statistics, at least in my opinion, the **central limit theorem (CLT)**.

In sampling distribution @sec-prob-samdist, we learned that if $X_i \stackrel{iid}{\sim} N(\mu, \sigma^2)$, then $\overline{X} \sim N\left(\mu, \frac{\sigma^2}{n} \right)$. But the question is <span style="color:blue"> what if the population distribution is **NOT** normal? </span> What does the sampling distribution of the sample mean look like if the population distribution is multimodal? or skewed? or not bell-shaped? Well, the central limit theorem gives us the answer!

<!-- <center> -->
<!-- The **central limit theorem (CLT)** gives us the answer! -->
<!-- </center> -->

<!-- <br> -->

**Central Limit Theorem (CLT)**: 

*Suppose $\overline{X}$ is the sample mean from a random sample of size $n$ and from a population distribution having mean $\mu$ and standard deviation $\sigma < \infty$. As $n$ increases, the sampling distribution of $\overline{X}$ looks more and more like $N(\mu, \sigma^2/n)$ regardless of the distribution from which we are sampling!*

```{r}
#| label: fig-clt
#| fig-cap: "Illustration of Central Limit Theorem. Source: Wiki."
#| echo: false
knitr::include_graphics("./images/img-prob/clt.png")
```

@fig-clt illustrates the CLT. First, the random sample $(X_1, \dots, X_n)$ can be collected from any population distribution, whether it is normal or not. The magic part is that the sampling distribution of the sample mean $\overline{X}$ always looks like normal distribution $N(\mu, \sigma^2/n)$ as long as the sample size $n$ is sufficiently large. The larger $n$ is, the more normal-like the sampling distribution of $\overline{X}$ is. One question is how large is enough for $n$. Amazingly the normal approximation is quite well when $n \ge 30$. The variance of the sampling distribution which is $\sigma^2/n$ is decreasing with $n$ as well.

Please try the [app](https://www.statcrunch.com/applets/type3&samplingdist) and see how the shape of the sampling distribution changes with the sample size $n$ and with the shape of the population distribution. You will find that it requires larger $n$ to get a more normal-like sampling distribution if the population distribution is very skewed. You can also see how the CLT works in @fig-clt-right and @fig-clt-u. The population distribution can be discrete, like binomial or Poisson distribution. Their sampling distribution of $\overline{X}$ will still look like normal although the sampling distribution is not continuous.

<!-- ------------------------------------------------------------------ -->

<!-- <span style="color:blue"> **Illustration of the Central Limit Theorem** </span> -->


```{r}
#| label: fig-clt-right
#| fig-asp: 0.3
#| fig-cap: "CLT Illustration: A Right-Skewed Distribution."
#| echo: false
par(mfrow = c(1, 4))
par(mar = c(4, 1, 1, 0), mgp = c(2, 0.5, 0), las = 1)
shape <- 1
rate <- 1/2
# gamma_sample <- rgamma(1000, shape = shape, rate = rate)
# hist(gamma_sample, breaks = 30, col = "#FFCC00", border = "white", xlab = "x",
#      main = "Population distr. w/ mean 2 var 4", prob = TRUE)
x <- seq(0, 12, length.out = 1000)
plot(x, dgamma(x, shape = shape, rate = rate), type="n", xlab="x", ylab="", axes = TRUE, ylim = c(0, 0.6),
     main = "Population distr. w/ mean 2 var 4", cex.main = 0.8)
lines(x, dgamma(x, shape = shape, rate = rate), col = "#003366", lwd = 3)
polygon(c(x,rev(x)), c(rep(0, 1000), rev(dgamma(x, shape = shape, rate = rate))), col="#FFCC00")

# lines(density(gamma_sample), col = "red", lwd = 4) 
n_s <- 2
n_m <- 10
n_l <- 1000
sample_s <- replicate(10000, rgamma(n_s, shape = shape, rate = rate))
sample_m <- replicate(10000, rgamma(n_m, shape = shape, rate = rate))
sample_l <- replicate(10000, rgamma(n_l, shape= shape, rate = rate))
sample_mean_s <- apply(sample_s, 2, mean)
sample_mean_m <- apply(sample_m, 2, mean)
sample_mean_l <- apply(sample_l, 2, mean)
x_s <- seq(-6, 8, length.out = 1000)
x_m <- seq(-1, 5, length.out = 1000)
x_l <- seq(1.7, 2.3, length.out = 1000)
hist(sample_mean_s, breaks = 30, col = 4, border = "white", xlim = range(x_s),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_s, ")"),
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_s, dnorm(x_s, shape/rate, sqrt(shape/rate^2)/sqrt(n_s)), lwd = 3, col = "red")
# lines(density(sample_mean_s), col = "red", lwd = 4) 
hist(sample_mean_m, breaks = 30, col = 4, border = "white", xlim = range(x_m),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_m, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_m, dnorm(x_m, shape/rate, sqrt(shape/rate^2)/sqrt(n_m)), lwd = 3, col = "red")
# lines(density(sample_mean_m), col = "red", lwd = 4) 
hist(sample_mean_l, breaks = 30, col = 4, border = "white", xlim = range(x_l),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_l, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
lines(x_l, dnorm(x_l, shape/rate, sqrt(shape/rate^2)/sqrt(n_l)), lwd = 1, col = "red")
# lines(density(sample_mean_l), col = "red", lwd = 4) 
# paste("Mean of the sample mean of size 1000 =", mean(sample_mean_l))
# paste("Variance of the sample mean of size 1000 =", var(sample_mean_l))
# paste("True mean of the sample mean =", shape / rate)
# paste("True variance of the sample mean =", (shape / rate ^ 2) / n_l)
```

```{r}
#| label: fig-clt-u
#| fig-cap: "CLT Illustration: A U-shaped Distribution."
#| echo: false
#| fig-asp: 0.3
par(mfrow = c(1, 4))
par(mar = c(4, 1, 1, 0), mgp = c(2, 0.5, 0), las = 1)
shape1 <- 1/2
shape2 <- 1/2
x <- seq(0, 1, length.out = 1000)
plot(x, dbeta(x, shape1 = shape1, shape2 = shape2), type="n", xlab="x", ylab="", axes = TRUE, ylim = c(0, 3), cex.main = 0.8,
     main = "Popu. distr. w/ mu 0.5 var 0.125")
lines(x, dbeta(x, shape1 = shape1, shape2 = shape2), col = "#003366", lwd = 3)
# polygon(c(x, rev(x)), c(rev(rep(0, 1000)), (dbeta(x, shape1 = shape1, shape2 = shape2))), col="#FFCC00")

# lines(density(gamma_sample), col = "red", lwd = 4) 
n_s <- 2
n_m <- 10
n_l <- 1000
sample_s <- replicate(10000, rbeta(n_s, shape1 = shape1, shape2 = shape2))
sample_m <- replicate(10000, rbeta(n_m, shape1 = shape1, shape2 = shape2))
sample_l <- replicate(10000, rbeta(n_l, shape1 = shape1, shape2 = shape2))
sample_mean_s <- apply(sample_s, 2, mean)
sample_mean_m <- apply(sample_m, 2, mean)
sample_mean_l <- apply(sample_l, 2, mean)
x_s <- seq(-0.5, 1.5, length.out = 1000)
x_m <- seq(0, 1, length.out = 1000)
x_l <- seq(0.45, 0.55, length.out = 1000)
hist(sample_mean_s, breaks = 20, col = 4, border = "white", xlim = range(x_s),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_s, ")"),
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_s, dnorm(x_s, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_s)), lwd = 3, col = "red")
# lines(density(sample_mean_s), col = "red", lwd = 4) 
hist(sample_mean_m, breaks = 20, col = 4, border = "white", xlim = range(x_m),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_m, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
# lines(x_m, dnorm(x_m, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_m)), lwd = 3, col = "red")
# lines(density(sample_mean_m), col = "red", lwd = 4) 
hist(sample_mean_l, breaks = 20, col = 4, border = "white", xlim = range(x_l),
     xlab = "sample mean", main = paste0("Distr. of X_bar (n = ", n_l, ")"), 
     prob = TRUE, yaxt = "n", ylab = "", cex.main = 0.9)
lines(x_l, dnorm(x_l, shape1/(shape1+shape2), sqrt(0.125)/sqrt(n_l)), lwd = 1, col = "red")
# lines(density(sample_mean_l), col = "red", lwd = 4) 
# paste("Mean of the sample mean of size 1000 =", mean(sample_mean_l))
# paste("Variance of the sample mean of size 1000 =", var(sample_mean_l))
# paste("Theoretical mean of the sample mean =", shape1/(shape1+shape2))
# paste("Theoretical variance of the sample mean =", 0.125 / n_l)
```




In sum, for a random sample $(X_1, \dots, X_n)$, if the population distribution is normally distributed, then of course with no surprise the sampling distribution of the sample mean is also exactly normally distributed. If the population distribution is not normally distributed, as long as its mean and variance exist, its sampling distribution of the sample mean will still look like a normal distribution when the sample size $n$ is large enough.


- $X_i \stackrel{iid}{\sim} N(\mu, \sigma^2)$. $\overline{X} \sim N\left(\mu, \frac{\sigma^2}{n} \right)$

- $X_i \stackrel{iid}{\sim}$ *any* distribution ($\mu, \sigma^2$). $\overline{X}$ looks like $N\left(\mu, \frac{\sigma^2}{n} \right)$ (for $n$ sufficiently large)



<!-- ------------------------------------------------------------------ -->

<!-- <span style="color:blue"> **** </span> -->

Why is the central limit theorem Important? Many well-developed statistical methods are based on the normal distribution assumption. With the central limit theorem, we can use these methods even if we are sampling from a non-normal distribution or if we have no idea what the population distribution is, provided that the sample size is large enough.

## Central Limit Theorem Example
::::{.columns}
:::{.column width="75%"}
Suppose that the selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000. 

In 100 randomly selected sales, what is the probability the *average* selling price is more than $400,000?
:::

:::{.column width="25%"}
```{r}
#| echo: false
knitr::include_graphics("./images/img-prob/house.jpeg")
```
:::
::::

Since the sample size is fairly large $(n = 100)$, by the central limit theorem, the sampling distribution of the average selling price is approximately normal with a mean of $382,000 and a standard deviation of $150,000 / \sqrt{100}$.

Then $P(\overline{X} > 400000) = P\left(\frac{\overline{X} - 382000}{150000/\sqrt{100}}  > \frac{400000 - 382000}{150000/\sqrt{100}}\right) \approx P(Z > 1.2)$ where $Z \sim N(0, 1)$.

<!-- - We need the sampling distribution of the average selling price in order to obtain the probability.  -->

Therefore using R we get the probability

```{r}
pnorm(1.2, lower.tail = FALSE)
pnorm(400000, mean = 382000, 
      sd = 150000/sqrt(100), lower.tail = FALSE)
```
