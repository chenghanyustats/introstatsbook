# Inference About Variances {#sec-infer-var}

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: packages
library(emoji)
library(knitr)
# library(kableExtra)
library(openintro)
library(ggplot2)
```

```{r}
#| echo: false
knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
```


<span style="color:blue"> **Why Inference for Population Variances?** </span>

We've learned how to do inference about one and two population means in the previous chapters. But we care about not only the center but also the dispersion or variability of some population distribution. In this chapter we are going to learn some inference methods for population variances. If the target population follows normal distribution $N(\mu, \sigma^2)$, our focus now is $\sigma^2$ or $\sigma$, not the mean $\mu$.

But why do we want to do inference about population variance? As population means, most of the time we do not know the true variance value, and in daily lives in some situations, we care about variation. For example, we would like to learn or control the variation in potency of some drug, say drug for lowering blood pressure. We hope the same amount or dose level of the drug provide the same effect on each individual. We don't want some patients' blood pressure is lowered a lot, but some other patient's blood pressure is lowered just a little bit. In other words, we hope the drug effect is consistent or it has small variability. We want the new treatment can provide consistent potency and efficacy for all patients.

We also pay a lot of attention to the variance of stock prices because the higher the variance, the riskier the investment. We may want to monitor our financial portfolio, and the variability of our investment value so that we have long term financial stability.

Moreover, we may want to know if two population variances are equal, so that a correct or better method can be used. For example, in order to get higher statistical testing power, we use the two sample pooled $t$-test if $\sigma_1 = \sigma_2$ or they are very close to each other.

<!-- ::::{.columns} -->
<!-- :::{.column width="50%"} -->
<!-- - <span style="color:blue"> The variation in potency of drugs</span>: *affects patients' health* -->

<!-- # ```{r, echo=FALSE, out.width="78%", fig.align='center'} -->
<!-- # knitr::include_graphics("./images/img-infer/drugs.jpeg") -->
<!-- # ``` -->
<!-- ::: -->

<!-- :::{.column width="50%"} -->
<!-- - <span style="color:blue"> The variance of stock prices </span>: *the higher the variance, the riskier the investment* -->
<!-- # ```{r, echo=FALSE, out.width="78%", fig.align='center'} -->
<!-- # knitr::include_graphics("./images/img-infer/stock.jpeg") -->
<!-- # ``` -->
<!-- ::: -->
<!-- :::: -->

<!-- We want to know if $\sigma_1 = \sigma_2$, because the method we use depends on whether or not this is true.  -->

<!-- :::{.callout-note icon=false} -->
<!-- ## Which test did we learn that needs $\sigma_1 = \sigma_2$? -->
<!-- - Two-sample pooled test -->
<!-- ::: -->

<!-- <center> -->
<!-- **In some situations, we care about *variation*!** -->
<!-- </center> -->

<!-- <br> -->




## Inference for One Population Variance

<!-- ------------------------------------------------------------------ -->

<span style="color:blue"> **Inference for Population Variances** </span>

Let's start with inference for one population variance. For point estimation, the most intuitive and straightforward point estimator for $\sigma^2$ is the sample variance $S^2$ defined as 

$$S^2 = \frac{\sum_{i=1}^n(X_i - \overline{X})^2}{n-1}.$$ 

Note that the the denominator is $n-1$, not the sample size $n$. Note that $S^2$ is a random variable because each data point $X_i$ and $\overline{X}$ are assumed random variables. Dividing by $n-1$ instead of $n$ actually means something. $S^2$ is an **unbiased estimator** for $\sigma^2$, i.e., $E(S^2) = \sigma^2$, which is a good property of a point estimator. If we were able to repeatedly collect data sets of the same size $n$ lots of times, and for each data set we obtain its sample variance, then the average sample variance will be, if not exactly, very close to the true population variance.

The inference methods for $\sigma^2$ introduced in this chapter require the population to be *normally distributed*.  

<!-- - $S^2$ is an **unbiased estimator** for $\sigma^2$. -->
<!--   + This means $E(S^2) = \sigma^2$. -->


:::{.callout-important}
:::{style="font-size: 1.1em;"}
The inference methods can *work poorly if normality is violated, even if the sample is large*.
:::
:::

<!-- # ```{r, echo=FALSE, out.width="40%", fig.align='center'} -->
<!-- # knitr::include_graphics("./images/img-infer/normal_dist.jpeg") -->
<!-- # ``` -->

---------------------------------------------------------------------

<span style="color:blue"> **Chi-Square $\chi^2$ Distribution** </span>

Remember that when we do the inference for population means, we use either normal distribution or Student's $t$ distribution. For the inference about the population variances, we use another distribution called [**chi-square $\chi^2$ distribution**](http://sctc.mscs.mu.edu:3838/sample-apps/Calculator/) because the distribution is related to the sampling distribution of some variable involving $S^2$ and $\sigma^2$. We'll talk about that later. Let's first learn a little about the chi-square $\chi^2$ distribution.





<!-- - The inference for $\sigma^2$ involves the so called $\chi^2$ distribution. -->

<!-- ::::{.columns} -->
<!-- :::{.column width="50%"} -->

<!-- <br> -->

As Student's $t$ distribution, the chi-square $\chi^2$ distribution has one parameter, degrees of freedom $df$. To denote a specific chi-square distribution, we write $\chi^2_{df}$ like $\chi^2_{2}$ for a chi-square distribution or variable with degrees of freedom two. @fig-chi-dist shows $\chi^2$ distributions with varying degrees of freedom. It is in general a *right-skewed* distribution, but it gets more and more symmetric as $df$ gets larger. Also, the $\chi^2$ distribution is defined over *positive* numbers, one hint why we use it for inferring variances because variance is non-negative, but normal or $t$ distribution is defined on the whole real line.



<!-- - Parameter: degrees of freedom $df$ -->
<!-- - *Right-skewed* distribution -->
<!-- - Defined over *positive* numbers -->
<!-- - More symmetric as $df$ gets larger -->
<!-- - [Chi-Square Distribution](http://sctc.mscs.mu.edu:3838/sample-apps/Calculator/) -->
<!-- ::: -->

<!-- :::{.column width="50%"} -->
```{r}
#| label: fig-chi-dist
#| fig-cap: Illustration of $\chi^2$ distributions with varying degrees of freedom 
par(mgp = c(2.2, 0.5, 0))
par(mar = c(4,4,2,1))
x <- seq(0, 40, length=1000)
df_vec <- c(3, 5, 10, 20)
hx <- dchisq(x, df = df_vec[1])
plot(x, hx, type="l", lty=1, xlab="x", axes = FALSE, col = 1, lwd = 2,
  ylab="Density", main= "Chi-Square Distributions with different dfs")
axis(1)
axis(2, las = 2)
text(df_vec[1], max(dchisq(x, df = df_vec[1])), paste("df =", df_vec[1]), col = 1)
for (i in 2:length(df_vec)) {
  hx <- dchisq(x, df = df_vec[i])
  lines(x, hx, col = i, lwd = 2)
  text(df_vec[i]+2, max(dchisq(x, df = df_vec[i]))+0.01, 
       paste("df =", df_vec[i]), col = i)
}
```
<!-- ::: -->
<!-- :::: -->

<span style="color:red"> ***Upper Tail and Lower Tail of Chi-Square*** </span>

Like standard normal and $t$ distribution, to do the inference for variances, we get to find or define critical values of a chi-square distribution. With some probability $\alpha$, we define 

- $\chi^2_{\frac{\alpha}{2},\, df}$ is a $\chi^2$ value of a $\chi^2$ distribution with degrees of freedom $df$ such that it has area to the **right** of $\alpha/2$.

- $\chi^2_{1-\frac{\alpha}{2},\, df}$ is a $\chi^2$ value of a $\chi^2$ distribution with degrees of freedom $df$ such that it has area to the **left** of $\alpha/2$. In other words, it has area to the **right** of $1 - \alpha/2$, and that's why it has a subscript $1-\frac{\alpha}{2}$.

@fig-chisq-alpha illustrates the $\chi^2$ critical values. Notice that in $N(0, 1)$, $z_{1-\frac{\alpha}{2}} = -z_{\frac{\alpha}{2}}$. Because of the symmetry of the distribution, the $z$-value having area $\alpha/2$ on the right is the $z$-value having area $\alpha/2$ on the left with a negative sign. However, the chi-square distribution is not symmetric, so $\chi^2_{1-\frac{\alpha}{2},\,df} \ne -\chi^2_{\frac{\alpha}{2},\,df}$.


<!-- but $\chi^2_{1-\frac{\alpha}{2},\,df} \ne -\chi^2_{\frac{\alpha}{2},\,df}$. -->

```{r}
#| label: fig-chisq-alpha
#| fig-cap: Illustration of $\alpha/2$ significance levels for $\chi^2_{df}$ distribution
par(mar = c(2, 0, 0, 0), mgp = c(2.1, 0.6, 0))
chi_sq_upper_tail <- function(U, df, xlim = c(0, 10), col = fadeColor("black", "22"),
                               axes = TRUE, ...) {
    x <- c(0, seq(xlim[1], xlim[2] + 3, length.out = 300))
    y <- c(0, stats::dchisq(x[-1], df))
    graphics::plot(x, y, type = "l", axes = FALSE, xlim = xlim)
    graphics::abline(h = 0)
    if (axes) {
        axis(1)
    }
    these <- which(x >= U)
    X <- x[c(these[1], these, rev(these)[1])]
    Y <- c(0, y[these], 0)
    graphics::polygon(X, Y, col = col)
}
chi_sq_lower_tail <- function (L, df, xlim = c(0, 10), 
                               col = fadeColor("black", "22"),
                               axes = TRUE, ...) {
    x <- c(0, seq(xlim[1], xlim[2] + 3, length.out = 300))
    y <- c(0, stats::dchisq(x[-1], df))
    graphics::plot(x, y, type = "l", axes = FALSE, xlim = xlim)
    graphics::abline(h = 0)
    if (axes) {
        axis(1)
    }
    these <- which(x <= L)
    X <- x[c(these[1], these, rev(these)[1])]
    Y <- c(0, y[these], 0)
    graphics::polygon(X, Y, col = col)
}
chi_sq_two_tail <- function (U, L, df, xlim = c(0, 10), 
                             col = fadeColor("black", "22"), axes = TRUE, ...) {
    x <- c(0, seq(xlim[1], xlim[2] + 3, length.out = 300))
    y <- c(0, stats::dchisq(x[-1], df))
    graphics::plot(x, y, type = "l", axes = FALSE, xlim = xlim)
    graphics::abline(h = 0)
    if (axes) {
        axis(1)
    }
    these_U <- which(x >= U)
    X_U <- x[c(these_U[1], these_U, rev(these_U)[1])]
    Y_U <- c(0, y[these_U], 0)
    graphics::polygon(X_U, Y_U, col = col)
    these_L <- which(x <= L)
    X_L <- x[c(these_L[1], these_L, rev(these_L)[1])]
    Y_L <- c(0, y[these_L], 0)
    graphics::polygon(X_L, Y_L, col = col)
}

chi_sq_two_tail(U = 15, L = 2, df = 6, xlim = c(0, 20), col = 4, axes = FALSE)
axis(1, at = c(0, 2, 15), labels = c(0, expression(chi[1-frac(alpha, 2)]^2),
                                  expression(chi[frac(alpha, 2)]^2)), font = 3,
     cex.axis = 2, tick = FALSE)
text(5, 0.05, expression(1-alpha), cex = 3)
text(1, 0.02, expression(frac(alpha, 2)), cex = 2)
text(18, 0.02, expression(frac(alpha, 2)), cex = 2)
```

<span style="color:red"> ***Sampling Distribution*** </span> 

When a random sample of size $n$ is from $\color{red}{N(\mu, \sigma^2)}$, the following sample statistic has the $\chi^2$ sampling distribution with degrees of freedom $n-1$:
$$ \frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}. $$

I would like to stress again that the inference method for $\sigma^2$ introduced here can work poorly if the normality assumption is violated, even for large samples.

The sample statistic involves $S^2$ and $\sigma^2$, so we have manipulate its sampling distribution to obtain the confidence interval for $\sigma^2$, and use it as a test statistic in the hypothesis testing for $\sigma^2$.


<span style="color:red"> ***$(1-\alpha)100\%$ Confidence Interval for $\sigma^2$*** </span>

The $(1-\alpha)100\%$ confidence interval for $\sigma^2$ is 
$$\color{blue}{\boxed{\left( \frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}, \, n-1}}, \frac{(n-1)S^2}{\chi^2_{1-\frac{\alpha}{2}, \, n-1}} \right)}}$$ 

How do we get this interval? We start with the sampling distribution of $\frac{(n-1)S^2}{\sigma^2}$.

- $P\left(\chi^2_{1-\frac{\alpha}{2}, \,n-1} < \frac{(n-1)S^2}{\sigma^2} < \chi^2_{\frac{\alpha}{2},\, n-1} \right) = 1 - \alpha$ (Goal: isolate $\sigma^2$)
- $P\left(\frac{\chi^2_{1-\frac{\alpha}{2}, \,n-1}}{(n-1)S^2} < \frac{1}{\sigma^2} < \frac{\chi^2_{\frac{\alpha}{2},\, n-1}}{(n-1)S^2} \right) = 1 - \alpha$ (divided by $(n-1)S^2$) 
- $P\left(\frac{(n-1)S^2}{\chi^2_{1-\frac{\alpha}{2}, \, n-1}} > \sigma^2 > \frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}, \,n-1}} \right) = 1 - \alpha$ (take reciprocal)
- $P\left(\frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}, \, n-1}} < \sigma^2 < \frac{(n-1)S^2}{\chi^2_{1-\frac{\alpha}{2}, \,n-1}} \right) = 1 - \alpha$ (smaller value on the left)


Be careful that the interval for $\sigma^2$ *cannot* be expressed as $(S^2 - m, S^2 + m)$ for some margin of error $m$ anymore!

<!-- :::{.callout-note} -->
<!-- - The CI for $\sigma^2$ **cannot** be expressed as $(S^2-m, S^2+m)$ anymore! -->
<!-- ::: -->


----------------------------------------------------------------------

<span style="color:blue"> **Example: Supermodel Heights** </span>

::::{.columns}
:::{.column width="80%"}
Listed below are heights (cm) for the simple random sample of 16 female supermodels.

```{r}
#| echo: true
heights <- c(178, 177, 176, 174, 175, 178, 175, 178, 
             178, 177, 180, 176, 180, 178, 180, 176)
```

The supermodels' heights are normally distributed. Please construct a $95\%$ confidence interval for population standard deviation $\sigma$.
:::

:::{.column width="20%"}
```{r}
knitr::include_graphics("./images/img-infer/models.jpeg")
```
:::
::::

We just need to get what we need for constructing the interval from the sample data, sample size $n$, sample variance $s^2$, $\alpha$, critical values $\chi^2_{\alpha/2, n-1}$ and $\chi^2_{1-\alpha/2, n-1}$.

- $n = 16$, $s^2 = 3.4$, $\alpha = 0.05$.
- $\chi^2_{\alpha/2, n-1} = \chi^2_{0.025, 15} = 27.49$
- $\chi^2_{1-\alpha/2, n-1} = \chi^2_{0.975, 15} = 6.26$

Note that we want the interval for $\sigma$, not $\sigma^2$, we take a square root of the lower and upper bound of the interval for $\sigma^2$. The $95\%$ CI for $\sigma$ is $\small \left( \sqrt{\frac{(n-1)s^2}{\chi^2_{\frac{\alpha}{2}, \, n-1}}}, \sqrt{\frac{(n-1)s^2}{\chi^2_{1-\frac{\alpha}{2}, \, n-1}}} \right) = \left( \sqrt{\frac{(16-1)(3.4)}{27.49}}, \sqrt{\frac{(16-1)(3.4)}{6.26}}\right) = (1.36, 2.85)$

We are 95% confident that the height of supermodels has standard deviation between 1.36 and 2.85.


<span style="color:red"> ***Confidence Interval Computation in R*** </span>

We use `qchisq()` to get the $\chi^2$ critical values. The probability $p$ and the degrees of freedom `df` need to be specified. You should be able to understand the rest part of the code. Enjoy it.


```{r}
#| echo: true

## set values
n <- 16
s2 <- var(heights)
alpha <- 0.05

## two chi-square critical values
chi2_right <- qchisq(p = alpha/2, df = n - 1, lower.tail = FALSE)
chi2_left <- qchisq(p = alpha/2, df = n - 1, lower.tail = TRUE)

## two bounds of CI for sigma2
ci_sig2_lower <- (n - 1) * s2 / chi2_right
ci_sig2_upper <- (n - 1) * s2 / chi2_left

## two bounds of CI for sigma
(ci_sig_lower <- sqrt(ci_sig2_lower))
(ci_sig_upper <- sqrt(ci_sig2_upper))
```

<span style="color:red"> ***Testing*** </span>

Back to the example. Use $\alpha = 0.05$ to test the claim that "*supermodels have heights with a standard deviation that is less than the standard deviation, $\sigma = 7.5$ cm, for the population of women*". 

<span style="color:red"> Step 1 </span>

- We are comparing the $\sigma$ of heights of supermodels with the $\sigma$ of heights of women in general which is 7.5. So the hypothesize value $\sigma_0$ is 7.5. We wonder if supermodel height standard deviation is smaller than 7.5. Therefore, we have test $H_0: \sigma = \sigma_0$ vs. $H_1: \sigma < \sigma_0$, where $\sigma_0 = 7.5$ cm.

<span style="color:red"> Step 2 </span>

- $\alpha = 0.05$

<span style="color:red"> Step 3 </span>

- The test statistic comes from the variable $\chi_{test}^2 = \frac{(n-1)S^2}{\sigma^2}$ that follows $\chi^2_{n-1}$ distribution. Under $H_0$,  we have $\chi_{test}^2 = \frac{(n-1)s^2}{\sigma_0^2} = \frac{(16-1)(3.4)}{7.5^2} = 0.91$, drawn from $\chi^2_{n-1}$.

<!-- ::::{.columns} -->
<!-- :::{.column width="50%"} -->
<span style="color:red"> Step 4-c </span>

- This is a left-tailed test. 
- The critical value is $\chi_{1-\alpha, df}^2 = \chi_{0.95, 15}^2 = 7.26$

<span style="color:red"> Step 5-c </span>

- Reject $H_0$ in favor of $H_1$ if $\chi_{test}^2 < \chi_{1-\alpha, df}^2$.
- Since $0.91 < 7.26$, we reject $H_0$.
<!-- ::: -->

<!-- :::{.column width="50%"} -->

<!-- <br>  -->

```{r}
#| fig-asp: 0.5
## | label: fig-chisq-models
## | fig-cap: $\chi_{0.95,15}^2$ distribution for supermodel heights
par(mar = c(2, 0, 0, 0), mgp = c(1, 1, 0))

chi_sq_lower_tail <- function(L, df, xlim = c(0, 10), 
                               col = fadeColor("black", "22"),
                               axes = TRUE, ...) {
    x <- c(0, seq(xlim[1], xlim[2] + 3, length.out = 300))
    y <- c(0, stats::dchisq(x[-1], df))
    graphics::plot(x, y, type = "l", axes = FALSE, xlim = xlim, xlab = "")
    graphics::abline(h = 0)
    if (axes) {
        axis(1)
    }
    these <- which(x <= L)
    X <- x[c(these[1], these, rev(these)[1])]
    Y <- c(0, y[these], 0)
    graphics::polygon(X, Y, col = col)
}


chi_sq_lower_tail(L = 7.26, df = 15, xlim = c(0, 50), col = 4, axes = FALSE)
# axis(1, at = c(2, 15), labels = c(expression(chi[1-frac(0.05, 2)]^2),
#                                   expression(chi[frac(0.05, 2)]^2)), font = 3,
#      cex.axis = 2, tick = FALSE)
axis(1, at = c(-0.6, 0.907, 7.26), labels = c(0, expression(chi[test]^2), expression(chi[1-0.05]^2)), font = 3, 
     cex.axis = 1, tick = TRUE)
# text(0.907, 0.001, expression(chi[test]^2), cex = 1)
text(5.5, 0.0025, expression(0.05), cex = 0.9)
text(22, 0.05, expression(chi[15]^2), cex = 2)
# text(0, 0, 0, cex = 1)
# text(18, 0.02, expression(frac(0.05, 2)), cex = 2)
```
<!-- ::: -->
<!-- :::: -->


<span style="color:red"> Step 6 </span>

- There is sufficient evidence to support the claim that supermodels have heights with a standard deviation that is less than the standard deviation for the population of all women.

We conclude that the *heights of supermodels vary less than heights of women in the general women population.*

---------------------------------------------------------------

<span style="color:blue"> **Back to Pooled t-Test** </span>

In a two sample pooled t-test, we assume

  - <span style="color:blue"> $n_1 \ge 30$ and $n_2 \ge 30$ or that both samples are drawn from normal populations. </span>
  - <span style="color:blue"> $\sigma_1 = \sigma_2$ </span>

We can use a QQ-plot (and normality tests, Anderson, Shapiro, etc.) to check the assumption of a normal distribution. We now learn how to check the assumption $\sigma_1 = \sigma_2$.

## Inference for Comparing Two Population Variances

<span style="color:blue"> **F Distribution** </span>

For comparing *two* population variances, we need another distribution called [**$F$ distribution**](http://sctc.mscs.mu.edu:3838/sample-apps/Calculator/). The $F$ distribution has two parameters $df_1$ and $df_2$, a hint why it is used for comparing *two* variances. We write a specific $F$ distribution $F_{df_1, df_2}$. Second, the $F$ distribution is also right-skewed. Like $\chi^2$ distribution, the $F$ distribution is defined over *positive* numbers. @fig-f-dist illustrates $F$ distribution with different parameters. You can see that when $df_1$ and $df_2$ are both large, the $F$ distribution looks bell-shaped.

<!-- - We use the **$F$ distribution** for inference about **two** population variances. -->

<!-- ::::{.columns} -->
<!-- :::{.column width="40%"} -->

<!-- <br> -->

<!-- - Two parameters: $df_1$, $df_2$ -->
<!-- - *Right-skewed* distribution -->
<!-- - Defined over *positive* numbers -->
<!-- - R Shiny app: [F Distribution](http://sctc.mscs.mu.edu:3838/sample-apps/Calculator/) -->
<!-- ::: -->

<!-- :::{.column width="60%"} -->
```{r}
#| label: fig-f-dist
#| fig-cap: F distributions with different parameters.
par(mgp = c(2.5, 1, 0))
x <- seq(0, 3, length=1000)
df1_vec <- c(3, 100)
df2_vec <- c(5, 10, 100)
# hx <- df(x, df1 = df1_vec[1], df2 = df2_vec[1])
# plot(x, hx, type="l", lty=1, xlab="x", axes = FALSE, col = 1, lwd = 2, ylim = c(0, 2.2),
#   ylab="Density", main= "F Distribution with different df_1 and df_2")
# axis(1)
# axis(2, las = 2)
# f_mode <- ((df1_vec[1] - 2) / df1_vec[1]) * (df2_vec[1] / (df2_vec[1] + 2))
# text(f_mode, max(df(x, df1 = df1_vec[1], df2 = df2_vec[1])), 
#      paste("df1 =", df1_vec[1], "df2 =", df2_vec[1]), col = 1)
legend_text <- paste("df1 = ", df1_vec[1], ", df2 = ", df2_vec[1], sep = "")
col_idx <- c(1)
k <- 2
for (i in 1:length(df1_vec)) {
    for (j in 1:length(df2_vec)) {
        if (i == 1 && j == 1) {
            hx <- df(x, df1 = df1_vec[1], df2 = df2_vec[1])
            plot(x, hx, type="l", lty=1, xlab="x", axes = FALSE, col = 1, lwd = 3, ylim = c(0, 2.2),
                    ylab="Density", main= "F Distribution with different df_1 and df_2")
            axis(1)
            axis(2, las = 2)
            # legend_text <- c(legend_text, paste("df1 =", df1_vec[i], "df2 =", df2_vec[j]))
        } else {
            hx <- df(x, df1 = df1_vec[i], df2 = df2_vec[j])
            lines(x, hx, col = k, lwd = 3)
            legend_text <- c(legend_text, paste("df1 = ", df1_vec[i], ", df2 = ", df2_vec[j], sep = ""))
            col_idx <- c(col_idx, k)
        }
        k <- k + 1
        # f_mode <- ((df1_vec[i] - 2) / df1_vec[i]) * (df2_vec[j] / (df2_vec[j] + 2))
        # text(f_mode, max(df(x, df1 = df1_vec[i], df2 = df2_vec[j])) + 0.1,
        #      paste("df1 =", df1_vec[i], "df2 =", df2_vec[j]), col = k)
    }
    k <- k + 1
}
# print(k)
legend("topright", legend_text, lwd = rep(3, 6), col = col_idx, bty = "n")
```
<!-- ::: -->
<!-- :::: -->

<span style="color:red"> ***Upper and Lower Tail of F Distribution*** </span>

We denote $F_{\alpha, \, df_1, \, df_2}$ as the $F$ quantile such that $P(F_{df_1, df_2} > F_{\alpha, \, df_1, \, df_2}) = \alpha$. With it, we can find the critical values $F_{\frac{\alpha}{2}, \, df_1, \, df_2}$ and $F_{1-\frac{\alpha}{2}, \, df_1, \, df_2}$ used in constructing the confidence interval for the ratio $\sigma^2_1/\sigma^2_2$ discussed next.

```{r}
#| label: fig-f-sig-level
#| fig-cap: Illustration of $\alpha/2$ significance levels for $F_{df_1, df_2}$ distribution
#| fig-asp: 0.5
par(mar = c(2, 0, 0, 0), mgp = c(2.1, 0.6, 0))
f_upper_tail <- function(U, df1, df2, xlim = c(0, 10), col = fadeColor("black", "22"),
                               axes = TRUE, ...) {
    x <- c(0, seq(xlim[1], xlim[2] + 3, length.out = 300))
    y <- c(0, stats::df(x[-1], df1, df2))
    graphics::plot(x, y, type = "l", axes = FALSE, xlim = xlim)
    graphics::abline(h = 0)
    if (axes) {
        axis(1)
    }
    these <- which(x >= U)
    X <- x[c(these[1], these, rev(these)[1])]
    Y <- c(0, y[these], 0)
    graphics::polygon(X, Y, col = col)
}
f_lower_tail <- function (L, df1, df2, xlim = c(0, 10), 
                               col = fadeColor("black", "22"),
                               axes = TRUE, ...) {
    x <- c(0, seq(xlim[1], xlim[2] + 3, length.out = 300))
    y <- c(0, stats::df(x[-1], df1, df2))
    graphics::plot(x, y, type = "l", axes = FALSE, xlim = xlim)
    graphics::abline(h = 0)
    if (axes) {
        axis(1)
    }
    these <- which(x <= L)
    X <- x[c(these[1], these, rev(these)[1])]
    Y <- c(0, y[these], 0)
    graphics::polygon(X, Y, col = col)
}
f_two_tail <- function (U, L, df1, df2, xlim = c(0, 10), 
                             col = fadeColor("black", "22"), axes = TRUE, ...) {
    x <- c(0, seq(xlim[1], xlim[2] + 3, length.out = 300))
    y <- c(0, stats::df(x[-1], df1, df2))
    graphics::plot(x, y, type = "l", axes = FALSE, xlim = xlim, 
                   xlab = "", ylab = "")
    graphics::abline(h = 0)
    if (axes) {
        axis(1)
    }
    these_U <- which(x >= U)
    X_U <- x[c(these_U[1], these_U, rev(these_U)[1])]
    Y_U <- c(0, y[these_U], 0)
    graphics::polygon(X_U, Y_U, col = col)
    these_L <- which(x <= L)
    X_L <- x[c(these_L[1], these_L, rev(these_L)[1])]
    Y_L <- c(0, y[these_L], 0)
    graphics::polygon(X_L, Y_L, col = col)
}

f_two_tail(U=2.5,L=0.3, df1=3, df2=100, xlim = c(0, 6), col = 4, axes = FALSE)
axis(1, at = c(0.3, 2.5), labels = c(expression(F[1-frac(alpha, 2)]),
                                     expression(F[frac(alpha, 2)])), 
     font = 3, cex.axis = 1.5,
     tick = FALSE)
text(1, 0.2, expression(1-alpha), cex = 2)
text(2.7, 0.05, expression(frac(alpha, 2)), cex = 1.5)
text(0.15, 0.05, expression(frac(alpha, 2)), cex = 1.5)
```

<sample style="color:red"> ***Sampling Distribution*** </span>

When random samples of sizes $n_1$ and $n_2$ have been *independently* drawn from two normally distributed populations, $N(\mu_1, \sigma_1^2)$ and $N(\mu_2, \sigma_2^2)$ respectively, the ratio $\frac{S_1^2/S_2^2}{\sigma_1^2/\sigma_2^2}$ has the $F$ sampling distribution $$\frac{S_1^2/S_2^2}{\sigma_1^2/\sigma_2^2} \sim F_{n_1-1, \, n_2-1}.$$



:::{.callout-important}
:::{style="font-size: 1.1em;"}
The order of degrees of freedom matters! $F_{n_1-1, \, n_2-1} \ne F_{n_2-1, \, n_1-1}$. Please don't mess around. 
:::
:::


<span style="color:red"> ***$(1-\alpha)100\%$ Confidence Interval for $\sigma_1^2 / \sigma_2^2$*** </span>


From the sampling distribution of the ratio, the $(1-\alpha)100\%$ confidence interval for $\sigma_1^2 / \sigma_2^2$ is 
$$\color{blue}{\boxed{\left( \frac{s_1^2/s_2^2}{F_{\alpha/2, \, n_1 - 1, \, n_2 - 1}}, \frac{s_1^2/s_2^2}{F_{1-\alpha/2, \, \, n_1 - 1, \, n_2 - 1}} \right)}}$$ 

How do we get the interval? Not surprising. We start with the sampling distribution of $\frac{S_1^2/S_2^2}{\sigma_1^2/\sigma_2^2}$.

- $P\left(F_{1-\alpha/2, \, n_1 - 1, \, n_2 - 1} < \frac{S_1^2/S_2^2}{\sigma_1^2/\sigma_2^2} < F_{\alpha/2, \, n_1 - 1, \, n_2 - 1} \right) = 1 - \alpha$ (Goal: isolate $\sigma_1^2/\sigma_2^2$)

- $P\left(\frac{1}{F_{\alpha/2, \, n_1 - 1, \, n_2 - 1}} < \frac{\sigma_1^2/\sigma_2^2}{S_1^2/S_2^2} < \frac{1}{F_{1-\alpha/2, \, n_1 - 1, \, n_2 - 1}} \right) = 1 - \alpha$  (take reciprocal)

- $P\left(\frac{S_1^2/S_2^2}{F_{\alpha/2, \, n_1 - 1, \, n_2 - 1}} < \frac{\sigma_1^2}{\sigma_2^2} < \frac{S_1^2/S_2^2}{F_{1-\alpha/2, \, n_1 - 1, \, n_2 - 1}} \right) = 1 - \alpha$ (times $S_1^2/S_2^2$)


:::{.callout-note}
The confidence interval for $\sigma_1^2 / \sigma_2^2$ *cannot* be expressed as $\left(\frac{s_1^2}{s_2^2}-m, \frac{s_1^2}{s_2^2} + m\right)$ anymore!
:::





-------------------------------------------------------------------

<span style="color:blue"> **F test for comparing $\sigma_1^2$ and $\sigma_2^2$** </span>

<span style="color:red"> Step 1 </span>

For comparing $\sigma_1^2$ and $\sigma_2^2$, the test can be either right-tailed or two-tailed. Left-tailed testing is not necessary because we can always define the population whose variance is hypothetically larger than the variance of another population as the first population.


- Right-tailed: <span style="color:blue"> $\small \begin{align} &H_0: \sigma_1 \le \sigma_2 \\ &H_1: \sigma_1 > \sigma_2 \end{align}$ </span>
- Two-tailed: <span style="color:green"> $\small \begin{align} &H_0: \sigma_1 = \sigma_2 \\ &H_1: \sigma_1 \ne \sigma_2 \end{align}$ </span>

<span style="color:red"> Step 2 </span>

- $\alpha = 0.05$

<span style="color:red"> Step 3 </span>

- Under $H_0$, $\sigma_1 = \sigma_2$, and the test statistic is $$\small F_{test} = \frac{s_1^2/s_2^2}{\sigma_1^2/\sigma_2^2} = \frac{s_1^2}{s_2^2} \sim F_{n_1-1, \, n_2-1}$$
The denominator is gone because the ratio is one.

<span style="color:red"> Step 4-c </span>

- Right-tailed: <span style="color:blue"> $F_{\alpha, \, n_1-1, \, n_2-1}$ </span>. 
- Two-tailed: <span style="color:green"> $F_{\alpha/2, \, n_1-1, \, n_2-1}$ or $F_{1-\alpha/2, \, n_1-1, \, n_2-1}$ </span>

<span style="color:red"> Step 5-c </span>

- Right-tailed: reject $H_0$ if <span style="color:blue"> $F_{test} \ge F_{\alpha, \, n_1-1, \, n_2-1}$</span>. 
- Two-tailed: reject $H_0$ if <span style="color:green"> $F_{test} \ge F_{\alpha/2, \, n_1-1, \, n_2-1}$ or $F_{test} \le F_{1-\alpha/2, \, n_1-1, \, n_2-1}$</span>

-------------------------------------------------------------------

<span style="color:blue"> **Example: Weight Loss** </span>

This is our previous example.

::::{.columns}
:::{.column width="70%"}

*A study was conducted to see the effectiveness of a weight loss program. Two groups (Control and Experimental) of 10 subjects were selected. The two populations are normally distributed and have the same standard deviation.*
:::

:::{.column width="30%"}
```{r}
#| out-width: 89%
knitr::include_graphics("./images/img-infer/weight.jpeg")
```
:::
::::

*The data on weight loss was collected at the end of six months.*

+ **Control**: $n_1 = 10$, $\overline{x}_1 = 2.1\, lb$, $s_1 = 0.5\, lb$
+ **Experimental**: $n_2 = 10$, $\overline{x}_2 = 4.2\, lb$, $s_2 = 0.7\, lb$
  
<!-- - Assumptions: -->
<!--   + <span style="color:blue"> $\sigma_1 = \sigma_2$ </span> -->
<!--   + The weight loss for both groups are normally distributed. -->

<span style="color:red"> ***Check if $\sigma_1 = \sigma_2$*** </span>

- $n_1 = 10$, $s_1 = 0.5 \, lb$
- $n_2 = 10$, $s_2 = 0.7 \, lb$

<span style="color:red"> Step 1 </span>

- $\begin{align} &H_0: \sigma_1 = \sigma_2 \\ &H_1: \sigma_1 \ne \sigma_2 \end{align}$

<span style="color:red"> Step 2 </span>

- $\alpha = 0.05$

<span style="color:red"> Step 3 </span>

- The test statistic is $F_{test} = \frac{s_1^2}{s_2^2} = \frac{0.5^2}{0.7^2} = 0.51$.

<span style="color:red"> Step 4-c </span>

- This is a two-tailed test.
- The critical value is $F_{0.05/2, \, 10-1, \, 10-1} = 4.03$ or $F_{1-0.05/2, \, 10-1, \, 10-1} = 0.25$.

```{r}
#| fig-asp: 0.5
## | label: fig-weight-fdist
## | fig-cap: F Distribution for Weight Loss Example
par(mar = c(2, 0, 0, 0))
f_two_tail(U=4.03,L=0.22, df1=9, df2=9, xlim = c(0, 6), col = 4, axes = FALSE)
axis(1, at = c(0.22, 0.51, 4.03), 
     labels = c(expression(F[.975]),
                expression(F[test]),
                expression(F[.025])), 
     font = 3, cex.axis = 0.73, tick = TRUE)
abline(v = 0.51, col = 2, lwd = 0.8)
# text(0.51, 0.01, expression(1-alpha), cex = 2)


text(2.5, 0.2, expression(F[paste(9, ",", 9)]), cex = 2)
# text(2.7, 0.05, expression(frac(alpha, 2)), cex = 1.5)
# text(0.15, 0.05, expression(frac(alpha, 2)), cex = 1.5)
```

<span style="color:red"> Step 5-c </span>

- Is $F_{test} > 4.03$ or $F_{test} < 0.25$?
  - No. 

<span style="color:red"> Step 6 </span> 

- The evidence is not sufficient to reject the claim that $\sigma_1 = \sigma_2$.

<span style="color:red"> ***95% CI for $\sigma_1^2 / \sigma_2^2$*** </span>

<!-- ::::{.columns} -->
<!-- :::{.column width="50%"} -->
- $\small F_{\alpha/2, \, df_1, \, df_2} = F_{0.05/2, \, 10-1, \, 10-1} = 4.03$ 
- $\small F_{1-\alpha/2, \, df_1, \, df_2} = F_{1-0.05/2, \, 10-1, \, 10-1} = 0.25$
- $\small \frac{s_1^2}{s_2^2} = \frac{0.5^2}{0.7^2} = 0.51$
- The 95% CI for $\sigma_1^2 / \sigma_2^2$ is 
$$\small \begin{align} &\left( \frac{s_1^2/s_2^2}{F_{\alpha/2, \, df_1, \, df_2}}, \frac{s_1^2/s_2^2}{F_{1-\alpha/2, \, df_1, \, df_2}} \right) \\ &= \left( \frac{0.51}{4.03}, \frac{0.51}{0.25} \right) = \left(0.127, 2.05\right)\end{align}$$
<!-- ::: -->

<!-- :::{.column width="50%"} -->
```{r}
#| fig-asp: 0.5
##| label: fig-weight-sig-level
##| fig-cap: F Distribution for significance level 0.05

par(mar = c(2, 0, 0, 0))
f_two_tail(U=4.03,L=0.22, df1=9, df2=9, xlim = c(0, 6), col = 4, axes = FALSE)
axis(1, at = c(0.22, 4.03), 
     labels = c(expression(F[.975]),
                expression(F[.025])), 
     font = 3, cex.axis = 0.73, tick = TRUE)
# abline(v = 0.51, col = 2, lwd = 0.4)
# text(0.51, 0.01, expression(1-alpha), cex = 2)


text(2.5, 0.2, expression(F[paste(9, ",", 9)]), cex = 2)
# text(2.7, 0.05, expression(frac(alpha, 2)), cex = 1.5)
# text(0.15, 0.05, expression(frac(alpha, 2)), cex = 1.5)
```
<!-- ::: -->
<!-- :::: -->

We are 95% confident that the ratio $\sigma_1^2 / \sigma_2^2$ is between 0.127 and 2.04. Because one is included in this interval, meaning that $\sigma^2_1 = \sigma^2_2$, it leads to the same conclusion as the F test.

<span style="color:red"> ***Implementing F-test in R*** </span>

We use `qf()` to find the $F$ critical values. 
<!-- - **`qf(p = alpha, df1 = n1 - 1, df2 = n2 - 1, lower.tail = FALSE)`** to find $F_{\alpha, \, n_1-1, \, n_2-1}$. -->

<!-- ::::{.columns} -->
<!-- :::{.column width="49%"} -->
```{r}
#| echo: true
#| label: f-test-two-sigma-ci

## set values
n1 <- 10; n2 <- 10
s1 <- 0.5; s2 <- 0.7
alpha <- 0.05

## 95% CI for sigma_1^2 / sigma_2^2
f_small <- qf(p = alpha / 2, df1 = n1 - 1, df2 = n2 - 1,lower.tail = TRUE)
f_big <- qf(p = alpha / 2, df1 = n1 - 1, df2 = n2 - 1, lower.tail = FALSE)

## lower bound
(s1 ^ 2 / s2 ^ 2) / f_big

## upper bound
(s1 ^ 2 / s2 ^ 2) / f_small
```
<!-- ::: -->

<!-- :::{.column width="2%"} -->
<!-- ::: -->

<!-- :::{.column width="49%"} -->
```{r}
#| echo: true
#| label: f-test-two-sigma-test
## Testing sigma_1 = sigma_2
(test_stats <- s1 ^ 2 / s2 ^ 2)
```
<!-- ::: -->
<!-- :::: -->



:::{.callout-note}
:::{style="font-size: 1.1em;"}
If we have the entire two samples data, we can use the R built-in function `var.test(x, y, alternative = "two.sided")` to perform a $F$ test to compare the variances of two samples from normal populations. The arguments `x` and `y` are numeric vectors of data values.  Argument `alternative` must be one of `"two.sided"` (default), `"greater"` or `"less"`.
:::
:::



## Comparing More Than Two Population Variances*

So far we use $\chi^2$ distribution to do inference for one population variance, and $F$ distribution to compare two variances. What if we want to compare more than two population variances? For example, what do we do if we have this kind of test:

<span style="color:blue"> $\begin{align} 
  &H_0: \sigma_1 = \sigma_2 = \dots = \sigma_t\\ 
  &H_1: \text{Population variances are not all equal}
  \end{align}$ </span>

where we have $t > 2$ different groups. In this scenario, we can use the so-called **Brown-Forsythe-Levene (BFL) test**. The BFL test assumes that we have $t > 2$ independent samples. Then BFL replaces $y_{ij}$, the $j$-th observation in group $i$, with $z_{ij}$. $z_{ij} = |y_{ij} - \tilde{y}_i|$, where $\tilde{y}_i$ is the sample **median** or **mean** of group $i$. 

:::{.callout-note}
Brown-Forsythe-Levene test actually represents two tests: *Brown-Forsythe test* and *Levene's test*, where Brown-Forsythe test uses the sample **medians** in $z_{ij}$ and Levene's test uses the sample **means**. This is the only difference between the two tests, and therefore some will refer them as one Brown-Forsythe-Levene test, or some even refer one to the other. For example, the R function `car::leveneTest()` used in the example actually perform the Brown-Forsythe test by default with its argument `center=median`.

Using means has larger statistical power for symmetric, moderate-tailed, distributions. But when data are non-normal, either skewed or heavy tailed, using medians is recommended because of good robustness against many types of non-normal data and while retaining good statistical power.
:::
  
The test statistic is $\color{blue}{L_{test} = \frac{\sum_{i=1}^tn_i(\overline{z}_{i\cdot} - \overline{z}_{\cdot\cdot})^2 / (t - 1)}{\sum_{i=1}^t \sum_{j = 1}^{n_i}(z_{ij} - \overline{z}_{i\cdot})^2 / (N-t)}}$, where $\overline{z}_{i\cdot} = \sum_{j = 1}^{n_i}z_{ij}/n_{i}$ is the mean of the $z_{ij}$s from the group $i$, $\overline{z}_{\cdot\cdot} = \sum_{i=1}^t\sum_{j = 1}^{n_i}z_{ij}/N$ is the mean of all $z_{ij}$s, and $N = \sum_{i=1}^tn_i$.

The decision rule is that we reject $H_0$ if $\color{blue}{L_{test} > F_{\alpha, \, t-1, \, N-t}}$.



<!-- ## Idea of Brown-Forsythe-Levene (BFL) test -->

<!-- - $\color{blue}{L = \frac{\sum_{i=1}^tn_i(\overline{z}_{i\cdot} - \overline{z}_{\cdot\cdot})^2 / (t - 1)}{\sum_{i=1}^t \sum_{j = 1}^{n_i}(z_{ij} - \overline{z}_{i\cdot})^2 / (N-t)}}$ -->

<!-- - $z_{ij} = |y_{ij} - \tilde{y}_i|$, $\overline{z}_{i\cdot} = \sum_{j = 1}^{n_i}z_{ij}/n_{i}$, $\overline{z}_{\cdot\cdot} = \sum_{i=1}^t\sum_{j = 1}^{n_i}z_{ij}/N$ -->

<!-- - We actually compare variation between samples. -->

There are other tests for testing the homogeneity of variances, but the BFL test performs better when data are from a **non-normal** distribution. It corrects for the skewness of distribution by using deviations from group *medians*. The BFL test is more **robust**. It is less likely to incorrectly declare that the assumption of equal variances has been violated.


-------------------------------------------------------------------

<span style="color:blue"> **Example: Automobile Additives (Example 7.8 in SMD)** </span>

Three different additives that are marketed for increasing the miles per gallon (mpg) for automobiles. The percentage increase in mpg was recorded for a 250-mile test drive for each additive for 10 randomly assigned cars. Is there a difference between the three additives with respect to their variability? The data are saved in R data frame `data_additive`.

```{r}
par(mar = c(2,6,1,1))
y1 <- c(4.2, 2.9, 0.2, 25.7, 6.3, 7.2, 2.3, 9.9, 5.3, 6.5)
y2 <- c(0.2, 11.3, 0.3, 17.1, 51, 10.1, 0.3, 0.6, 7.9, 7.2)
y3 <- c(7.2, 6.4, 9.9, 3.5, 10.6, 10.8, 10.6, 8.4, 6.0, 11.9)
# data_ex78 <- data.frame("mpg_increase" = c(y1, y2, y3), "additive" = factor(rep(1:3, each = 10)))
data_additive <- data.frame("mpg_increase" = c(y1, y2, y3), "additive" = factor(rep(1:3, each = 10)))
```

```{r}
#| echo: true
data_additive
```

```{r}
#| label: additive-boxplot
#| echo: false
#| fig-height: 3.6
#| fig-width: 10
par(mar = c(4, 5, 1.5, 0))
boxplot(mpg_increase~additive, data = data_additive, axes = FALSE, ylab = "",
        main = "Boxplots for three additives", horizontal = TRUE)
axis(1)
axis(2, las = 2, at = 1:3, labels = paste("Additive", 1:3))
```

The boxplot suggests that Additive 2 has larger variation than the other two in terms of the percentage increase in mpg. We will use the BFL test to decide whether or not their variation difference is just a random sampling consequences or it is statistically significant.

<!-- ## Example 7.8 Data {.columns-2 .larger} -->
<!-- # ```{r ex7-8data, echo=FALSE} -->
<!-- # data_additive[1:15, ] -->
<!-- # data_additive[16:30, ] -->
<!-- # ``` -->

<!-- # ```{r ex7-8data, echo=FALSE} -->
<!-- # data_ex78[1:10, ] -->
<!-- # data_ex78[11:20, ] -->
<!-- # data_ex78[21:30, ] -->
<!-- # ``` -->

<!-- <div class="columns-2"> -->
<!-- ```{r ex7-8, echo=FALSE} -->
<!-- data_ex78[1:10, ] -->
<!-- ``` -->
<!-- ```{r ex7-81, echo=FALSE} -->
<!-- data_ex78[11:20, ] -->
<!-- ``` -->
<!-- ```{r ex7-82, echo=FALSE} -->
<!-- data_ex78[21:30, ] -->
<!-- ``` -->
<!-- </div> -->

<!-- ::: {.tiny} -->

<!-- ::: -->





<!-- ## Example 7.8 Check Normality -->
<!-- ```{r ex7-8qqplot, echo=FALSE, fig.height=2.5, fig.width=10} -->
<!-- library(car) -->
<!-- library(nortest) -->
<!-- par(mgp = c(2.5, 1, 0)) -->
<!-- par(mar = c(4, 4, 2, 1)) -->
<!-- par(mfrow = c(1, 3)) -->
<!-- for (i in 1:3) { -->
<!--     qqPlot(data_additive[data_additive$additive == i, 1], las = 2, ylab = "sample quantiles", col = "#003366",  -->
<!--        col.lines = "red", pch = 16, grid = FALSE, main = paste("QQ-plot Additive", i), cex = 2, id = FALSE) -->
<!-- }  -->
<!-- ad.test(data_additive[data_additive$additive == 1, 1]) -->
<!-- ad.test(data_additive[data_additive$additive == 2, 1]) -->
<!-- ad.test(data_additive[data_additive$additive == 3, 1]) -->
<!-- ``` -->


<!-- ## Example 7.8 BFL Test -->
The procedure of BLT test is shown step by step as follows.

- **Step 1**: <span style="color:blue"> $\begin{align} 
  &H_0: \sigma_1 = \sigma_2 = \sigma_3\\ 
  &H_1: \text{Population variances are not all equal}
  \end{align}$ </span>
  
- **Step 2**: $\alpha = 0.05$

- **Step 3**: Test statistic $\color{blue}{L_{test} = \frac{\sum_{i=1}^tn_i(\overline{z}_{i\cdot} - \overline{z}_{\cdot\cdot})^2 / (t - 1)}{\sum_{i=1}^t \sum_{j = 1}^{n_i}(z_{ij} - \overline{z}_{i\cdot})^2 / (N-t)}} = \frac{235.8/(3-1)}{1742.6/(30-3)} = 1.872$.

- **Step4-c**: Critical value is $F_{\alpha, \, t-1, \, N-t} = F_{0.05, \, 3-1, \, 30-3} = 3.35$.

- **Step5-c**: Since $L_{test}  = 1.872 < F_{\alpha, \, t-1, \, N-t} = 3.35$, we do not reject $H_0$.

- **Step6**: There is insufficient evidence to support the claim that there is a difference in the population variances of the percentage increase in mpg for the three additives.

Once you understand the idea and the process of doing the test, in practice, we use R function `leveneTest()` in the `car` ([Companion to Applied Regression](https://www.john-fox.ca/Companion/index.html)) package to help us perform the test.



<!-- ## Example 7.8 BFL Test in R -->
<!-- - The test statistic is $\color{blue}{L_{test} = \frac{\sum_{i=1}^tn_i(\overline{z}_{i\cdot} - \overline{z}_{\cdot\cdot})^2 / (t - 1)}{\sum_{i=1}^t \sum_{j = 1}^{n_i}(z_{ij} - \overline{z}_{i\cdot})^2 / (N-t)}}$, where $\overline{z}_{i\cdot}$ is the mean of the $z_{ij}$s from the group $i$, $\overline{z}_{\cdot\cdot}$ is the mean of all $z_{ij}$s, and $N = \sum_{i=1}^tn_i$. -->

```{r}
#| echo: true
#| message: false
library(car) # Load the package car ()
leveneTest(y = data_additive$mpg_increase, 
           group = data_additive$additive)
```

In the function, we put the response variable data in `y`, and the factor defining groups in `group`. By default, it will use sample medians to perform the test. The p-value is greater than 0.05 and it leads to the same conclusion that there is insufficient evidence to say there is a difference in the population variances of the percentage increase in mpg for the three additives.

<!-- Yes. Just one line of code after you install the `car` R package. -->

<!-- - Check Table 7.5 in the textbook if you really want to obtain the test statistic by hand. -->





## Exercises

1. The data about male and female pulse rates are summarized below.
    (a) Construct a 95\% CI for $\sigma_{male}$ of pulse rates for males. 
    (b) Construct a 95\% CI for $\sigma_{male}/\sigma_{female}$.
    (c) Does it appear that the population standard deviations for males and females are different? Why or why not?


<center>
|                | Male       | Female |
|----------------|------------|--------|
| $\overline{x}$ | 71         | 75     |
| $s$            | 9          | 12     |
| $n$            | 14         | 12     |
</center>
