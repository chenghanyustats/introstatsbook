# Multiple Linear Regression* {#sec-model-mlr}


<!-- ## Why Multiple Regression? -->




Our target response may be affected by several factors.
- Total sales $(Y)$ and amount of money spent on advertising on YouTube (YT) $(X_1)$, Facebook (FB) $(X_2)$, Instagram (IG) $(X_3)$.


<!-- ![](./img/tv.jpeg) ![](./img/yt.jpeg) -->


:::: {.columns}

::: {.column width="33.3%"}
<br>
```{r}
#| out-width: 100%
knitr::include_graphics("./images/img-model/yt.svg")
```
:::

::: {.column width="33.3%"}
```{r}
#| out-width: 100%
knitr::include_graphics("./images/img-model/fb.jpeg")
```
:::

::: {.column width="33.3%"}
```{r}
#| out-width: 100%
knitr::include_graphics("./images/img-model/ig.jpeg")
```
:::


::::

- Predict sales based on the three advertising expenditures and see which medium is more effective.



::: notes

- The first question you may ask is Why we want Multiple Regression?
- In practice, we often have more than one predictor. 
- How amount of money spent on advertising on different media affect the total sales of some product, we may need more than one predictors. 
- Because usually company will spend money on several different media, not just one.
- Data: total sales $(Y)$ and amount of money spent on advertising on TV $(X_1)$, YouTube $(X_2)$, and Instagram $(X_3)$.
- We want to predict sales based on the three advertising expenditures and see which medium is more effective.

:::

## Fit Separate Simple Linear Regression Models

Fit three separate independent SLR models:

```{r}
#| fig-asp: 0.35
#| out-width: 100%
advertising_data <- read.csv("./data/Advertising.csv")
advertising_data <- advertising_data[, 2:5]
colnames(advertising_data) <- c("youtube", "facebook", "instagram", "sales")
par(mfrow = c(1, 3))
par(mar = c(3, 3, 2, 1), mgp = c(2, 1, 0))
plot(advertising_data$facebook, advertising_data$sales, xlab = "$ on YouTube ", ylab = "Sales", col = 4, pch = 16, las = 1, main = "YouTube")
abline(lm(advertising_data$sales ~ advertising_data$facebook), col = "red", lwd = 2)
plot(advertising_data$youtube, advertising_data$sales, xlab = "$ on Facebook", ylab = "Sales", col = 4, pch = 16, las = 1, main = "Facebook")
abline(lm(advertising_data$sales ~ advertising_data$youtube), col = "red", lwd = 2)
plot(advertising_data$instagram, advertising_data$sales, xlab = "$ on Instagram", ylab = "Sales", col = 4, pch = 16, las = 1, main = "Instagram")
abline(lm(advertising_data$sales ~ advertising_data$instagram), col = "red", lwd = 2)
```


. . .

`r emo::ji('x')` Fitting a separate SLR model for each predictor is not satisfactory. 

::: notes

- You may wonder, how about we just fit three Separate Simple Linear Regression Models, one for each predictor. 
- Yes, we could do that. And if we do this, we'll see that advertising on the 3 media is valuable because the more the money we put in, the higher sales of products we'll get.
- Fitting a separate SLR model for each predictor is not satisfactory. That's see why.

:::

## Don't Fit a Separate Simple Linear Regression

- `r emo::ji('point_right')` How to make a *single* prediction of sales given levels of the 3 advertising media budgets?
  + <span style="color:blue"> How to predict the sales when the amount spent on YT is 50, 100 on FB and 30 on IG? </span>
  
  
. . .

- `r emo::ji('point_right')` Each regression equation ignores the other 2 media in forming coefficient estimates.
  + <span style="color:blue"> The effect of FB advertising on sales may be increased or decreased when YT and IG advertising are in the model. </span>
  + <span style="color:blue"> IG advertising may have no impact on sales when YT and FB advertising are in the model. </span>



. . .

- `r emo::ji('thumbsup')``r emo::ji('thumbsup')` Better approach: *extend the SLR model so that it can __directly accommodate multiple predictors__.*

::: notes

- Fitting a separate SLR model for each predictor is not satisfactory. 
  + It is unclear how to make a single prediction of sales given levels of the three advertising media budgets, since each of the budgets is associated with a separate regression equation.
  + Each of the three regression equations ignores the other two media in forming estimates for the regression coefficients.
  + If the three media budgets are correlated with each other, this can lead to very misleading estimates of the individual media effects on sales.
- A better approach is to extend the simple linear regression model so that it can directly accommodate multiple predictors.  

:::


##

**I hope you don't feel...**

::: small

```{r}
#| fig-cap: "Source: https://memegenerator.net/instance/62248186/first-world-problems-multiple-regression-more-like-multiple-depression"
#| out-width: 46%
knitr::include_graphics("./images/img-model/mlr_depress.jpeg")
```

:::



##

**What I hope is...**

::: small

```{r}
#| fig-cap: "https://www.tldrpharmacy.com/content/how-to-be-awesome-at-biostatistics-and-literature-evaluation-part-iii"
#| out-width: 82%
knitr::include_graphics("./images/img-model/mlr_meme.jpeg")
```
:::



## Multiple Linear Regression (MLR) Model
- We have $k$ distinct predictors. The (population) multiple linear regression model:
$$Y_i= \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} + \dots + \beta_kX_{ik} + \epsilon_i$$
- $X_{ij}$: $j$-th regressor value on $i$-th measurement, $j = 1, \dots, k$.
- $\beta_j$: $j$-th coefficient quantifying the association between $X_j$ and $Y$.

. . .

- In the advertising example, $k = 3$ and
$$\texttt{sales} = \beta_0 + \beta_1 \times \texttt{YouTube} + \beta_2 \times  \texttt{Facebook} + \beta_3 \times \texttt{Instagram} + \epsilon$$

. . .

- Assumptions as SLR
  + $\epsilon_i \stackrel{iid}{\sim} N(0, \sigma^2)$
  

. . .

- When $k = 1$, MLR is reduced to SLR.

::: notes
- Two subscripts of X.
- Later, we will learn how to interpret the coefficients correctly.
- We interpret $\beta_j$, $j = 1, \dots, p$, as the average effect on $Y$ of a one unit increase in $X_j$, **holding all other predictors fixed**.
:::

. . .

::: question
How many parameters are there in the model?
:::


## Sample MLR Model
- Given the training sample data $(x_{11}, \dots, x_{1k}, y_1), (x_{21}, \dots, x_{2k}, y_2), \dots, (x_{n1}, \dots, x_{nk}, y_n),$
- The sample MLR model:
$$\begin{align}
y_i &= \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_k x_{ik} + \epsilon_i \\
&= \beta_0 + \sum_{j=1}^k\beta_j x_{ij} + \epsilon_i, \quad i = 1, 2, \dots, n
\end{align}$$

```{r}
#| out-width: 48%
knitr::include_graphics("./images/img-model/mlr_data_matrix.png")
```


::: notes
- Writing our data and model without using matrix starts getting tedious.
- Later we'll see our to write it in a more clean way using matrix notation.
- our mpg data set in the homework has this type of structure.
- In R we usually store this kind of data set in R structure call data frame.
:::


## Regression Hyperplane
- **SLR**: regression line
- **MLR: regression hyperplane or response surface**
- $y = \beta_0 + \beta_1x_1 + \beta_{2}x_2 + \epsilon$
- $E(y \mid x_1, x_2) = 50 + 10x_1 + 7x_2$
```{r}
x1 <- runif(50, 0, 10)
x2 <- runif(50, 0, 10)
x1x2 <- x1 * x2
x1_2 <- x1 ^ 2
x2_2 <- x2 ^ 2
mu <- 50 + 10 * x1 + 7 * x2 
mu_interact <- 50 + 10 * x1 + 7 * x2 + 5 * x1x2
mu_order2_interact <- 800 + 10 * x1 + 7 * x2 - 8.5 * x1_2 - 5 * x2_2 + 4 * x1x2
df <- data.frame("x1" = x1, "x2" = x2, "y" = mu)
# df_interact <- data.frame("x1" = x1, "x2" = x2, "x1x2" = x1x2, "y" = mu_interact)
# df_order2_interact <- data.frame("x1" = x1, "x2" = x2, "x1_sq" = x1_2,
#                                  "x2_sq" = x2_2, "x1x2" = x1x2,
#                                  "y" = mu_order2_interact)
my_lm <- lm(mu ~ x1 + x2, data = df)
my_lm_interact <- lm(mu_interact ~ x1*x2, data = df)
# my_lm_interact <- lm(mu_interact ~ x1*x2, data = df)
# my_lm_order2_interact <- lm(mu_order2_interact ~ x1 + x2 + x1_sq + x2_sq + x1x2, data = df_order2_interact)
my_lm_order2_interact <- lm(mu_order2_interact ~ poly(x1, x2, degree = 2, 
                                                      raw = TRUE), 
                            data = df)
```

:::: {.columns}

::: {.column width="50%"}
```{r}
#| out-width: 110%
par(mar = c(0, 0, 0, 0))
surf <- rsm::persp.lm(my_lm, x2 ~ x1, zlab = "E(y)", xlim = c(0, 10), ylim = c(0, 10), zlim = c(0, 250), theta = 30, phi = 15, 
                      col = grDevices::adjustcolor( "lightblue", alpha.f = 0.8))
points(trans3d(x1, x2, mu + rnorm(50, sd = 20), 
               pmat = surf$`x1 ~ x2`$transf), col = 2, pch = 16)
```
:::



::: {.column width="50%"}
```{r}
#| out-width: 110%
par(mar = c(4, 4, 0.5, 0.5))
rsm::contour.lm(my_lm, x2 ~ x1, image = TRUE, img.col = cm.colors(50),
                labcex = 1.4, lwd = 2, cex.axis = 1.5, las = 1)
```
:::
::::


::: notes
- Now I'm gonna show you what a MLR looks like when we fit it to the data.
- If we have two predictors, we will have a sample regression plane.
- If we have more than two predictors in the model, we are not able to visualize it, but the idea is the same.
- We will have a something called hyperplane or response surface that basically play the same role as the regression plane in 2D or regression line in 1D.
- The plot on the right is the contour plot when we project the plot onto the x1-x2 plane. You can see that basically the higher x1 and/or the higher x2, the higher value of y.
- Moreover, you can see that the level curves are straight and parallel, meaning that the effect of x1 on y does not change with the values of x2 or the effect does not depend on the level of x2.

- https://cran.r-project.org/web/packages/rsm/vignettes/rsm-plots.pdf
- http://www.sthda.com/english/wiki/scatterplot3d-3d-graphics-r-software-and-data-visualization 
- https://stackoverflow.com/questions/18147595/plot-3d-plane-true-regression-surface

:::


## Response Surface : Interaction Model
- $y = \beta_0 + \beta_1x_1 + \beta_{2}x_2 + \beta_{12}x_1x_2 + \epsilon$
- This is in fact a linear regression model: let $\beta_3 = \beta_{12}, x_3 = x_1x_2$.
- $E(y \mid x_1, x_2) = 50 + 10x_1 + 7x_2 + 5x_1x_2$
- `r emo::ji('sunglasses')` `r emo::ji('nerd_face')` **A linear model generates a nonlinear response surface!**

:::: {.columns}

::: {.column width="50%"}
```{r}
#| out-width: 110%
par(mar = c(0, 0, 0, 0))
surf_interact <- rsm::persp.lm(my_lm_interact, x2 ~ x1, zlab = "E(y)", xlim = c(0, 10), ylim = c(0, 10), zlim = c(0, 800), theta = 30, phi = 15, col = grDevices::adjustcolor( "lightblue", alpha.f = 0.8))
points(trans3d(x1, x2, mu_interact + rnorm(50, sd = 50), 
               pmat = surf_interact$`x1 ~ x2`$transf), col = 2, pch = 16)
```
:::



::: {.column width="50%"}
```{r}
#| out-width: 110%
par(mar = c(4, 4, 0.5, 0.5))
rsm::contour.lm(my_lm_interact, x2 ~ x1, image = TRUE, 
                img.col = cm.colors(50), las = 1,
                labcex = 1.4, lwd = 2, cex.axis = 1.5)
```
:::
::::



::: notes
- Remember in SLR, we can have a liner model that describes a nonlinear relationship.
- Same in MLR. We can have a linear model that generates a nonlinear response surface!
- $y = \beta_0 + \beta_1x_1 + \beta_{2}x_2 + \beta_{12}x_1x_2 + \epsilon$
- This is in fact a linear regression model: let $\beta_3 = \beta_{12}, x_3 = x_1x_2$.
:::




## Response Surface : 2nd Order with Interaction Model
- $y = \beta_0 + \beta_1x_1 + \beta_{2}x_2 + \beta_{11}x_1^2 + \beta_{22}x_2^2 + \beta_{12}x_1x_2 + \epsilon$
- $E(y) = 800+10x_1+7x_2 -8.5x_1^2-5x_2^2- 4x_1x_2$
- `r emo::ji('sunglasses')` `r emo::ji('nerd_face')` A **linear** regression model can describe a **complex nonlinear relationship** between the response and predictors!


:::: {.columns}
::: {.column width="50%"}
```{r}
#| out-width: 110%
par(mar = c(0, 0, 0, 0))
surf_order2_interact <- rsm::persp.lm(my_lm_order2_interact, x2 ~ x1, zlab = "E(y)", xlim = c(0, 10), ylim = c(0, 10), zlim = c(0, 1000), theta = 30, phi = 15, col = grDevices::adjustcolor( "lightblue", alpha.f = 0.8))
points(trans3d(x1, x2, mu_order2_interact + rnorm(50, sd = 50), 
               pmat = surf_order2_interact$`x1 ~ x2`$transf), col = 2, pch = 16)
```
:::

::: {.column width="50%"}
```{r}
#| out-width: 110%
par(mar = c(4, 4, 0.5, 0.5))
rsm::contour.lm(my_lm_order2_interact, x2 ~ x1, image = TRUE, 
                img.col = cm.colors(50), las = 1,
                labcex = 1.4, lwd = 2, cex.axis = 1.5)
```
:::
::::




# Point Estimation of Model Parameters



## Least Square Estimation of the Coefficients

$$\begin{align}
y_i &= \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \dots + \beta_k x_{ik} + \epsilon_i \\
&= \beta_0 + \sum_{j=1}^k\beta_j x_{ij} + \epsilon_i, \quad i = 1, 2, \dots, n
\end{align}$$

- The least-squares function is
$$S(\alpha_0, \alpha_1, \dots, \alpha_k) = \sum_{i=1}^n\left(y_i - \alpha_0 - \sum_{j=1}^k\alpha_j x_{ij}\right)^2$$
The function $S(\cdot)$ must be minimized with respect to the coefficients, i.e.,
$$(b_0, b_1, \dots, b_k) = \underset{{\alpha_0, \alpha_1, \dots, \alpha_k}}{\mathrm{arg \, min}}  S(\alpha_0, \alpha_1, \dots, \alpha_k)$$


::: notes
- As SLR, we can define the least-squares function as the sum of squares of epsilon.
- The idea is to minimize the function $S$ w.r.t. the model coefficients $\beta_0, \beta_1, \dots, \beta_k$.
- In other words, we are going to choose the sample statistics $b_0$, $b_1$, ..., $b_k$ as the estimates of $\beta_0, \beta_1, \dots, \beta_k$ so that $S(.)$ is minimized when $b_0$, $b_1$, ..., $b_k$ are plugged in the function.
:::



## Geometry of Least Square Estimation

```{r}
# scatter plot

advertising_data <- read.csv("./data/Advertising.csv")
x <- advertising_data$TV
y <- advertising_data$radio
z <- advertising_data$sales
par(mgp = c(2, 0.8, 0), las = 1, mar = c(4, 4, 0, 0))
plot3d <- scatterplot3d::scatterplot3d(advertising_data$TV,
                                       advertising_data$radio,
                                       advertising_data$sales,
              xlab = "X1", ylab = "X2", zlab = "Y",
              color = rgb(0, 0, 1, 0.4), mar = c(3, 3, 0, 2),
              angle = 30, pch = 16, box = FALSE, cex.symbols = 0.8)
# regression plane
adv_lm <- lm(sales ~ TV + radio, data = advertising_data)
plot3d$plane3d(adv_lm, lty.box = "solid", draw_lines = TRUE,
               draw_polygon = TRUE, lwd = 0.1,
               polygon_args = list(border = "green", col = rgb(0, 1, 0, 0.2)))

# overlay positive residuals
res_pos <- resid(adv_lm) > 0
plot3d$points3d(x[res_pos], y[res_pos], z[res_pos], pch = 16, col = "blue", cex = 0.8)

# compute locations of segments
orig     <- plot3d$xyz.convert(x, y, z)
plane    <- plot3d$xyz.convert(x, y, fitted(adv_lm))
i.negpos <- 1 + (resid(adv_lm) > 0) # which residuals are above the plane?

# draw residual distances to regression plane
segments(orig$x, orig$y, plane$x, plane$y, col = 1, lty = c(2, 1)[i.negpos],
         lwd = 0.8)
#
# # draw the regression plane
# s3d$plane3d(LM, draw_polygon = TRUE, draw_lines = TRUE,
#             polygon_args = list(col = rgb(0.8, 0.8, 0.8, 0.8)))
#
# redraw positive residuals and segments above the plane
# wh <- resid(adv_lm) > 0
# segments(orig$x[res_pos], orig$y[res_pos], plane$x[res_pos], plane$y[res_pos],
#          col = "red", lty = 1, lwd = 1.5)
# s3d$points3d(x[wh], z[wh], y[wh], pch = 19)
# knitr::include_graphics("./img/multiple_reg_fit.png")
```

::: notes
- If we look at the geometry of Least Square Estimation of the MLR, we have a visualization like this.
- Again, in SLR, different $b_0$ and $b_1$s give us different sample regression lines.
- In MLR, suppose we have two predictors, and different $b_0$ and $b_1$ and $b_2$ give us different sample regression planes.
- so geometrically speaking, we are trying to find a sample regression plane such that the sum of the squared distance between the observations (denoted by those blue points) and the plane is minimized.
- For more than 2 predictor case, we are not able to visualize it because we live a 3D world, but the idea is exactly the same. And we called the regression plane a hyperplane. OK.
:::




## Least-squares Normal Equations
$$\begin{align}
\left.\frac{\partial S}{\partial\alpha_0}\right\vert_{b_0, b_1, \dots, b_k} &= -2 \sum_{i=1}^n\left(y_i - b_0 - \sum_{j=1}^k b_j x_{ij}\right) = 0\\
\left.\frac{\partial S}{\partial\alpha_j}\right\vert_{b_0, b_1, \dots, b_k} &= -2 \sum_{i=1}^n\left(y_i - b_0 - \sum_{j=1}^k b_j x_{ij}\right)x_{ij} = 0, \quad j = 1, 2, \dots, k
\end{align}$$

- $p = k + 1$ equations with $p$ unknown parameters.
- The ordinary least squares estimators are the solutions to the normal equations.

::: notes
- So again similar to SLR, we can take derivative w.r.t $\beta_0$, $\beta_1$, to the $\beta_k$.
- And we are gonna have $p = k + 1$ equations with $p$ unknown parameters.
- So we can find one and only one solution to $\beta_0$, $\beta_1$, to the $\beta_k$, which are $b_0$, $b_1$, ..., $b_k$.
- And the $p$ equations are the least squares normal questions.
- You guys solve the equations when k = 1.
- Do you think it is easy to solve the questions when the number of predictors is 2 or more than 2?
:::

. . .

::: question
`r emo::ji("tropical_drink")` `r emo::ji("beer")` `r emo::ji("cocktail")`  `r emo::ji("clinking_glasses")`  I buy you a drink if you solve the equations for $k \ge 2$ by hand without using matrix notations or operations!
:::


::: notes
- Here I am happy to treat you and buy you a drink if you can solve the equations for $k \ge 2$ by hand without using matrix notations or operations!
- Come to my office hours and show your work. And we go to a bar together. Sounds good?
- In fact, if you try, you are gonna deal with lots of algebra, but if we write the system of equations in a matrix form, the solution becomes very clean.
- That's see why.
:::


## [R Lab]{.pink} [Delivery Time Data](./data/data-ex-3-1.csv)

:::: {.columns}

::: {.column width="70%"}

```{r}
#| echo: true
# Load the data set
delivery <- read.csv(file = "./data/data-ex-3-1.csv",
                     header = TRUE)
delivery_data <- delivery[, -1]
colnames(delivery_data) <- c("time", "cases", "distance")
str(delivery_data)
```

- $y$: the amount of time required by the route driver to stock the vending machines with beverages
- $x_1$: the number of cases stocked
- $x_2$: the distance walked by the driver
:::



::: {.column width="30%"}

::: midi
```{r}
#| echo: true
#| class-output: my_class800
delivery_data
```
:::
:::
::::


::: notes
- $y$: the amount of time required by the route driver to stock the vending machines with beverages in an outlet.
- $x_1$: the number of cases stocked
- $x_2$: the distance walked by the driver
- Goal: fit a MLR model $y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon$ to the amount of time required by the route driver to service the vending machines
(use readr::read_csv if the file is large)
:::




## [R Lab]{.pink} Scatterplot Matrix
```{r}
#| echo: !expr c(2)
#| out-width: 78%
par(mar = c(2, 2, 0, 0), mgp = c(2, 1, 0))
pairs(delivery_data)
```


::: notes
- Each plot shows the relationship between a pair of variables.
:::


## [R Lab]{.pink} 3D Scatterplot
```{r}
#| out-width: 75%
#| code-fold: true
#| echo: true
par(mgp = c(2, 0.8, 0), las = 1, mar = c(4, 4, 0, 0))
library(scatterplot3d)
scatterplot3d(x = delivery_data$cases, y = delivery_data$distance, z = delivery_data$time,
              xlab ="cases", ylab = "distance", zlab = "time",
              xlim = c(2, 30), ylim = c(36, 1640), zlim = c(8, 80),
              box = TRUE, color = "blue", mar = c(3, 3, 0, 2), angle = 30, pch = 16)
```


::: notes
- Sometimes a 3D scatterplot is useful in visualizing the relationship between the response and the regressors when there are only two regressors.
:::



## [R Lab]{.pink} Multiple Linear Regression
```{r}
#| echo: true
delivery_lm <- lm(time ~ cases + distance, data = delivery_data)
delivery_lm$coef
```

$$\hat{y} = 2.34 + 1.62x_1 + 0.014x_2$$

- $b_1$: **All else held constant**, for one case of product stocked increase, we expect the delivery time to be longer, *on average*, by 1.62 minutes.

- $b_2$: **All else held constant**, one additional foot walked by the driver causes the delivery time, *on average*, to be 0.014 minutes longer.

- $b_0$: The delivery time with no number of cases of product stocked and no distance walked by the driver is expected to be 2.34 minutes. (Make sense?!)

## [R Lab]{.pink} Regression Plane
```{r}
#| out-width: 82%
par(mgp = c(2, 0.8, 0), las = 1)
x <- delivery_data$cases
y <- delivery_data$distance
z <- delivery_data$time
# scatter plot
plot3d <- scatterplot3d(x, y, z,
              xlab ="cases", ylab = "distance", zlab = "time",
              xlim = c(2, 30), ylim = c(36, 1640), zlim = c(8, 80),
              color = rgb(0, 0, 1, 0.4), mar = c(3, 3, 0, 2),
              angle = 45, pch = 16, box = FALSE, las = 1, cex.symbols = 0.8)
# regression plane
plot3d$plane3d(delivery_lm, lty.box = "solid", draw_lines = TRUE,
               draw_polygon = TRUE, lwd = 0.1,
               polygon_args = list(border = "green", col = rgb(0, 1, 0, 0.2)))

# overlay positive residuals
res_pos <- resid(delivery_lm) > 0
plot3d$points3d(x[res_pos], y[res_pos], z[res_pos], pch = 16, col = "blue", cex = 0.8)

# compute locations of segments
orig     <- plot3d$xyz.convert(x, y, z)
plane    <- plot3d$xyz.convert(x, y, fitted(delivery_lm))
i.negpos <- 1 + (resid(delivery_lm) > 0) # which residuals are above the plane?

# draw residual distances to regression plane
segments(orig$x, orig$y, plane$x, plane$y, col = "black", lty = c(2, 1)[i.negpos],
         lwd = 1)
#
# # draw the regression plane
# s3d$plane3d(LM, draw_polygon = TRUE, draw_lines = TRUE,
#             polygon_args = list(col = rgb(0.8, 0.8, 0.8, 0.8)))
#
# redraw positive residuals and segments above the plane
# wh <- resid(LM) > 0
# segments(orig$x[res_pos], orig$y[res_pos], plane$x[res_pos], plane$y[res_pos],
#          col = "red", lty = 1, lwd = 1.5)
# s3d$points3d(x[wh], z[wh], y[wh], pch = 19)
```



## Estimation of $\sigma^2$
- $SS_{res} = \sum_{i=1}^ne_i^2 = \sum_{i=1}^n(y_i - \hat{y}_i)^2$.

- $MS_{res} = \frac{SS_{res}}{n - p}$ with $p = k + 1$.

- $S^2 = MS_{res}$ is *unbiased* for $\sigma^2$, i.e., $E[MS_{res}] = \sigma^2$.

<!-- - $\hat{\sigma}^2 = MS_{res}$ is model dependent. Its value varies with change of the model. -->

- $S^2$ of SLR may be quite larger than the $S^2$ of MLR.

- $S^2$ measures the variation of the *unexplained* noise about the fitted regression line/hyperplane, so we prefer a small residual mean square.


::: notes
- $SS_{res} = \sum_{i=1}^ne_i^2 = {\bf e'e} = {\bf (y-Xb)'(y-Xb)} = {\bf y'y - b'X'y} = \sum_{i=1}^n(y_i - \hat{y}_i)^2$.
- If our model is specified correctly, $\hat{\sigma}^2$ depends on our data quality.
- If our data have lots of noise itself, there is nothing we can do.
:::


## [R Lab]{.pink} Estimation of $\sigma^2$
```{r}
#| echo: true

## method 1
summ_delivery <- summary(delivery_lm)  ## check names(summ_delivery)
summ_delivery$sigma ^ 2

```

. . .

```{r}
#| echo: true

## method 2
n <- length(delivery_lm$residuals)
(SS_res <- sum(delivery_lm$residuals * delivery_lm$residuals))
SS_res / (n - 3)
```

. . .

```{r}
#| echo: true

## method 3
(SS_res1 <- sum((delivery_data$time - delivery_lm$fitted.values) ^ 2))
SS_res1 / (n - 3)
```

::: notes
```{r, eval=FALSE}
summ_delivery <- summary(delivery_lm)
summ_delivery$sigma ^ 2
# ## Why this is SS_res? Check what crossprod() is doing!
# (SS_res1 <- crossprod(y) - crossprod(fitted_y, y))
# SS_res1 / (n - 3)
## Why this is SS_res?
n <- length(delivery_lm$residuals)
(SS_res <- sum(delivery_lm$residuals * delivery_lm$residuals))
SS_res / (n - 3)
```
:::



# Confidence Interval
<h2> CI for Coefficients </h2>
<h2> CI for the Mean Response  </h2>
<h2> PI for New Observations  </h2>


## Properties of LSEs
- `r emo::ji('thumbsup')` ${\bf b} = (b_0, b_1, \dots, b_k)'$ is BLUE.
- <span style="color:#DE3163"> **Linear** </span>: Each $b_j$ is a linear combination of $y_1, \dots, y_n$.
- <span style="color:#DE3163"> **Unbiased** </span>: Each $b_j$ is normally distributed with mean $\beta_j$.
- <span style="color:#DE3163"> **Best** </span>: Each $b_j$ has the minimum variance, comparing to all other unbiased estimator for $\beta_j$ that is a linear combo of $y_1, \dots, y_n$.


## Wald CI for Coefficients
The $(1-\alpha)100\%$ Wald CI for $\beta_j$, $j = 0, 1, \dots, k$ is
$$\left(b_j- t_{\alpha/2, n-p}~se(b_j), \quad b_j + t_{\alpha/2, n-p}~ se(b_j)\right)$$

```{r}
#| echo: true
(ci <- confint(delivery_lm))
```

- These are **marginal** CIs seperately for each $b_j$.


::: notes
- Each of the statistics $\frac{b_j - \beta_j}{\sqrt{\hat{\sigma}^2C_{jj}}}, j = 0, 1, 2, \dots, k$ follows $t_{n-p}$ distribution, where $\hat{\sigma}^2 = MS_{res}$, and $C_{jj}$ is the $j$-th diagonal element of ${\bf (X'X)}^{-1}$.
- The $(1-\alpha)100\%$ CI for $\beta_j$, $j = 0, 1, \dots, k$ is
$\left(b_j- t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2C_{jj}}, \quad b_j + t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2C_{jj}}\right)$
- These interval estimates do not take correlation of coefficients into account. One coefficient may be higher when another is lower.
- We can only use the intervals one at a time when doing interval estimation. When we want to do interval estimation for several coefficients together at the same time, these intervals are not that accurate.
:::


## Correlated Coefficients {visibility="hidden"}

::: fact
Covariance of random variables $X$ and $Y$, $\cov(X, Y)$ is defined as 
$$\small \cov(X, Y) = E[(X - E(X))(Y - E(Y))]$$
::: 


::: question
What is $\cov(X, X)$?
:::

. . .

:::: {.columns}

::: {.column width="50%"}
::: fact
The covariance matrix of the coefficient vector ${\bf b} = (b_0, b_1, b_2)'$ is  $$\scriptsize \begin{align} \cov({\bf b}) &= \begin{bmatrix} \cov(b_0, b_0) & \cov(b_0, b_1) & \cov(b_0, b_2) \\ \cov(b_1, b_0) & \cov(b_1, b_1) & \cov(b_1, b_2) \\ \cov(b_2, b_0) & \cov(b_2, b_1) & \cov(b_2, b_2) \end{bmatrix} \end{align}$$
:::
:::



::: {.column width="50%"}
::: fact
The correlation matrix of the coefficient vector ${\bf b} = (b_0, b_1, b_2)'$ is  $$\scriptsize \begin{align} \cor({\bf b}) &= \begin{bmatrix} 1 & r_{01} & r_{02} \\ r_{10} & 1 & r_{12} \\ r_{20} & r_{21} & 1 \end{bmatrix} \end{align}$$
:::
:::
::::


> <span style="color:blue"> But all $b_j$ are correlated! </span>

## Correlated Coefficients

:::: {.columns}
::: {.column width="50%"}

```{r}
#| echo: true
## variance-covariance matrix
V <- vcov(delivery_lm)

## standard error
sqrt(diag(V))
```

:::

::: {.column width="50%"}
```{r}
#| echo: true
## correlation matrix
cov2cor(V)
```
:::
::::

- $b_1$ and $b_2$ are negatively correlated.

- Individual CI ignores the correlation between $b_j$s.

. . .

::: question
How do we specify a confidence level that applies **simultaneously** to a **set** of interval estimates? For example, a $95\%$ confidence "interval" for both $b_1$ and $b_2$.
:::


## Confidence Region
- The $(1-\alpha)100\%$ CI for a **set** of $b_j$s will be an **elliptically-shaped region**!

<!-- :::: {.columns} -->

<!-- ::: {.column width="50%"} -->
```{r}
#| echo: true
#| eval: true
#| code-fold: true
par(mgp = c(2.8, 0.9, 0), mar = c(4, 4, 2, 0))
## confidence region
car::confidenceEllipse(
    delivery_lm, 
    levels = 0.95, 
    which.coef = c("cases", "distance"), 
    main = expression(
        paste("95% Confidence Region for ", 
              beta[1], " and ",  beta[2])
        )
    )
## marginal CI for cases
abline(v = ci[2, ], lty = 2, lwd = 2)  
## marginal CI for distance
abline(h = ci[3, ], lty = 2, lwd = 2)
points(x = 1.4, y = 0.01, col = "red", cex = 2, pch = 16)
points(x = 2, y = 0.008, col = "black", cex = 2, pch = 16)
```
<!-- ::: -->


<!-- ::: {.column width="50%"} -->
<!-- # ```{r} -->
<!-- # #| out-width: 100% -->
<!-- # par(mgp = c(2.8, 0.9, 0), mar = c(4, 4, 2, 0)) -->
<!-- # car::confidenceEllipse(delivery_lm, levels = 0.95, which.coef = c("cases", "distance"),  -->
<!-- #                        main = expression(paste("95% Confidence Region for ", beta[1], " and ",  beta[2]))) -->
<!-- # abline(v = ci[2, ], lty = 2, lwd = 2)  ## marginal CI for cases -->
<!-- # abline(h = ci[3, ], lty = 2, lwd = 2)  ## marginal CI for distance -->
<!-- # points(x = 1.4, y = 0.01, col = "red", cex = 2, pch = 16) -->
<!-- # points(x = 2, y = 0.008, col = "black", cex = 2, pch = 16) -->
<!-- # ``` -->
<!-- ::: -->
<!-- :::: -->

- Some points (red) within the marginal intervals are implausible according to the joint region.
- The joint region includes values of the coefficient for cases (black), for example, that are excluded from the marginal interval.

::: notes
- Bonferroni
- Scheffe S-method
- maximum modulus t procedure
With repeated sampling, 95% of
such ellipses will simultaneously include β1 and β2, if the fitted model is correct and normality holds. The orientation of the ellipse reflects the negative correlation between the estimates. 
Contrast the 95% confidence ellipse with the marginal 95% confidence intervals, also shown on the plot. Some points within the marginal intervals—with larger values for both of the coefficients, for example—are implausible according to the joint region. Similarly, the joint region includes values of the coefficient for income, for example, that are excluded from the marginal interval.
:::


## CI for the Mean Response $E(y \mid {\bf x}_0)$
- The fitted value at a point ${\bf x}_0 = (1, x_{01}, x_{02}, \dots, x_{0k})'$ is $$\hat{y}_0 = b_0 + b_1x_{01} + \cdots + b_kx_{0k}$$
- This is an unbiased estimator for $E(y \mid {\bf x}_0)$
<!-- - $\frac{\hat{y}_0 - E(y|{\bf x}_0)}{\sqrt{\hat{\sigma}^2{\bf x}_0'{\bf (X'X)}^{-1}{\bf x}_0}}\sim t_{n-p}$. -->
- The $(1-\alpha)100\%$ CI for $E(y \mid {\bf x}_0)$ is
$$\left(\hat{y}_0 - t_{\alpha/2, n-p} ~ se(\hat{y}_0), \quad \hat{y}_0 + t_{\alpha/2, n-p} ~ se(\hat{y}_0)\right)$$

```{r}
#| echo: true
predict(delivery_lm,
        newdata = data.frame(cases = 8, distance = 275),
        interval = "confidence", level = 0.95)
```


::: notes
- The fitted value at a point ${\bf x}_0 = (1, x_{01}, x_{02}, \dots, x_{0k})'$ is $\hat{y}_0 = {\bf x}_0'\bf b$.
- This is an unbiased estimator for $E(y \mid {\bf x}_0)$.
- $\frac{\hat{y}_0 - E(y|{\bf x}_0)}{\sqrt{\hat{\sigma}^2{\bf x}_0'{\bf (X'X)}^{-1}{\bf x}_0}}\sim t_{n-p}$.
- The $(1-\alpha)100\%$ CI for $E(y \mid {\bf x}_0)$ is
$\left(\hat{y}_0 - t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2{\bf x}_0'{\bf (X'X)}^{-1}{\bf x}_0}, \quad \hat{y}_0 + t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2{\bf x}_0'{\bf (X'X)}^{-1}{\bf x}_0}\right)$

```{r}
predict(delivery_lm,
        newdata = data.frame(cases = 8, distance = 275),
        interval = "confidence", level = 0.95)
```
:::


## PI for New Observations
- Predict the future observation $y_0$ when ${\bf x} = {\bf x}_0$.
- A point estimate is $\hat{y}_0 = b_0 + b_1x_{01} + \cdots + b_kx_{0k}$.
- The $(1-\alpha)100\%$ PI for $y_0$ is
$$\left(\hat{y}_0 - t_{\alpha/2, n-p} ~se(y_0 - \hat{y}_0), \quad \hat{y}_0 + t_{\alpha/2, n-p} ~se(y_0 - \hat{y}_0)\right)$$

```{r}
#| echo: true
predict(delivery_lm,
        newdata = data.frame(cases = 8, distance = 275),
        interval = "predict", level = 0.95)
```


::: notes
- Predict the future observation $y_0$ when ${\bf x} = {\bf x}_0$.
- A point estimate is $\hat{y}_0 = {\bf x}_0'\bf b$.
- The $(1-\alpha)100\%$ PI for $y_0$ is
$\left(\hat{y}_0 - t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2\left(1+{\bf x}_0'{\bf (X'X)}^{-1}{\bf x}_0\right)}, \quad \hat{y}_0 + t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2\left(1+{\bf x}_0'{\bf (X'X)}^{-1}{\bf x}_0\right)}\right)$

```{r}
predict(delivery_lm,
        newdata = data.frame(cases = 8, distance = 275),
        interval = "predict", level = 0.95)
```
:::


## Predictor Effect Plots
- A complete picture of the regression surface requires drawing a $p$-dimensional graph.

- **Predictor effect plots** look at 1 or 2D plots for each predictor.

. . .

- The predictor effect plot for $x_1$: 
  + fix the values of all other predictors ( $x_2$ in the example)
  + substitute these fixed values into the fitted regression equation.

- Fix $x_2$ at its average, $\hat{y} = 2.34 + 1.62 ~x_1 + 0.014 (409.28)$

. . .

- **Same** slope for *any* choice of fixed values of other predictors.

- The intercepts depend on the values of other predictors.



::: notes
- A complete picture of the regression surface generated by the fitted model requires drawing a $p$-dimensional graph.

- Fix $x_1$ at its average, $\hat{y} = 2.34 + 1.62 (8.76) + 0.014 ~x_2$

- The slope would be the **same** for *any* choice of fixed values of other predictors.
:::



## [R Lab]{.pink} Predictor Effect Plots
```{r}
#| eval: false
#| echo: true
library(effects)
plot(effects::predictorEffects(mod = delivery_lm))
```

:::: {.columns}

::: {.column width="50%"}
```{r}
#| out-width: 100%
#| fig-asp: 0.7

par(mar = c(3, 3, 1, 0))
plot(effects::predictorEffects(predictor = "cases", mod = delivery_lm))
```
:::



::: {.column width="50%"}
```{r}
#| out-width: 100%
#| fig-asp: 0.7

par(mar = c(3, 3, 1, 0))
plot(effects::predictorEffects(predictor = "distance", mod = delivery_lm))
```
:::
::::


::: notes
- rug plot
- the mean of time increases linearly as cases or distance increases. 
- Given x2 variable in the model, what is the effect on x1 on the response.
- Without correction for simultaneous statistical inference
- plot(effects::predictorEffects(mod = delivery_lm, confint = list(type = "Scheffe")))
'arg' should be one of “pointwise”, “Scheffe”, “scheffe”
:::

## Extrapolation {visibility="hidden"}
- In MLR, it's easy to inadvertently extrapolate since the regressors jointly define the region containing the data.

```{r}
#| out-width: 70%
X <- cbind(1, delivery_data[, c(2, 3)])
X <- as.matrix(X)
y <- as.matrix(delivery_data$time)
con.hull.pos <- grDevices::chull(delivery_data[, 2:3])
con.hull <- rbind(delivery_data[con.hull.pos, ], delivery_data[con.hull.pos[1], ])[, 2:3]
# plot(delivery_data$cases, delivery_data$distance, 
#      xlab = "cases", ylab = "distance")
# lines(con.hull, col = "red")
H <- X %*% solve(t(X) %*% X) %*% t(X)
xa <- c(8, 275)
xb <- c(20, 250)
# t(c(1, xa)) %*% solve(t(X) %*% X) %*% c(1, xa)
# t(c(1, xb)) %*% solve(t(X) %*% X) %*% c(1, xb)
par(mar = c(3, 3, 0, 0))
plot(X[, -1], pch = 16, las = 1, xlab = "cases", ylab = "distance")
points(xa[1], xa[2], col = 2, pch = 15)
points(xb[1], xb[2], col = 4, pch = 10)
legend("topleft", c("interpolate", "extrapolate"), col = c(2, 4), 
       pch = c(15, 10), bty = "n")
lines(con.hull, col = 3, lwd = 4)
```

::: notes
- define the smallest convex set containing all of the original $n$ regressor points, as the regressor vairiable hull (RVH).
- a point within the ranges of x1 and x2 may not be necessarily a interpolation point.

```{r}
with(delivery_data,
     car::dataEllipse(cases, distance,
        levels = c(0.5, 0.75, 0.9, 0.95)))

```
:::


# Hypothesis Testing
<h2> Test for Significance of Regression </h2>
<h2> Tests on Individual Coefficients </h2>
<h2> Tests on Subsets of Coefficients (Later) </h2>


::: notes
- Test for Significance: test if there is any regressor that has a significant effect on predicting $Y$ or explaining the variation of $Y$.
- In a Test for Significance, we are looking all the regressors.
- Tests on Individual Coefficients: we are only interested in single one predictor's effect.
- But sometimes, we may be interested in a subset of Coefficients, not all, not single one, but two or three coefficients. And we'll see how to do this kind of test later.
- Finally, we'll learn the so-called General Linear Hypothesis that can perform a quite general test.
- The three tests above are in fact special case of the General Linear Hypothesis.
:::


## Test for Significance of Regression
- **Test for significance**: Determine if there is a **linear** relationship between the response and **any** of the regressor variables.
- <span style="color:blue">
$H_0: \beta_1 = \beta_2 = \cdots = \beta_k = 0 \quad H_1: \beta_j \ne 0 \text{ for at least one } j$ </span>
<!-- - The $F$ test of ANOVA performs the test. -->

<!-- # ```{r} -->
<!-- # knitr::include_graphics("./images/img-model/anova_mlr.png") -->
<!-- # ``` -->
<!-- - $SS_T = {\bf y'y} - \frac{\left(\sum_{i=1}^n y_i \right)^2}{n}$, $SS_R = {\bf b'X'y}-\frac{\left(\sum_{i=1}^n y_i \right)^2}{n}$, $SS_{res} = {\bf y'y} - {\bf b'X'y}$ -->

. . .


| Source of Variation | SS | df | MS | F | $p$-value |
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
| Regression | $SS_R$| $k$ | $MS_R$  |  $\frac{MS_R}{MS_{res}} = F_{test}$ |  $P(F_{k, n-k-1} > F_{test})$ |
| Residual | $SS_{res}$ | $n-k-1$   |  $MS_{res}$ |   |  | 
| Total | $SS_{T}$ | $n-1$   |   |   |  | 

- Reject $H_0$ if $F_{test} > F_{\alpha, k, n - k - 1}$.

::: notes
- In SLR, test for significance is the same as testing individual coefficient $\beta_1 = 0$.
- We may reject the null when the truth is all beta's are nonzero, or only one single beta is nonzero.
- If there is only one nonzero beta, and we reject the null, basically, the nearly all the variation of Y is explained by the corresponding predictor that has the nonzero beta.
- We can still say the model is significant because that particular predictor has a significant effect on explaining y.
- Another way to represent SS_T
:::


## [R Lab]{.pink} Test for Significance

<span style="color:blue">
$H_0: \beta_{1} = \beta_{2} = 0 \quad H_1: \beta_j \ne 0 \text{ for at least one } j$ </span>

```{r}
#| echo: true
#| output.lines: !expr c(9:19)
#| class-output: my_class800
summ_delivery
```


::: notes
- How do we obtain the ANOVA Table?
:::


## [R Lab]{.pink} Wrong ANOVA Table

| Source of Variation | SS | df | MS | F | $p$-value |
|:-----:|:-----:|:-----:|:-----:|:-----:|:-----:|
| Regression | $SS_R$| $k$ | $MS_R$  |  $\frac{MS_R}{MS_{res}} = F_{test}$ |  $P(F_{k, n-k-1} > F_{test})$ |
| Residual | $SS_{res}$ | $n-k-1$   |  $MS_{res}$ |   |  | 
| Total | $SS_{T}$ | $n-1$   |   |   |  | 


```{r}
#| echo: true
anova(delivery_lm) ## This is for sequential F-test
```

- This is so called **Type-I** ANOVA table for a *sequential F-test*, which is **NOT** what we want.


<!-- # ```{r} -->
<!-- #  -->
<!-- # knitr::include_graphics("./images/img-model/anova_mlr.png") -->
<!-- # ``` -->



## [R Lab]{.pink} ANOVA Table: Null vs. Full
::: alert
Testing coefficients is like model comparison: **Compare the full model with the model under $H_0$**!
:::

:::: {.columns}

::: {.column width="50%"}
+ **Full model**: including both `cases` and `distance` predictors
+ **Null model**: no predictors $(\beta_1 = \beta_2 = 0)$


```{r}
#| out-width: 100%
knitr::include_graphics("./images/img-model/anova_ex.png")
```

:::

::: {.column width="50%"}
::: {.fragment}
```{r}
#| echo: true
#| output.lines: !expr c(1:7)

## regression with intercept only
null_model <- lm(time ~ 1,
                 data = delivery_data)
anova(null_model, delivery_lm)
```
:::
:::
::::




::: notes
- first row: info of SS_T
- second row: info of SS_R and SS_res
- does not show MS
:::




## $R^2$ and Adjusted $R^2$
- $R^2 = \frac{SS_R}{SS_T} = 1 - \frac{SS_{res}}{SS_T}$
   + calculated as in SLR.
   + accesses how well the regression model fits the data.
   + measures the proportion of variability in $Y$ that is explained by the regression or the $k$ predictors.
   + The model with one additional predictor **always gets a higher $R^2$**.


::: notes
- There are other criterion or metrics for accessing model adequacy or model fit that may be better than R^2
- The model with one additional predictor **always gets a higher $R^2$** even the new predictor has no explanatory power or useless in predicting y. Because the predictor may capture the random noise variation, leading to decrease in $SS_{res}$.
- If we use R^2 as the measure of model fit, then we will tend to include lots of regressors in the model, and we will always choose the model that has the most predictors, even though lots of predictors have no or little contribution to the fitting or prediction.
- The idea is a little bit like overfitting in machine learning.
- A complex or a larger model has several disadvantages.
  + First, it's more difficult to interpret your model and results. It reduces the interpretability.
  + Second, the computing or running time is usually longer, and sometimes much longer depending on the order of complexity.
  + Also, the data or the model itself may consume lots of memory spaces.
  
:::


. . .

::: alert
**Occam's Razor: Don't use a complex model if a simpler model can perform equally well!**
:::


. . .

- **Adjusted $R^2$**
  + $R^2_{adj} = 1 - \frac{SS_{res}/(n-p)}{SS_T/(n-1)}$
  + applies a penalty (through $p$) for number of variables included in the model.


::: notes
- Not the more the better anymore
- Adjusted R2 doesn't increase if the new variable does not provide very little information for prediction
- Adjusted R2 will only increase on adding a variable to the model if the addition of the regressor reduces $MS_{res}$
- This makes adjusted R2 a preferable metric for model selection in multiple regression models.
:::


## Adjusted $R^2$ Example
- For a model with 3 predictors, $SS_{res} = 90$, $SS_T = 245$, and $n = 15$.
$$R^2_{adj} = 1 - \frac{90/(15-4)}{245/(15-1)} = 0.53$$
- The 4-th regressor is added into the model, and $SS_{res} = 88$ (always decreases). Then
$$R^2_{adj} = 1 - \frac{88/(15-5)}{245/(15-1)} = 0.49$$

. . .

**Intuition**: The new added regressor should have explanatory power for $y$ large enough, so that $MS_{res}$ is decreased.


## [R Lab]{.pink} $R^2$ and Adjusted $R^2$
```{r}
#| echo: true
#| output.lines: !expr c(16:18)
summ_delivery
```
```{r}
#| echo: true
summ_delivery$r.squared
summ_delivery$adj.r.squared
```


## Tests on Individual Regression Coefficients

- Hypothesis test on any **single** regression coefficient.
- <span style="color:blue">
$H_0: \beta_{j} = 0 \quad H_1: \beta_j \ne 0$ </span>
- $t_{test} = \frac{b_j}{se(b_j)}$
- Reject $H_0$ if $|t_{test}| > t_{\alpha/2, n-k-1}$
- This is a **partial** or **marginal test**: a test of the **contribution of $X_j$ given ALL other regressors in the model**.

::: notes
- We can also do a one-side test for sure. But I am not sure if there is a R function to do a one-sided test.
- But we can always compute the test statistic or the p-value ourselves.
- Hypothesis test on any **single** regression coefficient.
- <span style="color:blue">
$H_0: \beta_{j} = 0 \quad H_1: \beta_j \ne 0$ </span>
- $t_{test} = \frac{b_j}{\sqrt{\hat{\sigma}^2C_{jj}}}$, where $C_{jj}$ is the $j$-th diagonal element of $({\bf X'X})^{-1}$.
- Reject $H_0$ if $|t_{test}| > t_{\alpha/2, n-k-1}$
- This is a **partial** or **marginal test**: a test of the **contribution of $X_j$ given ALL other regressors in the model**.
:::



## [R Lab]{.pink} Tests on Individual Coefficients
- Assess the value of $x_2$ (distance) given that $x_1$ (cases) is in the model.
- <span style="color:blue">
$H_0: \beta_{2} = 0 \quad H_1: \beta_2 \ne 0$ </span>

```{r}
#| echo: true
summ_delivery$coefficients
```

:::  notes
qt(0.05/2, df = delivery_lm$df.residual, lower.tail = FALSE) ## t critical value df = n - 3 = 22
```{r}
XtX_inv_diag <- diag(solve(t(X) %*% X)) ## diagonal elements of (XtX)^-1
se_b <- sqrt(summ_delivery$sigma ^ 2 * XtX_inv_diag) ## standard error
(t_test <- (delivery_lm$coefficients[3] - 0) / se_b[3]) ## t test statistic
2 * pt(q = t_test, df = 22, lower.tail = FALSE) ## p-value
```
:::


## Inference Pitfalls
::: danger
- The test $H_0:\beta_j = 0$ will always be rejected as long as the sample size is large enough, even $x_j$ has a very small effect on $y$.
  + Consider the **practical significance** of the result, not just the statistical significance.
  + Use the confidence interval to draw conclusions instead of relying only p-values.

:::

. . .

<br>

::: danger
- If the sample size is small, there may not be enough evidence to reject $H_0:\beta_j = 0$.
  + DON'T immediately conclude that the variable has no association with the response.
  + There may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.

:::

