# Confidence Interval {#sec-infer-ci}

```{r packages, echo=FALSE, message=FALSE, warning=FALSE}
library(openintro)
library(knitr)
library(emoji)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_knit$set(global.par = TRUE)
# options(reindent.spaces = 4)
```

## Foundations for Inference

<span style="color:blue"> **Inference Framework** </span>

- **Inferential statistics uses sample data to learn about an unknown population**.
- **Idea**: Assume the target population follows some distribution but with **unknown** parameters.
  + <span style="color:blue"> Assume the population is normally distributed but its mean and/or variance are unknown. </span>
- **Goal**: Learn the unknown parameters of the assumed population distribution.

::::{.columns}
:::{.column width="50%"}
```{r sample, cache=TRUE, echo=FALSE, out.width="80%",fig.align='center'}
#| label: fig-sampling
#| fig-cap: Sampling from a population
par(mar = 0*c(1,1,1,1))
plot(c(0, 2),
     c(0, 1.1),
     type = 'n',
     axes = FALSE, xlab = "", ylab = "")
temp <- seq(0, 2 * pi, 2 * pi / 100)
x <- 0.5 + 0.5 * cos(temp)
y <- 0.5 + 0.5 * sin(temp)
lines(x, y)

s <- matrix(runif(700), ncol = 2)
S <- matrix(NA, 350, 2)
j <- 0
for (i in 1:nrow(s)) {
  if(sum((s[i, ] - 0.5)^2) < 0.23){
    j <- j + 1
    S[j, ] <- s[i, ]
  }
}
points(S, col = COL[1, 3], pch = 20)
text(0.5, 1, 'Population', pos = 3)

set.seed(50)
N <- sample(j, 25)
lines((x - 0.5) / 2 + 1.5, (y - 0.5) / 2 +  0.5, pch = 20)

SS <- (S[N, ] - 0.5) / 2 + 0.5
these <- c(2, 5, 11, 10, 12)
points(SS[these, 1] + 1,
       SS[these, 2],
       col = COL[4, 2],
       pch = 20,
       cex = 1.5)
text(1.5, 0.75, 'Sample', pos = 3)

for (i in these) {
  arrows(S[N[i], 1], S[N[i], 2],
         SS[i, 1] + 1 - 0.03, SS[i, 2],
         length = 0.08, col = COL[5], lwd = 1.5)
}
```
:::

:::{.column width="50%"}
```{r, echo=FALSE, out.width="77%",fig.align='center'}
#| label: fig-data-generating
#| fig-cap: Relationship between probabiltity and statistical inference
# knitr::include_graphics("./img/data_generating.png")
# fig.cap="Figure 1.1 in All of Statistics (Wasserman 2003)"
par(mar = c(0, 0, 0, 0))
plot(0, 0, type = "n", axes = FALSE, xlab = "", ylab = "")
plotrix::draw.ellipse(x = -0.56, y = 0, a = 0.52, b = 0.4, lwd = 2)
plotrix::draw.ellipse(x = 0.56, y = 0, a = 0.45, b = 0.34, lwd = 2)
text(x = -0.56, y = 0, labels = "Data Generating Process", cex = 1.5)
text(x = 0.56, y = 0, labels = "Observed Data", cex = 1.5)
diagram::curvedarrow(from = c(-0.56, 0.47), to = c(0.56, 0.47), 
                     curve = -0.2, arr.pos = 0.98)
diagram::curvedarrow(from = c(0.56, -0.47), to = c(-0.56, -0.47), 
                     curve = -0.2, arr.pos = 0.98)
text(x = 0, y = 0.8, labels = "Probability", cex = 1.5)
text(x = 0, y = -0.8, labels = "Statistical Inference", cex = 1.5)
# plot(0, 0, type = "n", axes = FALSE, xlab = "", ylab = "")
# plotrix::draw.ellipse(x = -0.3, y = 0.5, a = 0.65, b = 0.45, lwd = 2)
# plotrix::draw.ellipse(x = -0.3, y = 0.4, a = 0.35, b = 0.2, lwd = 2, lty = 2)
# text(x = -0.3, y = 0.7, labels = "Set of all measurements: Population", cex = 1.2)
# plotrix::draw.ellipse(x = 0.5, y = -0.5, a = 0.36, b = 0.21, lwd = 2, lty = 1)
# diagram::curvedarrow(from = c(-0.3, 0.4), to = c(0.5, -0.3), 
#                      curve = -0.2, arr.pos = 0.98)
# text(x = 0.5, y = -0.5, labels = "Sample", cex = 1.2)
# text(x = 0, y = -0.4, labels = "Set of data selected from the population:", cex = 1.2)
```
:::
::::

- There are two approaches in parameter learning.
  - **Estimation** 
  - **Hypothesis Testing**

## Point Estimator

:::{.callout-note icon=false}
## If you could **only use a single number** to guess the unknown population mean, $\mu$, what number would you like to use?
:::

- The single point used to estimate the unknown parameter is known as a **point estimator**.
- A **point estimator** is any function of data $(X_1, X_2, \dots, X_n)$.
  + **Any statistic is considered a point estimator** *(before actually being collected).*
- A **point estimate** is a value of a point estimator used to estimate a population parameter. 
  + This is a value calculated from the collected data.
- The sample mean, $(\overline{X})$, is a statistic and a point estimator for the population mean, $\mu$.

--------------------------------------------------------------------

<span style="color:blue"> **Sample Mean as an Point Estimator** </span>

- Draw 5 values from the population that follows $N(2, 1)$ as sample data $(x_1, x_2, x_3, x_4, x_5)$.
```{r, echo=FALSE}
## Generate sample data x1, x2, x3, x4, x5, each from population distribution N(2, 1)
x_data_1 <- rnorm(n = 5, mean = 2, sd = 1)
```


```{r, echo=FALSE}
x_vec <- c(x_data_1, mean(x_data_1))
names(x_vec) <- c("x1", "x2", "x3", "x4", "x5", "sample mean")
kable(t(x_vec), digits = 2, align = "c")
```

- $\mu = 2$, and we use the point estimate $\overline{x}=$ `r round(mean(x_data_1), 2)` to estimate it.

:::{.callout-note icon=false}
## Why is $\overline{x}$ not equal to $\mu$?
- Due to its **randomness** nature
:::


```{r, echo=FALSE, out.width="35%", fig.align='center'}
#| label: fig-sample-mean
#| fig-cap: Sample mean has randomness associated with it
par(mar = c(2, 0, 1, 0), mgp = c(3, 1, 0))
z <- 2 + seq(-3, 3, by = 0.001)
hz <- dnorm(z, mean = 2)
plot(z, hz, type = "n", xlab = "", ylab = "", ylim = c(0, dnorm(0, 0, 1)),
    axes = FALSE, main = "Population")
lines(z, hz, col = 4, lwd = 4)
axis(1)
points(x_data_1, y = rep(0, length(x_data_1)), col = "red", pch = 16, cex = 2)
```

- If another sample of size $5$ is drawn from the same population,

```{r, echo=FALSE}
x_data_2 <- rnorm(5, mean = 2, sd = 1)
```


```{r, echo=FALSE}
x_vec <- c(x_data_2, mean(x_data_2))
names(x_vec) <- c("x1", "x2", "x3", "x4", "x5", "sample mean")
kable(t(x_vec), digits = 2, align = "c")
```

- The second sample mean, $\overline{x} =$ `r round(mean(x_data_2), 2)`, is different from the first one.

:::{.callout-note icon=false}
## Why do the first sample and the second sample give us different sample means?
- A point estimator has its own *sampling distribution*.
:::


```{r, echo=FALSE, out.width="35%", fig.align='center'}
#| label: fig-samp-dist
#| fig-cap: Sampling Distribution of Sampling Mean
par(mar = c(3, 0, 2, 0), mgp = c(3, 1, 0))
z <- 2 + seq(-3, 3, by = 0.001)
hz <- dnorm(z, mean = 2, sd = 1/sqrt(5))
plot(z, hz, type = "n", xlab = "", ylab = "", ylim = c(0, dnorm(0, 0, 1/sqrt(5))),
    axes = FALSE, main = expression(paste("Sampling Distribution of ", bar(X))))
lines(z, hz, col = 4, lwd = 4)
axis(1)
points(c(mean(x_data_1), mean(x_data_2)), 
       y = rep(0, 2), col = c("red", "blue"), 
       pch = 16, cex = 2)
```

--------------------------------------------------------------------

<span style="color:blue"> **Why Point Estimates Are Not Enough** </span>

:::{.callout-note icon=false}
## If you want to estimate $\mu$, would you prefer to report a range of values the parameter might be in or a single estimate like $\overline{x}$?
:::

:::{.callout-note icon=false}
## If you want to catch a fish, would you prefer to use a spear or a net?
:::

::::{.columns}
:::{.column width="50%"}
```{r echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("./images/img-infer/spear.png")
```
:::

:::{.column width="50%"}
```{r echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("./images/img-infer/net.png")
```
:::
::::

- Due to the variation of $\overline{X}$, if we report a point estimate $\overline{x}$, we probably won't hit the exact $\mu$.
- If we report *a range of plausible values*, we have a better shot at capturing the parameter!

## Confidence Intervals

- A plausible range of values for $\mu$ is called a **confidence interval (CI)**.
  - This range depends on how *precise* and *reliable* our statistic is as an estimate of the parameter.
- To construct a CI for $\mu$, we first need to quantify the variability of our sample mean.
- Quantifying this uncertainty requires a measurement of how much we would expect the sample statistic to vary from sample to sample. 
  + This is the *variance* of the sampling distribution of the sample mean!

:::{.callout-note}
The larger the variation of $\overline{X}$ is, the wider the CI for $\mu$ will be given the same "level of confidence".
:::


- Do we know the variance of $\overline{X}$? 
  - By CLT, $\overline{X} \sim N(\mu, \sigma^2/n)$ **regardless of what the population distribution is**.


--------------------------------------------------------------------

<span style="color:blue"> **Precision vs. Reliability** </span>

:::{.callout-note icon=false}
## If we want to be very certain that we capture $\mu$, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?
:::

```{r echo=FALSE, out.width="100%"}
#| label: fig-garfield
#| fig-cap: Balance between precision and reliability
knitr::include_graphics("./images/img-infer/garfield.png")
```

- With a fixed sample size, precision and reliability have a trade-off relationship.
  - Narrower intervals are more precise but less reliable, while wider intervals are more reliable but less precise.


--------------------------------------------------------------------

<span style="color:blue"> **A Confidence Interval Is for a Parameter** </span>

- A confidence interval is **for a parameter, NOT a statistic.**
  + For example, we use the sample mean to form a confidence interval for the population mean.
- We **never say** *"The confidence interval of the sample mean, $\overline{X}$, is ..."*
- We **say** *"The confidence interval for the true population mean, $\mu$, is ..."*
- In general, a confidence interval for $\mu$ has the form

<center>
$\large \overline{x} \pm m = (\overline{x} - m, \overline{x} + m)$
</center>

- The $m$ is called the **margin of error**.
- $\overline{x} - m$ is the **lower bound** and $\overline{x} + m$ is the **upper bound** of the confidence interval.
- The point estimate, $\overline{x}$, and margin of error, $m$, can be obtained from known quantities and our data once sampled.

--------------------------------------------------------------------

<span style="color:blue"> **$(1 - \alpha)100\%$ Confidence Intervals** </span>

- The **confidence level** $1-\alpha$: **the proportion of times that the CI contains the population parameter, assuming that the estimation process is repeated a large number of times**.
- Common choices for the confidence level include
  + 90% $(\alpha = 0.10)$
  + 95% $(\alpha = 0.05)$
  + 99% $(\alpha = 0.01)$
- 95% is the most common level because it has a good balance between precision (width of the CI) and reliability (confidence level).
  + <span style="color:blue"> **High reliability and Low precision**: I am 100% confident that the mean height of Marquette students is between 3'0" and 8'0". </span> 
    - Duh...`r emoji('shrug')`
  + <span style="color:blue"> **Low reliability and High precision**: I am 20% confident that mean height of Marquette students is between 5'6" and 5'7". </span> 
    - This is far from the truth... `r emoji('no_good')`

--------------------------------------------------------------------

<span style="color:blue"> **$95\%$ Confidence Intervals for $\mu$** </span>

<span style="color:red"> ***Z-score*** </span>

::::{.columns}
:::{.column width="50%"}
- $\alpha = 0.05$
- Start with the sampling distribution of $\overline{X} \sim N\left(\mu, \frac{\sigma^2}{n}\right)$
- $\overline{x}$ will be within 1.96 SDs of the population mean, $\mu$, $95\%$ of the time.
- The $z$-score of 1.96 is associated with 2.5% area *to the right* and is called a **critical value** denoted as **$z_{0.025}$ **.
:::

:::{.column width="50%"}
```{r ci_95, echo=FALSE, out.width='100%'}
par(mar = c(6, 0, 1.5, 0), mgp = c(3, 2, 0), las = 1)
z <- seq(-3, 3, by = 0.001)
hz <- dnorm(z)
plot(z, hz, type = "n", xlab = "", ylab = "", ylim = c(0, dnorm(0, 0, 1)),
    axes = FALSE, main = expression(paste("Sampling distribution of ", bar(X))))
lines(z, hz, col = 4, lwd = 4)
these <- (qnorm(0.025) <= z & z <= qnorm(0.975))
polygon(c(qnorm(0.025), z[these], qnorm(0.975)),
          c(0, hz[these], 0),
          col = "lightblue",
          border = 4)
z_cri <- qnorm(0.975)
# segments(x0 = 0, y0 = 0, y1 = dnorm(0, 0, 1), col = 4, lwd = 1, lty = 2)
# segments(x0 = 1, y0 = 0, y1 = dnorm(1, 0, 1), col = 4, lwd = 1, lty = 2)
segments(x0 = z_cri, y0 = 0, y1 = dnorm(z_cri, 0, 1), col = 4, lwd = 2, lty = 2)
# segments(x0 = 2, y0 = 0, y1 = dnorm(2, 0, 1), col = 4, lwd = 1, lty = 2)
# segments(x0 = 3, y0 = 0, y1 = 2*dnorm(3, 0, 1), col = 4, lwd = 1, lty = 2)
# segments(x0 = -1, y0 = 0, y1 = dnorm(-1, 0, 1), col = 4, lwd = 1, lty = 2)
segments(x0 = -z_cri, y0 = 0, y1 = dnorm(-z_cri, 0, 1), col = 4, lwd = 2, lty = 2)
# segments(x0 = -2, y0 = 0, y1 = dnorm(-2, 0, 1), col = 4, lwd = 1, lty = 2)
# segments(x0 = -3, y0 = 0, y1 = 2*dnorm(-3, 0, 1), col = 4, lwd = 1, lty = 2)
# axis(1, at = seq(-3, 3, 1), pos = 0, line = 1)


text(0, 0.3*dnorm(0), "95%", cex = 3, col = "#003366")
text(2.2, 0.5*dnorm(2.2), "2.5%", cex = 1.2, col = "#003366")
text(-2.2, 0.5*dnorm(-2.2), "2.5%", cex = 1.2, col = "#003366")
labels <- c("", expression(mu - 1.96 * frac(sigma, sqrt(n)),
                     # mu - sigma,
                     mu,
                     # mu + sigma,
                     mu + 1.96 * frac(sigma, sqrt(n)), bar(X)))
axis(1, at = c(-3, -z_cri,0, z_cri, 3), labels, pos = 0, line = 1, cex.axis = 1.5)
par(mgp = c(3, 1, 0))
axis(1, at = c(-3, -z_cri,0, z_cri, 3), cex.axis = 1.5,
     labels = c("", -1.96, 0, 1.96, "Z"), tck = 0.01, line = 3)
```
:::
::::

<span style="color:red"> ***Probability*** </span>

::::{.columns}
:::{.column width="50%"}
$$P\left(\mu-1.96\frac{\sigma}{\sqrt{n}} < \overline{X} < \mu + 1.96\frac{\sigma}{\sqrt{n}} \right) = 0.95$$
 
:::{.callout-note icon=false}
## Is the interval $\left(\mu-1.96\frac{\sigma}{\sqrt{n}}, \mu+1.96\frac{\sigma}{\sqrt{n}} \right)$ our confidence interval?
- `r emoji('x')` **No! We don't know $\mu$, which is the quantity we want to estimate, but we're almost there!**
:::
:::

:::{.column width="50%"}
```{r ref.label='ci_95', echo=FALSE, out.width='100%'}

```
:::
::::

<span style="color:red"> ***Formula***

::::{.columns}
:::{.column width=50%}
$$\begin{align}
&P\left(\mu-1.96\frac{\sigma}{\sqrt{n}} < \overline{X} < \mu + 1.96\frac{\sigma}{\sqrt{n}} \right) = 0.95\\
&P\left( \boxed{\overline{X}-1.96\frac{\sigma}{\sqrt{n}} < \mu < \overline{X} + 1.96\frac{\sigma}{\sqrt{n}}} \right) = 0.95
\end{align}$$

- <span style="color:blue"> With sample data of size $n$, $\left(\overline{x}-1.96\frac{\sigma}{\sqrt{n}},  \overline{x} + 1.96\frac{\sigma}{\sqrt{n}}\right)$ is our $95\%$ CI for $\mu$ if $\sigma$ is **known** to us! </span>
- The margin of error $m = 1.96\frac{\sigma}{\sqrt{n}}$.
:::

:::{.column width="50%"}
```{r ref.label='ci_95', echo=FALSE, out.width='100%'}

```
:::
::::

## Confidence Intervals for $\mu$ When $\sigma$ is Known

- **Requirements** for estimating $\mu$ when $\sigma$ is known:
  + `r emoji('point_right')` The sample should be a **random sample**, such that all data $X_i$ are drawn from the same population and $X_i$ and $X_j$ are independent.
    - <span style="color:blue"> Any methods in this course are based on the assumption of a random sample </span>
  + `r emoji('point_right')` The population standard deviation, $\sigma$, is **known**.
  + `r emoji('point_right')` The population is either **normally distributed**, $n > 30$ or both, i.e., $X_i \sim N(\mu, \sigma^2)$.
    - <span style="color:blue"> $n > 30$ allows the Central Limit Theorem to be applied and hence normality is satisfied. </span>
  
-------------------------------------------------------------------

<span style="color:blue"> **$(1-\alpha)100\%$ Confidence Intervals for $\mu$** </span>

::::{.columns}
:::{.column width="50%"}
```{r ref.label='ci_95', echo=FALSE, out.width='100%'}

```

<center>
<span style="color:blue"> $\left(\overline{x}-1.96\frac{\sigma}{\sqrt{n}},  \overline{x} + 1.96\frac{\sigma}{\sqrt{n}}\right)$ </span>
<span style="color:blue"> $\left(\overline{x}-z_{0.025}\frac{\sigma}{\sqrt{n}},  \overline{x} + z_{0.025}\frac{\sigma}{\sqrt{n}}\right)$ </span>
</center>
:::

:::{.column width="50%"}
```{r ci_alpha, echo=FALSE, out.width='100%'}
par(mar = c(6, 0, 1.5, 0), mgp = c(3, 2, 0), las = 1)
z <- seq(-3, 3, by = 0.001)
hz <- dnorm(z)
plot(z, hz, type = "n", xlab = "", ylab = "", ylim = c(0, dnorm(0, 0, 1)),
     main = expression(paste("Sampling distribution of ", bar(X))), axes = FALSE)
lines(z, hz, col = 2, lwd = 2)
these <- (qnorm(0.025) <= z & z <= qnorm(0.975))
polygon(c(qnorm(0.025), z[these], qnorm(0.975)),
          c(0, hz[these], 0),
          col = "pink",
          border = 2)
z_cri <- qnorm(0.975)
segments(x0 = z_cri, y0 = 0, y1 = dnorm(z_cri, 0, 1), col = 2, lwd = 2, lty = 2)
segments(x0 = -z_cri, y0 = 0, y1 = dnorm(-z_cri, 0, 1), col = 2, lwd = 2, lty = 2)


text(0, 0.3*dnorm(0), expression(1 - alpha), cex = 3, col = "#003366")
text(2.2, 0.5*dnorm(2.2), expression(alpha/2), cex = 1.2, col = "#003366")
text(-2.2, 0.5*dnorm(-2.2), expression(alpha/2), cex = 1.2, col = "#003366")
labels <- c("", expression(mu - z[frac(alpha, 2)] * frac(sigma, sqrt(n)),
                     mu,
                     mu + z[frac(alpha, 2)] * frac(sigma, sqrt(n)), bar(X)))
axis(1, at = c(-3, -z_cri,0, z_cri, 3), labels, pos = 0, line = 1, 
     cex.axis = 1.5)
par(mgp = c(3, 1, 0))
axis(1, at = c(-3, -z_cri,0, z_cri, 3), cex.axis = 1.5,
     labels = c("", expression(-z[frac(alpha, 2)]), 0, expression(z[frac(alpha, 2)]), "Z"), tck = 0.01, line = 3)

```

<center>
<span style="color:red"> $\left(\overline{x}-z_{\alpha/2}\frac{\sigma}{\sqrt{n}},  \overline{x} + z_{\alpha/2}\frac{\sigma}{\sqrt{n}}\right)$ </span>
</center>
:::
::::

<br>

- **Procedures** for constructing a confidence interval for $\mu$ when $\sigma$ is known:
  1. Check that the **requirements** are satisfied.
  2. Decide $\alpha$ or the *confidence level* $(1 - \alpha)$.
  3. Find the *critical value*, $z_{\alpha/2}$.
  4. Evaluate *margin of error*, $m = z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}$. 
  5. Construct the $(1 - \alpha)100\%$ CI for $\mu$ using the sample mean, $\overline{x}$, and margin of error, $m$: 

<center>
<span style="color:red"> $$\boxed{\overline{x} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}} \text{  or  } \left( \overline{x} -z_{\alpha/2}\frac{\sigma}{\sqrt{n}}, \, \overline{x} + z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\right)}$$</span>
</center>

---------------------------------------------------------------------

<span style="color:blue"> **Example** </span>

::::{.columns}
:::{.column width="50%"}
- Suppose we want to know the mean systolic blood pressure (SBP) of a population.
- Assume that the population distribution is normal and has a standard deviation of 5 mmHg. 
- We have a random sample of 16 subjects from this population with a mean of 121.5 mmHg.
- Estimate the mean SBP with a 95% confidence interval.
:::

:::{.column width="50%"}
```{r, echo=FALSE, out.width="100%"}
knitr::include_graphics("./images/img-infer/blood_pressure.jpeg")
```
:::
::::

1. Requirements: 
    - <span style="color:blue">  Normality is assumed, $\sigma = 5$ is known and a random sample is collected.
2. Decide $\alpha$: 
    - <span style="color:blue">  $\alpha = 0.05$  </span>
3. Find the *critical value* $z_{\alpha/2}$: 
    - <span style="color:blue">  $z_{\alpha/2} = z_{0.025} = 1.96$  </span> 
4. Evaluate *margin of error* $m = z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$: 
    - <span style="color:blue"> $m = (1.96) \frac{5}{\sqrt{16}} = 2.45$ </span> 
5. Construct the $(1 - \alpha)100\%$ CI: 
    - <span style="color:blue"> The 95% CI for the mean SBP is $\overline{x} \pm z_{\alpha/2}\frac{\sigma}{\sqrt{n}} = (121.5 -2.45, 121.5 + 2.45) = (119.05, 123.95)$ </span> 


<span style="color:red"> ***Computation in R*** </span>

- Below is a demonstration of how to find the 95% CI for SBP using R.

```{r ci_normal, echo=TRUE, tidy=FALSE}
## save all information we have
alpha <- 0.05
n <- 16
x_bar <- 121.5
sig <- 5

## 95% CI
## z-critical value
(cri_z <- qnorm(p = alpha / 2, lower.tail = FALSE))  

## margin of error
(m_z <- cri_z * (sig / sqrt(n)))  

## 95% CI for mu when sigma is known
x_bar + c(-1, 1) * m_z  
```

:::{.callout-note icon=false}
## Construct a 99% CI for the mean SBP. Do you expect it to have a wider or narrower interval than the 95% CI? Why?
:::

<span style="color:red"> ***Interpreting the Confidence Interval***

- **WRONG** `r emoji('x')` *"There is a 95% chance/probability that the true population mean will fall between 119.1 mm and 123.9 mm."*
- **WRONG** `r emoji('x')` *"The probability that the true population mean falls between 119.1 mm and 123.9 mm is 95%."*
- <span style="color:blue"> `r emoji('point_right')` **The sample mean is a random variable with a sampling distribution, so it makes sense to compute a probability of it being in some interval.** </span>
- <span style="color:blue"> `r emoji('point_right')` **The population mean is unknown and FIXED, so we cannot assign or compute any probability of it.** </span> 
  + If we were using **Bayesian inference**, a different inference method, we could compute a probability associated with $\mu$ because $\mu$ is treated as a random variable. 
- Instead we say, <span style="color:blue"> ***"We are 95% confident that the mean SBP lies between 119.1 mm and 123.9 mm."*** </span>
- This means if we were able to collect our data many times and build the corresponding CIs, we would expect that about 95% of those intervals would contain the true population parameter, which, in this case, is the mean systolic blood pressure.
- <span style="color:blue"> Remember: **$\overline{x}$ varies from sample to sample and so does its corresponding CI **.</span>
  + This idea is shown in @fig-ci. 

```{r ci_mean_known_sig, echo=FALSE, out.width='80%', fig.align='center'}
#| label: fig-ci
#| fig-cap: Illustration of 100 95% confidence intervals
par(mar = c(4, 4, 2, 1), mgp = c(2.5, 1, 0))
mu <- 120; sigma <- 5
alpha <- 0.05
M <- 100
n <- 16
set.seed(529)
sample_rep <- replicate(M, rnorm(n, mu, sigma))
sample_mean_rep <- apply(sample_rep, 2, mean)
E <- qnorm(p = alpha / 2, mean = 0, sd = 1, lower.tail = FALSE) * sigma / sqrt(n)
ci_lower <- sample_mean_rep - E
ci_upper <- sample_mean_rep + E

plot(rep(mu, M), seq(M), type = 'l', xlim = c(min(ci_lower), max(ci_upper)), 
     col = "yellow", las = 1,
     xlab = "95% interval", ylab = "Sample", 
     main = paste("95% confidence Intervals from", M, "different samples"))

contained <- (mu < ci_lower | mu > ci_upper)
# points(sample_mean_rep, 1:100, col = "black", cex = 0.2, pch = 19)
segments(x0 = ci_lower, y0 = 1:M, x1 = ci_upper, y1 = 1:M, col = '#003366', lwd = 1)
segments(x0 = ci_lower[contained], y0 = (1:M)[contained], x1 = ci_upper[contained], 
         y1 = (1:M)[contained], col = 'red', lwd = 1)

abline(v = mu, col = "#FFCC00", lwd = 2)
```

:::{.callout-note}
- We never know with certainty that 95% of the intervals, or any single interval for that matter, contains the true population parameter.
:::


## Confidence Intervals for $\mu$ When $\sigma$ is Unknown

- $\sigma^2 = \frac{\sum_{i=1}^{N}(x_i - \mu)^2}{N}$, where $N$ is the population size.
- It's rare that we don't know $\mu$ but know $\sigma$, so what do we do if $\sigma$ is unknown?
  + We use the **Student t** distribution to construct a confidence interval for $\mu$ when $\sigma$ is **unknown**.
- To construct these confidence intervals we still need
  + A random sample
  + A population that is normally distributed and/or $n > 30$.
  
:::{.callout-note icon=false}
## What is a natural estimator for the unknown $\sigma$?
:::

- When $\sigma$ is unknown, we use the *sample standard deviation*, $S = \sqrt{\frac{\sum_{i=1}^{n}(X_i - \overline{X})^2}{n-1}}$, instead when constructing the CI.

---------------------------------------------------------------

<span style="color:blue"> **Student t Distribution** </span>

- If the population is normally distributed or $n > 30$,
  + $\overline{X} \sim N\left(\mu, \frac{\sigma^2}{n} \right)$
  + $Z = \frac{\overline{X} - \mu}{\color{red}\sigma/\sqrt{n}} \sim N(0, 1)$
  + <span style="color:blue">  $T = \frac{\overline{X} - \mu}{\color{red}S/\sqrt{n}} \sim t_{n-1}$ </span> 
  + $t_{n-1}$ denotes the Student t distribution with **degrees of freedom (df)** $n-1$.

<span style="color:red"> ***Properties*** </span>

- It is symmetric about the mean 0 and bell-shaped like $N(0, 1)$.
- It has more variability than $N(0, 1)$ (heavier tails and lower peak).
- The variability is different for different sample sizes (degrees of freedom).
  + As $n \rightarrow \infty$ $(df \rightarrow \infty)$, the Student t distribution approaches $N(0, 1)$.

```{r student_t, echo=FALSE, out.width='52%', fig.align='center'}
#| label: fig-student-t
#| fig-cap: Student t distributions with various degrees of freedom
# Display the Student's t distributions with various
# degrees of freedom and compare to the normal distribution
par(mar = c(4, 4, 1, 0), mgp = c(2, 1, 0))
x <- seq(-4, 4, length=100)
hx <- dnorm(x)
degf <- c(1, 3, 8)
colors <- 1:4
labels <- c("N(0, 1)", "t (df = 1)", "t (df = 3)", "t (df = 8)")

plot(x, hx, type="l", lty = 1, xlab = "x", lwd = 2, las = 1, 
  ylab = "Density", main = "Comparison of t Distributions")

for (i in 1:3){
  lines(x, dt(x, degf[i]), lwd = 2, col = colors[i+1])
}
abline(v = 0, lty = 2)
legend("topright", inset = .05, labels, lwd = 2, lty = c(1, 1, 1, 1), 
       col = colors, bty = "n")
```

<span style="color:blue"> **Critical Values of $t_{\alpha/2, n-1}$** </span>

- When $\sigma$ is unknown, we use $t_{\alpha/2, n-1}$ as the critical value, instead of $z_{\alpha/2}$.

```{r, echo=FALSE, out.width="60%", fig.align='center'}
#| label: fig-cv-t
#| fig-cap: Illustration of critical value for Student t distribution
par(mar = c(3, 0, 1, 0), mgp = c(2, 1, 0))
x <- seq(-4, 4, length=100)
hx <- dt(x, df = 1)
plot(x, hx, type="l", lty = 1, xlab = "", lwd = 2, las = 1, 
  ylab = "", main = "Student t", axes = F)
t_cri <- qt(0.8, df = 1)
abline(v = 0, lty = 2, lwd = 0.5)
text(-0.2, 0.3*dt(0, df = 1), expression(1 - alpha/2), cex = 2, col = "#003366")
text(2.2, 0.5*dt(2.2, df = 1), expression(alpha/2), cex = 1.5, col = "#003366")
segments(x0 = t_cri, y0 = 0, y1 = dt(t_cri, df = 1), col = 2, lwd = 2, lty = 2)
axis(1, at = c(-4, -t_cri,0, t_cri, 4), cex.axis = 1.5, pos = 0,
     labels = c("", "", 0, expression(t[frac(alpha, 2)]), expression(T[n-1])), tck = 0.01, line = 1)
```

:::{.callout-note icon=false}
## With the same $\alpha$, is $t_{\alpha, n-1}$ or $z_{\alpha}$ larger?
:::


```{r critical_value, echo=FALSE}
critical_t_5 <- round(c(qt(0.95, df=5), qt(0.975, df=5), qt(0.995, df=5)), 2)
critical_t_15 <- round(c(qt(0.95, df=15), qt(0.975, df=15), qt(0.995, df=15)), 2)
critical_t_30 <- round(c(qt(0.95, df=30), qt(0.975, df=30), qt(0.995, df=30)), 2)
critical_t_1000 <- round(c(qt(0.95, df=1000), qt(0.975, df=1000), qt(0.995, df=1000)), 2)
critical_t_inf <- round(c(qt(0.95, df=Inf), qt(0.975, df=Inf), qt(0.995, df=Inf)), 2)
critical_z <- round(c(qnorm(0.95), qnorm(0.975), qnorm(0.995)), 2)
critical_value_table <- data.frame("confidence_level" = c("90%", "95%", "99%"),
                                   "critical_t (df 5)" = critical_t_5, 
                                   "critical_t (df 15)" = critical_t_15, 
                                   "critical_t (df 30)" = critical_t_30, 
                                   "critical_t (df = 1000)" = critical_t_1000,
                                   "critical_t (df = inf)" = critical_t_inf,
                                   "critical_z" = critical_z)
kable(critical_value_table, col.names = c("Level",
                                         "t df = 5",
                                         "t df = 15",
                                         "t df = 30",
                                         "t df = 1000",
                                         "t df = inf",
                                         "z"), align = "c")
```

----------------------------------------------------------------------

<span style="color:blue"> **$(1-\alpha)100\%$ Confidence Intervals for $\mu$ When $\sigma$ is Unknown** </span> 

- The $(1-\alpha)100\%$ confidence interval for $\mu$ when $\sigma$ is **unknown** is <span style="color:blue"> $$\left(\overline{x} - t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}, \overline{x} + t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}\right)$$ </span>
- Given the same confidence level $1-\alpha$, $t_{\alpha/2, n-1} > z_{\alpha/2}$, which means the confidence interval for $\mu$ is wider when $\sigma$ is unknown.

:::{.callout-note}
- We are more "uncertain" when doing inference about $\mu$ because don't have information about $\sigma$ and replacing it with $s$ adds additional uncertainty.
:::

<span style="color:red"> ***Computation in R*** </span>

- Back to the systolic blood pressure (SBP) example. We have $n=16$ and $\overline{x} = 121.5$. 
- Estimate the mean SBP with a 95% confidence interval with **unknown $\sigma$ and $s = 5$.**

```{r ci_t, echo=TRUE, tidy=FALSE}
alpha <- 0.05
n <- 16
x_bar <- 121.5
s <- 5  ## sigma is unknown and s = 5

## t-critical value
(cri_t <- qt(p = alpha / 2, df = n - 1, lower.tail = FALSE)) 
## margin of error
(m_t <- cri_t * (s / sqrt(n)))  
## 95% CI for mu when sigma is unknown
x_bar + c(-1, 1) * m_t  
```

## Summary

|      | Numerical Data, $\sigma$ known | Numerical Data, $\sigma$ unknown   |
|:-------------:|:-----------------:|:------------:|
| **Parameter of Interest** | Population Mean $\mu$  | Population Mean $\mu$ |  
| **Confidence Interval**   | $\bar{x} \pm z_{\alpha/2} \frac{\sigma}{\sqrt{n}}$ | $\bar{x} \pm t_{\alpha/2, n-1} \frac{s}{\sqrt{n}}$           |


- Remember to check if the population is normally distributed and/or $n>30$.
- What if the population is not normal and $n \le 30$?
  + Use a so-called **nonparametric** method, for example **bootstrapping**. 

## Exercises

1. Here are summary statistics for randomly selected weights of newborn boys: $n =207$, $\bar{x} = 30.2$hg (1hg = 100 grams), $s = 7.3$hg.
    (a) Compute a 95% confidence interval for $\mu$, the mean weight of newborn boys.
    (b) Is the result in (a) very different from the 95% confidence interval if $\sigma = 7.3$?
    
2. A 95% confidence interval for a population mean $\mu$ is given as (18.635,
21.125). This confidence interval is based on a simple random sample of 32 observations. Calculate the sample mean and standard deviation. Assume that all conditions necessary for inference are satisfied. Use the t-distribution in any calculations. 

3. A market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation of savings is \$95. He wants to collect data such that he can get a margin of error of no more than \$12 at a 95% confidence level. How large of a sample should he collect?

4. The 95% confidence interval for the mean rent of one bedroom apartments in Chicago was calculated as (\$2400, \$3200). 
    (a) Interpret the meaning of the 95% interval. 
    (b) Find the sample mean rent from the interval.



