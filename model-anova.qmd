# Analysis of Variance {#sec-model-anova}

```{r}
#| echo: false
#| warning: false
#| message: false
#| label: packages
library(emoji)
library(knitr)
library(openintro)
library(car)
library(nortest)
```

```{r}
#| echo: false
knitr::opts_chunk$set(echo = FALSE, fig.align = "center")
```


<span style="color:blue"> **Comparing More Than Two Population Means** </span>

We have learned how to compare two population means in @sec-infer-twomean. However, in many research settings and questions, we want to compare three or more population means. For example,

::::{.columns}
:::{.column width="50%"}
- <span style="color:blue"> Are there differences in the mean readings of 4 types of devices used to determine the pH of soil samples? </span>

```{r}
#| out-width: 80%
#| fig-cap: "Source: Unsplash-Markus Spiske."
knitr::include_graphics("./images/img-model/soil.jpeg")
```
:::

:::{.column width="50%"}
- <span style="color:blue"> Do different treatments (None, Fertilizer, Irrigation, Fertilizer and Irrigation) affect the mean weights of poplar trees?  </span>

```{r}
#| out-width: 80%
#| fig-cap: "Source: Unsplash-Nareeta Martin."
knitr::include_graphics("./images/img-model/poplar.jpeg")
```
:::
::::

<!-- ------------------------------------------------------------------------ -->

In both cases, four means are compared. The two-sample $z$ or $t$ test cannot be used anymore. Our solution is the **analysis of variance**, or **ANOVA**.
 
 
 

## ANOVA Rationale

<span style="color:blue"> **One-Way Analysis of Variance** </span>

A **factor** (treatment) is a property or characteristic (categorical variable) that allows us to *distinguish the different populations from one another*. In fact, when we compare two population means, the two samples are from the populations separated by the categories of a factor. For example, we have gender as a factor that separates the human population into male and female populations, then we compare their mean salary. In our previous examples, *type of device* and *treatment of trees* are factors.

One-way ANOVA examines the effect of single *categorical variable* on the *mean of a numerical variable*. The ANOVA model is one-way because there is only one factor being considered. If two factors are examined to see how they affect the mean of a numerical variable, the model is called two-way ANOVA. The numerical variable in ANOVA or regression model (@sec-model-reg) is called the **response variable** because we want to see how the variable responses to the changes of factors or other variables.

Interestingly, we analyze <span style="color:red"> variances </span> to test the equality of three or more population  <span style="color:red"> means</span> `r emoji('thinking')`. It sounds counterintuitive, but later you will see why this makes sense.


  <!-- + The method is **one-way** because we use one single property (categorical variable) for categorizing the populations. -->

<span style="color:red"> ***Requirements*** </span>

The one-way ANONA model has the following requirements or assumptions.

- *Each sub-population formed by the category of the factor is normally distributed.* In the two-sample $z$ or $t$ test, we also have such assumption that the two samples are drawn independently from two normally distributed populations or at least both sample sizes are sufficiently large.

- *The populations have the same variance $\sigma^2$.* ANOVA does not ridiculously assume all the population variances are known. Instead, the model admits the variances are unknown, but assumes they are all equal. Also sounds ridiculous? Anyway, it is what it is, and this is one of the limitations of ANOVA, although equality of variances is a pretty loose requirement. It is also possible to transform our data, so that the transformed samples have similar magnitude of variance. Does this assumption remind you of something?  The two-sample pooled $t$-test has the equality of variance assumption too. In this point of view, one-way ANOVA is a generalization of the two-sample pooled $t$-test. The next two requirements will be not surprising at all because they are also requirements of the two-sample pooled $t$-test.

- *The samples are random samples.* Without mentioned explicitly, the inference methods and statistical models introduced in the book are based on random samples.


- *The samples are independent of each other.* They are not matched or paired in any way.


:::{.callout-note}
## Data and Math Notation for ANOVA
```{r}
#| out-width: 100%
#| fig-cap: "Source: Table 8.5 of SMD."
knitr::include_graphics("./images/img-model/anova_data.png")
```

- $y_{ij}$: $j$-th observation from population $i$. $\color{blue}{(y_{24}:\text{ 4-th observation from population 2})}$

- $n_i$: number of values in the $i$-th group/sample. ($i = 1, \dots, k = 5$) $\color{blue}{\small (n_1 = n_2 = \dots = n_5 = 4)}$

- $\bar{y}_{i\cdot}$: the average of values in the $i$-th sample,  $\bar{y}_{i\cdot} = \frac{\sum_{j=1}^{n_i}y_{ij}}{n_i}.$

- $N$: total number of values in all samples combined, $\small N = n_1 + \dots + n_k = \sum_{i=1}^kn_i$. $\color{blue}{\small (N = 4 + \dots + 4 = 20)}$

- $\bar{y}_{\cdot\cdot}$: the *grand* sample mean that is the average of all sample values combined, $\bar{y}_{\cdot\cdot} = \frac{\sum_{i=1}^{k}\sum_{j=1}^{n_i}y_{ij}}{N} = \frac{\sum_{i=1}^{k}n_i\bar{y}_{i\cdot}}{n_1 + \dots + n_t}$


Mathematically, suppose there are $k$ populations/samples, and for group $i =1, \dots, k$, $n$ observations are drawn. Then

$$y_{ij} \stackrel{iid}{\sim} N(\mu_i, \sigma^2), ~~ j = 1, 2, \dots, n.$$

Our goal is to test whether or not $\mu_1 = \mu_2 = \cdots = \mu_k.$ If we do not reject $H_0: \mu_1 = \mu_2 = \cdots = \mu_k,$ then all samples are viewed as samples from a common normal distribution, i.e., $N(\mu_i, \sigma^2)$ and $\mu = \mu_1 = \mu_2 = \cdots = \mu_k$.

$$y_{ij} \stackrel{iid}{\sim} N(\mu, \sigma^2), ~~ j = 1, 2, \dots, n.$$

:::

<span style="color:red"> ***Rationale*** </span>

The section is quite important because we discuss in detail why we use variance size to test whether or not population means are equal.

Suppose we have two data sets Data 1 and Data 2, and both have three groups to be compared. Interestingly, Data 1 and Data 2 have the same group sample means $\bar{y}_1$, $\bar{y}_2$ and $\bar{y}_3$ denoted as red dots in @fig-boxplots-var. However, they differ with regards to the variance within each group. Data 1 has small variance within samples, while Data 2 has large variance within samples. Within groups, the data points in Data 1 are quite tight and close each other, and therefore they are all close to their sample mean. On the contrary, the data points in Data 2 are quite distant each other, even they are in the same group. Such distribution pattern tells us that the populations from which the samples of Data 1 are drawn have small variance $\sigma^2$.

Before we go into more details, let's see if you have a great intuition. 

:::{.callout-note icon=false}
## For which data do you feel more confident in saying the population means $\mu_1$, $\mu_2$ and $\mu_3$ are not all the same?
:::


```{r}
#| label: fig-boxplots-var
#| fig-cap: Boxplots illustrating the variance within samples.
par(mfrow = c(2, 1), mar = c(2.5, 2.5, 1.5, 0), mgp = c(1.1, 0.5, 0), las = 1)
y1 <- rnorm(50, 6, 0.05)
y2 <- rnorm(50, 5.6, 0.05)
y3 <- rnorm(50, 5.2, 0.05)
small_var_data <- data.frame(y = c(y1, y2, y3), sample = rep(c(1,2,3), each = 50))
boxplot(y ~ sample, small_var_data, ylim = c(2, 9),
        main = "Data 1: Small Variance Within Samples", cex.main = 0.89, 
        col = "lightblue", horizontal = TRUE)
points(x=c(mean(y1), mean(y2), mean(y3)), y = c(1, 2,3), col = "red", pch = 16)

y1_l <- rnorm(50, sd = 1.0) + 6
y2_l <- rnorm(50, sd = 1.0) + 5.6
y3_l <- rnorm(50, sd = 1.0) + 5.2
large_var_data <- data.frame(y = c(y1_l, y2_l, y3_l), sample = rep(c(1,2,3), each = 50))
boxplot(y ~ sample, large_var_data, ylim = c(2, 9),
        main = "Data 2: Large Variance Within Samples", cex.main = 0.89, 
        col = "lightblue", horizontal = TRUE)
points(x=c(mean(y1_l), mean(y2_l), mean(y3_l)), y = c(1, 2,3), col = "red", pch = 16)
# points(c(5.9, 5.5, 5))
```

<!-- :::{.callout-note icon=false} -->
<!-- ## For which data do you feel more confident in saying the population means $\mu_1$, $\mu_2$ and $\mu_3$ are not all the same? -->
<!-- ::: -->

If your answer is Data 1, congratulations you are correct!

The difference in sample means in Data 1 is more likely due to the true difference in population means ($\mu_1, \mu_2, \mu_3$ not all the same). Because of the small variation within groups, in Data 1 a value drawn from group 1 is very unlikely to be drawn from another group because with the normality assumption, the chance to be drawn from another group is very tiny. @fig-normal-var clearly illustrates this idea. The samples of Data 1 are so well-separated that we are more confident to say they are drawn from three well-separated populations that are not overlapped each other, and have distinct population means $\mu_1, \mu_2$, and $\mu_3$.

If the variance within groups is large, as shown at the bottom of @fig-normal-var, all three samples are mixed up together, as if they are all from the common population, even though they are in fact are from three distinct populations $N(\mu_1, \sigma^2), N(\mu_2, \sigma^2), N(\mu_3, \sigma^2)$ and $\mu_1 \ne \mu_2 \ne \mu_3$. Note that the three populations still have their own distinct population mean which is actually identical to the mean with small variance, but it is hard for us to learn this fact from their mixed random samples. Because the three samples are indistinguishable, we don't have strong evidence to conclude that their corresponding population has its own population mean, and tend to conclude that the three samples are from a common population $N(\mu, \sigma^2)$.


<!-- Therefore, the blue points can only be the draws from blue normal distribution,  -->


```{r}
#| label: fig-normal-var
#| fig-cap: Populations with small and large variance within samples.
par(mfrow = c(2, 1), mar = c(1.5, 0, 1.5, 0), mgp = c(1.1, 0.5, 0), las = 1)
x <- seq(2, 9, length=1000)

#create a vector of values that shows the height of the probability distribution
#for each value in x
dy1 <- dnorm(x, 6, 0.05)
dy2 <- dnorm(x, 5.6, 0.05)
dy3 <- dnorm(x, 5.2, 0.05)

#plot x and y as a scatterplot with connected lines (type = "l") and add
#an x-axis with custom labels
plot(x, dy1, type = "l", lwd = 2,
     axes = FALSE, xlab = "", ylab = "", 
     col = 1, main = "Normal population w/ small variance (within samples)")
lines(x, dy2, col = 2, lwd = 2)
lines(x, dy3, col = 4, lwd = 2)
abline(h = 0, col = "grey", lwd = 2)
points(c(y1, y2, y3), y = jitter(rep(0, 150), factor = 5), 
       pch = 1, col = c(rep(1, 50), rep(2, 50), rep(4, 50)), cex = 0.5)
legend(7, 6, paste("sample/population", 1:3), bty = "n", 
       col = c(1, 2, 4), lwd = 2, pch = 1)

dy1_l <- dnorm(x, 6, 1)
dy2_l <- dnorm(x, 5.6, 1)
dy3_l <- dnorm(x, 5.2, 1)

plot(x, dy1_l, type = "l", lwd = 2,
     axes = FALSE, xlab = "", ylab = "", 
     col = 1, main = "Normal population w/ large variance (within samples)")
lines(x, dy2_l, col = 2, lwd = 2)
lines(x, dy3_l, col = 4, lwd = 2)
abline(h = 0, col = "grey", lwd = 2)
points(c(y1_l, y2_l, y3_l), y = jitter(rep(0, 150), factor = 0.5), 
       pch = 1, col = c(rep(1, 50), rep(2, 50), rep(4, 50)), cex = 1)
legend(7, 0.4, paste("sample/population", 1:3), bty = "n", 
       col = c(1, 2, 4), lwd = 2, pch = 1)
```


The three samples in @fig-normal-one are from a common population $N(\mu, \sigma^2)$. The samples look pretty similar to the samples with large variance in @fig-normal-var. In other words, either one common population or three populations with large variance can produce quite similar samples. With the samples only, we cannot tell which data generating mechanism is the true one generating such data.



```{r}
#| fig-asp: 0.36
#| label: fig-normal-one
#| fig-cap: Three samples drawn from a common normal population.
par(mar = c(1.5, 0, 1.5, 0), mgp = c(1.1, 0.5, 0), las = 1)
dy1_a <- dnorm(x, 5.6, 1)
dy2_a <- dnorm(x, 5.6, 1)
dy3_a <- dnorm(x, 5.6, 1)
y1_a <- rnorm(50, sd = 1.0) + 5.6
y2_a <- rnorm(50, sd = 1.0) + 5.6
y3_a <- rnorm(50, sd = 1.0) + 5.6
plot(x, dy1_a, type = "l", lwd = 2,
     axes = FALSE, xlab = "", ylab = "", 
     col = 1, main = "Three samples from the same normal population")
# lines(x, dy2_a, col = 2, lwd = 2)
# lines(x, dy3_a, col = 4, lwd = 2)
abline(h = 0, col = "grey", lwd = 2)
points(c(y1_a, y2_a, y3_a), y = jitter(rep(0, 150), factor = 0.5), 
       pch = 1, col = c(rep(1, 50), rep(2, 50), rep(4, 50)), cex = 1)
legend(7, 0.4, c(paste("sample", 1:3), "common population"), bty = "n", 
       col = c(1, 2, 4, 1), pch = c(1, 1, 1, NA), lwd = c(NA, NA, NA, 2))
```


:::{.callout-note}
Always keep in mind that all we have are samples, and we never know the true means $\mu_1$, $\mu_2$, and $\mu_3$. In the figures, we assume we know the true populations, and see what the samples look like. Statistical inference is trickier. We want to have a decision rule, so that we know how much the samples are well separated is enough to say they are from three different populations. 
:::


<!-- variation between samples is measured by the pairwise distance among the sample means. -->
<!-- variation within samples is measured by the how far away the data points away from each other in each sample group. -->


---------------------------------------------------------------

<span style="color:blue"> **Variation Between Samples & Variation Within Samples** </span>

There are two types of variation we need to consider in order to determine whether population means are identical. They are **variation between samples** and **variation within samples**.

We have discussed the variation within samples or variance within groups, which measures the variability of data points in each group. This is actually the sample point estimate of the population variance $\sigma^2$ because the data points in the $i$-th group are assumed from the $i$-th population $N(\mu_i, \sigma^2).$ There are $k$ sample variance, one for each group. Later, we will learn how to combine them to get one single variance within samples as an estimate of $\sigma^2$.

Variation between samples, on the other hand, measures variability of sample means. *The farther away from each other the sample means are, the larger variation between samples*. In our Data 1 and Data 2 example, their variation between samples are very close because the relative location of their sample means are basically the same. @fig-boxplot-bw illustrating the variance between samples. Data 3 and 4 have the same variance within samples, but Data 3 have small variation between samples and Data 4 have large variation between samples. Clearly, the sample means in Data 4 are farther away from each other, comparing to Data 3.


```{r}
#| label: fig-boxplot-bw
#| fig-cap: Boxplots illustrating the variance between samples.
par(mfrow = c(2, 1), mar = c(2.5, 2.5, 1.5, 0), mgp = c(1.1, 0.5, 0), las = 1)
y1_l <- rnorm(50, sd = 1.0) + 6
y2_l <- rnorm(50, sd = 1.0) + 5.6
y3_l <- rnorm(50, sd = 1.0) + 5.2
large_var_data <- data.frame(y = c(y1_l, y2_l, y3_l), sample = rep(c(1,2,3), each = 50))
boxplot(y ~ sample, large_var_data, ylim = c(0, 11),
        main = "Data 3: Small Variance Between Samples", cex.main = 0.89, 
        col = "lightblue", horizontal = TRUE)
points(x=c(mean(y1_l), mean(y2_l), mean(y3_l)), y = c(1, 2,3), col = "red", pch = 16)

y1_l <- rnorm(50, sd = 1.0) + 8
y2_l <- rnorm(50, sd = 1.0) + 5.5
y3_l <- rnorm(50, sd = 1.0) + 3
large_var_data <- data.frame(y = c(y1_l, y2_l, y3_l), sample = rep(c(1,2,3), each = 50))
boxplot(y ~ sample, large_var_data, ylim = c(0, 11),
        main = "Data 4: Large Variance Between Samples", cex.main = 0.89, 
        col = "lightblue", horizontal = TRUE)
points(x=c(mean(y1_l), mean(y2_l), mean(y3_l)), y = c(1, 2, 3), col = "red", pch = 16)


```

Let me ask you the same question for Data 3 and 4.

:::{.callout-note icon=false}
## For which data do you feel more confident in saying the population means $\mu_1$, $\mu_2$ and $\mu_3$ are not all the same?
:::

Your answer should be Data 4. When the sample means $\bar{y}_1$, $\bar{y}_2$ and $\bar{y}_3$ are far away from each other, and they serve as the unbiased point estimate of $\mu_1$, $\mu_2$ and $\mu_3$, respectively, we tend to claim that $\mu_1$, $\mu_2$ and $\mu_3$ are not identical. 

We have an important finding. *Whether or not there is a difference in population means depends on the relative size of variation between samples and variation within samples*. When variability between samples is *large* *in comparison to* the variation within samples, like Data 1 and Data 4, we tend to conclude that the population means are not all the same. When variation between samples is *small* relatively to the variation within samples, like Data 2 and Data 3, it's hard to exclude the possibility that all samples comes from the same population.

<!-- - Data 1: Variability between samples is *large* *in comparison to* the variation within samples. -->
<!-- - Data 2: Variation between samples is *small* relatively to the variation within samples. -->
<!-- - Data 3: Variability between samples is *small* *in comparison to* the variation within samples. -->
<!-- - Data 4: Variation between samples is *large* relatively to the variation within samples. -->





<!-- :::{.callout-note icon=false} -->
<!-- ## We are more confident concluding there is a difference in population means when variation **between** samples is *larger* than variation **within** samples. -->
<!-- ::: -->

<!-- # ```{r, out.width="55%", echo=FALSE, fig.align='center'} -->
<!-- # #| label: fig-var-within -->
<!-- # #| fig-cap: Illustration of small and large variance within samples -->
<!-- # knitr::include_graphics("./images/img-model/figure8-1.png") -->
<!-- # ``` -->


## ANOVA Procedures

ANOVA is usually done by providing the ANOVA table below. 

```{r}
knitr::include_graphics("./images/img-model/anova_table_k.png")
```

In this section, we are going to learn the meaning of every cell in the table, and how to use the table to do the testing of equality of population means.

<!-- We now introduce the ANOVA procedures for testing the equality of population means. -->

The hypotheses is <span style="color:blue"> $$\begin{align} &H_0: \mu_1 = \mu_2 = \cdots = \mu_k\\  &H_1: \text{Population means are not all equal} \end{align}$$ </span>

Note that the alternative hypothesis is not $H_1: \mu_1 \ne \mu_2 \ne \cdots \ne \mu_k$. This is just one scenario where $H_0$ is not satisfied. Any $\mu_i \ne \mu_j , i \ne j$ violates $H_0$, and should be a possibility of $H_1$.



<!-- - We use $F$-test.  -->
We learned that whether or not there is a difference in population means depends on the relative size of variation between samples and variation within samples. Statistician Ronald Fisher found a way to define a variable which is the ratio of variance between samples to variance within samples, and the variable follows the $F$ distribution:
$$\frac{\text{variance between samples}}{\text{variance within samples}} \sim F_{df_B,\, df_W}$$
The degrees of freedom $df_B$ is paired with variance between samples and $df_W$ is for variance within samples. Be careful the order matters.

ANONA uses F test. If the variance *between* samples is much larger than the variance *within* samples, then the $F$ test statistic $F_{test}$ will be much greater than 1, which may be over the $F$ critical value, and $H_0$ is rejected.

The key question is how variance between samples and variance within samples are defined so that the ratio is $F$ distributed.

<!-- :::{.callout-note icon=false} -->
<!-- ## Key -->
<!-- - Define variance between samples and variance within samples so that the ratio is $F$ distributed. -->
<!-- ::: -->

<!-- ------------------------------------------------------------------ -->

<span style="color:blue"> **Variance Within Samples** </span>

One-way ANOVA is a generalization of the two-sample pooled $t$-test. In the two-sample pooled $t$-test with equal variance $\sigma^2$, we have the pooled sample variance
$$s_p^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}$$

<!-- :::{.callout-note icon=false} -->
<!-- ## How about the pooled sample variance for $k$ samples? -->
<!-- ::: -->
How about the pooled sample variance for $k$ samples? ANOVA assumes the populations have the **same variance** such that $\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_k^2 = \sigma^2$. With the same logic, we can have the pooled sample variance from $k$ samples
$$\boxed{s_W^2 = \frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \cdots + (n_k-1)s_k^2}{n_1 + n_2 + \cdots + n_k - k}}$$
where $s_i^2$, $i = 1, \dots ,k$, is the sample variance of $i$-th group. $s_W^2$ represents a *combined* estimate of the common variance $\sigma^2$. It measures variability of the observations within the $k$ populations. Note that each $s_i^2$ measures the variability within the $i$-th sample. So this pooled estimate is the variance within samples.

```{r}
knitr::include_graphics("./images/img-model/var_within_group.jpg")
```

<!-- ----------------------------------------------------------------- -->

<span style="color:blue"> **Variance Between Samples** </span>

The variance between samples measures variability *among* sample means for the $k$ groups, which is defined as 
$$\boxed{s^2_{B} = \frac{\sum_{i=1}^k n_i (\bar{y}_{i\cdot} - \bar{y}_{\cdot\cdot})^2}{k-1}}$$ where 

- $\bar{y}_{i\cdot}$ is the $i$-th sample mean.
- $\bar{y}_{\cdot\cdot}$ is the *grand* sample mean with all data points in all groups combined.
- $n_i$ is the sample size of the $i$-th sample.

The variance between samples measures the magnitude of how large is the deviation from group means to the overall grand mean. When $\bar{y}_{i\cdot}$s are away from each other, $(\bar{y}_{i\cdot} - \bar{y}_{\cdot\cdot})^2$ will be large, so is $s^2_{B}$. 


```{r}
knitr::include_graphics("./images/img-model/var_between_group.jpg")
```


In fact, not only $s^2_{W}$ estimates $\sigma^2$, $s^2_{B}$ is also an estimate of $\sigma^2$. If $H_0$ is true $(\mu_1 = \cdots = \mu_k = \mu)$, any variation in the sample means is due to chance and randomness, so it shouldn't be too large. $\bar{y}_{1\cdot}, \cdots, \bar{y}_{k\cdot}$ should be close each other and should be close to $\bar{y}_{\cdot \cdot}$. This leads to a small $s^2_{B}$ and small ratio $s^2_{B}/s^2_{W}$ that is our $F$ test statistic. That's why when $\mu_1 = \cdots = \mu_k = \mu$, we have small $F$ test statistic and tend to not reject $H_0$.



<!-- ----------------------------------------------------------------- -->

<span style="color:blue"> **ANOVA Table: Sum of Squares** </span>

In the ANOVA table, there is a column Sum of Squares. There are three types of sum of squares. Let's learn what they are.

**Total Sum of Squares (SST)** measures the total variation around $\bar{y}_{\cdot\cdot}$ in all of the sample data combined (ignoring the groups), which is defined as:
$$\color{blue}{SST = \sum_{j=1}^{n_i}\sum_{i=1}^{k} \left(y_{ij} - \bar{y}_{\cdot\cdot}\right)^2}$$ where $y_{ij}$ is the $j$-th data point in the $i$-th group.



**Sum of Squares Between Samples (SSB)** measures the variation *between* sample means:
$$\color{blue}{SSB = \sum_{i=1}^{k}n_i \left(\bar{y}_{i\cdot} - \bar{y}_{\cdot\cdot}\right)^2}$$

**Sum of Squares Within Samples (SSW)** measures the variation of any value, $y_{ij}$, about its sample mean, $\bar{y}_{i\cdot}$:
$$\color{blue}{SSW = \sum_{i=1}^{k} \sum_{j=1}^{n_i} \left(y_{ij} - \bar{y}_{i\cdot}\right)^2 = \sum_{i=1}^{k} (n_i - 1)s_i^2}$$

<span style="color:red"> ***Sum of Squares Identity*** </span>

The three sum of squares are related by the identity
$$SST = SSB + SSW.$$

Intuitively, for data points $y_{ij}$, their squared distance from the grand sample mean $\bar{y}_{\cdot\cdot}$ can be decomposed into two parts: (1) the squared distance between their own group sample mean and the grand sample mean, and (2) the their squared distance from their own group sample mean.

Note that the sum of squares statistics have associated degrees of freedom. More interestingly, the three degrees of freedom also form an identity. So

$$df_{T} = df_{B} + df_{W}$$
where 

- $df_{T} = N-1$ is the degrees of freedom of $SST$
- $df_{B} = k - 1$ is the degrees of freedom of $SSB$
- $df_{W} = N - k$ is the degrees of freedom of $SSW$.

When a sum of squares divided by its degrees of freedom, we get its **mean square (MS)**, i.e., $$\text{mean square} = \dfrac{\text{sum of squares}}{\text{degrees of freedom}}.$$ We are particularly interested in the **mean square between** (**MSB**) and  **mean square within** (**MSW**):

- $MSB = \frac{SSB}{k-1} = s^2_{B}$
- $MSW = \frac{SSW}{N-k} = s^2_{W}$

<!-- $\begin{align} &df_{T} = df_{B} + df_{W} \\&n - 1 = (k-1) + (n - k) \end{align}$ -->

<!-- - $\text{Mean Square (MS)} = \frac{\text{sum of squares}}{\text{degrees of freedom}}$ -->
<!-- - $MSB = \frac{SSB}{k-1} = s^2_{B}$ -->
<!-- - $MSW = \frac{SSW}{N-k} = s^2_{W}$ -->

Please check the formula of $SSB$ and $SSW$. You will find that $MSB$ is our variance between samples and $MSW$ is our variance within samples! So our $F$ test statistic is $$F_{test} = \frac{MSB}{MSW}.$$

Under $H_0$, $s^2_{B}/s_W^2$ is a statistic from $F_{k-1, \, N-k}$ distribution. The first degrees of freedom is $df_{B} = k - 1$, and the second is $df_{W} = N - k$. They cannot be switched.

We reject $H_0$ in favor of $H_1$ if $F_{test} > F_{\alpha, \, k - 1,\, N-k}$, or $p$-value $P(F_{k - 1,\, N-k} > F_{test}) < \alpha$ for some significance level $\alpha$.

<span style="color:red"> ***ANOVA Table*** </span>

We are done! We've talked about every cell in the ANOVA table, and we can use the table to do the test and make a conclusion about the equality of population means.

```{r}
knitr::include_graphics("./images/img-model/anova_table_k.png")
```

## ANOVA Example

We hypothesize that a nutrient called "isoflavones" varies among three types of food: (1) cereals and snacks, (2) energy bars and (3) veggie burgers. 

::::{.columns}
:::{.column width="32%"}
```{r, echo=FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("./images/img-model/cereal.jpeg")
```
:::

:::{.column width="2%"}
:::

:::{.column width="32%"}
```{r, echo=FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("./images/img-model/energy_bar.png")
```
:::

:::{.column width="2%"}
:::

:::{.column width="32%"}
```{r, echo=FALSE, out.width = "100%", fig.align = "center"}
knitr::include_graphics("./images/img-model/veggie_burger.jpeg")
```
:::
::::

A sample of 5 is taken from each type of food and the amount of isoflavones is measured. Is there a sufficient evidence to conclude that the mean isoflavone levels vary among these food items at $\alpha = 0.05$?

---------------------------------------------------------------

::::: {.panel-tabset}

## R

We are going to learn to generate a ANOVA table using R.

<span style="color:blue"> **Data** </span>

In order to use the R built-in function for ANOVA, we get to make sure the data matrix is in the right format. The original data set we get may be something like object `data` below, where each column represents the five isoflavones measurements of a food type. It is not a *"wrong"* data format, but just not what we need for doing ANOVA. The data frame `data_anova` is the data format needed for ANOVA. There are two columns, our response variable isoflavones  measurement labelled `y`, and the factor or categorical variable that may affect the response value, which is food type labelled `food`. With this format, there will be totally $N=15$ observations and rows. Please transform any data into this kind of data format before you do ANOVA.

```{r}
load("./data/table08-7.rdata")
data <- `table08-7`
data_anova <- data.frame("y"=c(data[, 1], data[, 2], data[, 3]),
                      "food"=rep(c("cereals", "energy", "veggie"), each = 5))
data_anova[1, 1] <- 3
data[1, 1] <- 3
```

<!-- - We prefer a data format like the one shown on the right. -->

::::{.columns}
:::{.column width="49%"}
```{r}
#| echo: true
data
```

<!-- :::{.callout-note icon=false} -->
<!-- ## So tell me what is the value of $y_{23}$! -->
<!-- ::: -->
:::

:::{.column width="2%"}
:::

:::{.column width="49%"}
```{r}
#| echo: true
data_anova
```
:::
::::


The boxplot kind of gives us the isoflavones distribution by food type. It is hard to say whether the food type affects isoflavone level or not, and we need ANOVA to help us make the conclusion.

```{r}
#| label: fig-boxplot-isoflavone
#| fig-cap: Boxplot of the Isoflavone Content in 3 Types of Food
par(mfrow = c(1, 1))
boxplot(data, axes = FALSE, ylab = "",
        main = "Boxplot of Isoflavones", horizontal = FALSE)
axis(2, las = 1)
axis(1, las = 1, at = 1:3, 
     labels = c("Cereals", "Energy Bars", "Veggie Burger"))
```


## Python

We are going to learn to generate a ANOVA table using Python.

<span style="color:blue"> **Data** </span>

```{python}
# Load the dataset
import pandas as pd
import numpy as np
data = np.array([3, 17, 12, 10, 4, 19, 10, 9, 7, 5, 25, 15, 12, 9, 8])
data = data.reshape(5, 3, order='F')
```
```{python}
# Load the dataset
import pandas as pd
data_anova = pd.read_csv("./data/data_anova.csv")
```

::::{.columns}
:::{.column width="49%"}

```{python}
#| echo: true
data
```

<!-- :::{.callout-note icon=false} -->
<!-- ## So tell me what is the value of $y_{23}$! -->
<!-- ::: -->
:::

:::{.column width="2%"}
:::

:::{.column width="49%"}
```{python}
#| echo: true
data_anova
```
:::
::::




The boxplot kind of gives us the isoflavones distribution by food type. It is hard to say whether the food type affects isoflavone level or not, and we need ANOVA to help us make the conclusion.

```{python}
import matplotlib.pyplot as plt
data_anova.boxplot(column="y", by="food", grid=False)
plt.title("Boxplots of isoflavones")
plt.suptitle("")
plt.xlabel("Food")
plt.ylabel("y")
plt.show()
```


:::::
---------------------------------------------------------------

<span style="color:blue"> **Test Assumptions** </span>

Before implementing any statistical method, always check its method assumptions.

ANOVA requires 

+ $\sigma_1 = \sigma_2 = \sigma_3$
+ Data are generated from a normal distribution for each type of food.

Well we did not learn to test the equality of more than two population variances, but believe me I did the test, and the three variances are not significantly different from each other. [^1] Even the the variances are not all equal, the ANOVA performance will not be worse much. Equality of variances is not a strict requirement. George Box of UW-Madison showed that as long as the sample sizes are (nearly) equal, the largest variance can be up to 9 times the smallest one and the result of ANOVA will continue to be reliable. A general rule of thumb for equal variances is to compare the smallest and largest sample standard deviations. This is much like the rule of thumb for equal variances for the test for independent means. If the ratio of these two sample standard deviations falls within 0.5 to 2, then it may be that the assumption is not violated.

[^1]: The test I use is Brown-Forsythe test. It can be performed using the function `onewaytests::bf.test()`.


To check the normality, we can check their QQ plots. [^2] @fig-qqplots shows that there is no obvious non-normal pattern although two data points are outside the blue 95% confidence region. The normality is not very restrictive as well. As long as the distribution is not very skewed, ANOVA works pretty well.



[^2]: There are several tests for normality, such as Shapiro–Wilk test (`stats::shapiro.test()`), Anderson–Darling test (`nortest::ad.test()`), and Kolmogorov–Smirnov test (`stats::ks.test()`).

We say the ANOVA $F$ test is **robust** to the violation of the two assumptions.


<!-- https://cran.r-project.org/web/packages/onewaytests/index.html -->
<!-- Shapiro-Wilk’s method is widely recommended for normality test and it provides better power than K-S. It is based on the correlation between the data and the corresponding normal scores. -->
<!-- Empirical testing has found[5] that the Anderson–Darling test is not quite as good as the Shapiro–Wilk test, but is better than other tests -->



```{r}
#| label: fig-qqplots
#| fig-cap: QQ plots for each type of food
#| fig-asp: 0.35
par(mgp = c(2, 1, 0))
par(mar = c(3.5, 3.5, 1.5, 0))
qqPlot(y ~ food, data = data_anova, layout = c(1, 3), las = 1)
# ad.test(data_ex78[data_ex78$additive == 1, 1])
# ad.test(data_ex78[data_ex78$additive == 2, 1])
# ad.test(data_ex78[data_ex78$additive == 3, 1])
```

<span style="color:blue"> **ANOVA Testing** </span>

We are interested in the following test:

<span style="color:blue"> $\begin{align}&H_0: \mu_1 = \mu_2 = \mu_3\\&H_1: \mu_is \text{ not all equal} \end{align}$ </span>

where $\mu_1$, $\mu_2$, $\mu_3$ stand for the population mean level of isoflavones of food cereals and snacks, energy bars, and veggie burgers respectively.

We could follow the regular six-step testing procedure, but generating the ANOVA table is more straightforward. 


```{r}
knitr::include_graphics("./images/img-model/anova_table_k.png")
```


:::: {.panel-tabset}

## R

In R, we can do all the calculations and generate an ANOVA table using just one line of code. We first use the popular function `lm()` to implement ANOVA. The first argument is `formula` that has the form `response ~ factor` where `response` is the numeric response vector, and `factor` is the categorical factor vector. In our data `data_anova`, `y` is our response, and `food` is our factor. Note that if we just write `y` and `food` in the formula, R will render an error because R does not recognize `y` and `food` because they are not an R object but a column name of data `data_anova`. Therefore, we need to tell R where `y` and `food` are from by specifying the data set they are referred. If you want to specify the data name, you can get access to the response and factor vector by extracting them using `data_anova$y` and `data_anova$food`.


The word `lm` stands for **linear model**, and ANOVA is a linear model. If you just run `lm(formula = data_anova$y ~ data_anova$food)`, it will show the linear model output related to ANOVA. We don't need it at this moment, and we will discuss more about linear model in Regression @sec-model-reg. 

To obtain the ANOVA table, we apply the function `anova()` to the `lm` object. In the output, `food` is the source of variation between samples. `Residuals` is for the source of variation within samples. `F value` is the $F$ test statistic value, not the critical value. `Pr(>F)` is the $p$-value. Since $p$-value > 0.05, we do not reject $H_0$. The evidence is not sufficient to reject the equality of population means.

```{r}
#| echo: true
anova(lm(formula = y ~ food, data = data_anova))
anova(lm(formula = data_anova$y ~ data_anova$food))
```

:::{.callout-note}

There is another way to generate the ANOVA table in R. We use `summary()` and `aov()` functions.

```{r}
#| echo: true
summary(aov(y ~ food, data = data_anova))

# oneway.test(y ~ food, data = data_anova, var.equal = TRUE)
```
:::

## Python


In Python, we can do all the calculations and generate an ANOVA table using simple code. We first use the function [`formula.ols()`](https://www.statsmodels.org/stable/generated/statsmodels.formula.api.ols.html) in `statsmodels.api` to create a linear model from a formula and dataframe. `ols` means ordinary least squares which is a method for a linear method. We will discuss it in @sec-model-reg. The first argument is `formula` that has the form `response ~ factor` where `response` is the numeric response vector, and `factor` is the categorical factor vector. In our data `data_anova`, `y` is our response, and `food` is our factor. Note that the formula has to be a string. 

Then, we need to tell Python where `y` and `food` are from by specifying the data set they are referred in the argument `data`.

The code `sm.formula.ols(formula='y ~ food', data=data_anova)` only creates a model, but it is not actually fit by our data. We need to add `.fit()` to complete the modeling fitting using the ols approach.

```{python}
#| echo: true
import statsmodels.api as sm
from statsmodels import stats
model = sm.formula.ols(formula='y ~ food', data=data_anova).fit()
```

To obtain the ANOVA table, we apply the function `anova_lm()` to the fitted model. In the output, `food` is the source of variation between samples. `Residual` is for the source of variation within samples. `F` is the $F$ test statistic value, not the critical value. `PR(>F)` is the $p$-value. Since $p$-value > 0.05, we do not reject $H_0$. The evidence is not sufficient to reject the equality of population means.


```{python}
#| echo: true
sm.stats.anova_lm(model)
```


:::{.callout-note}

The function [`f_oneway()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html) in `scipy.stats` provides a fast way to do the ANOVA without outputting the ANOVA table. It just provides the F test statistic and p-value.

```{python}
#| echo: true
from scipy import stats
stats.f_oneway(data[:, 0], data[:, 1], data[:, 2])
```
:::

::::




## Unequal Variances

The assumption of normality and equality of variances are both loose requirements. George Box of UW-Madison showed that as long as the sample sizes are (nearly) equal, the largest variance can be up to 9 times the smallest one and the result of ANOVA will continue to be reliable.

If population variances differ by large amount, we can **transform the data**. If the original variable is $y$, and the variances associated with $y$ across the treatments are not equal (heterogeneous), it may be necessary to work with a new variable such $\sqrt{y}$, $\log y$, or some other transformed variable.

- <span style="color:blue"> If $\sigma^2 \propto \mu$, use $Y_T = \sqrt{Y}$ or $\sqrt{Y+0.375}$. </span>

- <span style="color:blue"> If $\sigma^2 \propto \mu^2$, use $Y_T = \log{(Y)}$ or $\log(Y+1)$. </span>

<!-- ## Heterogeneous Variances on Original $y$ Scale -->

The following example demonstrates how data transformation can significantly equalize variances across groups. In the original data  $y$, variance increases with the treatment type, which violates the assumption of homogeneity of variance necessary for performing ANOVA. Directly applying ANOVA to $y$ may result in unconvincing outcomes. However, by transforming the data from $y$ to $\sqrt{y}$, the variances across the three groups become much more similar, thereby satisfying the homogeneity of variance assumption and making the ANOVA results more reliable.
 
 

```{r, fig.height=8, fig.width=13}
# k <- 1
n <- 100
df_heter_var <- data.frame("y" = c(rpois(n, 5), rpois(n, 20), rpois(n, 50)),
                           "treatment" = c(rep(1, n), rep(2, n), rep(3, n)))
boxplot(y~treatment, data = df_heter_var, axes = FALSE, ylab = "", xlab = "",
        main = "Heterogeneous Variances on y", horizontal = FALSE)
axis(2)
axis(1, las = 1, at = 1:3, labels = c("Treatment 1", "Treatment 2", "Treatment 3"))
original_y <- df_heter_var$y
```



<!-- ## Square Root Transformation ($y \rightarrow \sqrt{y}$) -->

```{r, fig.height=8, fig.width=13}
boxplot(sqrt(original_y)~treatment, data = df_heter_var, axes = FALSE, ylab = "", xlab = "",
        main = expression(paste("Equal Variances on ", sqrt(y))), horizontal = FALSE)
axis(2)
axis(1, las = 1, at = 1:3, labels = c("Treatment 1", "Treatment 2", "Treatment 3"))
```

### Example of Unequal Variances (Example 8.4 SMD)

Biologists believe that Mississippi river causes the oxygen level to be depleted near the Gulf of Mexico. To test this hypothesis water samples are taken at different distances from the mouth of Mississippi river, and the amounts of dissolve oxygen (in ppm) are recorded.

```{r}
#| out-width: 100%
knitr::include_graphics("./images/img-model/table8-16.png")
```

<!-- ![](./figures/table8-16.png){height=400px width=900px} -->


<!-- ## Example 8.4 Cont'd: Check the Data -->

::: {.panel-tabset}

## R

The origin data set `data_oxygen` is saved as

```{r, fig.height=6, fig.width=10}
load("../../intro_stats/OTT_Final/R_WORKSPACE/CH08/table08-16.rdata")
data_8_16 <- `table08-16`  ## This is a data frame
# original_y <- df_heter_var$y
data_oxygen <- data_8_16[, 2:5]
data_oxygen
```

<!-- ## Example 8.4 Cont'd: Test for Equal Variances -->



First we learn that the homogeneity of variance assumption is violated by checking the boxplot.

```{r}
#| echo: false
#| eval: false
boxplot(data_oxygen)
```

```{r}
boxplot(data_8_16[, 2:5], axes = FALSE, ylab = "", xlab = "",
        main = "Boxplot of Oxygen Content", horizontal = FALSE)
axis(2)
axis(1, las = 1, at = 1:4, labels = paste(c(1, 5, 10, 20), "km"))
```


```{r}
#| echo: !expr c(2)
data_oxygen_tidy <- data.frame(oxygen = c(data_8_16[, 2], data_8_16[, 3], 
                                        data_8_16[, 4], data_8_16[, 5]),
                             km = c(rep("1", 10), rep("5", 10), 
                                    rep("10", 10), rep("20", 10)))
```

This is also verified by the Levene's test using the function `car::leveneTest()`. The Levene’s test $p$-value is 0.02, so we reject the $H_0$: Equality of variance.

```{r}
#| echo: true
library(car)
leveneTest(oxygen ~ km, data = data_oxygen_tidy)
```

Note that here we use the data `data_oxygen_tidy`. As mentioned before, when doing analysis or fitting a statistical model, we prefer this *tidy data* that each column represents a variable, and each row stands for one observed value.

```{r}
data_oxygen_tidy
```


## Python

The origin data set `data_oxygen` is saved as

```{python}
data_oxygen = pd.read_csv("./data/data_oxygen.csv")
data_oxygen
```


First we learn that the homogeneity of variance assumption is violated by checking the boxplot.


```{python}
#| echo: false
#| eval: true
# Boxplot for the oxygen data
import seaborn as sns
sns.boxplot(data=data_oxygen)
plt.show()
```


This is also verified by the Levene's test using the function [`levene()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.levene.html) in `scipy .stats`. The Levene’s test $p$-value is 0.02, so we reject the $H_0$: Equality of variance.


```{python}
#| echo: true
# Levene's test for homogeneity of variances
stats.levene(data_oxygen['1KM'], 
             data_oxygen['5KM'], 
             data_oxygen['10KM'],
             data_oxygen['20KM'])
```

:::

To get some understanding of how we should transform our data to make the variances equal, let’s calculate $\frac{s_i^2}{\bar{y}_i}$ for $i = 1, 2, 3, 4$.

- $\frac{s_1^2}{\bar{y}_1} = 0.99$, $\frac{s_2^2}{\bar{y}_2} = 0.97$, $\frac{s_3^2}{\bar{y}_3} = 1.06$, $\frac{s_4^2}{\bar{y}_4} = 0.97$

This tells us, nearly, the sample variance is proportional to the sample mean of the same group. So we can use the transformation $Y_T = \sqrt{Y+0.375}$.


<!-- ## Example 8.4 Cont'd: Transforming the Data -->

<!-- ![](./figures/table8-17.png){height=500px width=900px} -->


```{r}
#| out-width: 100%
knitr::include_graphics("./images/img-model/table8-17.png")
```

Now, the variances for each group of the transformed data are much reasonably close.


## ANOVA Mathematical Model*

To generalize the ANOVA, it is easier to think of one-factor ANOVA in the following way:
$$y_{ij} = \mu + \tau_i + \epsilon_{ij}, \quad j = 1, 2, \dots, n_i, \quad i = 1, 2, \dots, t$$

  + $\mu$ is the overall mean across all $t$ populations.
  + $\tau_i$ is the effect due to $i$-th treatment.
  + $\epsilon_{ij}$ is the random deviation of $y_{ij}$ about the $i$-th population mean $\mu_i = \mu+ \tau_i$.
  + $\mu$ and $\tau_i$ are unknown constants. 
  + ANOVA is a **linear** model.
  
The assumptions of ANOVA are

  + $\epsilon_{ij}$s are independent and normally distributed with mean 0.
  + $Var(\epsilon_{ij}) = \sigma^2$ (<span style="color:blue"> a constant value </span>)
  + $\epsilon_{ij} \sim N(0, \sigma^2)$
  
  
<!-- ## ANOVA Model -->

@fig-anova-model illustrate the ANOVA model structure. There are 4 treatments, and the overall mean $\mu = 75.5$. The 4 treatments are $\tau_1 = -5.5$,  $\tau_2 = -17.5$,  $\tau_3= -14.5$, and $\tau_4 = 8.5$.


```{r}
#| label: fig-anova-model
#| out-width: 100%
#| fig-cap: Illustration of ANOVA model structure. Figure 16.7 of Applied Linear Statistical Models, 5th ed.
knitr::include_graphics("./images/img-model/anova_model.png")
```
<!-- ![](./figures/anova_model.png){height=300px width=500px} -->

Let $y_{13}$ is the third measurement drawn from population 1, the its value can be decomposed into three parts: the overall mean, the adjustment due to being in the first treatment, and the random variation of the first population: $$y_{13} = \mu + \tau_1 + \epsilon_{13}.$$

If $y_{13} = 72$, then $$y_{13} = \mu + \tau_1 + \epsilon_{13} \iff 72 = 75.5 + (-5.5) + 2.$$


<!-- ### Hypothesis Testing of the ANOVA Model -->
Again, our model is
$y_{ij} = \mu + \tau_i + \epsilon_{ij} = \mu_i + \epsilon_{ij}, \quad j = 1, 2, \dots, n_i, \quad i = 1, 2, \dots, t.$

To test whether all samples come from the same population, we test whether all group means are equal:

 <span style="color:blue"> $\begin{align} 
  &H_0: \mu_1 = \mu_2 = \cdots = \mu_t\\ 
  &H_1: \text{Population means are not all equal}
  \end{align}$ </span>
  
This is equivalent to testing whether all treatment effects are zero, or there is no treatment/factor/group effect: 

<span style="color:blue"> $\begin{align} 
  &H_0: \tau_i = 0 \text{ for all } i = 1, 2, \dots, t\\ 
  &H_1: \tau_i \ne 0 \text{ for some } i
  \end{align}$ </span>
  
Test statistics and decision rule are same as before. $s^2_{B} = MSB = \frac{SSB}{df_B}$ and $s^2_{W} = MSW = \frac{SSW}{df_W}$. Then the test statistic is $F_{test} = \frac{s^2_{B}}{s_W^2}$. We reject $H_0$ if $F_{test} > F_{\alpha, \, df_{B},\, df_{W}}$ or the $p$-value $P(F > F_{test}) < \alpha$.


### Check Assumption that $\epsilon_{ij} \sim N(0, \sigma^2)$

We learn that $y_{ij} = \mu + \tau_i + \epsilon_{ij} =  \mu_i + \epsilon_{ij}$ where $\mu_i = \mu + \tau_i$. As before, we need to check model assumptions. One assumption is $\epsilon_{ij} \sim N(0, \sigma^2)$. Notice that it implies normality and homogeneity of variance assumptions.

For the model, we have $\epsilon_{ij} = y_{ij} - \mu_i$. However, $\mu_i$ is unknown. We are not able to get the values of $\epsilon_{ij}$ and check whether they are normally distributed. One solution is to find its estimate. We first estimate the errors $\epsilon_{ij}$ by $e_{ij}$ called **residuals**:
$$e_{ij} = y_{ij} - \hat{\mu}_i =  y_{ij} - \bar{y}_{i\cdot},$$ where $\hat{\mu}_i = \bar{y}_{i\cdot}$. We use a *hat* symbol $\hat{}$ to denote the estimated value for an unknown parameter, or the predicted value of the response. Here we don't know the group population means $\mu_i$, but they can be estimated by their corresponding group sample means $\bar{y}_{i\cdot}$.

<!-- - Even when $n_i$s are small, we would have $n_T$ residuals, which provide a sufficient number of values to evaluate normality condition. -->

- To test normal distribution of errors $\epsilon_{ij}$, we look at the normal probability plot of $e_{ij}$.

- To test that the $Var(\epsilon_{ij})$ is constant of $\sigma^2$, we look at the scatter plot of the residuals $e_{ij}$ and the predicted or **fitted values** $\hat{y}_{ij}$, where
$$\hat{y}_{ij} = \hat{\mu}_i = \bar{y}_{i\cdot}.$$
$\hat{y}_{ij}$ is the fitted value of the $j$th observation in the $i$th group, which is an estimate of the observed data point $y_{ij}$. The fitted value $\hat{y}_{ij}$ is the one produced from the model, and of course for the most of the time will not be the same as the observed value $y_{ij}$. From ANOVA, $\hat{y}_{ij} = \bar{y}_{i\cdot}$. That is, if we want to estimate any value drawn from the $i$th population, ANOVA gives us a point estimate, the $i$th group sample mean, to estimate any $j$th observation from the $i$th population. 

  The samples in the $i$th population are not the same due to the variation of the normal distribution. However, when estimating each individual observation, ANOVA would not provide different values for different observations, but one single common value for all individual observations. The sampling variation cannot be captured by ANOVA, and using the mean value $\bar{y}_{i\cdot}$ as an estimate is the best ANOVA can do.


<!-- ## Going Back to Example 8.4 -->

<!-- - Biologists believe that Mississippi river causes the oxygen level to be depleted near the Gulf of Mexico. To test this hypothesis water samples are taken at different distances from the mouth of Mississippi river, and the amounts of dissolve oxygen (in ppm) are recorded. -->

<!-- ![](./figures/table8-16.png){height=400px width=900px} -->

------------------------------------------

Let's go back to the oxygen level example. The original data look like 

<!-- ## Fitted Values $\hat{y}_{ij} = \bar{y}_{i\cdot}$ {.tiny} -->
<!-- <div class="columns-2"> -->
<!-- # ```{r} -->
<!-- #  -->
<!-- # ``` -->
<!-- </br> -->
<!-- # ```{r} -->
<!-- # #| echo: !expr c(-1, -2) -->
<!-- # ## original data -->
<!-- # data_oxygen <- data_8_16[, 2:5] -->
<!-- # data_oxygen -->
<!-- # ``` -->
<!-- </div> -->

To get the sample mean for each group, we can grab each column data, and calculate its average. For example, $\bar{y}_{1\cdot} = 2.2$.

::: {.panel-tabset}

## R

```{r}
#| echo: true
colMeans(data_oxygen)
```


The fitted value can also be obtained from the fitted result by the fitted linear model from `lm()`. `oxygen_fit$fitted.values` gives us fitted values for all observations. Note that the first 10 observations all have the same fitted value 2.2 because they are all from the first group (1 km). Similarly for other groups.

```{r}
#| echo: true
# data_oxygen_tidy
## fit a linear model to get fitted values and residuals
oxygen_fit <- lm(oxygen ~ km, data = data_oxygen_tidy)
oxygen_fit$fitted.values
```

We can organize the fitted values, saving them as a data frame like the original data `data_oxygen`.

```{r}
#| echo: true
fitted_val <- matrix(oxygen_fit$fitted.values, 10, 4)

## use the same column name as the original data
colnames(fitted_val) <- paste0(c(1, 5, 10, 20), "KM")

## fitted value data
as.data.frame(fitted_val)
```



------------------------------------------

Now let's check the residuals $e_{ij} = y_{ij} - \bar{y}_{i\cdot}$. We can obtain the residuals using its definition 

```{r}
#| echo: true
data_oxygen_tidy[, 1] - oxygen_fit$fitted.values
```

or we can simply grab them from the fitted result:

```{r}
#| echo: true
oxygen_fit$residuals
```



<!-- ## Residuals $e_{ij} = y_{ij} - \bar{y}_{i\cdot}$  -->
<!-- -We have relatively large total sample size $n_T = 40$. -->


The residuals must look like a normal distribution. The normal probability plot is somewhat close to a straight line. 

```{r, fig.height=5}
par(mgp = c(2.5, 1, 0))
par(mar = c(4, 4, 2, 1))
library(car)
qqPlot(oxygen_fit$residuals, pch = 19, id = FALSE, ylab = "residuals", 
       main = "Normal Probability Plot for Residuals")
```

The Shapiro test and Anderson-Darling test give us a different conclusion. We could either transform the data to make it look more normal, or collect more data points to gain more information about how the data are distributed.

```{r}
#| echo: true
shapiro.test(oxygen_fit$residuals)
ad.test(oxygen_fit$residuals)
```


<!-- ## Fitted Values versus Residuals -->

**Residual plots** is another good way to check the model assumptions, especially whether or not the homogeneity of variance is hold. A residual plot is a scatterplot of $e_{ij}$ (y-axis) versus $\hat{y}_{ij}$ (x-axis).

<!-- - Scatterplot of $\hat{y}_{ij}$ versus $e_{ij}$ -->

```{r}
#| echo: true
#| eval: false
plot(x = oxygen_fit$fitted.values, y = oxygen_fit$residuals)
```

```{r, fig.height=4}
par(mar = c(4, 4, 2, 1))
plot(oxygen_fit$fitted.values, oxygen_fit$residuals, xlab = "Fitted Value",
     ylab = "Residual", main = "Versus Fits", pch = 19, col = "red",
     xlim = c(0, 35))
abline(h = 0)
```

If the homogeneity of variance holds, the residual points should be scattered around value 0 with the same degree of spreadness. Due to the cone shape, we see that the residual variance is increasing with the fitted values or kilometers, a sign of non-constant variances. We can conclude that $Var(\epsilon_{ij})$ is not constant. We can further say that this variance is a function of $\bar{y}_{i\cdot}$.


<!-- ## Correct Analysis Based on Transformed Data -->

Since the model assumptions are violated, we transform our data to correct unequal variances. As we learned in the previous section, we use the transformation $Y_T = \sqrt{Y + 0.375}$. The transformed data set is called `data_oxygen_tidy_trans`.

```{r}
#| echo: true
data_oxygen_tidy_trans <- data_oxygen_tidy
data_oxygen_tidy_trans[, 1] <- sqrt(data_oxygen_tidy[, 1] + 0.375)
```



We then fit ANOVA model on the transformed data.

```{r}
#| echo: true
oxygen_fit_trans <- lm(oxygen ~ km, data = data_oxygen_tidy_trans)
anova(oxygen_fit_trans)
```

We can check the model assumptions on the transformed data too to make sure that fitting ANOVA to the data makes sense.

```{r}
#| fig-height: 5
#| fig-width: 11.5
library(car)
par(mgp = c(2.5, 1, 0))
par(mar = c(4, 4, 2, 1))
par(mfrow = c(1, 2))
qqPlot(oxygen_fit_trans$residuals, pch = 19, id = FALSE, ylab = "residuals", 
       main = "Normal Probability Plot for Residuals (transformed y)")
plot(oxygen_fit_trans$fitted.values, oxygen_fit_trans$residuals, xlab = "Fitted Value",
     ylab = "Residual", main = "Versus Fits (transformed y)", pch = 19, col = "red",
     xlim = c(0, 6))
abline(h = 0)
```

```{r}
shapiro.test(oxygen_fit_trans$residuals)
ad.test(oxygen_fit_trans$residuals)
```

From the qqplot and the testing results we can see that the transformed data are more like a normal distribution. There is no significant pattern in the residual plot, showing that unequal variances have been corrected.

This result is more convincing than the one using the original data that violates the ANOVA model assumptions. Although the two models both reject the null hypothesis, the numerical results obtained from the transformed data interpret the relationship between the response and the factors more accurately.

```{r}
#| echo: true
anova(oxygen_fit)
```


<!-- ## Checking Asumptions Based on Transformed Data -->



## Python
```{python}
#| echo: true
# Calculate the mean for each group
group_means = data_oxygen.mean()
group_means
```

```{python}
#| echo: true
data_oxygen_tidy = pd.read_csv("./data/data_oxygen_tidy.csv")
```

The fitted value can also be obtained from the fitted result by the fitted linear model from `formula.ols()`. `oxygen_fit.fittedvalues` gives us fitted values for all observations. Note that the first 10 observations all have the same fitted value 2.2 because they are all from the first group (1 km). Similarly for other groups.

In the formula, since the variable `km` is in fact categorical, we need to add `C()` to turn its type from integer to string.

```{python}
#| echo: true
oxygen_fit = sm.formula.ols(formula='oxygen ~ C(km)', data=data_oxygen_tidy).fit()
oxygen_fit.fittedvalues
```

We can organize the fitted values, saving them as a data frame like the original data `data_oxygen`.

```{python}
#| echo: true
fitted_val = np.reshape(oxygen_fit.fittedvalues.values, (10, 4), order='F')
pd.DataFrame(fitted_val, columns=['1KM', '5KM', '10KM', '20KM'])
```


------------------------------------------

Now let's check the residuals $e_{ij} = y_{ij} - \bar{y}_{i\cdot}$. We can obtain the residuals using its definition 

```{python}
#| echo: true
data_oxygen_tidy['oxygen'].values - oxygen_fit.fittedvalues.values
```

or we can simply grab them from the fitted result:

```{python}
#| echo: true
oxygen_fit.resid
```

```{python}
# https://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.qqplot.html
sm.qqplot(oxygen_fit.resid, line='r')
plt.title("Normal Probability Plot for Residuals")
plt.show()
```


The [Shapiro](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html) test and [Anderson-Darling](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html) test give us a different conclusion. We could either transform the data to make it look more normal, or collect more data points to gain more information about how the data are distributed.

```{python}
#| echo: true
# Shapiro-Wilk test for normality
stats.shapiro(oxygen_fit.resid)
```

```{python}
#| echo: true
# Anderson-Darling test for normality
stats.anderson(oxygen_fit.resid)
```


**Residual plots** is another good way to check the model assumptions, especially whether or not the homogeneity of variance is hold. A residual plot is a scatterplot of $e_{ij}$ (y-axis) versus $\hat{y}_{ij}$ (x-axis).



```{python}
#| echo: true
#| eval: false
plt.scatter(oxygen_fit.fittedvalues, oxygen_fit.resid)
```


```{python}
#| echo: false
# Residuals vs Fitted values plot
plt.scatter(oxygen_fit.fittedvalues, oxygen_fit.resid, c='red')
plt.axhline(y=0, color='black', linestyle='--')
plt.xlabel("Fitted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs Fitted Values")
plt.show()
```

If the homogeneity of variance holds, the residual points should be scattered around value 0 with the same degree of spreadness. Due to the cone shape, we see that the residual variance is increasing with the fitted values or kilometers, a sign of non-constant variances. We can conclude that $Var(\epsilon_{ij})$ is not constant. We can further say that this variance is a function of $\bar{y}_{i\cdot}$.


<!-- ## Correct Analysis Based on Transformed Data -->

Since the model assumptions are violated, we transform our data to correct unequal variances. As we learned in the previous section, we use the transformation $Y_T = \sqrt{Y + 0.375}$. The transformed data set is called `data_oxygen_tidy_trans`.

```{python}
#| echo: true
data_oxygen_tidy_trans = data_oxygen_tidy.copy()
# Apply the transformation to the first column (oxygen)
data_oxygen_tidy_trans['oxygen'] = np.sqrt(data_oxygen_tidy['oxygen'] + 0.375)
```

We then fit ANOVA model on the transformed data.

```{python}
#| echo: true
oxygen_fit_trans = sm.formula.ols(formula='oxygen ~ C(km)',
                                  data=data_oxygen_tidy_trans).fit()
sm.stats.anova_lm(oxygen_fit_trans)
```

We can check the model assumptions on the transformed data too to make sure that fitting ANOVA to the data makes sense.

```{python}
# QQ plot and residual plot for the transformed model
fig, axs = plt.subplots(1, 2, figsize=(12, 6))
sm.qqplot(oxygen_fit_trans.resid, line='r', ax=axs[0])
axs[0].set_title("Normal Probability Plot for Residuals (Transformed)")

axs[1].scatter(oxygen_fit_trans.fittedvalues, oxygen_fit_trans.resid, c='red')
axs[1].axhline(y=0, color='black', linestyle='--')
axs[1].set_xlabel("Fitted Values")
axs[1].set_ylabel("Residuals")
axs[1].set_title("Residuals vs Fitted Values (Transformed)")

plt.tight_layout()
plt.show()
```

```{python}
stats.shapiro(oxygen_fit_trans.resid)
stats.anderson(oxygen_fit_trans.resid)
```

From the qqplot and the testing results we can see that the transformed data are more like a normal distribution. There is no significant pattern in the residual plot, showing that unequal variances have been corrected.

This result is more convincing than the one using the original data that violates the ANOVA model assumptions. Although the two models both reject the null hypothesis, the numerical results obtained from the transformed data interpret the relationship between the response and the factors more accurately.

```{python}
#| echo: true
oxygen_fit = sm.formula.ols('oxygen ~ C(km)', data=data_oxygen_tidy).fit()
sm.stats.anova_lm(oxygen_fit)
```


:::


## Nonparametric Approach: Kruskal-Wallis Test

In ANOVA, the sample from each factor level is from a normal population. What if the distribution is **non-normal**? One solution is to use the **Kruskal-Wallis test**, which can be viewed as a generalization of the Wilcoxon rank sum test (Mann-Whitney U test). Suppose we have $k$ samples from $k$ populations. We like to test the hypothesis that the $k$ samples were drawn from populations with the same **median**:

- <span style="color:blue"> $\begin{align} 
  &H_0: \text{All $k$ populations have the same median}\\ 
  &H_1: \text{Not all the medians are the same}
  \end{align}$ </span>

The requirements of the Kruskal-Wallis test are

  + At least 3 **independent** samples
  + Each sample has at least 5 observations for approximating a $\chi^2$ distribution. If samples have fewer than 5 observations, the $\chi^2$ approximation doe snot work well, and another way to find critical values is needed.

There is **no** requirement that the populations have a normal distribution or any other particular distribution.

The testing details are skipped here, but let's see how we can perform this test to answer our question when the normality assumption of ANOVA is not satisfied.


<!-- ---------------------------------------------------------------------- -->



<!-- <span style="color:blue">  **Example: `airquality` data** </span> -->

::: {.panel-tabset}

## R

We use the R built-in data set `airquality` for illustration. The data collected daily air quality measurements in New York, May to September 1973.

```{r}
#| echo: true
str(airquality)
head(airquality)
## remove observations with missing values using complete.cases()
air_data <- airquality[complete.cases(airquality), ] 
```

Our goal is to test at .05 significance level if the monthly ozone densities in New York have the same median from May to September 1973.

First let's check the normality assumption. We can use `qqnorm()`, but here the function `qqPlot()` from the `car` package is used. The blue shaded area shows the 95\% confidence interval for straight line that indicates normality. Since there are not many observations in each group May to September, it may be hard to see whether normality is satisfied for every group. 

```{r}
#| label: normality-air
#| fig-height: 3
#| fig-width: 12
#| echo: true
library(car)
par(mgp = c(2.5, 1, 0))
par(mar = c(4, 4, 2, 1))
car::qqPlot(Ozone ~ Month, data = air_data, layout=c(1, 5))
```
<!-- kruskal.test(Ozone ~ Month, data = airquality)  -->

## Python

We use the R built-in data set `airquality` for illustration. The data collected daily air quality measurements in New York, May to September 1973. We use `dropna()` to remove observations with missing values.

```{python}
#| echo: true
# Load air quality data
airquality = pd.read_csv("./data/airquality.csv")
air_data = airquality.dropna()
air_data
```

:::


What we can do is to use a more formal testing procedure to decide whether are not the normality is satisfied. There are several methods for normality test out there such as **Kolmogorov-Smirnov (K-S) test**, **Shapiro-Wilk's test**, and **Anderson-Darling test**.

The K-S test can be used to compare a sample against any reference distribution, not just the normal distribution. However, the K-S test is more focused on the central part of the distribution and might miss deviations in the tails. It is generally less powerful for detecting deviations from normality compared to other tests like the Shapiro-Wilk or Anderson-Darling tests.

The Shapiro-Wilk test is specifically designed to assess the normality of a distribution. It is generally more powerful than the K-S and Anderson-Darling tests for detecting departures from normality, especially in small to moderately sized samples. The Anderson-Darling test is a modification of the K-S test and gives more weight to the tails of the distribution.

<!-- https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3693611/ -->

When we do the normality test, the null hypothesis of these tests is that *"sample distribution IS normal"*. If the test is significant, the distribution is non-normal.



::: {.panel-tabset}

## R

To perform Shapiro-Wilk test, we use `shapiro.test()` function in R. Note that May and Semptember are not normal.

```{r}
#| label: normality-test
shapiro.test(air_data[air_data$Month == 5, 1])
shapiro.test(air_data[air_data$Month == 9, 1])
```

To perform Anderson-Darling test, we use `ad.test()` function in the R package `nortest`.

```{r}
#| echo: true
#| eval: true
library(nortest)
ad.test(air_data[air_data$Month == 5, 1])
ad.test(air_data[air_data$Month == 9, 1])
```

For the K-S test, we use `ks.test()` function. We need to add "pnorm" in the function, so that it knows you are comparing your data with the normal distribution.

```{r}
ks.test(air_data[air_data$Month == 5, 1], "pnorm")
ks.test(air_data[air_data$Month == 9, 1], "pnorm")
```


We just conform that not all sub-samples are normally distributed, and using the Kruskal-Wallis test could be making more sense. It is super easy to perform the Kruskal-Wallis test. Just one line of code with the formula as `lm()` in the `kruskal.test()` function.
<!-- - Just one line of code in R to do Kruskal-Wallis test! -->

```{r}
#| label: kruskal-wallis-air
#| echo: true
kruskal.test(formula = Ozone ~ Month, data = air_data) 
```

It's an (approximate) $\chi^2$ test with degrees of freedom $k-1$. Here, we have $k=5$ months. Since $p$-value is approaching to 0, we reject $H_0$. We conclude that there is sufficient evidence to reject the claim that the 5 monthly ozone densities in New York have the same median.



## Python

To perform Shapiro-Wilk test, we use [`shapiro()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.shapiro.html) function in `scipy.stats`. Note that May and Semptember are not normal.

```{python}
#| echo: true
stats.shapiro(x=air_data[air_data['Month'] == 5]['Ozone'])
stats.shapiro(x=air_data[air_data['Month'] == 9]['Ozone'])
```

To perform Anderson-Darling test, we use [`anderson()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.anderson.html) function in `scipy.stats`.

```{python}
#| echo: true
#| eval: true
stats.anderson(x=air_data[air_data['Month'] == 5]['Ozone'])
stats.anderson(x=air_data[air_data['Month'] == 9]['Ozone'])
```

For the K-S test, we use [`kstest()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html) function `scipy.stats`. We need to add "norm" in the function, so that it knows you are comparing your data with the normal distribution.

```{python}
#| echo: true
stats.kstest(rvs=air_data[air_data['Month'] == 5]['Ozone'], cdf='norm')
stats.kstest(rvs=air_data[air_data['Month'] == 9]['Ozone'], cdf='norm')
```


We just conform that not all sub-samples are normally distributed, and using the Kruskal-Wallis test could be making more sense. We use [`kruskal()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kruskal.html) in `scipy.stats` to perform such test.
<!-- - Just one line of code in R to do Kruskal-Wallis test! -->

```{python}
#| echo: true
# Kruskal-Wallis test for Ozone levels across different months
stats.kruskal(air_data[air_data['Month'] == 5]['Ozone'],
              air_data[air_data['Month'] == 6]['Ozone'],
              air_data[air_data['Month'] == 7]['Ozone'],
              air_data[air_data['Month'] == 8]['Ozone'],
              air_data[air_data['Month'] == 9]['Ozone'])
```

::: {.callout-note}
A more concise way to write the code is
```{python}
#| echo: true
#| eval: false
# Kruskal-Wallis test for Ozone levels across different months
stats.kruskal(*[air_data[air_data['Month']==m]['Ozone'] for m in sorted(air_data['Month'].unique())])
```
:::


It's an (approximate) $\chi^2$ test with degrees of freedom $k-1$. Here, we have $k=5$ months. Since $p$-value is approaching to 0, we reject $H_0$. We conclude that there is sufficient evidence to reject the claim that the 5 monthly ozone densities in New York have the same median.

:::


<!-- ## Pairwise Comparison -->



<!-- ## Two-Way ANOVA* -->

<!-- To be added. -->
