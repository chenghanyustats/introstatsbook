[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistics",
    "section": "",
    "text": "This is the website for my introductory statistics book. This book serves as a main reference book for my MATH 4720 Statistical Methods and MATH 4740 Biostatistical Methods at Marquette University.1 Some topics can also be discussed in an introductory data science course. You‚Äôll learn basic probability and statistical concepts as well as data analysis techniques such as linear regression using R computing software. The book balances the following aspects of statistics:\n\nmathematical derivation and statistical computation\ndistribution-based and simulation-based inferences\nmethodology and applications\nclassical/frequentist and Bayesian approaches\n\nCourse materials are borrowed from the following books:\n\nOpenIntro Statsitics (data oriented)\nIntroduction to Modern Statistics (computation oriented)\nAn Introduction to Statistical Methods & Data Analysis, 7th edition (mathematics oriented)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Statistics and Data",
    "section": "",
    "text": "What is Statistics? What is Data Science? What are data?"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13¬† Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. ‚ÄúLiterate Programming.‚Äù Comput.\nJ. 27 (2): 97‚Äì111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Statistics",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License. If you‚Äôd like to give back, please consider reporting a typo or leaving a pull request at github.com/chenghanyustats/introstatsbook."
  },
  {
    "objectID": "intro-stats.html",
    "href": "intro-stats.html",
    "title": "1¬† Science of Data and Data Science",
    "section": "",
    "text": "In ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records. as shown in Figure¬†1.1\nInterestingly someone defines statistics as the only field where two experts, using identical data, may come to completely opposite conclusions Figure¬†1.2, which is true in some sense. And we‚Äôll see why later in this course. With the same data, different statistical methods may produce different results and lead to difference conclusions."
  },
  {
    "objectID": "intro-stats.html#the-r-user-interface",
    "href": "intro-stats.html#the-r-user-interface",
    "title": "1¬† What is Statistics",
    "section": "1.1 The R User Interface",
    "text": "1.1 The R User Interface\nBefore you can ask your computer to save some numbers, you‚Äôll need to know how to talk to it. That‚Äôs where R and RStudio come in. RStudio gives you a way to talk to your computer. R gives you a language to speak in. To get started, open RStudio just as you would open any other application on your computer. When you do, a window should appear in your screen like the one shown in ?fig-console.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you do not yet have R and RStudio installed on your computer‚Äìor do not know what I am talking about‚Äìvisit Appendix A. The appendix will give you an overview of the two free tools and tell you how to download them.\n\n\nThe RStudio interface is simple. You type R code into the bottom line of the RStudio console pane and then click Enter to run it. The code you type is called a command, because it will command your computer to do something for you. The line you type it into is called the command line.\nWhen you type a command at the prompt and hit Enter, your computer executes the command and shows you the results. Then RStudio displays a fresh prompt for your next command. For example, if you type 1 + 1 and hit Enter, RStudio will display:\n> 1 + 1\n[1] 2\n>\nYou‚Äôll notice that a [1] appears next to your result. R is just letting you know that this line begins with the first value in your result. Some commands return more than one value, and their results may fill up multiple lines. For example, the command 100:130 returns 31 values; it creates a sequence of integers from 100 to 130. Notice that new bracketed numbers appear at the start of the second and third lines of output. These numbers just mean that the second line begins with the 14th value in the result, and the third line begins with the 25th value. You can mostly ignore the numbers that appear in brackets:\n> 100:130\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n[14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n[25] 126 127 128 129 130\n\n\n\n\n\n\nTip\n\n\n\nThe colon operator (:) returns every integer between two integers. It is an easy way to create a sequence of numbers.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may hear me speak of R in the third person. For example, I might say, ‚ÄúTell R to do this‚Äù or ‚ÄúTell R to do that‚Äù, but of course R can‚Äôt do anything; it is just a language. This way of speaking is shorthand for saying, ‚ÄúTell your computer to do this by writing a command in the R language at the command line of your RStudio console.‚Äù Your computer, and not R, does the actual work.\nIs this shorthand confusing and slightly lazy to use? Yes. Do a lot of people use it? Everyone I know‚Äìprobably because it is so convenient.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn some languages, like C, Java, and FORTRAN, you have to compile your human-readable code into machine-readable code (often 1s and 0s) before you can run it. If you‚Äôve programmed in such a language before, you may wonder whether you have to compile your R code before you can use it. The answer is no. R is a dynamic programming language, which means R automatically interprets your code as you run it.\n\n\nIf you type an incomplete command and press Enter, R will display a + prompt, which means R is waiting for you to type the rest of your command. Either finish the command or hit Escape to start over:\n> 5 -\n+\n+ 1\n[1] 4\nIf you type a command that R doesn‚Äôt recognize, R will return an error message. If you ever see an error message, don‚Äôt panic. R is just telling you that your computer couldn‚Äôt understand or do what you asked it to do. You can then try a different command at the next prompt:\n> 3 % 5\nError: unexpected input in \"3 % 5\"\n>\nOnce you get the hang of the command line, you can easily do anything in R that you would do with a calculator. For example, you could do some basic arithmetic:\n2 * 3   \n## 6\n\n4 - 1   \n## 3\n\n6 / (4 - 1)   \n## 2\nDid you notice something different about this code? I‚Äôve left out the >‚Äôs and [1]‚Äôs. This will make the code easier to copy and paste if you want to put it in your own console.\nR treats the hashtag character, #, in a special way; R will not run anything that follows a hashtag on a line. This makes hashtags very useful for adding comments and annotations to your code. Humans will be able to read the comments, but your computer will pass over them. The hashtag is known as the commenting symbol in R.\nFor the remainder of the book, I‚Äôll use hashtags to display the output of R code. I‚Äôll use a single hashtag to add my own comments and a double hashtag, ##, to display the results of code. I‚Äôll avoid showing >s and [1]s unless I want you to look at them.\n\n\n\n\n\n\nImportant\n\n\n\nSome R commands may take a long time to run. You can cancel a command once it has begun by pressing ctrl + c.¬†Note that it may also take R a long time to cancel the command.\n\n\n\n\n\n\n\n\nExercise: Magic with Numbers\n\n\n\nThat‚Äôs the basic interface for executing R code in RStudio. Think you have it? If so, try doing these simple tasks. If you execute everything correctly, you should end up with the same number that you started with:\n\nChoose any number and add 2 to it.\nMultiply the result by 3.\nSubtract 6 from the answer.\nDivide what you get by 3.\n\n\n\nThroughout the book, I‚Äôll put exercises in chunks, like the one above. I‚Äôll follow each exercise with a model answer, like the one below.\nYou could start with the number 10, and then do the following steps:\n10 + 2\n## 12\n\n12 * 3\n## 36\n\n36 - 6\n## 30\n\n30 / 3\n## 10"
  },
  {
    "objectID": "intro-data.html",
    "href": "intro-data.html",
    "title": "2¬† Data",
    "section": "",
    "text": "Data: A set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\n\n\n\n\n\n\n\n\n\n\n\nAll right. Statistics is a Science of Data, so What is Data?\nLet‚Äôs define Data.\nA data set is a set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\nFor example, the data set right here is a set of Marquette basketball players.\nSo objects are individuals or players in the data.\nAnd each player has several characteristics or attributes shown in columns associated with him.\nFor example, his #, class, position, height, weight, hometown, and high school.\nThese characteristics are called variables because they vary form one to another. Clear?"
  },
  {
    "objectID": "intro-data.html#the-r-user-interface",
    "href": "intro-data.html#the-r-user-interface",
    "title": "2¬† Data",
    "section": "2.1 The R User Interface",
    "text": "2.1 The R User Interface"
  },
  {
    "objectID": "intro-r.html",
    "href": "intro-r.html",
    "title": "3¬† Tool foR Data",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away."
  },
  {
    "objectID": "intro-r.html#the-r-user-interface",
    "href": "intro-r.html#the-r-user-interface",
    "title": "3¬† Tool foR Data",
    "section": "3.2 The R User Interface",
    "text": "3.2 The R User Interface\n\nRStudio IDE includes\n\na viewable environment, a file browser, data viewer, and a plotting pane. üëç\nalso features integrated help, syntax highlighting, context-aware tab completion and more! üòÑ\n\n\n\n\n\nR\n\n\nR Studio"
  },
  {
    "objectID": "data-graphics.html",
    "href": "data-graphics.html",
    "title": "4¬† Data Visualization",
    "section": "",
    "text": "Descriptive Statistics"
  },
  {
    "objectID": "data-graphics.html#the-r-user-interface",
    "href": "data-graphics.html#the-r-user-interface",
    "title": "4¬† Data Visualization",
    "section": "4.1 The R User Interface",
    "text": "4.1 The R User Interface"
  },
  {
    "objectID": "data-numerics.html",
    "href": "data-numerics.html",
    "title": "5¬† Data Sample Statistics",
    "section": "",
    "text": "Numerical Summaries of Data"
  },
  {
    "objectID": "data-numerics.html#the-r-user-interface",
    "href": "data-numerics.html#the-r-user-interface",
    "title": "5¬† Data Sample Statistics",
    "section": "5.1 The R User Interface",
    "text": "5.1 The R User Interface"
  },
  {
    "objectID": "datasummary.html",
    "href": "datasummary.html",
    "title": "Summarizing Data",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "datasummary.html#the-r-user-interface",
    "href": "datasummary.html#the-r-user-interface",
    "title": "Summarizing Data",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "intro-stats.html#objects",
    "href": "intro-stats.html#objects",
    "title": "1¬† What is Statistics",
    "section": "1.2 Objects",
    "text": "1.2 Objects\nNow that you know how to use R, let‚Äôs use it to make a virtual die. The : operator from a couple of pages ago gives you a nice way to create a group of numbers from one to six. The : operator returns its results as a vector, a one-dimensional set of numbers:\n1:6\n## 1 2 3 4 5 6\nThat‚Äôs all there is to how a virtual die looks! But you are not done yet. Running 1:6 generated a vector of numbers for you to see, but it didn‚Äôt save that vector anywhere in your computer‚Äôs memory. What you are looking at is basically the footprints of six numbers that existed briefly and then melted back into your computer‚Äôs RAM. If you want to use those numbers again, you‚Äôll have to ask your computer to save them somewhere. You can do that by creating an R object.\nR lets you save data by storing it inside an R object. What is an object? Just a name that you can use to call up stored data. For example, you can save data into an object like a or b. Wherever R encounters the object, it will replace it with the data saved inside, like so:\na <- 1\na\n## 1\n\na + 2\n## 3\n\n\n\n\n\n\nNote\n\n\n\n\nTo create an R object, choose a name and then use the less-than symbol, <, followed by a minus sign, -, to save data into it. This combination looks like an arrow, <-. R will make an object, give it your name, and store in it whatever follows the arrow. So a <- 1 stores 1 in an object named a.\nWhen you ask R what‚Äôs in a, R tells you on the next line.\nYou can use your object in new R commands, too. Since a previously stored the value of 1, you‚Äôre now adding 1 to 2.\n\n\n\nSo, for another example, the following code would create an object named die that contains the numbers one through six. To see what is stored in an object, just type the object‚Äôs name by itself:\ndie <- 1:6\n\ndie\n## 1 2 3 4 5 6\nWhen you create an object, the object will appear in the environment pane of RStudio, as shown in ?fig-environment. This pane will show you all of the objects you‚Äôve created since opening RStudio.\n\nYou can name an object in R almost anything you want, but there are a few rules. First, a name cannot start with a number. Second, a name cannot use some special symbols, like ^, !, $, @, +, -, /, or *:\n\n\n\nGood names\nNames that cause errors\n\n\n\n\na\n1trial\n\n\nb\n$\n\n\nFOO\n^mean\n\n\nmy_var\n2nd\n\n\n.day\n!bad\n\n\n\n\n\n\n\n\n\nCapitalization\n\n\n\nR is case-sensitive, so name and Name will refer to different objects:\nName <- 1\nname <- 0  \n  \nName + 1  \n## 2  \n\n\nFinally, R will overwrite any previous information stored in an object without asking you for permission. So, it is a good idea to not use names that are already taken:\nmy_number <- 1\nmy_number \n## 1\n\nmy_number <- 999\nmy_number\n## 999\nYou can see which object names you have already used with the function ls:\nls()\n## \"a\"         \"die\"       \"my_number\" \"name\"     \"Name\"     \nYou can also see which names you have used by examining RStudio‚Äôs environment pane.\nYou now have a virtual die that is stored in your computer‚Äôs memory. You can access it whenever you like by typing the word die. So what can you do with this die? Quite a lot. R will replace an object with its contents whenever the object‚Äôs name appears in a command. So, for example, you can do all sorts of math with the die. Math isn‚Äôt so helpful for rolling dice, but manipulating sets of numbers will be your stock and trade as a data scientist. So let‚Äôs take a look at how to do that:\ndie - 1\n## 0 1 2 3 4 5\n\ndie / 2\n## 0.5 1.0 1.5 2.0 2.5 3.0\n\ndie * die\n## 1  4  9 16 25 36\nIf you are a big fan of linear algebra (and who isn‚Äôt?), you may notice that R does not always follow the rules of matrix multiplication. Instead, R uses element-wise execution. When you manipulate a set of numbers, R will apply the same operation to each element in the set. So for example, when you run die - 1, R subtracts one from each element of die.\nWhen you use two or more vectors in an operation, R will line up the vectors and perform a sequence of individual operations. For example, when you run die * die, R lines up the two die vectors and then multiplies the first element of vector 1 by the first element of vector 2. R then multiplies the second element of vector 1 by the second element of vector 2, and so on, until every element has been multiplied. The result will be a new vector the same length as the first two, as shown in ?fig-elementwise.\n\nIf you give R two vectors of unequal lengths, R will repeat the shorter vector until it is as long as the longer vector, and then do the math, as shown in Figure¬†1.4. This isn‚Äôt a permanent change‚Äìthe shorter vector will be its original size after R does the math. If the length of the short vector does not divide evenly into the length of the long vector, R will return a warning message. This behavior is known as vector recycling, and it helps R do element-wise operations:\n1:2\n## 1 2\n\n1:4\n## 1 2 3 4\n\ndie\n## 1 2 3 4 5 6\n\ndie + 1:2\n## 2 4 4 6 6 8\n\ndie + 1:4\n## 2 4 6 8 6 8\nWarning message:\nIn die + 1:4 :\n  longer object length is not a multiple of shorter object length\n\n\n\nFigure 1.4: R will repeat a short vector to do element-wise operations with two vectors of uneven lengths.\n\n\nElement-wise operations are a very useful feature in R because they manipulate groups of values in an orderly way. When you start working with data sets, element-wise operations will ensure that values from one observation or case are only paired with values from the same observation or case. Element-wise operations also make it easier to write your own programs and functions in R.\nBut don‚Äôt think that R has given up on traditional matrix multiplication. You just have to ask for it when you want it. You can do inner multiplication with the %*% operator and outer multiplication with the %o% operator:\ndie %*% die\n## 91\n\ndie %o% die\n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]    1    2    3    4    5    6\n## [2,]    2    4    6    8   10   12\n## [3,]    3    6    9   12   15   18\n## [4,]    4    8   12   16   20   24\n## [5,]    5   10   15   20   25   30\n## [6,]    6   12   18   24   30   36\nYou can also do things like transpose a matrix with t and take its determinant with det.\nDon‚Äôt worry if you‚Äôre not familiar with these operations. They are easy to look up, and you won‚Äôt need them for this book.\nNow that you can do math with your die object, let‚Äôs look at how you could ‚Äúroll‚Äù it. Rolling your die will require something more sophisticated than basic arithmetic; you‚Äôll need to randomly select one of the die‚Äôs values. And for that, you will need a function."
  },
  {
    "objectID": "intro-stats.html#functions",
    "href": "intro-stats.html#functions",
    "title": "1¬† What is Statistics",
    "section": "1.3 Functions",
    "text": "1.3 Functions\nR comes with many functions that you can use to do sophisticated tasks like random sampling. For example, you can round a number with the round function, or calculate its factorial with the factorial function. Using a function is pretty simple. Just write the name of the function and then the data you want the function to operate on in parentheses:\nround(3.1415)\n## 3\n\nfactorial(3)\n## 6\nThe data that you pass into the function is called the function‚Äôs argument. The argument can be raw data, an R object, or even the results of another R function. In this last case, R will work from the innermost function to the outermost, as in ?fig-pemdas.\nmean(1:6)\n## 3.5\n\nmean(die)\n## 3.5\n\nround(mean(die))\n## 4\n\nLucky for us, there is an R function that can help ‚Äúroll‚Äù the die. You can simulate a roll of the die with R‚Äôs sample function. sample takes two arguments: a vector named x and a number named size. sample will return size elements from the vector:\nsample(x = 1:4, size = 2)\n## 3 2\nTo roll your die and get a number back, set x to die and sample one element from it. You‚Äôll get a new (maybe different) number each time you roll it:\nsample(x = die, size = 1)\n## 2\n\nsample(x = die, size = 1)\n## 1\n\nsample(x = die, size = 1)\n## 6\nMany R functions take multiple arguments that help them do their job. You can give a function as many arguments as you like as long as you separate each argument with a comma.\nYou may have noticed that I set die and 1 equal to the names of the arguments in sample, x and size. Every argument in every R function has a name. You can specify which data should be assigned to which argument by setting a name equal to data, as in the preceding code. This becomes important as you begin to pass multiple arguments to the same function; names help you avoid passing the wrong data to the wrong argument. However, using names is optional. You will notice that R users do not often use the name of the first argument in a function. So you might see the previous code written as:\nsample(die, size = 1)\n## 2\nOften, the name of the first argument is not very descriptive, and it is usually obvious what the first piece of data refers to anyways.\nBut how do you know which argument names to use? If you try to use a name that a function does not expect, you will likely get an error:\nround(3.1415, corners = 2)\n## Error in round(3.1415, corners = 2) : unused argument(s) (corners = 2)\nIf you‚Äôre not sure which names to use with a function, you can look up the function‚Äôs arguments with args. To do this, place the name of the function in the parentheses behind args. For example, you can see that the round function takes two arguments, one named x and one named digits:\nargs(round)\n## function (x, digits = 0) \n## NULL\nDid you notice that args shows that the digits argument of round is already set to 0? Frequently, an R function will take optional arguments like digits. These arguments are considered optional because they come with a default value. You can pass a new value to an optional argument if you want, and R will use the default value if you do not. For example, round will round your number to 0 digits past the decimal point by default. To override the default, supply your own value for digits:\nround(3.1415)\n## 3\n\nround(3.1415, digits = 2)\n## 3.14\nYou should write out the names of each argument after the first one or two when you call a function with multiple arguments. Why? First, this will help you and others understand your code. It is usually obvious which argument your first input refers to (and sometimes the second input as well). However, you‚Äôd need a large memory to remember the third and fourth arguments of every R function. Second, and more importantly, writing out argument names prevents errors.\nIf you do not write out the names of your arguments, R will match your values to the arguments in your function by order. For example, in the following code, the first value, die, will be matched to the first argument of sample, which is named x. The next value, 1, will be matched to the next argument, size:\nsample(die, 1)\n## 2\nAs you provide more arguments, it becomes more likely that your order and R‚Äôs order may not align. As a result, values may get passed to the wrong argument. Argument names prevent this. R will always match a value to its argument name, no matter where it appears in the order of arguments:\nsample(size = 1, x = die)\n## 2\n\n1.3.1 Sample with Replacement\nIf you set size = 2, you can almost simulate a pair of dice. Before we run that code, think for a minute why that might be the case. sample will return two numbers, one for each die:\nsample(die, size = 2)\n## 3 4\nI said this ‚Äúalmost‚Äù works because this method does something funny. If you use it many times, you‚Äôll notice that the second die never has the same value as the first die, which means you‚Äôll never roll something like a pair of threes or snake eyes. What is going on?\nBy default, sample builds a sample without replacement. To see what this means, imagine that sample places all of the values of die in a jar or urn. Then imagine that sample reaches into the jar and pulls out values one by one to build its sample. Once a value has been drawn from the jar, sample sets it aside. The value doesn‚Äôt go back into the jar, so it cannot be drawn again. So if sample selects a six on its first draw, it will not be able to select a six on the second draw; six is no longer in the jar to be selected. Although sample creates its sample electronically, it follows this seemingly physical behavior.\nOne side effect of this behavior is that each draw depends on the draws that come before it. In the real world, however, when you roll a pair of dice, each die is independent of the other. If the first die comes up six, it does not prevent the second die from coming up six. In fact, it doesn‚Äôt influence the second die in any way whatsoever. You can recreate this behavior in sample by adding the argument replace = TRUE:\nsample(die, size = 2, replace = TRUE)\n## 5 5\nThe argument replace = TRUE causes sample to sample with replacement. Our jar example provides a good way to understand the difference between sampling with replacement and without. When sample uses replacement, it draws a value from the jar and records the value. Then it puts the value back into the jar. In other words, sample replaces each value after each draw. As a result, sample may select the same value on the second draw. Each value has a chance of being selected each time. It is as if every draw were the first draw.\nSampling with replacement is an easy way to create independent random samples. Each value in your sample will be a sample of size one that is independent of the other values. This is the correct way to simulate a pair of dice:\nsample(die, size = 2, replace = TRUE)\n## 2 4\nCongratulate yourself; you‚Äôve just run your first simulation in R! You now have a method for simulating the result of rolling a pair of dice. If you want to add up the dice, you can feed your result straight into the sum function:\ndice <- sample(die, size = 2, replace = TRUE)\ndice\n## 2 4\n\nsum(dice)\n## 6\nWhat would happen if you call dice multiple times? Would R generate a new pair of dice values each time? Let‚Äôs give it a try:\ndice\n## 2 4\n\ndice\n## 2 4\n\ndice\n## 2 4\nNope. Each time you call dice, R will show you the result of that one time you called sample and saved the output to dice. R won‚Äôt rerun sample(die, 2, replace = TRUE) to create a new roll of the dice. This is a relief in a way. Once you save a set of results to an R object, those results do not change. Programming would be quite hard if the values of your objects changed each time you called them.\nHowever, it would be convenient to have an object that can re-roll the dice whenever you call it. You can make such an object by writing your own R function."
  },
  {
    "objectID": "intro-stats.html#sec-write-functions",
    "href": "intro-stats.html#sec-write-functions",
    "title": "1¬† What is Statistics",
    "section": "1.4 Writing Your Own Functions",
    "text": "1.4 Writing Your Own Functions\nTo recap, you already have working R code that simulates rolling a pair of dice:\ndie <- 1:6\ndice <- sample(die, size = 2, replace = TRUE)\nsum(dice)\nYou can retype this code into the console anytime you want to re-roll your dice. However, this is an awkward way to work with the code. It would be easier to use your code if you wrapped it into its own function, which is exactly what we‚Äôll do now. We‚Äôre going to write a function named roll that you can use to roll your virtual dice. When you‚Äôre finished, the function will work like this: each time you call roll(), R will return the sum of rolling two dice:\nroll()\n## 8 \n\nroll()\n## 3\n\nroll()\n## 7\nFunctions may seem mysterious or fancy, but they are just another type of R object. Instead of containing data, they contain code. This code is stored in a special format that makes it easy to reuse the code in new situations. You can write your own functions by recreating this format.\n\n1.4.1 The Function Constructor\nEvery function in R has three basic parts: a name, a body of code, and a set of arguments. To make your own function, you need to replicate these parts and store them in an R object, which you can do with the function function. To do this, call function() and follow it with a pair of braces, {}:\nmy_function <- function() {}\nfunction will build a function out of whatever R code you place between the braces. For example, you can turn your dice code into a function by calling:\nroll <- function() {\n  die <- 1:6\n  dice <- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\n\n\n\n\n\nNote\n\n\n\nNotice that I indented each line of code between the braces. This makes the code easier for you and me to read but has no impact on how the code runs. R ignores spaces and line breaks and executes one complete expression at a time.\n\n\nJust hit the Enter key between each line after the first brace, {. R will wait for you to type the last brace, }, before it responds.\nDon‚Äôt forget to save the output of function to an R object. This object will become your new function. To use it, write the object‚Äôs name followed by an open and closed parenthesis:\nroll()\n## 9\nYou can think of the parentheses as the ‚Äútrigger‚Äù that causes R to run the function. If you type in a function‚Äôs name without the parentheses, R will show you the code that is stored inside the function. If you type in the name with the parentheses, R will run that code:\nroll\n## function() {\n##   die <- 1:6\n##   dice <- sample(die, size = 2, replace = TRUE)\n##   sum(dice)\n## }\n\nroll()\n## 6\nThe code that you place inside your function is known as the body of the function. When you run a function in R, R will execute all of the code in the body and then return the result of the last line of code. If the last line of code doesn‚Äôt return a value, neither will your function, so you want to ensure that your final line of code returns a value. One way to check this is to think about what would happen if you ran the body of code line by line in the command line. Would R display a result after the last line, or would it not?\nHere‚Äôs some code that would display a result:\ndice\n1 + 1\nsqrt(2)\nAnd here‚Äôs some code that would not:\ndice <- sample(die, size = 2, replace = TRUE)\ntwo <- 1 + 1\na <- sqrt(2)\nDo you notice the pattern? These lines of code do not return a value to the command line; they save a value to an object."
  },
  {
    "objectID": "intro-stats.html#arguments",
    "href": "intro-stats.html#arguments",
    "title": "1¬† What is Statistics",
    "section": "1.5 Arguments",
    "text": "1.5 Arguments\nWhat if we removed one line of code from our function and changed the name die to bones, like this?\nroll2 <- function() {\n  dice <- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow I‚Äôll get an error when I run the function. The function needs the object bones to do its job, but there is no object named bones to be found:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   object 'bones' not found\nYou can supply bones when you call roll2 if you make bones an argument of the function. To do this, put the name bones in the parentheses that follow function when you define roll2:\nroll2 <- function(bones) {\n  dice <- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow roll2 will work as long as you supply bones when you call the function. You can take advantage of this to roll different types of dice each time you call roll2. Dungeons and Dragons, here we come!\nRemember, we‚Äôre rolling pairs of dice:\nroll2(bones = 1:4)\n##  3\n\nroll2(bones = 1:6)\n## 10\n\nroll2(1:20)\n## 31\nNotice that roll2 will still give an error if you do not supply a value for the bones argument when you call roll2:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   argument \"bones\" is missing, with no default\nYou can prevent this error by giving the bones argument a default value. To do this, set bones equal to a value when you define roll2:\nroll2 <- function(bones = 1:6) {\n  dice <- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow you can supply a new value for bones if you like, and roll2 will use the default if you do not:\nroll2()\n## 9\nYou can give your functions as many arguments as you like. Just list their names, separated by commas, in the parentheses that follow function. When the function is run, R will replace each argument name in the function body with the value that the user supplies for the argument. If the user does not supply a value, R will replace the argument name with the argument‚Äôs default value (if you defined one).\nTo summarize, function helps you construct your own R functions. You create a body of code for your function to run by writing code between the braces that follow function. You create arguments for your function to use by supplying their names in the parentheses that follow function. Finally, you give your function a name by saving its output to an R object, as shown in ?fig-functions.\nOnce you‚Äôve created your function, R will treat it like every other function in R. Think about how useful this is. Have you ever tried to create a new Excel option and add it to Microsoft‚Äôs menu bar? Or a new slide animation and add it to Powerpoint‚Äôs options? When you work with a programming language, you can do these types of things. As you learn to program in R, you will be able to create new, customized, reproducible tools for yourself whenever you like. Project 3: Slot Machine will teach you much more about writing functions in R."
  },
  {
    "objectID": "intro-stats.html#scripts",
    "href": "intro-stats.html#scripts",
    "title": "1¬† What is Statistics",
    "section": "1.6 Scripts",
    "text": "1.6 Scripts\nWhat if you want to edit roll2 again? You could go back and retype each line of code in roll2, but it would be so much easier if you had a draft of the code to start from. You can create a draft of your code as you go by using an R script. An R script is just a plain text file that you save R code in. You can open an R script in RStudio by going to File > New File > R script in the menu bar. RStudio will then open a fresh script above your console pane, as shown in ?fig-script.\nI strongly encourage you to write and edit all of your R code in a script before you run it in the console. Why? This habit creates a reproducible record of your work. When you‚Äôre finished for the day, you can save your script and then use it to rerun your entire analysis the next day. Scripts are also very handy for editing and proofreading your code, and they make a nice copy of your work to share with others. To save a script, click the scripts pane, and then go to File > Save As in the menu bar.\n\nRStudio comes with many built-in features that make it easy to work with scripts. First, you can automatically execute a line of code in a script by clicking the Run button, as shown in ?fig-run.\nR will run whichever line of code your cursor is on. If you have a whole section highlighted, R will run the highlighted code. Alternatively, you can run the entire script by clicking the Source button. Don‚Äôt like clicking buttons? You can use Control + Return as a shortcut for the Run button. On Macs, that would be Command + Return.\n\nIf you‚Äôre not convinced about scripts, you soon will be. It becomes a pain to write multi-line code in the console‚Äôs single-line command line. Let‚Äôs avoid that headache and open your first script now before we move to the next chapter.\n\n\n\n\n\n\nExtract function\n\n\n\nRStudio comes with a tool that can help you build functions. To use it, highlight the lines of code in your R script that you want to turn into a function. Then click Code > Extract Function in the menu bar. RStudio will ask you for a function name to use and then wrap your code in a function call. It will scan the code for undefined variables and use these as arguments.\nYou may want to double-check RStudio‚Äôs work. It assumes that your code is correct, so if it does something surprising, you may have a problem in your code."
  },
  {
    "objectID": "intro-stats.html#summary",
    "href": "intro-stats.html#summary",
    "title": "1¬† What is Statistics",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nYou‚Äôve covered a lot of ground already. You now have a virtual die stored in your computer‚Äôs memory, as well as your own R function that rolls a pair of dice. You‚Äôve also begun speaking the R language.\nAs you‚Äôve seen, R is a language that you can use to talk to your computer. You write commands in R and run them at the command line for your computer to read. Your computer will sometimes talk back‚Äìfor example, when you commit an error‚Äìbut it usually just does what you ask and then displays the result.\nThe two most important components of the R language are objects, which store data, and functions, which manipulate data. R also uses a host of operators like +, -, *, /, and <- to do basic tasks. As a data scientist, you will use R objects to store data in your computer‚Äôs memory, and you will use functions to automate tasks and do complicated calculations. We will examine objects in more depth later in Project 2: Playing Cards and dig further into functions in Project 3: Slot Machine. The vocabulary you have developed here will make each of those projects easier to understand. However, we‚Äôre not done with your dice yet.\nIn Packages and Help Pages, you‚Äôll run some simulations on your dice and build your first graphs in R. You‚Äôll also look at two of the most useful components of the R language: R packages, which are collections of functions writted by R‚Äôs talented community of developers, and R documentation, which is a collection of help pages built into R that explains every function and data set in the language."
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Probability",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "prob.html#the-r-user-interface",
    "href": "prob.html#the-r-user-interface",
    "title": "Probability",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "prob-define.html",
    "href": "prob-define.html",
    "title": "6¬† Definition of Probability",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "prob-define.html#the-r-user-interface",
    "href": "prob-define.html#the-r-user-interface",
    "title": "6¬† Definition of Probability",
    "section": "6.1 The R User Interface",
    "text": "6.1 The R User Interface"
  },
  {
    "objectID": "prob-rule.html",
    "href": "prob-rule.html",
    "title": "7¬† Probability Rules",
    "section": "",
    "text": "Warning: package 'emoji' was built under R version 4.2.2"
  },
  {
    "objectID": "prob-rule.html#the-r-user-interface",
    "href": "prob-rule.html#the-r-user-interface",
    "title": "7¬† Probability Rules",
    "section": "7.1 The R User Interface",
    "text": "7.1 The R User Interface"
  },
  {
    "objectID": "prob-rv.html",
    "href": "prob-rv.html",
    "title": "8¬† Random Variables",
    "section": "",
    "text": "Recap\n[1] Usually in statistics, a capital \\(X\\) represents a random variable and a small \\(x\\) represents a realized value of \\(X\\)."
  },
  {
    "objectID": "prob-rv.html#the-r-user-interface",
    "href": "prob-rv.html#the-r-user-interface",
    "title": "8¬† Random Variables",
    "section": "8.1 The R User Interface",
    "text": "8.1 The R User Interface"
  },
  {
    "objectID": "prob-disc.html",
    "href": "prob-disc.html",
    "title": "9¬† Discrete Probability Distributions",
    "section": "",
    "text": "Example:ü™ôü™ô Toss a fair coin twice independently and \\(X\\) is the number of heads.\nGraph\nMean\nVariance  - Suppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\). - The variance of \\(X\\), denoted by \\(Var(X)\\) or \\(\\sigma^2\\), is \\[\\small Var(X) := (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) = \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i)\\] - The standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance."
  },
  {
    "objectID": "prob-disc.html#the-r-user-interface",
    "href": "prob-disc.html#the-r-user-interface",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.1 The R User Interface",
    "text": "9.1 The R User Interface"
  },
  {
    "objectID": "prob-cont.html",
    "href": "prob-cont.html",
    "title": "10¬† Continuous Probability Distributions",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "prob-cont.html#the-r-user-interface",
    "href": "prob-cont.html#the-r-user-interface",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.1 The R User Interface",
    "text": "10.1 The R User Interface"
  },
  {
    "objectID": "prob-samdist.html",
    "href": "prob-samdist.html",
    "title": "11¬† Sampling Distribution",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "prob-samdist.html#the-r-user-interface",
    "href": "prob-samdist.html#the-r-user-interface",
    "title": "11¬† Sampling Distribution",
    "section": "11.1 The R User Interface",
    "text": "11.1 The R User Interface"
  },
  {
    "objectID": "prob-llnclt.html",
    "href": "prob-llnclt.html",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "",
    "text": "We know if \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) , then \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\).\n\n But what if the population distribution is NOT normal? \n\n\n\nThe central limit theorem (CLT) gives us the answer!\n\n\n\nCentral Limit Theorem (CLT):\n\nSuppose \\(\\overline{X}\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma < \\infty\\).\nAs \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\) regardless of the distribution from which we are sampling!\n\n\n\n\n\n\n\nFigure¬†12.1: Illustration of Central Limit Theorem (https://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png)\n\n\n\n\n\n Illustration of the Central Limit Theorem \n\n\n\n\n\nFigure¬†12.2: CLT Illustration: A Right-Skewed Distribution\n\n\n\n\n\n\n\n\n\nFigure¬†12.3: CLT Illustration: A U-shaped Distribution\n\n\n\n\n\n Why is the Central Limit Theorem Important? \n\nMany well-developed statistical methods are based on the normal distribution assumption.\nWith the Central Limit Theorem, we can use these methods even if we are sampling from a non-normal distribution or if we have no idea what the population distribution is, provided that the sample size is large enough."
  },
  {
    "objectID": "prob-llnclt.html#the-r-user-interface",
    "href": "prob-llnclt.html#the-r-user-interface",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.1 The R User Interface",
    "text": "12.1 The R User Interface"
  },
  {
    "objectID": "infer.html",
    "href": "infer.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer.html#the-r-user-interface",
    "href": "infer.html#the-r-user-interface",
    "title": "Statistical Inference",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "infer-ci.html",
    "href": "infer-ci.html",
    "title": "13¬† Confidence Interval",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer-ci.html#the-r-user-interface",
    "href": "infer-ci.html#the-r-user-interface",
    "title": "13¬† Confidence Interval",
    "section": "13.1 The R User Interface",
    "text": "13.1 The R User Interface"
  },
  {
    "objectID": "infer-bt.html",
    "href": "infer-bt.html",
    "title": "14¬† Bootstrapping",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer-bt.html#the-r-user-interface",
    "href": "infer-bt.html#the-r-user-interface",
    "title": "14¬† Bootstrapping",
    "section": "14.1 The R User Interface",
    "text": "14.1 The R User Interface"
  },
  {
    "objectID": "infer-ht.html",
    "href": "infer-ht.html",
    "title": "15¬† Hypothesis Testing",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer-ht.html#the-r-user-interface",
    "href": "infer-ht.html#the-r-user-interface",
    "title": "15¬† Hypothesis Testing",
    "section": "15.1 The R User Interface",
    "text": "15.1 The R User Interface"
  },
  {
    "objectID": "infer-twomean.html",
    "href": "infer-twomean.html",
    "title": "16¬† Comparing Two Population Means",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer-twomean.html#the-r-user-interface",
    "href": "infer-twomean.html#the-r-user-interface",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.1 The R User Interface",
    "text": "16.1 The R User Interface"
  },
  {
    "objectID": "infer-var.html",
    "href": "infer-var.html",
    "title": "17¬† Inference About Variances",
    "section": "",
    "text": "##Inference for One Population Variance\nWhy Inference for Population Variances?"
  },
  {
    "objectID": "infer-var.html#the-r-user-interface",
    "href": "infer-var.html#the-r-user-interface",
    "title": "17¬† Inference About Variances",
    "section": "17.1 The R User Interface",
    "text": "17.1 The R User Interface"
  },
  {
    "objectID": "infer-prop.html",
    "href": "infer-prop.html",
    "title": "18¬† Inference About Proportions",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer-prop.html#the-r-user-interface",
    "href": "infer-prop.html#the-r-user-interface",
    "title": "18¬† Inference About Proportions",
    "section": "18.1 The R User Interface",
    "text": "18.1 The R User Interface"
  },
  {
    "objectID": "infer-goodnessfit.html",
    "href": "infer-goodnessfit.html",
    "title": "19¬† Inference about Categorical Data",
    "section": "",
    "text": "Categorical Variable with More Than 2 Categories \n\n\n\nA categorical variable has \\(k\\) categories \\(A_1, \\dots, A_k\\).\n\n\n\n\nSubject\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(A_k\\)\n\n\n\n\n1\nx\n\n\n\n\n\n\n2\n\nx\n\n\n\n\n\n3\n\n\n\n\nx\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\\(n\\)\n\n\n\nx\n\n\n\n\n\n\n\n\nWith the size \\(n\\), for categories \\(A_1, \\dots , A_k\\), their observed count is \\(O_1, \\dots, O_k\\), and \\(\\sum_{i=1}^kO_i = n\\).\nOne-way count table:\n\n\n\n\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(A_k\\)\nTotal\n\n\n\n\n\\(O_1\\)\n\\(O_2\\)\n\\(\\cdots\\)\n\\(O_k\\)\n\\(n\\)\n\n\n\n\n\n Example \n\n\n\nAre the selected jurors racially representative of the population?\nIf the jury is representative of the population, the proportions in the sample should reflect the proportions of the population of eligible jurors (i.e.¬†registered voters).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nRepresentation in juries\n205\n26\n25\n19\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nRepresentation in juries\n0.745\n0.095\n0.091\n0.069\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\n\n\nAre the proportions of juries close enough to the proportions of registered voters, so that we are confident saying that the jurors really were randomly sampled from the registered voters?\n\n\n\n\n\n\n\n Goodness-of-Fit Test \n\nA goodness-of-fit test tests the hypothesis that the observed frequency distribution fits or conforms to some claim distribution.\n\n\n\n\n\n\n\nIn the jury example, what is our observed frequency distribution, and what is our claim distribution?\n\n\n\n\n\n\n\n\n\n\n\n\nIf the individuals are randomly selected to serve on a jury, about how many of the 275 people would we expect to be white? How about black?\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\nAbout \\(72\\%\\) of the population is white, so we would expect about \\(72\\%\\) of the jurors to be white.\n\n\\(0.72 \\times 275 = 198\\).\n\nWe expect about \\(7\\%\\) of the jurors to be black.\n\nThis corresponds to about \\(0.07 \\times 275 = 19.25\\) black jurors.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n205\n26\n25\n19\n\n\nExpected Count\n198\n19.25\n33\n24.75\n\n\nPopulation Proportion \\((H_0)\\)\n0.72\n0.07\n0.12\n0.09\n\n\n\n\nThe observed count and expected count will be similar if there was no bias in selecting the members of the jury.\nWe want to test whether the differences are strong enough to provide convincing evidence that the jurors were not selected from a random sample of all registered voters.\n\n Example \n\n \\(\\begin{align} &H_0: \\text{No racial bias in who serves on a jury, and } \\\\ &H_1: \\text{There is racial bias in juror selection} \\end{align}\\) \n \\(\\begin{align} &H_0: \\pi_1 = \\pi_1^0, \\pi_2 = \\pi_2^0, \\dots, \\pi_k = \\pi_k^0\\\\ &H_1: \\pi_i \\ne \\pi_i^0 \\text{ for some } i \\end{align}\\) \n Under \\(H_0\\), \\(\\chi^2_{test} = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} + \\cdots + \\frac{(O_k - E_k)^2}{E_k}\\), \\(E_i = n\\pi_i^0, i = 1, \\dots, k\\) \nReject \\(H_0\\) if  \\(\\chi^2_{test} > \\chi^2_{\\alpha, df}\\), \\(df = k-1\\) \nRequire each \\(E_i \\ge 5\\), \\(i = 1, \\dots, k\\).\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\n\nProportion under \\(H_0\\)\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\n\n\n\nUnder \\(H_0\\), \\(\\chi^2_{test} = \\frac{(205 - 198)^2}{198} + \\frac{(26 - 19.25)^2}{19.25} + \\frac{(25 - 33)^2}{33} + \\frac{(19 - 24.75)^2}{24.75} = 5.89\\)\n\\(\\chi^2_{0.05, 3} = 7.81\\).\nBecause \\(5.89 < 7.81\\), we fail to reject \\(H_0\\) in favor of \\(H_1\\).\nWe do not have convincing evidence of racial bias in the juror selection process.\n\n Goodness-of-Fit Test in R \n\nBelow is an example of how to perform a Goodness-of-Fit test in R.\n\n\nobs <- c(205, 26, 25, 19)\npi_0 <- c(0.72, 0.07, 0.12, 0.09)\n\n## Use chisq.test() function\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 5.8896, df = 3, p-value = 0.1171"
  },
  {
    "objectID": "infer-goodnessfit.html#the-r-user-interface",
    "href": "infer-goodnessfit.html#the-r-user-interface",
    "title": "19¬† Test of Goodness of Fit",
    "section": "19.1 The R User Interface",
    "text": "19.1 The R User Interface"
  },
  {
    "objectID": "infer-indep.html",
    "href": "infer-indep.html",
    "title": "20¬† Test of Independence",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer-indep.html#the-r-user-interface",
    "href": "infer-indep.html#the-r-user-interface",
    "title": "20¬† Test of Independence",
    "section": "20.1 The R User Interface",
    "text": "20.1 The R User Interface"
  },
  {
    "objectID": "infer-bayes.html",
    "href": "infer-bayes.html",
    "title": "21¬† Bayesian Inference",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer-bayes.html#the-r-user-interface",
    "href": "infer-bayes.html#the-r-user-interface",
    "title": "21¬† Bayesian Inference",
    "section": "21.1 The R User Interface",
    "text": "21.1 The R User Interface"
  },
  {
    "objectID": "infer-nonpar.html",
    "href": "infer-nonpar.html",
    "title": "22¬† Nonparametric Inference",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "infer-nonpar.html#the-r-user-interface",
    "href": "infer-nonpar.html#the-r-user-interface",
    "title": "22¬† Nonparametric Inference",
    "section": "22.1 The R User Interface",
    "text": "22.1 The R User Interface"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Statistical Models",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "model.html#the-r-user-interface",
    "href": "model.html#the-r-user-interface",
    "title": "Statistical Models",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "model-anova.html",
    "href": "model-anova.html",
    "title": "23¬† Analysis of Variance",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "model-anova.html#the-r-user-interface",
    "href": "model-anova.html#the-r-user-interface",
    "title": "23¬† Analysis of Variance",
    "section": "23.1 The R User Interface",
    "text": "23.1 The R User Interface"
  },
  {
    "objectID": "model-reg.html",
    "href": "model-reg.html",
    "title": "24¬† Linear Regression",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "model-reg.html#the-r-user-interface",
    "href": "model-reg.html#the-r-user-interface",
    "title": "24¬† Linear Regression",
    "section": "24.1 The R User Interface",
    "text": "24.1 The R User Interface"
  },
  {
    "objectID": "model-logistic.html",
    "href": "model-logistic.html",
    "title": "25¬† Logistic Regression",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "model-logistic.html#the-r-user-interface",
    "href": "model-logistic.html#the-r-user-interface",
    "title": "25¬† Logistic Regression",
    "section": "25.1 The R User Interface",
    "text": "25.1 The R User Interface"
  },
  {
    "objectID": "model-bayes.html",
    "href": "model-bayes.html",
    "title": "26¬† Bayesian Linear Regression",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don‚Äôt worry if you‚Äôve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer‚Äôs memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer‚Äôs memory.\nLet‚Äôs work on saving these numbers first, and then consider a method for ‚Äúrolling‚Äù our die."
  },
  {
    "objectID": "model-bayes.html#the-r-user-interface",
    "href": "model-bayes.html#the-r-user-interface",
    "title": "26¬† Bayesian Linear Regression",
    "section": "26.1 The R User Interface",
    "text": "26.1 The R User Interface"
  },
  {
    "objectID": "model-survival.html",
    "href": "model-survival.html",
    "title": "27¬† Survival Analysis",
    "section": "",
    "text": "Is the Cluster of Deaths Significantly High?"
  },
  {
    "objectID": "model-survival.html#the-r-user-interface",
    "href": "model-survival.html#the-r-user-interface",
    "title": "27¬† Survival Analysis",
    "section": "27.1 The R User Interface",
    "text": "27.1 The R User Interface"
  },
  {
    "objectID": "intro-stats.html#statistics-as-a-discipline",
    "href": "intro-stats.html#statistics-as-a-discipline",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.2 Statistics as a Discipline",
    "text": "1.2 Statistics as a Discipline\n\nForget about that useless definition.\n\n\n\n\n\n\nFigure¬†1.2: Wiki definition of statistics (Source: https://en.wikipedia.org/wiki/Statistics)\n\n\n\n\n\nFrom Wiki, Statistics is formally defined as the discipline that concerns the collection, organization, analysis, interpretation and presentation of data.\nStatistics is a Science of Data.\nA science of data using statistical thinking, methods and models.\nThere might be another science of data. I‚Äôm not saying statistics is THE science of data.\n\n\n\n\n\nü§î But wait, then what is DATA SCIENCE ‚ùì"
  },
  {
    "objectID": "intro-stats.html#difference-between-statistics-and-data-science",
    "href": "intro-stats.html#difference-between-statistics-and-data-science",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.2 Difference between Statistics and Data Science",
    "text": "1.2 Difference between Statistics and Data Science\n\n Data Science \n\nBecause of their shared attributes, many find it hard to differentiate between statistics and data science.\nThe tweets below poke fun at the lack of clarity surrounding the definition of data science/data scientists (Figure¬†1.4).\n\n\n\n\n\n\nFigure¬†1.4: Tweets about what Data Science is\n\n\n\nA more formal definition of data science can be found on Investopedia.\nThis site defines Data Science as a field of applied mathematics and statistics that provides useful information based on large amounts of complex data or big data.\nAlthough this definition is helpful for understanding data science, Dan Ariely, a famous behavioral economist at Duke, joked about their use of the term big data in his tweet below (Figure¬†1.5).\n\n\n\n\nFigure¬†1.5: Professor Ariely on Big Data\n\n\n\nMore information can be gathered about the differences between these two fields from looking at the courses offered in the Statistics Department at UC Santa Cruz.\nFrom Figure¬†1.6 below, one can see that statistics primarily focuses on data analysis, methods and models.\n\n\n\n\n\n\nFigure¬†1.6: Courses offered by the Department of Statistics at UC Santa Cruz (Source: https://courses.soe.ucsc.edu/)\n\n\n\n\n\nThis statistics department, in particular, doesn‚Äôt talk a lot about data collection, organization, data presentation or data visualization.\nAlthough statistics does not focus on these concepts, they are encompassed within the field of data science.\nThe data science process includes the collection, organization, analysis, interpretation and presentation of data (Figure¬†1.7).\n\n\n\n\n\n\nFigure¬†1.7: The data science process created at Harvard by Joe Blitzstein and Hanspeter Pfister\n\n\n\n\n\nIn typical statistics departments, there isn‚Äôt much instruction or research done on data collection, cleaning, storage, database management, and data visualization.\nBecause statistics continues to focus on data analysis and modeling, Data Science now addresses these other processes that statistics passes over."
  },
  {
    "objectID": "intro-stats.html#uc-santa-cruz-department-of-statistics-courses",
    "href": "intro-stats.html#uc-santa-cruz-department-of-statistics-courses",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.2 UC Santa Cruz Department of Statistics Courses",
    "text": "1.2 UC Santa Cruz Department of Statistics Courses\n\n\n\n\n\nFigure 1.6: Course offering of department of statistics at UC Santa Cruz. (Source: https://courses.soe.ucsc.edu/)\n\n\n\n\n\nThis shows statistics courses offered by UC Santa Cruz, the university I graduated from.\nYou can see that statistics focuses much more on data analysis, methods and models.\nThe stats department doesn‚Äôt talk a lot about data collection, organization, data presentation or data visualization."
  },
  {
    "objectID": "intro-stats.html#data-science-is-now-a-broader-view-of-statistics",
    "href": "intro-stats.html#data-science-is-now-a-broader-view-of-statistics",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.2 Data Science Is Now a Broader View of Statistics",
    "text": "1.2 Data Science Is Now a Broader View of Statistics\n\nCollection, organization, analysis, interpretation and presentation of data.\n\n\n\n\n\n\nFigure 1.7: The Data Science process. Created at Harvard by Joe Blitzstein and Hanspeter Pfister.\n\n\n\n\n\nData science is more like a broader view of statistics.\nBecause again, in typical statistics departments, we don‚Äôt really teach or do much research on data collection, cleaning, storage, database management, and data visualization, which all are now a part of DS.\nStatistics focuses very much on data analysis and modeling.\nAnyway, please don‚Äôt worry about the names.\nThe important thing is you learn useful methods to help you analyze your data no matter what it is called, statistics or data science."
  },
  {
    "objectID": "intro-stats.html#what-do-we-learn-in-this-course",
    "href": "intro-stats.html#what-do-we-learn-in-this-course",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.3 What Do We Learn In this Course?",
    "text": "1.3 What Do We Learn In this Course?\n\n\n\n\n\n\n\n\n\n\n\nIn particular, we will spend most of our time talking about probability and statistical inference methods.\nWe will focus on various statistical methods for analyzing data.\nWe will also learn useful information\n\nabout the population we are interested in\nfrom our sample data\nthrough statistical inferential methods, including estimation and testing\n\nEach of these terms will be discussed in detail throughout this textbook."
  },
  {
    "objectID": "intro-stats.html#we-focus-on-statistical-inference",
    "href": "intro-stats.html#we-focus-on-statistical-inference",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.4 We Focus On Statistical Inference",
    "text": "1.4 We Focus On Statistical Inference\n\nWe spend most of time on various statistical methods for analyzing data.\nLearn useful information\n\nabout the population we are interested\nfrom our sample data\nthrough statistical inferential methods, including estimation and testing\n\nDon‚Äôt worry if you have no idea what these terms are.\nThese are what we will discuss throughout the course, and I‚Äôll explain each term in detail later in class."
  },
  {
    "objectID": "intro-data.html#data-matrix",
    "href": "intro-data.html#data-matrix",
    "title": "2¬† Data",
    "section": "2.2 Data Matrix",
    "text": "2.2 Data Matrix\n\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns.\n\n\n\n\n\n\n\n\n\n\n\nAnd we usually store a data set in a matrix form that has rows and columns.\nEach row corresponds to a unique case or observational unit, or the object.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns.\nThe first step in conducting a study is to identify questions to be investigated.\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\nTarget Population: The complete collection of data we‚Äôd like to make inference about.\nWhat is the average GPA of currently enrolled Marquette students?\n\n\n\n\n\n\n\n\n\n\n\nTarget Population: The complete collection of data we‚Äôd like to make inference about.\nSo the population is a set of all objects which we are interested in studying from.\nBecause All Marquette undergrads that are currently enrolled is the complete collection of data we‚Äôd like to make inference about.\nEach currently enrolled Marquette undergrad is an object.\nNote that students who are not currently enrolled or students that are already graduated are not our interest, and they shouldn‚Äôt be a part of target population.\nCan anybody tell me what variable associated with Marquette undergrads is our interest?\nSo average GPA is the variable or population property we like to make inference about."
  },
  {
    "objectID": "intro-data.html#sample-data",
    "href": "intro-data.html#sample-data",
    "title": "2¬† Data",
    "section": "2.3 Sample Data",
    "text": "2.3 Sample Data\n\nSometimes, it‚Äôs possible to collect data of all cases we are interested in.\nMost of the time, it is too expensive to collect data for every case in a population.\nWhat about the average GPA of all students in Illinois? The U.S.? The world? üò± üò± üò±\n\n\n\n\n\n\n\nSampling is our solution to it.\nA sample is a subset of cases selected from a population.\nThe idea is that we are not able to compute the average GPA of a population, but we can collect a sample from that population which has way less objects than the population.\nThen we compute the average GPA of the sample data.\nWe hope the sample average GPA can be close to the population average GPA because the population GPA is our main interest, not the sample GPA.\nTo have sample average GPA close to population GPA, we want the sample to look like the population so that the sample and the population share similar attributes including GPA."
  },
  {
    "objectID": "intro-data.html#statistics-is-a-science-of-data-so-what-is-data",
    "href": "intro-data.html#statistics-is-a-science-of-data-so-what-is-data",
    "title": "2¬† Data",
    "section": "2.1 Statistics is a Science of Data, so What is Data?",
    "text": "2.1 Statistics is a Science of Data, so What is Data?\n\nData: A set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\nFor example, the data set right here is a set of Marquette basketball players.\n\n\n\n\n\n\n\n\n\n\n\nThe objects are individuals or players in the data.\nAnd each player has several characteristics or attributes shown in columns associated with him.\n\nFor example, his #, class, position, height, weight, hometown, and high school.\n\nThese characteristics are called variables because they vary from one to another."
  },
  {
    "objectID": "intro-data.html#good-sample-vs.-bad-sample",
    "href": "intro-data.html#good-sample-vs.-bad-sample",
    "title": "2¬† Data",
    "section": "2.3 Good Sample vs.¬†Bad Sample",
    "text": "2.3 Good Sample vs.¬†Bad Sample\n\n\n\n\n\n\nIs this 4740/5740 class a sample of the target population Marquette students?\n\n\n\n\n\n\n\n\n\n\n\n\nIs this 4740/5740 class a ‚Äúgood‚Äù sample of the target population?\n\n\n\n\n\n\n\nDoes this 4740 class look like the target population?\nThe sample is convenient to be collected, but it is NOT representative of the population.\nYou are mostly STEM majors, and so with a high chance, your average GPA is not the same as the GPA of humanities or business students.\nBiased sample: The average GPA of the class may be far from that of all MU undergrads.\nSample data must be collected in an appropriate way. If not GIGO.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Average GPAs for different majors (https://www.dailycal.org/2018/04/19/integrative-biology-computer-science-sociology-majors-lowest-gpa/)"
  },
  {
    "objectID": "intro-data.html#how-and-why-a-representative-sample",
    "href": "intro-data.html#how-and-why-a-representative-sample",
    "title": "2¬† Data",
    "section": "2.3 How and Why a Representative Sample?",
    "text": "2.3 How and Why a Representative Sample?\n\nWe always seek to randomly select a sample from a population.\nRandom sampling usually give us a representative Sample, as long as the sample size, or the number of objects in the sample, is not too small.\nLots of statistical methods are based on randomness assumption."
  },
  {
    "objectID": "intro-data.html#data-collection",
    "href": "intro-data.html#data-collection",
    "title": "2¬† Data",
    "section": "2.3 Data Collection",
    "text": "2.3 Data Collection\n Two Types of Studies to Collect Sample Data \n\nThere are two types of studies that are used to collect data: observational studies and experimental studies.\nAn observational study is a study in which those collecting the data observe and measure characteristics/variables, but do NOT attempt to modify or intervene with the subjects being studied.\n\n Example: Sample from 1Ô∏è‚É£ the heart disease and 2Ô∏è‚É£ heart disease-free populations and record the fat content of the diets for the two groups. \n\nIn an experimental study, some treatment(s) is applied and then those collecting data proceed to observe its responses or effects on the individuals (experimental units).\n\n Example: Assign volunteers to one of several diets with different levels of dietary fat (treatments) and compare the treatments with respect to the incidence of heart disease after a period of time. \n\n\n\n\n\n\n\n\nObservational or Experimental?\n\n\n\n\nRandomly select 40 males and 40 females to see the difference in blood pressure levels between male and female.\nTest the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo).\n\n\n\n Limitation of Observational Studies: Confounding Variables \n\nA confounder is a variable NOT included in a study that affects the variables in the study.\nFor example, a person observes past data that shows that increases in ice cream sales are associated with increases in drownings and concludes that eating ice cream causes drownings. üò±üòï‚ÅâÔ∏è\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the confounder that is not in the data but affects ice cream sales and the number of drownings?\n\n\n\n\nTemperature\n\n\n\n\nAs temperature increases, ice cream sales increase and the number of drownings also rises because more people go swimming (Figure¬†2.6).\n\n Causal Relationships \n\nMaking causal conclusions based on experimental data is often more reasonable than making the same causal conclusions based on observational data.\nObservational studies are generally only sufficient to show associations, not causality.\n\n\n\n\n\n\nFigure¬†2.6: Temperature acting as a confounder\n\n\n\n\n\n Types of Random Samples \n\nAs previously mentioned, many statistical methods are based on the randomness assumption.\nTherefore, it‚Äôs important to understand what a random sample is and how to collect it.\nIn a random sample, each member of a population is equally likely to be selected.\n\n Simple Random Sample \n\nFor a simple random sample (SRS), every possible sample of sample size \\(n\\) has the same chance to be chosen.\nExample: If I were to sample 100 students from all 10,000 Marquette students, I would randomly assign each student a number (from 1 to 10,000) and then randomly select 100 numbers.\n\n\n\n\n\n\n\n\nFigure¬†2.7: Simple Random Sample\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.8: Simple random sample from a population of 15 (https://research-methodology.net/sampling-in-primary-data-collection/random-sampling/)\n\n\n\n\n\n\n Stratified Random Sample \n\nFor stratified sampling, subdivide the population into different subgroups (strata) that share the same characteristics, then draw a simple random sample from each subgroup.\nHomogeneous within strata; Non-homogeneous between strata (Figure¬†2.9)\n\n\n\n\n\n\nFigure¬†2.9: Stratified Sampling\n\n\n\n\n\nExample: Divide Marquette students into groups by colleges, then perform a SRS for each group (Figure¬†2.10).\n\n\n\n\n\n\nFigure¬†2.10: Stratified sampling of Marquette Students\n\n\n\n\n Cluster Sampling \n\nFor cluster sampling, divide the population into clusters, then randomly select some of those clusters, and then choose all the members from those selected clusters.\nHomogeneous between clusters; Non-homogeneous within clusters (Figure¬†2.11)\n\n\n\n\n\n\nFigure¬†2.11: Cluster Sampling\n\n\n\n\n\nExample: Study 4720 students‚Äô drinking habits by dividing the students into 9 groups, and then randomly selecting 3 and interviewing all of the students in each of those clusters (Figure¬†2.12).\n\n\n\n\n\n\nFigure¬†2.12: Cluster sampling of Marquette students"
  },
  {
    "objectID": "intro-data.html#causal-relationship",
    "href": "intro-data.html#causal-relationship",
    "title": "2¬† Data",
    "section": "2.4 Causal Relationship",
    "text": "2.4 Causal Relationship\n\nMaking causal conclusions based on experiments is often more reasonable than making the same causal conclusions based on observational data.\nObservational studies are generally only sufficient to show associations, not causality."
  },
  {
    "objectID": "intro-data.html#simple-random-sample",
    "href": "intro-data.html#simple-random-sample",
    "title": "2¬† Data",
    "section": "2.4 Simple Random Sample",
    "text": "2.4 Simple Random Sample\n\nRandom Sample: Each member of a population is equally likely to be selected.\nSimple Random Sample (SRS): Every possible sample of sample size \\(n\\) has the same chance to be chosen.\nExample: If I sample 100 students from all, say 10,000 Marquette students, I would randomly assign each student a number (from 1 to 10,000) and then randomly select 100 numbers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://research-methodology.net/sampling-in-primary-data-collection/random-sampling/"
  },
  {
    "objectID": "intro-data.html#stratified-random-sample",
    "href": "intro-data.html#stratified-random-sample",
    "title": "2¬† Data",
    "section": "2.4 Stratified Random Sample",
    "text": "2.4 Stratified Random Sample\n\nStratified Sampling: Subdivide the population into different subgroups (strata) that share the same characteristics, then draw a simple random sample from each subgroup.\nHomogeneous within strata; Non-homogeneous between strata"
  },
  {
    "objectID": "intro-data.html#cluster-sampling",
    "href": "intro-data.html#cluster-sampling",
    "title": "2¬† Data",
    "section": "2.4 Cluster Sampling",
    "text": "2.4 Cluster Sampling\n\nCluster Sampling: Divide the population into clusters, then randomly select some of those clusters, and then choose all the members from those selected clusters.\nHomogeneous between clusters; Non-homogeneous within clusters"
  },
  {
    "objectID": "intro-data.html#categorical-vs.-numerical-variables",
    "href": "intro-data.html#categorical-vs.-numerical-variables",
    "title": "2¬† Data",
    "section": "2.5 Categorical vs.¬†Numerical Variables",
    "text": "2.5 Categorical vs.¬†Numerical Variables\n\nA categorical variable provides non-numerical information which can be placed in one (and only one) category from two or more categories.\n\nGender (Male üë®, Female üë©, Trans üè≥Ô∏è‚Äçüåà) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA üá∫üá∏, Canada üá®üá¶, UK üá¨üáß, Germany üá©üá™, Japan üáØüáµ, Korea üá∞üá∑) \n\nA numerical variable is recorded in a numerical value representing counts or measurements.\n\n GPA \n The number of relationships you‚Äôve had \n Height"
  },
  {
    "objectID": "intro-data.html#numerical-variables-can-be-discrete-or-continuous",
    "href": "intro-data.html#numerical-variables-can-be-discrete-or-continuous",
    "title": "2¬† Data",
    "section": "2.5 Numerical Variables can be Discrete or Continuous",
    "text": "2.5 Numerical Variables can be Discrete or Continuous\n\nA discrete variable takes on values of a finite or countable number.\nA continuous variable takes on values anywhere over a particular range without gaps or jumps.\n\n GPA is continuous because it can be any value between 0 and 4. \n The number of relationships you‚Äôve had is discrete because you can count the number and it is finite.\n Height is continuous because it can be any number within a range."
  },
  {
    "objectID": "intro-data.html#categorical-variables-are-usually-recorded-as-numbers",
    "href": "intro-data.html#categorical-variables-are-usually-recorded-as-numbers",
    "title": "2¬† Data",
    "section": "2.5 Categorical Variables are Usually Recorded as Numbers",
    "text": "2.5 Categorical Variables are Usually Recorded as Numbers\n\nGender (Male = 0, Female = 1, Trans = 2) \nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5) \nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301) \nUnited Airlines boarding groups\nThe numbers represent categories only; differences between them are meaningless.\n\nCanada - USA = 101 - 100 = 1?\nGraduate - Sophomore = 5 - 2 = 3 = Junior?\n\nWe need to learn the level of measurements to know whether or which arithmetic operations are meaningful."
  },
  {
    "objectID": "intro-data.html#levels-of-measurements-nominal-and-ordinal-for-categorical-variables",
    "href": "intro-data.html#levels-of-measurements-nominal-and-ordinal-for-categorical-variables",
    "title": "2¬† Data",
    "section": "2.5 Levels of Measurements: Nominal and Ordinal for Categorical Variables",
    "text": "2.5 Levels of Measurements: Nominal and Ordinal for Categorical Variables\n\nNominal: The data can NOT be ordered in a meaningful or natural way.\n\nGender (Male = 0, Female = 1, Trans = 2)  is nominal because Male, Female and Trans cannot be ordered.\nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301)  is nominal.\n\nOrdinal: The data can be arranged in some meaningful order, but differences between data values can NOT be determined or are meaningless.\n\nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5)  is ordinal because Sophomore is one class higher than Freshman."
  },
  {
    "objectID": "intro-data.html#converting-numerical-to-categorical",
    "href": "intro-data.html#converting-numerical-to-categorical",
    "title": "2¬† Data",
    "section": "2.5 Converting Numerical to Categorical",
    "text": "2.5 Converting Numerical to Categorical\n\nYou‚Äôve already seen an example.\n\n\n\n\n\n \n  \n    Grade \n    Percentage \n  \n \n\n  \n    A \n    [94, 100] \n  \n  \n    A- \n    [90, 94) \n  \n  \n    B+ \n    [87, 90) \n  \n  \n    B \n    [83, 87) \n  \n  \n    B- \n    [80, 83) \n  \n  \n    C+ \n    [77, 80) \n  \n  \n    C \n    [73, 77) \n  \n  \n    C- \n    [70, 73) \n  \n  \n    D+ \n    [65, 70) \n  \n  \n    D \n    [60, 65) \n  \n  \n    F \n    [0, 60) \n  \n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify data type of each variable in the Marquette men‚Äôs basketball player data"
  },
  {
    "objectID": "intro-stats.html#statistics-as-numeric-records",
    "href": "intro-stats.html#statistics-as-numeric-records",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.1 Statistics as Numeric Records",
    "text": "1.1 Statistics as Numeric Records\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records as shown in Figure¬†1.1.\n\n\n\n\n\nFigure¬†1.1: Example of statistics or numeric records. (Source: https://www.nba.com/stats/player/893/career)\n\n\n\n\n\n\nInterestingly someone defines statistics as the only field where two experts, using identical data, may come to completely opposite conclusions Figure¬†1.2, which is true in some sense. We‚Äôll see why later in this course. With the same data, different statistical methods may produce different results and lead to difference conclusions.\n\n\n\n\n\n\nFigure¬†1.2: Statistics Shirt (Source: shorturl.at/vEMNS)"
  },
  {
    "objectID": "intro-data.html#data",
    "href": "intro-data.html#data",
    "title": "2¬† Data",
    "section": "2.1 Data",
    "text": "2.1 Data\n What is Data? \n\nBecause statistics is a science of data, we first need to understand what data is.\nData can be described as a set of objects on which we observe or measure one or more characteristics.\n\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is also called a variable because it varies from one object to another.\n\nWe usually store a data set in a matrix form that has rows and columns.\n\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\n\nThis structure allows new cases to be added as rows or new variables to be added as columns.\n\n\n Example \n\nFigure¬†2.1 below is a data set of Marquette basketball players stored in matrix form.\nThe objects are individuals or players in the data and each have their own associated row.\nEach player has several characteristics or attributes shown in the columns associated with them.\n\nThese include jersey number, class, position, height, weight, hometown and high school.\nThese characteristics can also be referred to as variables because they vary from one player to another.\n\n\n\n\n\n\n\nFigure¬†2.1: Data set of 2019 Marquette men‚Äôs basketball players"
  },
  {
    "objectID": "intro-data.html#target-population",
    "href": "intro-data.html#target-population",
    "title": "2¬† Data",
    "section": "2.2 Target Population",
    "text": "2.2 Target Population\n\nThe first step in conducting a study is to identify questions to be investigated.\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\nTarget Population: The complete collection of data we‚Äôd like to make inference about.\nWhat is the average GPA of currently enrolled Marquette students?\n\n\n\n\n\n\n\n\n\n\n\nThe population is a set of all objects which we are interested in studying from.\nBecause all Marquette undergrads that are currently enrolled is the complete collection of data we‚Äôd like to make inference about, each currently enrolled Marquette undergrad is an object.\n\nNote that students who are not currently enrolled or students that are already graduated are not our interest, and they shouldn‚Äôt be a part of target population.\n\nAverage GPA is the variable or population property we would like to make inference about."
  },
  {
    "objectID": "intro-data.html#two-types-of-studies-to-collect-sample-data",
    "href": "intro-data.html#two-types-of-studies-to-collect-sample-data",
    "title": "2¬† Data",
    "section": "2.3 Two Types of Studies to Collect Sample Data",
    "text": "2.3 Two Types of Studies to Collect Sample Data\n\nObservational Study: Observe and measure characteristics/variables, and do NOT attempt to modify or intervene with the subjects being studied.\n\n Sample from 1Ô∏è‚É£ the heart disease and 2Ô∏è‚É£ heart disease-free populations. Then record the fat content of the diets for the two groups. \n\nExperimental Study: Apply some treatment(s) and then proceed to observe its responses or effects on the individuals (experimental units).\n\nAssign volunteers to one of several diets with different levels of dietary fat (treatments). Then compare the treatments with respect to the incidence of heart disease after a period of time. \n\n\n\n\n\n\n\n\nObservational or Experimental?\n\n\n\n\nRandomly select 40 males and 40 females to see the difference in blood pressure levels between male and female. \n Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo)."
  },
  {
    "objectID": "intro-data.html#limitation-of-observational-studies-confounding",
    "href": "intro-data.html#limitation-of-observational-studies-confounding",
    "title": "2¬† Data",
    "section": "2.4 Limitation of Observational Studies: Confounding",
    "text": "2.4 Limitation of Observational Studies: Confounding\n\nConfounder: A variable NOT included in a study but that affects the variables in the study.\nObserve past data that shows that increases in ice cream sales are associated with increases in drownings, and we conclude that eating ice cream causes drownings. üò± üòï ‚ÅâÔ∏è\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the confounder that is not in the data but affects ice cream sales and the number of drownings?\n\n\n\n\n\n\n\nTemperature: As temperature increases, ice cream sales increase and the number of drownings goes up because more people swim."
  },
  {
    "objectID": "intro-data.html#stratified-random-sample-example",
    "href": "intro-data.html#stratified-random-sample-example",
    "title": "2¬† Data",
    "section": "2.4 Stratified Random Sample Example",
    "text": "2.4 Stratified Random Sample Example\n\nExample: Divide Marquette students into groups by colleges, then SRS for each group."
  },
  {
    "objectID": "intro-data.html#cluster-sampling-example",
    "href": "intro-data.html#cluster-sampling-example",
    "title": "2¬† Data",
    "section": "2.5 Cluster Sampling Example",
    "text": "2.5 Cluster Sampling Example\n\nExample: Studying 4720 students‚Äô drinking habits by dividing the students into 9 groups, then randomly selecting 3 and interviewing all of the students in each of those clusters."
  },
  {
    "objectID": "intro-data.html#levels-of-measurements-interval-and-ratio-for-numerical-variables",
    "href": "intro-data.html#levels-of-measurements-interval-and-ratio-for-numerical-variables",
    "title": "2¬† Data",
    "section": "2.5 Levels of Measurements: Interval and Ratio for Numerical Variables",
    "text": "2.5 Levels of Measurements: Interval and Ratio for Numerical Variables\n\nInterval: The data have meaningful differences between any two values but the data do NOT have a natural zero or starting point. The data can do \\(\\color{red} +\\) and \\(\\color{red} -\\), but can‚Äôt reasonably do \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\nTemperature is interval because \\(80^{\\circ}\\)F is 40 degrees higher than \\(40^{\\circ}\\)F \\((80-40=40)\\), but \\(0^{\\circ}\\) does not mean NO heat and \\(80^{\\circ}\\)F is NOT twice as hot as \\(40^{\\circ}\\)F.\n\nRatio: The data have both meaningful differences and ratios, and there is a natural zero starting point that indicates none of the quantity. The data can do \\(\\color{red} +\\), \\(\\color{red} -\\), \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\nDistance is ratio because \\(80\\) miles is twice as far as \\(40\\) miles \\((80/40 = 2)\\), and \\(0\\) mile means no distance."
  },
  {
    "objectID": "intro-data.html#data-type",
    "href": "intro-data.html#data-type",
    "title": "2¬† Data",
    "section": "2.4 Data Type",
    "text": "2.4 Data Type\n\n\n\n\n\nFigure¬†2.13: Types of Data\n\n\n\n\n Categorical vs.¬†Numerical Variables \n\nA categorical variable provides non-numerical information which can be placed in one (and only one) category from two or more categories.\n\nGender (Male üë®, Female üë©, Trans üè≥Ô∏è‚Äçüåà) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA üá∫üá∏, Canada üá®üá¶, UK üá¨üáß, Germany üá©üá™, Japan üáØüáµ, Korea üá∞üá∑) \n\nA numerical variable is recorded in a numerical value representing counts or measurements.\n\n GPA \n The number of relationships you‚Äôve had \n Height \n\n\n Numerical Variables \n\nNumerical variables can be discrete or continuous.\nA discrete variable takes on values of a finite or countable number.\nA continuous variable takes on values anywhere over a particular range without gaps or jumps.\n\n GPA is continuous because it can be any value between 0 and 4. \n The number of relationships you‚Äôve had is discrete because you can count the number and it is finite.\n Height is continuous because it can be any number within a range. \n\n\n Categorical Variables \n\nCategorical variables are usually recorded as numbers.\nGender (Male = 0, Female = 1, Trans = 2) \nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5) \nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301) \nUnited Airlines boarding groups\nThe numbers represent categories only; differences between them are meaningless.\n\nCanada - USA = 101 - 100 = 1?\nGraduate - Sophomore = 5 - 2 = 3 = Junior?\n\nWe need to learn the level of measurements to know which arithmetic operations are meaningful.\n\n\n Levels of Measurements \n Nominal and Ordinal for Categorical Variables \n\nNominal: The data can NOT be ordered in a meaningful or natural way.\n\nGender (Male = 0, Female = 1, Trans = 2)  is nominal because Male, Female and Trans cannot be ordered.\nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301)  is nominal.\n\nOrdinal: The data can be arranged in some meaningful order, but differences between data values can NOT be determined or are meaningless.\n\nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5)  is ordinal because Sophomore is one class higher than Freshman.\n\n\n Interval and Ratio for Numerical Variables \n\nInterval: The data have meaningful differences between any two values but the data do NOT have a natural zero or starting point. The data can do \\(\\color{red} +\\) and \\(\\color{red} -\\), but can‚Äôt reasonably do \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\nTemperature is interval because \\(80^{\\circ}\\)F is 40 degrees higher than \\(40^{\\circ}\\)F \\((80-40=40)\\), but \\(0^{\\circ}\\) does not mean NO heat and \\(80^{\\circ}\\)F is NOT twice as hot as \\(40^{\\circ}\\)F.\n\nRatio: The data have both meaningful differences and ratios, and there is a natural zero starting point that indicates none of the quantity. The data can do \\(\\color{red} +\\), \\(\\color{red} -\\), \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\nDistance is ratio because \\(80\\) miles is twice as far as \\(40\\) miles \\((80/40 = 2)\\), and \\(0\\) mile means no distance.\n\n\n\n Converting Numerical to Categorical \n\nYou‚Äôve already seen an example of this with the class grading scale (Figure¬†2.14).\n\n\n\n\n\n\n\n\n\nGrade\n\n\nPercentage\n\n\n\n\n\n\nA\n\n\n[94, 100]\n\n\n\n\nA-\n\n\n[90, 94)\n\n\n\n\nB+\n\n\n[87, 90)\n\n\n\n\nB\n\n\n[83, 87)\n\n\n\n\nB-\n\n\n[80, 83)\n\n\n\n\nC+\n\n\n[77, 80)\n\n\n\n\nC\n\n\n[73, 77)\n\n\n\n\nC-\n\n\n[70, 73)\n\n\n\n\nD+\n\n\n[65, 70)\n\n\n\n\nD\n\n\n[60, 65)\n\n\n\n\nF\n\n\n[0, 60)\n\n\n\n\n\nFigure¬†2.14: Grading scale for this class\n\n\n\n\n Practice \n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify the data type of each variable in the Marquette men‚Äôs basketball player data\n\n\n\n\n\n\n\nFigure¬†2.15: 2019 Marquette men‚Äôs basketball player data set"
  },
  {
    "objectID": "intro-r.html#r-and-rstudio",
    "href": "intro-r.html#r-and-rstudio",
    "title": "3¬† Tool foR Data",
    "section": "3.1 R and RStudio",
    "text": "3.1 R and RStudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR: free open-source programming language üìà\nR is mainly for doing data science with strength in statistical modeling, computing and data visualization\n\n\n\nRStudio 1: interface for R, Python, etc called an IDE (integrated development environment), e.g.¬†‚ÄúI write R code in the RStudio IDE‚Äù.\nRStudio is not a requirement for programming with R, but it‚Äôs commonly used by R developers, statisticians and data scientists.\n\n[1] RStudio company is becoming Posit starting October 2022."
  },
  {
    "objectID": "intro-r.html#rstudio-cloud---statistics-wo-hardware-hassles",
    "href": "intro-r.html#rstudio-cloud---statistics-wo-hardware-hassles",
    "title": "3¬† Tool foR Data",
    "section": "3.2 ‚òÅÔ∏è RStudio Cloud - Statistics w/o hardware hassles",
    "text": "3.2 ‚òÅÔ∏è RStudio Cloud - Statistics w/o hardware hassles\n\nüòé We can implement R programs without installing R and RStudio in your laptop!\nüòé RStudio Cloud lets you do, share and learn data science online for free!\n\n\n\n\n3.2.1 üòû R/RStudio: Lots of friction\n\nDownload and install R\nDownload and install RStudio\nInstall wanted R packages:\n\nrmarkdown\ntidyverse\n‚Ä¶\n\nLoad these packages\nDownload and install tools like Git\n\n\n\n3.2.2 ü§ì RStudio Cloud: Much less friction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo to https://rstudio.cloud/\nLog in\n\n\n\n\n\n\n\n\n>hello R!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüòÉ We do statistical analysis using RStudio IDE, directly from a web browser!"
  },
  {
    "objectID": "intro-r.html#install-rstudio-cloud",
    "href": "intro-r.html#install-rstudio-cloud",
    "title": "3¬† Tool foR Data",
    "section": "3.2 Install RStudio Cloud",
    "text": "3.2 Install RStudio Cloud\n\n\n\n\n\n\nLab time!\n\n\n\n\nStep 1: In the RStudio website https://rstudio.com/, please choose Products > RStudio Cloud as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab time!\n\n\n\n\nStep 2: Click GET STARTED FOR FREE.\nStep 3: Free > Sign Up. Sign up using your Marquette email address or the one you prefer."
  },
  {
    "objectID": "intro-r.html#new-projects",
    "href": "intro-r.html#new-projects",
    "title": "3¬† Tool foR Data",
    "section": "3.2 New Projects",
    "text": "3.2 New Projects\nIn RStudio Cloud, click New Project > New RStudio Project, then you are all set!"
  },
  {
    "objectID": "intro-r.html#first-r-code-in-rstudio-cloud",
    "href": "intro-r.html#first-r-code-in-rstudio-cloud",
    "title": "3¬† Tool foR Data",
    "section": "3.3 First R Code in RStudio Cloud!",
    "text": "3.3 First R Code in RStudio Cloud!\n\nGive your project a nice name (click Untitled Project), math-4720 for example.\nFirst R code: \"Hello WoRld!\" or 2 + 4 after > in the Console pane.\nChange the editor theme: Tools > Global Options > Appearance"
  },
  {
    "objectID": "intro-r.html#working-in-rstudiorstudio-cloud",
    "href": "intro-r.html#working-in-rstudiorstudio-cloud",
    "title": "3¬† Tool foR Data",
    "section": "3.2 Working in RStudio/RStudio Cloud",
    "text": "3.2 Working in RStudio/RStudio Cloud\n RStudio Panes \n\n\n\n\n\nFigure¬†3.5: RStudio Panes\n\n\n\n\n R Script \n\nA R script is a .R file that contains R code.  \nTo create a R script, go to File > New > R Script, or click the green-plus icon on the topleft corner, and select R Script.\n\n\n\n\n\n\nFigure¬†3.6: Creating an R script\n\n\n\n\n Run Code \n\n Run : run the current line or selection of code.\n Icon right to the Run : re-run the previous selected code.\n\n\n\n\n\n\n\n\nFigure¬†3.7: Running R Code\n\n\n\n\n\n Environment Tab \n\nThe (global) environment is our workspace (NOT the RStudio Cloud workspace).\nAnything created or imported into the current R session is stored in our environment and shown in the Environment tab.\nAfter we run the R script, objects stored in the environment are\n\nData set mtcars\nObject x storing integer values 1 to 10.\nObject y storing three numeric values 3, 5, 9.\n\n\n\n\n\n\n\nFigure¬†3.8: Environment Pane\n\n\n\n\n\n Help \n\nDon‚Äôt know how a function works or what a data set is about ‚ùì\nüëâ Simply type ? followed by the data name or function name like\n\n\n?mean\n?mtcars\n\n\nA document will show up in the Help tab, teaching you how to use the function or explaining the data set.\n\n\n\n\n\n\n\nWhat does the function mean() do?\n\n\n\n\n\n\n Resources \nIn RStudio Cloud sidebar,\n\nR and RStudio Cheat Sheats\nLearn more RStudio and statistical data science: Primers\n\n\n\n\n\n\nFigure¬†3.9: Resources for help with R/RStudio\n\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nWhat is the size of mtcars data?\nType mtcars and hit Enter in the Console to see the data set.\nDiscuss data type of each variable.\nType mtcars[, 1] and hit Enter in the Console. What do you see?"
  },
  {
    "objectID": "intro-r.html#install-r",
    "href": "intro-r.html#install-r",
    "title": "3¬† Tool foR Data",
    "section": "3.3 Install R",
    "text": "3.3 Install R\n\n3.3.1 Step 1\n\nGo to https://cloud.r-project.org\nClick Download R for [your operating system]\n\n\n\n\n3.3.2 Step 2\n\nIf you are a Mac user, you should see the page as below. You are recommended to download and install the latest version of R (now R-4.2.1 (Funny-Looking Kid)), if your OS version allows to do so. Otherwise, choose a previous version, R-3.6.3.\nIf you are a Windows user, after clicking Download R for Windows, please choose base version, then click Download R-4.2.1 for Windows. \n\n\n\n\n3.3.3 Step 3\n\nOnce you install R successfully, when you open R, you should be able to see the following R terminal or console:\n\n\n\n\nWindows \n\n\nMac"
  },
  {
    "objectID": "intro-r.html#install-rstudio",
    "href": "intro-r.html#install-rstudio",
    "title": "3¬† Tool foR Data",
    "section": "3.5 Install RStudio",
    "text": "3.5 Install RStudio\n\n3.5.1 Step 1\n\nIn the RStudio website, please choose Products > RStudio as shown below.\n\n\n\n\n3.5.2 Step 2\n\nChoose RStudio Desktop and click DOWNLOAD RSTUDIO DESKTOP for the free version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.3 Step 3\n\nClick DOWNLOAD RSTUDIO FOR [YOUR SYSTEM]. Then follow standard installation steps, you should get the software soon.\nBe careful that R should be installed successfully in your computer before you download and install RStudio.\n[Note]: The latest version of RStudio is 2022.07.1+554."
  },
  {
    "objectID": "intro-r.html#rstudio-screen",
    "href": "intro-r.html#rstudio-screen",
    "title": "3¬† Tool foR Data",
    "section": "3.4 RStudio Screen",
    "text": "3.4 RStudio Screen\n\nWhen you open RStudio, you should see something similar to the figure below.\nIf you do, congratulations! You are able to do every statistical computation in R using RStudio locally in your computer."
  },
  {
    "objectID": "intro-r.html#r-is-a-calculator---arithmetic-operators",
    "href": "intro-r.html#r-is-a-calculator---arithmetic-operators",
    "title": "3¬† Tool foR Data",
    "section": "3.4 R is a Calculator - Arithmetic Operators",
    "text": "3.4 R is a Calculator - Arithmetic Operators\n\n\n\n\n\n\n\n\n\n\n3.4.1 Examples\n\n2 + 3 * 5 + 4\n\n[1] 21\n\n2 + 3 * (5 + 4)\n\n[1] 29\n\n\n\nWe have to do the operation in the parenthesis first"
  },
  {
    "objectID": "intro-r.html#r-does-comparisons---logical-operators",
    "href": "intro-r.html#r-does-comparisons---logical-operators",
    "title": "3¬† Tool foR Data",
    "section": "3.5 R Does Comparisons - Logical Operators",
    "text": "3.5 R Does Comparisons - Logical Operators\n\n\n\n\n\n\n\n\n\n\n3.5.1 Examples\n\n\n\n5 <= 5\n\n[1] TRUE\n\n5 <= 4\n\n[1] FALSE\n\n# Is 5 is NOT equal to 5? FALSE\n5 != 5\n\n[1] FALSE\n\n\n\n## Is TRUE not equal to FALSE?\nTRUE != FALSE\n\n[1] TRUE\n\n## Is not TRUE equal to FALSE?\n!TRUE == FALSE\n\n[1] TRUE\n\n## TRUE if either one is TRUE or both are TRUE\nTRUE | FALSE\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\nWhat does TRUE & FALSE return?"
  },
  {
    "objectID": "intro-r.html#built-in-functions",
    "href": "intro-r.html#built-in-functions",
    "title": "3¬† Tool foR Data",
    "section": "3.5 Built-in Functions",
    "text": "3.5 Built-in Functions\n\nR has lots of built-in functions, especially for mathematics, probability and statistics.\n\n\n\n\n\n\n\n\n\n\n\n3.5.1 Examples\n\n\n\nsqrt(144)\n\n[1] 12\n\nexp(1)  ## Euler's number\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nabs(-7)\n\n[1] 7\n\n\n\nfactorial(5)\n\n[1] 120\n\n## without specifying base value\n## it is a natural log with base e\nlog(100)\n\n[1] 4.60517\n\n## log function and we specify base = 2\nlog(100, base = 10)\n\n[1] 2"
  },
  {
    "objectID": "intro-r.html#commenting",
    "href": "intro-r.html#commenting",
    "title": "3¬† Tool foR Data",
    "section": "3.5 Commenting",
    "text": "3.5 Commenting\n\n\n\n\n\n\nYou‚Äôve seen comments a lot! How do we write a comment in R?\n\n\n\n\n\n\n\nUse # to add a comment so that the text after # is not read as an R command.\nWriting (good) comments is highly recommended: help readers and more importantly yourself understand what the code is doing.\nComments should explain the why, not the what.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://www.reddit.com/r/ProgrammerHumor/comments/8w54mx/code_comments_be_like/"
  },
  {
    "objectID": "intro-r.html#objects-and-funtions-in-r",
    "href": "intro-r.html#objects-and-funtions-in-r",
    "title": "3¬† Tool foR Data",
    "section": "3.5 Objects and Funtions in R",
    "text": "3.5 Objects and Funtions in R\n\n Everything that exists is an object. \n Everything that happens is a function call. \n‚Äì John Chambers, the creator of the S programming language.\n\n\nWe have made lots of things happened!\nEven arithmetic and logical operators are functions!\n\n\n`+`(x = 2, y = 3)\n\n[1] 5\n\n`&`(TRUE, FALSE)\n\n[1] FALSE"
  },
  {
    "objectID": "intro-r.html#creating-variables",
    "href": "intro-r.html#creating-variables",
    "title": "3¬† Tool foR Data",
    "section": "3.5 Creating Variables",
    "text": "3.5 Creating Variables\n\nA variable stores a value that can be changed according to our need.\nUse <- operator to assign a value to the variable. (Highly recommendedüëç)\n\n\nx <- 5  ## we create an object, value 5, and call it x, which is a variable.\nx  ## type the variable name to see the value stored in the object x\n\n[1] 5\n\n\n\n\n(x <- x + 6)  # We can reassign any value to the variable we created\n\n[1] 11\n\nx == 5  # We can perform any operations on variables\n\n[1] FALSE\n\nlog(x) # Variables can also be used in any built-in functions\n\n[1] 2.397895"
  },
  {
    "objectID": "intro-r.html#bad-naming",
    "href": "intro-r.html#bad-naming",
    "title": "3¬† Tool foR Data",
    "section": "3.5 Bad Naming",
    "text": "3.5 Bad Naming\n\n‚ùå Unless you have a very good reason, don‚Äôt create a variable whose name is the same as any R built-in constant or function!\nüòü It causes lots of confusion when your code is long and when others read your code.\n\n\n## THIS IS BAD CODING! DON'T DO THIS!\npi  ## pi is a built-in constant\n\n[1] 3.141593\n\n(pi <- 20)\n\n[1] 20\n\nabs ## abs is a built-in function\n\nfunction (x)  .Primitive(\"abs\")\n\n(abs <- abs(pi))\n\n[1] 20"
  },
  {
    "objectID": "intro-r.html#object-types",
    "href": "intro-r.html#object-types",
    "title": "3¬† Tool foR Data",
    "section": "3.5 Object Types",
    "text": "3.5 Object Types\n Types of Variables \n\nUse typeof() to check which type a variable belongs to.\nCommon types include character, double, integer and logical.\nTo check if it‚Äôs of a specific type, use is.character(), is.double(), is.integer(), is.logical().\n\n\n\n\ntypeof(5)\n\n[1] \"double\"\n\ntypeof(5L)\n\n[1] \"integer\"\n\ntypeof(\"I_love_stats!\")\n\n[1] \"character\"\n\n\n\n\n\n\ntypeof(1 > 3)\n\n[1] \"logical\"\n\nis.double(5L)\n\n[1] FALSE\n\n\n\n\n Variable Types in R and in Statistics \n\nType character and logical correspond to categorical variables.\n\nType logical is a special type of categorical variables that has only two categories (binary).\n\nWe usually call it a binary variable.\n\n\nType double and integer correspond to numerical variables. (an exception later)\n\nType double is for continuous variables\nType integer is for discrete variables."
  },
  {
    "objectID": "intro-r.html#atomic-vector",
    "href": "intro-r.html#atomic-vector",
    "title": "3¬† Tool foR Data",
    "section": "3.6 (Atomic) Vector",
    "text": "3.6 (Atomic) Vector\n\nTo create a vector, use c(), short for concatenate or combine.\nAll elements of a vector must be of the same type.\n\n\n\n\n(dbl_vec <- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(int_vec <- c(1L, 6L, 10L))\n\n[1]  1  6 10\n\n## TRUE and FALSE can be written as T and F\n(log_vec <- c(TRUE, FALSE, F))  \n\n[1]  TRUE FALSE FALSE\n\n(chr_vec <- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5\n\n\n\n\n\n3.6.1 Operations on Vectors\n\nWe can do any operations on vectors as we do on a scalar variable (vector of length 1).\n\n\n\n\n# Create two vectors\nv1 <- c(3, 8)\nv2 <- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10\n\n\n\n\n\n\n3.6.2 Recycling of Vectors\n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\nv1 <- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 <- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16\n\n\n\n\n3.6.3 Subsetting Vectors\n\nTo extract element(s) in a vector, use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The first element\nv1[1] \n\n[1] 3\n\n## The second element\nv2[2]  \n\n[1] 100\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5"
  },
  {
    "objectID": "intro-r.html#factor",
    "href": "intro-r.html#factor",
    "title": "3¬† Tool foR Data",
    "section": "3.7 Factor",
    "text": "3.7 Factor\n\nA vector of type factor can be ordered in a meaningful way.\nCreate a factor by factor(). It is a type of integer, not character. üò≤ üôÑ\n\n\nfac <- factor(c(\"med\", \"high\", \"low\"))\ntypeof(fac)\n\n[1] \"integer\"\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically.\n\n[1] \"high\" \"low\"  \"med\" \n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\norder_fac <- factor(c(\"med\", \"high\", \"low\"), levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1"
  },
  {
    "objectID": "intro-r.html#list-generic-vectors",
    "href": "intro-r.html#list-generic-vectors",
    "title": "3¬† Tool foR Data",
    "section": "3.8 List (Generic Vectors)",
    "text": "3.8 List (Generic Vectors)\n\nLists are different from vectors: Elements can be of any type, including lists.\nConstruct a list by using list() instead of c().\n\n\n\n\n## a list of 3 elements of different types\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\nx_lst\n\n$idx\n[1] 1 2 3\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1]  TRUE FALSE\n\n\n\nstr(x_lst)\n\nList of 3\n $ idx: int [1:3] 1 2 3\n $    : chr \"a\"\n $    : logi [1:2] TRUE FALSE\n\nnames(x_lst)\n\n[1] \"idx\" \"\"    \"\"   \n\nlength(x_lst)\n\n[1] 3\n\n\n\n\n\n3.8.1 Subsetting a List\n\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\n\n\n\n\nReturn an  element  of a list\n\n\nReturn a  sub-list  of a list\n\n\n\n\n## subset by name (a vector)\nx_lst$idx  \n\n[1] 1 2 3\n\n## subset by indexing (a vector)\nx_lst[[1]]  \n\n[1] 1 2 3\n\ntypeof(x_lst$idx)\n\n[1] \"integer\"\n\n\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n\n$idx\n[1] 1 2 3\n\n## subset by indexing (still a list)\nx_lst[1]  \n\n$idx\n[1] 1 2 3\n\ntypeof(x_lst[\"idx\"])\n\n[1] \"list\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\n‚Äî @RLangTip, https://twitter.com/RLangTip/status/268375867468681216"
  },
  {
    "objectID": "intro-r.html#matrix",
    "href": "intro-r.html#matrix",
    "title": "3¬† Tool foR Data",
    "section": "3.7 Matrix",
    "text": "3.7 Matrix\n\nA matrix is a two-dimensional analog of a vector.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat <- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n\n3.7.1 Subsetting a Matrix\n\nTo extract a sub-matrix, use the same indexing approach as vectors on rows and columns.\nUse comma , to separate row and column index.\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\n[1] 4 5 6\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\n[1] 2 5\n\n## The 1st and 3rd rows\nmat[c(1, 3), ] \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    3    6\n\n\n\n\n\n\n3.7.2 Binding Matrices\n\nWe can generalize c() used in vectors to cbind() (binding matrices by adding columns) and rbind() (binding matrices by adding rows) for matrices.\nWhen matrices are combined by columns (rows), they should have the same number of rows (columns).\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nmat_c <- matrix(data = c(7, 0, 0, 8, 2, 6), \n                nrow = 3, ncol = 2)\n## should have the same number of rows\ncbind(mat, mat_c)  \n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7    8\n[2,]    2    5    0    2\n[3,]    3    6    0    6\n\n\n\nmat_r <- matrix(data = 1:4, \n                nrow = 2, \n                ncol = 2)\n## should have the same number of columns\nrbind(mat, mat_r)  \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n[4,]    1    3\n[5,]    2    4"
  },
  {
    "objectID": "intro-r.html#data-frame-the-most-common-way-of-storing-data",
    "href": "intro-r.html#data-frame-the-most-common-way-of-storing-data",
    "title": "3¬† Tool foR Data",
    "section": "3.7 Data frame: The Most Common Way of Storing Data",
    "text": "3.7 Data frame: The Most Common Way of Storing Data\n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nMore general than matrix: Different columns can have different types.\nTo create a data frame, use data.frame() that takes named vectors as input.\n\n\n\n\n## data frame w/ an dbl column named  \n## and char column named grade.\n(df <- data.frame(age = c(19,21,40), \n                  gender = c(\"m\",\"f\",\"m\")))\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n## a data frame has a list structure\nstr(df)  \n\n'data.frame':   3 obs. of  2 variables:\n $ age   : num  19 21 40\n $ gender: chr  \"m\" \"f\" \"m\"\n\n\n\n## must set column names\n## or they are ugly and non-recognizable\ndata.frame(c(19, 21, 40), c(\"m\",\"f\", \"m\")) \n\n  c.19..21..40. c..m....f....m..\n1            19                m\n2            21                f\n3            40                m\n\n\n\n\n\n3.7.1 Properties of Data Frames\n\nData frame has properties of matrix and list.\n\n\n\n\nnames(df)  ## df as a list\n\n[1] \"age\"    \"gender\"\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\"    \"gender\"\n\nlength(df) ## df as a list\n\n[1] 2\n\nncol(df) ## df as a matrix\n\n[1] 2\n\ndim(df) ## df as a matrix\n\n[1] 3 2\n\n\n\n## rbind() and cbind() can be used on df\n\ndf_r <- data.frame(age = 10, \n                   gender = \"f\")\nrbind(df, df_r)\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n4  10      f\n\ndf_c <- \n    data.frame(col = c(\"red\",\"blue\",\"gray\"))\n(df_new <- cbind(df, df_c))\n\n  age gender  col\n1  19      m  red\n2  21      f blue\n3  40      m gray\n\n\n\n\n\n\n3.7.2 Subsetting a Data Frame\n\nWhen we subset data frames, we can use either list or matrix subsetting methods.\n\n\n\n\ndf_new\n\n  age gender  col\n1  19      m  red\n2  21      f blue\n3  40      m gray\n\n## Subset rows\ndf_new[c(1, 3), ]\n\n  age gender  col\n1  19      m  red\n3  40      m gray\n\n## select the row where age == 21\ndf_new[df_new$age == 21, ]\n\n  age gender  col\n2  21      f blue\n\n\n\n## Subset columns\n## like a list\ndf_new$age\n\n[1] 19 21 40\n\ndf_new[c(\"age\", \"gender\")]\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n## like a matrix\ndf_new[, c(\"age\", \"gender\")]\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nCreate a vector object called x that has 5 elements 3, 6, 2, 9, 14.\nCompute the average of elements of x.\nSubset the mtcars data set by selecting variables mpg and disp.\nSelect the cars (rows) in mtcars that have 4 cylinders."
  },
  {
    "objectID": "data-graphics.html#descriptive-statistics-data-summary",
    "href": "data-graphics.html#descriptive-statistics-data-summary",
    "title": "4¬† Data Visualization",
    "section": "4.1 Descriptive Statistics (Data Summary)",
    "text": "4.1 Descriptive Statistics (Data Summary)\n\nBefore doing inferential statistics, let‚Äôs first learn to understand our data by describing or summarizing it using a table, graph, or some important values, so that appropriate methods can be performed for better inference results."
  },
  {
    "objectID": "data-graphics.html#frequency-table-for-categorical-variable",
    "href": "data-graphics.html#frequency-table-for-categorical-variable",
    "title": "4¬† Data Visualization",
    "section": "4.1 Frequency Table for Categorical Variable",
    "text": "4.1 Frequency Table for Categorical Variable\n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nBelow is an example of a frequency table for categorical data with \\(n\\) being the total number of data values.\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\n\\(C_1\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(C_2\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(C_k\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\nHere is another example of a categorical variable color that has three categories.\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\nRed üî¥\n8\n8/50 = 0.16\n\n\nBlue üîµ\n26\n26/50 = 0.52\n\n\nBlack ‚ö´\n16\n16/50 = 0.32\n\n\n\n Categorical Frequency Table in R \nloan50\n\nBelow is the loan50 data set from the openintro package in R.\n\n\n# install.packages(\"openintro\")\nlibrary(openintro)\nstr(loan50)\n\ntibble [50 √ó 18] (S3: tbl_df/tbl/data.frame)\n $ state                  : Factor w/ 51 levels \"\",\"AK\",\"AL\",\"AR\",..: 32 6 41 6 36 16 35 25 11 11 ...\n $ emp_length             : num [1:50] 3 10 NA 0 4 6 2 10 6 3 ...\n $ term                   : num [1:50] 60 36 36 36 60 36 36 36 60 60 ...\n $ homeownership          : Factor w/ 3 levels \"rent\",\"mortgage\",..: 1 1 2 1 2 2 1 2 1 2 ...\n $ annual_income          : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 80000 ...\n $ verified_income        : Factor w/ 4 levels \"\",\"Not Verified\",..: 2 2 4 2 2 3 3 2 2 3 ...\n $ debt_to_income         : num [1:50] 0.558 1.306 1.056 0.574 0.238 ...\n $ total_credit_limit     : int [1:50] 95131 51929 301373 59890 422619 349825 15980 258439 87705 330394 ...\n $ total_credit_utilized  : int [1:50] 32894 78341 79221 43076 60490 72162 2872 28073 23715 32036 ...\n $ num_cc_carrying_balance: int [1:50] 8 2 14 10 2 4 1 3 10 4 ...\n $ loan_purpose           : Factor w/ 14 levels \"\",\"car\",\"credit_card\",..: 4 3 4 3 5 5 4 3 3 4 ...\n $ loan_amount            : int [1:50] 22000 6000 25000 6000 25000 6400 3000 14500 10000 18500 ...\n $ grade                  : Factor w/ 8 levels \"\",\"A\",\"B\",\"C\",..: 3 3 6 3 3 3 5 2 2 4 ...\n $ interest_rate          : num [1:50] 10.9 9.92 26.3 9.92 9.43 ...\n $ public_record_bankrupt : int [1:50] 0 1 0 0 0 0 0 0 0 1 ...\n $ loan_status            : Factor w/ 7 levels \"\",\"Charged Off\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ has_second_income      : logi [1:50] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ total_income           : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 192000 ...\n\n\n\nhomeownership\n\nThe values as well as the frequency table for the variable homeownership from the loan50 data set are shown below.\n\n\n# 50 values (rent, mortgage, own) of categorical homeownership in loan50 data\n(x <- loan50$homeownership)\n\n [1] rent     rent     mortgage rent     mortgage mortgage rent     mortgage\n [9] rent     mortgage rent     mortgage rent     mortgage rent     mortgage\n[17] rent     rent     rent     mortgage mortgage mortgage mortgage rent    \n[25] mortgage rent     mortgage own      mortgage mortgage rent     mortgage\n[33] mortgage rent     rent     own      mortgage rent     mortgage rent    \n[41] mortgage rent     rent     mortgage mortgage mortgage mortgage rent    \n[49] own      mortgage\nLevels: rent mortgage own\n\n## frequency table\ntable(x)\n\nx\n    rent mortgage      own \n      21       26        3 \n\n\n\n\n\n\n\n\n\nIf we want to create a frequency table shown in definition, which R data structure we can use?\n\n\n\n\n\n\n\nfreq <- table(x)\nrel_freq <- freq / sum(freq)\ncbind(freq, rel_freq)\n\n         freq rel_freq\nrent       21     0.42\nmortgage   26     0.52\nown         3     0.06\n\n\n\n Visualizing a Frequency Table \n Bar Chart \n\nBelow is a bar chart that visualizes the homeownership frequency table.\n\n\nbarplot(height = table(x), main = \"Bar Chart\", xlab = \"Homeownership\")\n\n\n\n\n Pie Chart \n\nThe homeownership frequency table can also be visualized using a pie chart.\n\n\npie(x = table(x), main = \"Pie Chart\")"
  },
  {
    "objectID": "data-graphics.html#frequency-distribution-for-numerical-variables",
    "href": "data-graphics.html#frequency-distribution-for-numerical-variables",
    "title": "4¬† Data Visualization",
    "section": "4.2 Frequency Distribution for Numerical Variables",
    "text": "4.2 Frequency Distribution for Numerical Variables\n\nTo create a frequency distribution for numerical variables, one must\n\nDivide the data into \\(k\\) non-overlapping groups of intervals (classes).\nConvert the data into \\(k\\) categories with an associated class interval.\nCount the number of measurements falling in a given class interval (class frequency).\n\n\n\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\n\\(1\\)\n\\([a_1, a_2]\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(2\\)\n\\((a_2, a_3]\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(k\\)\n\\((a_k, a_{k+1}]\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\n\\((a_2 - a_1) = (a_3 - a_2) = \\cdots = (a_{k+1} - a_k)\\). All class widths are the same!\n\n\n\n\n\n\n\nCan our grade conversion be used for creating a frequency distribution?\n\n\n\n\nNo, because the class widths are not all the same as seen in Figure¬†4.2.\n\n\n\n\n\n\n\n\n\n\n\nGrade\n\n\nPercentage\n\n\n\n\n\n\nA\n\n\n[94, 100]\n\n\n\n\nA-\n\n\n[90, 94)\n\n\n\n\nB+\n\n\n[87, 90)\n\n\n\n\nB\n\n\n[83, 87)\n\n\n\n\nB-\n\n\n[80, 83)\n\n\n\n\nC+\n\n\n[77, 80)\n\n\n\n\nC\n\n\n[73, 77)\n\n\n\n\nC-\n\n\n[70, 73)\n\n\n\n\nD+\n\n\n[65, 70)\n\n\n\n\nD\n\n\n[60, 65)\n\n\n\n\nF\n\n\n[0, 60)\n\n\n\n\n\nFigure¬†4.2: Grading scale for this class\n\n\n\n\n Interest Rate Data loan50 [OI] \n\nBelow is data for the interest rate variable in the loan 50 data set.\n\n\n(int_rate <- round(loan50$interest_rate, 1))\n\n [1] 10.9  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n[16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n[31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n[46] 10.4 21.4 10.9  9.4  6.1\n\n\n\n\n\n\n\n\n\n\n\n Frequency Distribution of Interest Rate \n\n\n\n\n Class Class_Intvl Freq Rel_Freq\n     1     5%-7.5%   11     0.22\n     2    7.5%-10%   15     0.30\n     3   10%-12.5%    8     0.16\n     4   12.5%-15%    5     0.10\n     5   15%-17.5%    4     0.08\n     6   17.5%-20%    4     0.08\n     7   20%-22.5%    1     0.02\n     8   22.5%-25%    1     0.02\n     9   25%-27.5%    1     0.02\n\n\n\nrange(int_rate)\n\n[1]  5.3 26.3\n\n\n\n\nAll class widths are the same (2.5%)!\nThe number of classes should not be too big or too small.\nThe lower limit of the 1st class should not be greater than the minimum value of the data.\n\nThe lower limit of the 1st class is 5%, which is less than the minimum value of 5.3%.\n\nThe upper limit of the last class should not be smaller than the maximum value of the data.\n\nThe upper limit of the last class is 27.5%, which is greater than the maximum value of 26.3%.\n\n\n\n\n\n\n\n\n\n\nHow do we choose the number of classes or the class width?\n\n\n\nR decides the number of classes for us when we visualize the frequency distribution by a histogram.\n\n\n\n Visualizing Frequency Distribution by a Histogram \n\n\n\nUse default breaks (no need to specify)\n\n\nhist(x = int_rate, \n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Defualt)\")\n\n\n\n\n\n\n\n\nUse customized breaks\n\n\nclass_boundary\n\n [1]  5.0  7.5 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5\n\nhist(x = int_rate, \n     breaks = class_boundary, #<<\n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Ours)\")\n\n\n\n\n\n\n Skewness \n\nKey characteristics of distributions include the shape, center and spread.\nSkewness provides a way to summarize the shape of a distribution.\n\n\n\n\n\n\nFigure¬†4.3: Distribution characteristics\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs the interest rate histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†4.4: Interest Rate Histogram\n\n\n\n\n\n\n\n\n\n\nFigure¬†4.5: Trick for remembering skewness (Biostatistics for the Biological and Health Sciences p.53)\n\n\n\n\n\n\n\n Scatterplot for Two Numerical Variables \n\n\nA scatterplot provides a case-by-case view of data for two numerical variables.\nBelow is a scatterplot of Loan Amount vs.¬†Total Income from the loan 50 data.\n\n\nplot(x = loan50$total_income, y = loan50$loan_amount,\n     xlab = \"Total Income\", ylab = \"Loan Amount\",\n     pch = 16, col = 4)"
  },
  {
    "objectID": "data-graphics.html#skewness",
    "href": "data-graphics.html#skewness",
    "title": "4¬† Data Visualization",
    "section": "4.2 Skewness",
    "text": "4.2 Skewness\n\nKey characteristics of distributions includes shape, center and spread.\nSkewness provides a way to summarize the shape of a distribution."
  },
  {
    "objectID": "data-graphics.html#remembering-skewness",
    "href": "data-graphics.html#remembering-skewness",
    "title": "4¬† Data Visualization",
    "section": "4.3 Remembering Skewness",
    "text": "4.3 Remembering Skewness\n\n\n\n\n\n\nIs the interest rate histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.53"
  },
  {
    "objectID": "data-graphics.html#scatterplot-for-two-numerical-variables",
    "href": "data-graphics.html#scatterplot-for-two-numerical-variables",
    "title": "4¬† Data Visualization",
    "section": "4.2 Scatterplot for Two Numerical Variables",
    "text": "4.2 Scatterplot for Two Numerical Variables\n\n\nA scatterplot provides a case-by-case view of data for two numerical variables.\n\n\nplot(x = loan50$total_income, y = loan50$loan_amount,\n     xlab = \"Total income\", ylab = \"Loan amount\",\n     pch = 16, col = 4)"
  },
  {
    "objectID": "data-numerics.html#numerical-summaries-of-data",
    "href": "data-numerics.html#numerical-summaries-of-data",
    "title": "5¬† Data Sample Statistics",
    "section": "5.1 Numerical Summaries of Data",
    "text": "5.1 Numerical Summaries of Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure of Center: We typically use the middle point. (What does ‚Äúmiddle‚Äù mean?)\nMeasure of Variation: What values tell us how much variation a variable has?\n\n\n\n\n\n\n\n\n\n\nIf you need to choose one value that represents the entire data, what value would you choose?"
  },
  {
    "objectID": "data-numerics.html#measures-of-center-mean",
    "href": "data-numerics.html#measures-of-center-mean",
    "title": "5¬† Data Sample Statistics",
    "section": "5.1 Measures of Center: Mean",
    "text": "5.1 Measures of Center: Mean\n\nThe (arithmetic) mean or average is adding up all of the values, then dividing by the total number of them.\nLet \\(x_1, x_2, \\dots, x_n\\) denote the measurements observed in a sample of size \\(n\\). Then the sample mean is defined as \\[\\overline{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + x_2 + \\dots + x_n}{n}\\]\nIn the interest rate example, \\[\\overline{x} = \\frac{10.9\\% + 9.9\\% + \\cdots + 6.1\\%}{50} = 11.56\\%\\] \n\n\nmean(int_rate)\n\n[1] 11.558\n\n\n\n5.1.1 Balancing Point\n\nThink of the mean as the balancing point of the distribution."
  },
  {
    "objectID": "data-numerics.html#measures-of-center-median",
    "href": "data-numerics.html#measures-of-center-median",
    "title": "5¬† Data Sample Statistics",
    "section": "5.2 Measures of Center: Median",
    "text": "5.2 Measures of Center: Median\n\nMedian: the middle value when data values are sorted.\nHalf of the values are less than or equal to the median, and the other half are greater than it.\nTo find the median, we first sort the values.\n\\(n\\) is odd: the median is located in the exact middle of the ordered values.\n\n Data: (0, 2, 10, 14, 8) \n Sorted Data: (0, 2, 8, 10, 14) \n The median is \\(8\\) \n\n\\(n\\) is even: the median is the average of the two middle numbers.\n\n Data: (0, 2, 10, 14, 8, 12) \n Sorted Data: (0, 2, 8, 10, 12, 14) \n The median is \\(\\frac{8 + 10}{2} = 9\\) \n\n\n\n5.2.1 Calculate Median in R\n\nmedian(int_rate)  ## Compute the median using command median()\n\n[1] 9.9\n\n\n\n## Compute the median using definition\n(sort_rate <- sort(int_rate))  ## sort data\n\n [1]  5.3  5.3  5.3  6.1  6.1  6.1  6.7  6.7  7.3  7.3  7.3  8.0  8.0  8.0  8.0\n[16]  9.4  9.4  9.4  9.4  9.4  9.9  9.9  9.9  9.9  9.9  9.9 10.4 10.4 10.9 10.9\n[31] 10.9 10.9 10.9 12.0 12.6 12.6 12.6 14.1 15.0 16.0 17.1 17.1 17.1 18.1 18.4\n[46] 19.4 20.0 21.4 24.9 26.3\n\nlength(int_rate)  ## Check sample size is odd or even\n\n[1] 50\n\n(sort_rate[25] + sort_rate[26]) / 2  ## Verify the answer\n\n[1] 9.9\n\n\n\n(int_rate[25] + int_rate[26]) / 2  ## Using un-sorted data leads to a wrong answer!!\n\n[1] 8.1"
  },
  {
    "objectID": "data-numerics.html#measures-of-center-mode",
    "href": "data-numerics.html#measures-of-center-mode",
    "title": "5¬† Data Sample Statistics",
    "section": "5.2 Measures of Center: Mode",
    "text": "5.2 Measures of Center: Mode\n\nMode: the value that occurs most frequently.\nFor continuous numerical data, it is common to have no observations with the same value.\nPractical definition: A mode is represented by a prominent peak in the distribution.\n\n\n## Create a frequency table \n(table_data <- table(int_rate))\n\nint_rate\n 5.3  6.1  6.7  7.3    8  9.4  9.9 10.4 10.9   12 12.6 14.1   15   16 17.1 18.1 \n   3    3    2    3    4    5    6    2    5    1    3    1    1    1    3    1 \n18.4 19.4   20 21.4 24.9 26.3 \n   1    1    1    1    1    1 \n\n\n\n## Sort the table to find the mode that occurs most frequently\n## the number that happens most frequently will be the first one\nsort(table_data, decreasing = TRUE)\n\nint_rate\n 9.9  9.4 10.9    8  5.3  6.1  7.3 12.6 17.1  6.7 10.4   12 14.1   15   16 18.1 \n   6    5    5    4    3    3    3    3    3    2    2    1    1    1    1    1 \n18.4 19.4   20 21.4 24.9 26.3 \n   1    1    1    1    1    1"
  },
  {
    "objectID": "data-numerics.html#comparison-of-mean-median-and-mode",
    "href": "data-numerics.html#comparison-of-mean-median-and-mode",
    "title": "5¬† Data Sample Statistics",
    "section": "5.2 Comparison of Mean, Median and Mode",
    "text": "5.2 Comparison of Mean, Median and Mode\n\nThe mode is applicable for both categorical and numerical data, while the median and mean work for numerical data only.\nIt is also possible to have more than one mode, but there is only one median and one mean.\nThe mean is sensitive to extreme values (outliers).\nThe median and mode are more robust than the mean.\n\nBeing more robust means these measures of center are more resistant to the addition of extreme values to the data.\nAn example in R is shown below:\n\n\n\ndata_extreme\n\n [1] 90.0  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n[16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n[31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n[46] 10.4 21.4 10.9  9.4  6.1\n\n\n\nmean(data_extreme)  ## Large mean! Original mean is 11.56\n\n[1] 13.14\n\nmedian(data_extreme)  ## Median does not change!\n\n[1] 9.9\n\nnames(sort(table(data_extreme), decreasing = TRUE))[1] ## Mode does not change either!\n\n[1] \"9.9\"\n\n\n\nBelow is a figure that shows the differences in where the mean, median, and mode lie for skewed distributions vs.¬†symmetric distributions.\n\n\n\n\n\n\nFigure¬†5.2: Comparison of mean, median, and mode for symmetrical vs.¬†skewed distributions"
  },
  {
    "objectID": "data-numerics.html#measures-of-variation",
    "href": "data-numerics.html#measures-of-variation",
    "title": "5¬† Data Sample Statistics",
    "section": "5.3 Measures of Variation",
    "text": "5.3 Measures of Variation\n\nMeasures of variation, just like measures of center, affect the shape of the distribution (Figure¬†5.3).\n\n\n\n\n\n\nFigure¬†5.3: Effects of variation on the shape of distributions\n\n\n\n\n\n p-th percentile \n\n\n\nThe p-th percentile (quantile) is a data value such that\n\nat most \\(p\\%\\) of the values are below it\nat most \\((1-p)\\%\\) of the values are above it\n\n\n\n\n\n\n\n\nThere are two data sets with the same mean 20.\n\n\n\n\nOne data set has 99-th percentile = 30, and 1-st percentile = 10.\nThe other has 99-th percentile = 40, and 1-st percentile = 0.\nWhich data set has larger variation?\n\n\n\n\n\n\n\n\n\nFigure¬†5.4: Percentiles for ACT scores (https://en.wikipedia.org/wiki/ACT_(test))\n\n\n\n\n\n\n\n Interquartile Range (IQR) \n\nFirst Quartile (Q1): the 25-th percentile\nSecond Quartile (Q2): the 50-th percentile (Median)\nThird Quartile (Q3): the 75-th percentile\nInterquartile Range (IQR): Q3 - Q1\n\n\n\n\n## Use quantile() to find any percentile \n## through specifying the probability\nquantile(x = int_rate, \n         probs = c(0.25, 0.5, 0.75))\n\n   25%    50%    75% \n 8.000  9.900 13.725 \n\n## IQR by definition\nquantile(x = int_rate, probs = 0.75) - \n  quantile(x = int_rate, probs = 0.25) \n\n  75% \n5.725 \n\n\n\n\n\n\n## IQR()\nIQR(int_rate)  \n\n[1] 5.725\n\n## summary() to get the numeric summary\nsummary(int_rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.30    8.00    9.90   11.56   13.72   26.30 \n\n\n\n\n\n\n\n\n\n\nDoes a larger IQR means more or less variation?\n\n\n\n\n\n\n\n Variance and Standard Deviation \n\nThe distance of an observation from its mean, \\(x_i - \\overline{x}\\), is its deviation.\nSample Variance is defined as \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1} \\]\nSample Standard Deviation (SD) is defined as the square root of the variance. \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}} \\]\nThe corresponding population variance and SD are denoted as \\(\\sigma^2\\) and \\(\\sigma\\) respectively.\nThe variance is the average of squared deviation from the sample mean \\(\\overline{x}\\) or the mean squared deviation from the mean.\nThe standard deviation is the root mean squared deviation from the mean.\n\nIt measures, on average, how far the data spread out around the average.\n\n\n Compute Variance and SD in R \n\nvar(int_rate)\n\n[1] 25.54942\n\nsqrt(var(int_rate))\n\n[1] 5.054644\n\nsd(int_rate)\n\n[1] 5.054644"
  },
  {
    "objectID": "data-numerics.html#visualizing-data-variation-boxplot",
    "href": "data-numerics.html#visualizing-data-variation-boxplot",
    "title": "5¬† Data Sample Statistics",
    "section": "5.4 Visualizing Data Variation: Boxplot",
    "text": "5.4 Visualizing Data Variation: Boxplot\nWhen plotting the whiskers,\n\nminimum means the minimal value that is not an potential outlier.\nmaximum means the maximal value that is not an potential outlier.\n\n\n\n\n\n\nhttps://www.leansigmacorporation.com/box-plot-with-minitab/\n\n\n\n\n\n5.4.1 Interest Rate Boxplot\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Boxplot in R\n\n\n\nboxplot(int_rate,ylab =\"Interest Rate (%)\")\n\n\n\n\n\nsort(int_rate, decreasing = TRUE)[1:5]\n\n[1] 26.3 24.9 21.4 20.0 19.4\n\nsort(int_rate)[1:5]\n\n[1] 5.3 5.3 5.3 6.1 6.1\n\nQ3 <- quantile(int_rate, probs = 0.75, \n               names = FALSE)\nQ1 <- quantile(int_rate, probs = 0.25, \n               names = FALSE)\nIQR <- Q3 - Q1\nQ1 - 1.5 * IQR\n\n[1] -0.5875\n\nQ3 + 1.5 * IQR\n\n[1] 22.3125"
  },
  {
    "objectID": "prob-define.html#why-study-probability",
    "href": "prob-define.html#why-study-probability",
    "title": "6¬† Definition of Probability",
    "section": "6.1 Why Study Probability",
    "text": "6.1 Why Study Probability\n\nWe live in a world full of chances and uncertainty! (Sept 19, 2022)"
  },
  {
    "objectID": "prob-define.html#why-probability-before-statistics",
    "href": "prob-define.html#why-probability-before-statistics",
    "title": "6¬† Definition of Probability",
    "section": "6.2 Why Probability Before Statistics?",
    "text": "6.2 Why Probability Before Statistics?\n\n\n Probability : We know the process generating the data and are interested in properties of observations.\n Statistics : We observed the data (sample) and are interested in determining what is the process generating the data (population)."
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability-relative-frequency",
    "href": "prob-define.html#interpretation-of-probability-relative-frequency",
    "title": "6¬† Definition of Probability",
    "section": "6.2 Interpretation of Probability: Relative Frequency",
    "text": "6.2 Interpretation of Probability: Relative Frequency\n\n Relative Frequency : The probability that some outcome of a process will be obtained is interpreted as the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\n\n\n\n      Frequency Relative Frequency\nHeads         7                0.7\nTails         3                0.3\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       507              0.507\nTails       493              0.493\nTotal      1000              1.000\n---------------------\n\n\n\nIf we repeat tossing the coin 10 times, the probability of obtaining a head is 70%.\nIf 1000 times, the probability is 50.7%.\n\n\n\n\n\n\n\nAny issue of relative frequency probability?"
  },
  {
    "objectID": "prob-define.html#issues-of-relative-frequency",
    "href": "prob-define.html#issues-of-relative-frequency",
    "title": "6¬† Definition of Probability",
    "section": "6.3 Issues of Relative Frequency",
    "text": "6.3 Issues of Relative Frequency\n\nüòï How large of a number is large enough?\nüòï Meaning of ‚Äúunder similar conditions‚Äù\nüòï The relative frequency is reliable under identical conditions?\nüëâ We only obtain an approximation instead of exact value.\nüòÇ How do you compute the probability that Chicago Cubs wins the World Series next year?"
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability-classical-approach",
    "href": "prob-define.html#interpretation-of-probability-classical-approach",
    "title": "6¬† Definition of Probability",
    "section": "6.3 Interpretation of Probability: Classical Approach",
    "text": "6.3 Interpretation of Probability: Classical Approach\n\n Classical probability : The probability is based on the concept of equally likely outcomes.\nIf the outcome of some process must be one of \\(n\\) different outcomes, the probability of each outcome is \\(1/n\\).\nExample:\n\ntoss a fair coin (2 outcomes) p\u001f*\u0019\nroll a well-balanced die (6 outcomes) üé≤\ndraw one from a deck of cards (52 outcomes) üÉè\n\n\n\n\n\n\n\n\nAny issue of classical probability?\n\n\n\n\n\n\n\nThe probability that [you name it] wins the World Series next year is 1/30?!"
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability-subjective-approach",
    "href": "prob-define.html#interpretation-of-probability-subjective-approach",
    "title": "6¬† Definition of Probability",
    "section": "6.4 Interpretation of Probability: Subjective Approach",
    "text": "6.4 Interpretation of Probability: Subjective Approach\n\n Subjective probability : The probability is assigned or estimated using people‚Äôs knowledge, beliefs and information about the data generating process.\nA person‚Äôs subjective probability of an outcome, rather than the true probability of that outcome.\nI think ‚Äúthe probability that Milwaukee Brewers wins the World Series this year is 30%‚Äù.\nMy probability that Milwaukee Brewers wins the World Series next year is different from an ESPN MLB analyst‚Äôs probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny probability operations and rules do NOT depend on interpretation of probability!"
  },
  {
    "objectID": "prob-rule.html#experiments-events-and-sample-space",
    "href": "prob-rule.html#experiments-events-and-sample-space",
    "title": "7¬† Probability Rules",
    "section": "7.1 Experiments, Events and Sample Space",
    "text": "7.1 Experiments, Events and Sample Space\n\nExperiment: any process in which the possible outcomes can be identified ahead of time.\nEvent: a set of possible outcomes of the experiment.\nSample space \\((\\mathcal{S})\\) of an experiment: the collection of ALL possible outcomes of the experiment.\n\n\n\n\n\n\n\n\n\n\nExperiment\nPossible Outcomes\nSome Events\nSample Space\n\n\n\n\nFlip a coin p\u001f*\u0019\nHeads, Tails\n{Heads}, {Heads, Tails}, ‚Ä¶\n{Heads, Tails}\n\n\nRoll a die üé≤\n1, 2, 3, 4, 5, 6\n{1, 3, 5}, {2, 4, 6}, {2}, {3, 4, 5, 6}, ‚Ä¶\n{1, 2, 3, 4, 5, 6}\n\n\n\n\n\n\n\n\n\nIs the sample space also an event?\n\n\n\n\n\n\n\nYes, the sample space itself is an event because it is also a set of possible outcomes of the experiment."
  },
  {
    "objectID": "prob-rule.html#set-concept-example-of-rolling-a-six-side-balanced-die",
    "href": "prob-rule.html#set-concept-example-of-rolling-a-six-side-balanced-die",
    "title": "7¬† Probability Rules",
    "section": "7.2 Set Concept: Example of Rolling a six-side balanced die",
    "text": "7.2 Set Concept: Example of Rolling a six-side balanced die\n\n\n\n\n\n\nTip\n\n\n\nDraw a Venn Diagram every time you get stuck!\n\n\n\n Complement  of an event (set) \\(A\\),  \\(A^c\\) : a set of all outcomes (elements) of \\(\\mathcal{S}\\) in which \\(A\\) does not occur.\n\nLet \\(A\\) be an event that a number greater than 2. Then \\(A = \\{3, 4, 5, 6\\}\\) and \\(A^c = \\{1, 2\\}\\).\n\n Union \\((A \\cup B)\\) : a set of all outcomes of \\(\\mathcal{S}\\) in \\(A\\) or \\(B\\).\n\nLet \\(B\\) be an event that an even number is obtained. (What is \\(B\\) in terms of a set?)\n \\(B = \\{2, 4, 6\\}\\), \\(A \\cup B = \\{2, 3, 4, 5, 6\\}\\).\n\n Intersection \\((A \\cap B)\\) : a set of all outcomes of \\(\\mathcal{S}\\) in both \\(A\\) and \\(B\\).\n\n \\(A \\cap B = \\{4, 6\\}\\).\n\n\\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if they have no outcomes in common \\((A \\cap B = \\emptyset)\\).\n\n\\(\\emptyset\\) means an empty set, \\(\\{\\}\\), i.e., no elements in the set.\n Let \\(C\\) be an event that an odd number is obtained. Then \\(C = \\{1, 3, 5\\}\\) and \\(B \\cap C = \\emptyset\\). \n\nContainment \\((A \\subset B)\\): every elements of \\(A\\) also belongs to \\(B\\). If \\(A\\) occurs then so does \\(B\\).\n\n\n\n\n\n\n\n\n\n\n\n \\(B\\) is an event that an even number is obtained. \n \\(D\\) is an event that a number greater than 1 is obtained. \n \\(B = \\{2, 4, 6\\}\\) and \\(D = \\{2, 3, 4, 5, 6\\}\\). \n\n\n\n\n\n\n\nIs \\(B \\subset D\\) or \\(D \\subset B\\)?"
  },
  {
    "objectID": "prob-rule.html#probability-rules",
    "href": "prob-rule.html#probability-rules",
    "title": "7¬† Probability Rules",
    "section": "7.2 Probability Rules",
    "text": "7.2 Probability Rules\nDenote the probability of an event \\(A\\) on a sample space \\(\\mathcal{S}\\) as \\(P(A)\\).\n\n\n\n\n\n\nTip\n\n\n\nTreat the probability of an event as the area of the event in the Venn diagram.\n\n\n\nAxioms\n\n\\(P(\\mathcal{S}) = 1\\)\nFor any event \\(A\\), \\(P(A) \\ge 0\\)\nIf \\(A\\) and \\(B\\) are disjoint, \\(P(A \\cup B) = P(A) + P(B)\\)\n\nProperties\n\n\\(P(\\emptyset) = 0\\).\n\\(0 \\le P(A) \\le 1\\)\n\\(P(A^c) = 1 - P(A)\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (Addition Rule)\nIf \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)"
  },
  {
    "objectID": "prob-rule.html#venn-diagram-illustration",
    "href": "prob-rule.html#venn-diagram-illustration",
    "title": "7¬† Probability Rules",
    "section": "7.2 Venn Diagram Illustration",
    "text": "7.2 Venn Diagram Illustration\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\n\n\n\n\n\n\n\n\n\nDisjoint case: \\(P(A \\cup B) = P(A) + P(B)\\) because \\(P(A \\cap B) = 0\\)!"
  },
  {
    "objectID": "prob-rule.html#example-mm-colors",
    "href": "prob-rule.html#example-mm-colors",
    "title": "7¬† Probability Rules",
    "section": "7.2 Example: M&M Colors",
    "text": "7.2 Example: M&M Colors\nThe makers of the candy M&Ms report that their plain M&Ms are composed of\n\n15% Yellow; 10% Red; 20% Orange; 25% Blue; 15% Green; 15% Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\n\n\n\nIt is brown.\n\n\nIt is red or green.\n\n\nIt is not blue.\n\n\nIt is red and brown.\n\n\n\n\n\n\nSolution:\n\n\\(P(\\mathrm{Brown}) = 0.15\\)\n\\(\\small \\begin{align} P(\\mathrm{Red} \\cup \\mathrm{Green}) &= P(\\mathrm{Red}) + P(\\mathrm{Green}) - P(\\mathrm{Red} \\cap \\mathrm{Green}) \\\\ &= 0.10 + 0.15 - 0 = 0.25 \\end{align}\\)\n\\(P(\\text{Not Blue}) = 1 - P(\\text{Blue}) = 1 - 0.25 = 0.75\\)\n\\(P(\\text{Red and Brown}) = P(\\emptyset) = 0\\)\n\n\n\n\n\n\n\nBy the way, which interpretation of probability is used in this question?"
  },
  {
    "objectID": "prob-rule.html#conditional-probability",
    "href": "prob-rule.html#conditional-probability",
    "title": "7¬† Probability Rules",
    "section": "7.3 Conditional Probability",
    "text": "7.3 Conditional Probability\n\nThe conditional probability of \\(A\\) given \\(B\\) is \\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)} \\] if \\(P(B) > 0\\), and it is undefined if \\(P(B) = 0\\). \n‚ÄúGiven \\(B\\)‚Äù means that event \\(B\\) has already occurred.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplication Rule: \\(P(A \\cap B) = P(A \\mid B)P(B) = P(B \\mid A)P(A)\\)\n\\(P(A)\\) and \\(P(B)\\) are unconditional or marginal probabilities."
  },
  {
    "objectID": "prob-rule.html#difference-between-pa-and-pa-mid-b",
    "href": "prob-rule.html#difference-between-pa-and-pa-mid-b",
    "title": "7¬† Probability Rules",
    "section": "7.3 Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)",
    "text": "7.3 Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)"
  },
  {
    "objectID": "prob-rule.html#example-peanut-butter-and-jelly",
    "href": "prob-rule.html#example-peanut-butter-and-jelly",
    "title": "7¬† Probability Rules",
    "section": "7.3 Example: Peanut Butter and Jelly",
    "text": "7.3 Example: Peanut Butter and Jelly\n\nSuppose 80% of people like peanut butter, 89% like jelly, and 78% like both.\nGiven that a randomly sampled person likes peanut butter, what is the probability that she also likes jelly?\n\n\n\n\n\n\n\n\n\n\nWe want \\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)}\\).\nFrom the problem we have \\(P(PB) = 0.8\\), \\(P(J) = 0.89\\), \\(P(PB \\cap J) = 0.78\\)\n\\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)} = \\frac{0.78}{0.8} = 0.975\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf we don‚Äôt know if the person loves peanut butter, the probability that she loves jelly is 89%.\nIf we do know she loves peanut butter, the probability that she loves jelly is going up to 97.5%."
  },
  {
    "objectID": "prob-rule.html#independence",
    "href": "prob-rule.html#independence",
    "title": "7¬† Probability Rules",
    "section": "7.3 Independence",
    "text": "7.3 Independence\n\n\\(A\\) and \\(B\\) are independent if \\(\\begin{align} P(A \\mid B) &= P(A) \\text{ or }\\\\ P(B \\mid A) &= P(B) \\text{ or } \\\\P(A\\cap B) &= P(A)P(B)\\end{align}\\) \\(\\text{ if } P(A) > 0 \\text{ and } P(B) > 0\\)\nIntuition: Knowing \\(B\\) occurs does not change the probability that \\(A\\) occurs, and vice versa.\n\n\n\n\n\n\n\nCan we compute \\(P(A \\cap B)\\) if we only know \\(P(A)\\) and \\(P(B)\\)?\n\n\n\n\n\n\n\nNo, we cannot compute \\(P(A \\cap B)\\) since we do not know if \\(A\\) and \\(B\\) are independent.\nWe only could if \\(A\\) and \\(B\\) were independent.\nIn general, we need the multiplication rule \\(P(A \\cap B) = P(A \\mid B)P(B)\\)."
  },
  {
    "objectID": "prob-rule.html#venn-diagram-explanation-of-independence",
    "href": "prob-rule.html#venn-diagram-explanation-of-independence",
    "title": "7¬† Probability Rules",
    "section": "7.3 Venn Diagram Explanation of Independence",
    "text": "7.3 Venn Diagram Explanation of Independence"
  },
  {
    "objectID": "prob-rule.html#independence-example",
    "href": "prob-rule.html#independence-example",
    "title": "7¬† Probability Rules",
    "section": "7.4 Independence Example",
    "text": "7.4 Independence Example\n\n\n\n\n\n\nAssuming that events \\(A\\) and \\(B\\) are independent. \\(P(A) = 0.3\\) and \\(P(B) = 0.7\\).\n\n\n\n\n\\(P(A \\cap B)\\)?\n\\(P(A \\cup B)\\)?\n\\(P(A \\mid B)\\)?\n\n\n\n\n\n\n\\(P(A \\cap B) = P(A)P(B)=0.21\\)\n\\(P(A \\cup B) = P(A)+P(B)-P(A\\cap B) = 0.3+0.7-0.21=0.79\\)\n\\(P(A \\mid B) = P(A) = 0.3\\)"
  },
  {
    "objectID": "prob-rule.html#why-bayes-formula",
    "href": "prob-rule.html#why-bayes-formula",
    "title": "7¬† Probability Rules",
    "section": "7.3 Why Bayes‚Äô Formula?",
    "text": "7.3 Why Bayes‚Äô Formula?\n\nOften, we know \\(P(B \\mid A)\\) but are much more interested in \\(P(A \\mid B)\\).\nExample: diagnostic tests provide \\(P(\\text{positive test result} \\mid \\text{COVID})\\), but we are interested in \\(P(\\text{COVID} \\mid \\text{positive test result})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes‚Äô formula provides a way for finding \\(P(A \\mid B)\\) from \\(P(B \\mid A)\\)"
  },
  {
    "objectID": "prob-rule.html#bayes-formula",
    "href": "prob-rule.html#bayes-formula",
    "title": "7¬† Probability Rules",
    "section": "\n7.3 Bayes‚Äô Formula",
    "text": "7.3 Bayes‚Äô Formula\n Why Bayes‚Äô Formula? \n\nOften, we know \\(P(B \\mid A)\\), but we are much more interested in \\(P(A \\mid B)\\).\nExample: Diagnostic tests provide \\(P(\\text{positive test result} \\mid \\text{COVID})\\), but we are also interested in \\(P(\\text{COVID} \\mid \\text{positive test result})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes‚Äô formula provides a way to find \\(P(A \\mid B)\\) from \\(P(B \\mid A)\\)\n\n\n\n Formula \n\nIf \\(A\\) and \\(B\\) are events whose probabilities are not 0 or 1, then \\[\\begin{align*} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\quad ( \\text{def. of cond. prob.}) \\\\ &= \\frac{P(A \\cap B)}{P((B \\cap A) \\cup (B \\cap A^c))} \\quad ( \\text{partition } B) \\\\ &= \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}  \\quad ( \\text{multiplication rule}) \\end{align*}\\]\n\n\n\n\n\n\nFigure¬†7.6: Venn Diagram illustration for Bayes‚Äô formula\n\n\n\n\n Example: Passing Rate \n\n\nAfter taking MATH 4720, \\(80\\%\\) of students understand the Bayes‚Äô formula.\n\nOf those who understood the Bayes‚Äô formula,\n\n\n\\(95\\%\\) passed\n\n\nOf those who did not understand the Bayes‚Äô formula,\n\n\n\\(60\\%\\) passed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the probability that a student understands the Bayes‚Äô formula given the fact that she passed.\n\n\n\n\n\n\n\n\n Step 1: Formulate what we would like to compute \n\n\\(P(\\text{understood} \\mid \\text{passed})\\)\n\n\n\n Step 2: Define relevant events in the formula: \\(A\\), \\(A^c\\) and \\(B\\) \n\nLet \\(A =\\) understood and \\(B =\\) passed. Then \\(A^c =\\) didn‚Äôt understand and \\(P(\\text{understood} \\mid \\text{passed}) = P(A \\mid B)\\).\n\n\n\n Step 3: Find probabilities in the Bayes‚Äô formula using provided information. \n\n\\(P(B \\mid A) = P(\\text{passed} \\mid \\text{understood}) = 0.95\\)\n\n\\(P(B \\mid A^c) = P(\\text{passed} \\mid \\text{didn't understand}) = 0.6\\)\n\n\\(P(A) = P(\\text{understood}) = 0.8\\)\n\\(P(A^c) = 1 - P(A) = 0.2\\)\n\n\n\n Step 4: Apply Bayes‚Äô formula. \n\n\\(\\small P(\\text{understood} \\mid \\text{passed}) = P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)} = \\frac{(0.95)(0.8)}{(0.95)(0.8) + (0.6)(0.2)} = 0.86\\)\n\n\n\n Tree Diagram Illustration \n\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understood the Bayes‚Äô formula, \\(95\\%\\) passed (\\(5\\%\\) failed).\nOf those who did not understand the formula, \\(60\\%\\) passed (\\(40\\%\\) failed).\n\n\n\n\n\n\n\nFigure¬†7.7: Tree Diagram illustration of Passing Rate example\n\n\n\n\n\n\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{P(\\text{pass | yes})P(\\text{yes})}{P(\\text{pass | yes})P(\\text{yes}) + P(\\text{pass | no})P(\\text{no})} \\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]"
  },
  {
    "objectID": "prob-rule.html#example-passing-rate",
    "href": "prob-rule.html#example-passing-rate",
    "title": "7¬† Probability Rules",
    "section": "7.4 Example: Passing Rate",
    "text": "7.4 Example: Passing Rate\n\n\n\n\nAfter taking MATH 4720, \\(80\\%\\) of students understand the Bayes‚Äô formula.\n\nOf those who understand the Bayes‚Äô formula,\n\n\\(95\\%\\) passed\n\nOf those who do not understand the Bayes‚Äô formula,\n\n\\(60\\%\\) passed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the probability that a student understand the Bayes‚Äô formula given the fact that she passed."
  },
  {
    "objectID": "prob-rule.html#bayes-formula-step-by-step",
    "href": "prob-rule.html#bayes-formula-step-by-step",
    "title": "7¬† Probability Rules",
    "section": "7.4 Bayes Formula: Step-by-Step",
    "text": "7.4 Bayes Formula: Step-by-Step\n\n Step 1: Formulate what we would like to compute \n\n\\(P(\\text{understand} \\mid \\text{passed})\\)\n\n Step 2: Define relevant events in the formula: \\(A\\), \\(A^c\\) and \\(B\\) \n\nLet \\(A =\\) understand. \\(B =\\) passed. Then \\(A^c =\\) don‚Äôt understand and \\(P(\\text{understand} \\mid \\text{passed}) = P(A \\mid B)\\).\n\n Step 3: Find probabilities in the Bayes‚Äô formula using provided information. \n\n\\(P(B \\mid A) = P(\\text{passed} \\mid \\text{understand}) = 0.95\\), \\(P(B \\mid A^c) = P(\\text{passed} \\mid \\text{don't understand}) = 0.6\\)\n\\(P(A) = P(\\text{understand}) = 0.8\\), \\(P(A^c) = 1 - P(A) = 0.2\\).\n\n Step 4: Apply Bayes‚Äô formula. \n\n\\(\\small P(\\text{understand} \\mid \\text{passed}) = P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)} = \\frac{(0.95)(0.8)}{(0.95)(0.8) + (0.6)(0.2)} = 0.86\\)"
  },
  {
    "objectID": "prob-rule.html#bayes-formula-tree-diagram-illustration",
    "href": "prob-rule.html#bayes-formula-tree-diagram-illustration",
    "title": "7¬† Probability Rules",
    "section": "7.4 Bayes Formula: Tree Diagram Illustration",
    "text": "7.4 Bayes Formula: Tree Diagram Illustration\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understand the Bayes‚Äô formula, \\(95\\%\\) passed ( \\(5\\%\\) failed).\nOf those who do not understand the formula, \\(60\\%\\) passed ( \\(40\\%\\) failed).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{P(\\text{pass | yes})P(\\text{yes})}{P(\\text{pass | yes})P(\\text{yes}) + P(\\text{pass | no})P(\\text{no})} \\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]"
  },
  {
    "objectID": "prob-rv.html#random-variables",
    "href": "prob-rv.html#random-variables",
    "title": "8¬† Random Variables",
    "section": "8.1 Random Variables",
    "text": "8.1 Random Variables\n\nRecap: A variable in a data set is a characteristic that varies from one to another.\n\nA variable can be either categorical or numerical.\nNumerical variables can be either discrete or continuous.\n\nA random variable, usually written as \\(X\\) 1, is a variable whose possible values are numerical outcomes determined by chance or randomness of a procedure or experiment.\n\n Toss a coin 2 times. \\(X\\) = # of heads. \n \\(X\\) = # of accidents in W. Wisconsin Ave. per day.\n\n\nA random variable has a probability distribution associated with it, accounting for its randomness.\n\n[1] Usually in statistics, a capital \\(X\\) represents a random variable and a small \\(x\\) represents a realized value of \\(X\\).\n Discrete and Continuous Random Variables \n\nA discrete random variable takes on a finite or countable number of values.\nA continuous random variable has infinitely many values, and the collection of values is uncountable.\n The number of relationships you‚Äôve ever had is discrete variable because we can count the number and it is finite.\n\nIf we can further determine the probability that the number is 0, 1, 2, or any possible number, it is a discrete random variable.\n\n Height is continuous because it can be any number within a range. \n\nIf we have a way to quantify the probability that the height is from any value \\(a\\) to any value \\(b\\), it is a continuous random variable.\n\n\n\n Probability Distributions A Statistician Should Know \n\n\n\n\n\nhttps://github.com/rasmusab/distribution_diagrams"
  },
  {
    "objectID": "prob-rv.html#discrete-and-continuous-random-variables",
    "href": "prob-rv.html#discrete-and-continuous-random-variables",
    "title": "8¬† Random Variables",
    "section": "8.2 Discrete and Continuous Random Variables",
    "text": "8.2 Discrete and Continuous Random Variables\n\nA discrete random variable takes on a finite or countable number of values.\n\n The number of relationships you‚Äôve ever had is discrete variable because we can count the number and it is finite.\nIf we can further determine the probability that the number is 0, 1, 2 or any possible number, it is a discrete random variable.\n\nA continuous random variable has infinitely many values, and the collection of values is uncountable.\n\n Height is a continuous variable because it can be any number within a range. \nIf we have a way to quantify the probability that the height is from any value \\(a\\) to any value \\(b\\), it is a continuous random variable.\n\nFigure¬†8.1 below shows the many different types of probability distributions that exist.\n\n\n\n\n\n\nFigure¬†8.1: Probability distributions a statistician should know (https://github.com/rasmusab/distribution_diagrams)"
  },
  {
    "objectID": "prob-disc.html#a-statistician-should-know",
    "href": "prob-disc.html#a-statistician-should-know",
    "title": "9¬† Discrete Probability Distributions",
    "section": "\n9.1 A Statistician Should Know",
    "text": "9.1 A Statistician Should Know\n\n\n\n\nhttps://github.com/rasmusab/distribution_diagrams"
  },
  {
    "objectID": "prob-disc.html#discrete-probability-distribution",
    "href": "prob-disc.html#discrete-probability-distribution",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.1 Discrete Probability Distribution",
    "text": "9.1 Discrete Probability Distribution\n\nThe probability (mass) function of a discrete random variable (rv) \\(X\\) is a function \\(P(X = x)\\) (or \\(p(x)\\)) that assigns a probability for every possible number \\(x\\).\nThe probability distribution for a discrete r.v. \\(X\\) displays its probability function.\nThe display can be a table, graph, or mathematical formula of \\(P(X = x)\\).\n\n Example:ü™ôü™ô Toss a fair coin twice independently and \\(X\\) is the number of heads. \n\nThe probability distribution of \\(X\\) as a table is\n\n\n\n\n\n\n  \n    x \n    0 \n    1 \n    2 \n  \n  \n    P(X = x) \n    0.25 \n    0.5 \n    0.25 \n  \n\n\n\n\n\n\n\n\n\n\n\nüëâ \\(\\{X = x\\}\\) is an event corresponding to an event of some experiment.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the event that \\(\\{X = 0\\}\\) corresponds to?\n\n\n\n\n\n\n\n\n\n\n\n\nHow do we get \\(P(X = 0)\\), \\(P(X=1)\\) and \\(P(X=2)\\) ?"
  },
  {
    "objectID": "prob-disc.html#discrete-probability-distribution-as-a-graph",
    "href": "prob-disc.html#discrete-probability-distribution-as-a-graph",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.2 Discrete Probability Distribution as a Graph",
    "text": "9.2 Discrete Probability Distribution as a Graph\n\n\n\n\n\n\n\n\n\n\n\n\\(0 \\le P(X = x) \\le 1\\) for every value \\(x\\) of \\(X\\).\n\n \\(x = 0, 1, 2\\) \n\n\\(\\sum_{x}P(X=x) = 1\\), where \\(x\\) assumes all possible values.\n\n \\(P(X=0) + P(X = 1) + P(X = 2) = 1\\) \n\nThe probabilities for a discrete r.v. are additive because \\(\\{X = a\\}\\) and \\(\\{X = b\\}\\) are disjoint for any possible values \\(a \\ne b\\).\n\n \\(P(X = 1 \\text{ or } 2) = P(\\{X = 1\\} \\cup \\{X = 2\\}) = P(X = 1) + P(X = 2)\\)."
  },
  {
    "objectID": "prob-disc.html#mean-of-a-discrete-random-variable",
    "href": "prob-disc.html#mean-of-a-discrete-random-variable",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.3 Mean of a Discrete Random Variable",
    "text": "9.3 Mean of a Discrete Random Variable\n\nSuppose \\(X\\) takes values \\(x_1, \\dots, x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\).\nThe mean or expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[E(X) := x_1 \\times P(X = x_1) + \\dots + x_k \\times P(X = x_k) = \\sum_{i=1}^kx_iP(X=x_i)\\]\nThe Greek letter \\(\\mu\\) may be used in place of the notation \\(E(X)\\).\n\n\n\n\n\n\n\nüëâ The mean of a discrete random variable \\(X\\) is the weighted average of possible values \\(x\\) weighted by their corresponding probability.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the mean of \\(X\\) (the number of heads) in the previous example?"
  },
  {
    "objectID": "prob-disc.html#variance-of-a-discrete-random-variable",
    "href": "prob-disc.html#variance-of-a-discrete-random-variable",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.4 Variance of a Discrete Random Variable",
    "text": "9.4 Variance of a Discrete Random Variable\n\nSuppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\).\nThe variance of \\(X\\), denoted by \\(Var(X)\\) or \\(\\sigma^2\\), is \\[\\small Var(X) := (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) = \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i)\\]\nThe standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance.\n\n\n\n\n\n\n\nüëâ The variance of a discrete random variable \\(X\\) is the weighted sum of squared deviation from the mean weighted by probability values.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the variance of \\(X\\) (the number of heads) in the previous example?"
  },
  {
    "objectID": "prob-disc.html#binomial-experiment-and-random-variable",
    "href": "prob-disc.html#binomial-experiment-and-random-variable",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.2 Binomial Experiment and Random Variable",
    "text": "9.2 Binomial Experiment and Random Variable\n\nA binomial experiment is the one having the following properties:\n\nüëâ The experiment consists of a fixed number of identical trials \\(n\\).\nüëâ Each trial results in one of exactly two outcomes (success (S) and failure (F)).\nüëâ Trials are independent, meaning that the outcome of any trial does not affect the outcome of any other trial.\nüëâ The probability of success is constant for all trials.\n\nIf \\(X\\) is defined as  the number of successes observed in \\(n\\) trials , \\(X\\) is a binomial random variable.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe word success just means one of the two outcomes, and does not necessarily mean something good. \nüò≤ We can define Drug abuse as success and No drug abuse as failure."
  },
  {
    "objectID": "prob-disc.html#binomial-distribution",
    "href": "prob-disc.html#binomial-distribution",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.2 Binomial Distribution",
    "text": "9.2 Binomial Distribution\n Binomial Experiment and Random Variable \n\nA binomial experiment is one that has the following properties:\n\nüëâ The experiment consists of a fixed number of identical trials \\(n\\).\nüëâ Each trial results in one of exactly two outcomes (success (S) and failure (F)).\nüëâ Trials are independent, meaning that the outcome of one trial does not affect the outcome of any other trial.\nüëâ The probability of success is constant for all trials.\n\nIf \\(X\\) is defined as  the number of successes observed in \\(n\\) trials , then \\(X\\) is a binomial random variable.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe word success just means one of the two outcomes and does not necessarily mean something good. \nüò≤ We can define Drug abuse as success and No drug abuse as failure.\n\n\n\n\n Distribution \n\nThe probability function \\(P(X = x)\\) of a binomial r.v. \\(X\\) can be fully determined by\n\nthe number of trials, \\(n\\)\nprobability of success, \\(\\pi\\)\n\nDifferent \\((n, \\pi)\\) pairs generate different binomial probability distributions.\n\\(X\\) is said to follow a binomial distribution with parameters \\(n\\) and \\(\\pi\\), written as \\(\\color{blue}{X \\sim binomial(n, \\pi)}\\).\nThe binomial probability function is \\[ \\color{blue}{P(X = x \\mid n, \\pi) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x}, \\quad x = 0, 1, 2, \\dots, n}\\]\nThis distribution has a mean \\(\\mu = E(X) = n\\pi\\) and variance \\(\\sigma^2 = Var(X) = n\\pi(1-\\pi)\\).\n\n\n\n\n\n\n\nIf we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial r.v.?\n\n\n\n\n\n\n Example \n\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose it‚Äôs a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit.\n\\(X \\sim binomial(15, 0.2)\\).\n\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, n}\\]\n\n\n\n\n\nFigure¬†9.2: Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)\n\n\n\n\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\n\n\n\n\n\nTip\n\n\n\nNever do this by hand. We can compute them using R!\n\n\n\n Computation in R\n\n\nWith size being the number of trials and prob being the probability of success,\n\nuse dbinom(x, size, prob) to compute \\(P(X = x)\\)\nuse pbinom(q, size, prob) to compute \\(P(X \\le q)\\)\nuse pbinom(q, size, prob, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n## 1. P(X = 6)\ndbinom(x = 6, size = 15, prob = 0.2) \n\n[1] 0.04299262\n\n## 2. P(X >= 6) = 1 - P(X <= 5)\n1 - pbinom(q = 5, size = 15, prob = 0.2) \n\n[1] 0.06105143\n\n\n\n\n\n\n## 2. P(X >= 6) = P(X > 5)\npbinom(q = 5, size = 15, prob = 0.2, \n       lower.tail = FALSE)  \n\n[1] 0.06105143\n\n\n\n\n\nBelow is an example of how to generate the binomial probability distribution as a graph.\n\n\nplot(x = 0:15, y = dbinom(0:15, size = 15, prob = 0.2), \n     type = 'h', xlab = \"x\", ylab = \"P(X = x)\", \n     lwd = 5, main = \"Binomial(15, 0.2)\")"
  },
  {
    "objectID": "prob-disc.html#binomial-distribution-example",
    "href": "prob-disc.html#binomial-distribution-example",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.3 Binomial Distribution Example",
    "text": "9.3 Binomial Distribution Example\n\n\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose it is a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit.\n\\(X \\sim binomial(15, 0.2)\\).\n\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, n}\\]\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\n\n\n\n\n\nTip\n\n\n\nNever do this by hand. We can compute them using R!"
  },
  {
    "objectID": "prob-disc.html#binomial-distribution-example-x-sim-binomial15-0.2",
    "href": "prob-disc.html#binomial-distribution-example-x-sim-binomial15-0.2",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.3 Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)",
    "text": "9.3 Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)"
  },
  {
    "objectID": "prob-disc.html#binomial-distribution-example-1",
    "href": "prob-disc.html#binomial-distribution-example-1",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.9 Binomial Distribution Example",
    "text": "9.9 Binomial Distribution Example\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\n\n\n\n\n\nTip\n\n\n\nNever do this by hand. We compute them using R!"
  },
  {
    "objectID": "prob-disc.html#binomial-example-computation-in-r",
    "href": "prob-disc.html#binomial-example-computation-in-r",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.3 Binomial Example Computation in R",
    "text": "9.3 Binomial Example Computation in R\n\n\nWith size the number of trials and prob the probability of success,\n\ndbinom(x, size, prob) to compute \\(P(X = x)\\)\npbinom(q, size, prob) to compute \\(P(X \\le q)\\)\npbinom(q, size, prob, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n## 1. P(X = 6)\ndbinom(x = 6, size = 15, prob = 0.2) \n\n[1] 0.04299262\n\n## 2. P(X >= 6) = 1 - P(X <= 5)\n1 - pbinom(q = 5, size = 15, prob = 0.2) \n\n[1] 0.06105143\n\n\n\n## 2. P(X >= 6) = P(X > 5)\npbinom(q = 5, size = 15, prob = 0.2, \n       lower.tail = FALSE)  \n\n[1] 0.06105143"
  },
  {
    "objectID": "prob-disc.html#binomial15-0.2",
    "href": "prob-disc.html#binomial15-0.2",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.3 Binomial(15, 0.2)",
    "text": "9.3 Binomial(15, 0.2)\n\nplot(x = 0:15, y = dbinom(0:15, size = 15, prob = 0.2), \n     type = 'h', xlab = \"x\", ylab = \"P(X = x)\", \n     lwd = 5, main = \"Binomial(15, 0.2)\")"
  },
  {
    "objectID": "prob-disc.html#poisson-random-variables",
    "href": "prob-disc.html#poisson-random-variables",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.3 Poisson Random Variables",
    "text": "9.3 Poisson Random Variables\n\nIf we like to count the number of occurrences of some event over a unit of time or space (region) and its associated probability, we could consider the Poisson distribution.\n\nNumber of COVID patients arriving at ICU in one hour\nNumber of Marquette students logging onto D2L in one day\nNumber of dandelions per square meter in Marquette campus\n\nLet \\(X\\) be a Poisson r.v. Then \\(\\color{blue}{X \\sim Poisson(\\lambda)}\\), where \\(\\lambda\\) is the parameter representing the mean number of occurrences of the event in the interval. \\[\\color{blue}{P(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0, 1, 2, \\dots}\\] with both mean and variance being equal to \\(\\lambda\\)."
  },
  {
    "objectID": "prob-disc.html#assumptions-and-properties-of-poisson-variables",
    "href": "prob-disc.html#assumptions-and-properties-of-poisson-variables",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.4 Assumptions and Properties of Poisson Variables",
    "text": "9.4 Assumptions and Properties of Poisson Variables\n\nüëâ Events occur one at a time; two or more events do not occur at the same time or in the same space or spot.\nüëâ The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a nonoverlapping time period or region of space.\nüëâ \\(\\lambda\\) is constant of any period or region.\n\n\n\n\n\n\n\nCan you find the difference between Binomial and Poisson distributions?\n\n\n\n\n\n\n\nThe Poisson distribution\n\nis determined by one single parameter \\(\\lambda\\)\nhas possible values \\(x = 0, 1, 2, \\dots\\) with no upper limit (countable), while a binomial variable has possible values \\(0, 1, 2, \\dots, n\\) (finite)"
  },
  {
    "objectID": "prob-disc.html#poisson-distribution-example",
    "href": "prob-disc.html#poisson-distribution-example",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.4 Poisson Distribution Example",
    "text": "9.4 Poisson Distribution Example\n\n\n\n\nLast year there were 4200 births at the University of Wisconsin Hospital. Assume \\(X\\) be the number of births in a given day at the center, and \\(X \\sim Poisson(\\lambda)\\). Find\n\n\\(\\lambda\\), the mean number of births per day.\nthe probability that on a randomly selected day, there are exactly 10 births.\n\\(P(X > 10)\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\small \\lambda = \\frac{\\text{Number of birth in a year}}{\\text{Number of days}} = \\frac{4200}{365} = 11.5\\)\n\\(\\small P(X = 10 \\mid \\lambda = 11.5) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = \\frac{11.5^{10} e^{-11.5}}{10!} = 0.113\\)\n\\(\\small P(X > 10) = p(11) + p(12) + \\dots + p(20) + \\dots\\) (No end!) \\(\\small P(X > 10) = 1 - P(X \\le 10) = 1 - (p(1) + p(2) + \\dots + p(10))\\)."
  },
  {
    "objectID": "prob-disc.html#poisson-example-compuatation-in-r",
    "href": "prob-disc.html#poisson-example-compuatation-in-r",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.4 Poisson Example Compuatation in R",
    "text": "9.4 Poisson Example Compuatation in R\n\n\n\nWith lambda the mean of Poisson distribution,\n\ndpois(x, lambda) to compute \\(P(X = x)\\)\nppois(q, lambda) to compute \\(P(X \\le q)\\)\nppois(q, lambda, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n(lam <- 4200 / 365)\n\n[1] 11.50685\n\n## P(X = 10)\ndpois(x = 10, lambda = lam)  \n\n[1] 0.112834\n\n\n\n## P(X > 10) = 1 - P(X <= 10)\n1 - ppois(q = 10, lambda = lam)  \n\n[1] 0.5990436\n\n## P(X > 10)\nppois(q = 10, lambda = lam, \n      lower.tail = FALSE) \n\n[1] 0.5990436"
  },
  {
    "objectID": "prob-disc.html#poisson11.5",
    "href": "prob-disc.html#poisson11.5",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.4 Poisson(11.5)",
    "text": "9.4 Poisson(11.5)\n\n\\(X\\) has no upper limit. The graph is truncated at \\(x = 24\\).\n\n\nplot(0:24, dpois(0:24, lambda = lam), type = 'h', lwd = 5, \n     ylab = \"P(X = x)\", xlab = \"x\", main = \"Poisson(11.5)\")"
  },
  {
    "objectID": "prob-cont.html#continuous-probability-distributions",
    "href": "prob-cont.html#continuous-probability-distributions",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.1 Continuous Probability Distributions",
    "text": "10.1 Continuous Probability Distributions\n Introduction \n\nA continuous r.v. can take on any values from an interval of the real line.\nInstead of probability functions, a continuous r.v. \\(X\\) has the probability density function (pdf) \\(f(x)\\) such that for any real value \\(a < b\\), \\[P(a < X < b) = \\int_{a}^b f(x) dx\\]\nThe cumulative distribution function (cdf) of \\(X\\) is defined as \\[F(x) := P(X \\le x) = \\int_{-\\infty}^x f(t)dt\\]\nEvery pdf must satisfy  (1) \\(f(x) \\ge 0\\) for all \\(x\\); (2) \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\) \n\nüòé Luckily we don‚Äôt deal with integrals in this course.\n Density Curve \n\nA pdf generates a graph called the density curve that shows the likelihood of a random variable at all possible values.\n\\(P(a < X < b) = \\int_{a}^b f(x) dx\\): The area under the density curve between \\(a\\) and \\(b\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\): The total area under any density curve is equal to 1.\n\n\n\n\n\n\n\n\n\n\n Commonly Used Continuous Distributions \n\nR Shiny app is a Continuous Distribution calculator.\nIn this course, we will touch on normal (Gaussian), student‚Äôs t, chi-square, F\nSome other common distributions include uniform, exponential, gamma, beta, inverse gamma, Cauchy, etc. (MATH 4700)"
  },
  {
    "objectID": "prob-cont.html#density-curve",
    "href": "prob-cont.html#density-curve",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.2 Density Curve",
    "text": "10.2 Density Curve\n\nA pdf generates a graph called the density curve that shows the likelihood of a random variable at all possible values.\n\\(P(a < X < b) = \\int_{a}^b f(x) dx\\): The area under the density curve between \\(a\\) and \\(b\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\): The total area under any density curve is equal to 1."
  },
  {
    "objectID": "prob-cont.html#commonly-used-continuous-distributions",
    "href": "prob-cont.html#commonly-used-continuous-distributions",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.3 Commonly Used Continuous Distributions",
    "text": "10.3 Commonly Used Continuous Distributions\n\nR Shiny app is at Continuous Distribution\nIn this course, we will touch normal (Gaussian), student‚Äôs t, chi-square, F\nSome other common distributions include uniform, exponential, gamma, beta, inverse gamma, Cauchy, etc. (MATH 4700)"
  },
  {
    "objectID": "prob-cont.html#normal-gaussian-distribution",
    "href": "prob-cont.html#normal-gaussian-distribution",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.2 Normal (Gaussian) Distribution",
    "text": "10.2 Normal (Gaussian) Distribution\n\nThe normal distribution, \\(N(\\mu, \\sigma^2\\)), has the probability distribution function given by \\[\\small f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty < x < \\infty\\]\n\nThis distribution has two parameters: mean, \\(\\mu\\), and variance, \\(\\sigma^2\\) (standard deviation \\(\\sigma\\)).\nIt is always bell-shaped and symmetric about the mean, \\(\\mu\\).\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), \\(N(0, 1)\\) is called standard normal.\n\nBelow are examples of normal distribution curves and how they change with different means and standard deviations.\n\n\n\n\n\n\nFigure¬†10.2: Normal density curve with mean 100 and standard deviation 15\n\n\n\n\n\n\n\n\n\nFigure¬†10.3: Normal density curves with varying means and standard deviations"
  },
  {
    "objectID": "prob-cont.html#normal-density-curves",
    "href": "prob-cont.html#normal-density-curves",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.3 Normal Density Curves",
    "text": "10.3 Normal Density Curves"
  },
  {
    "objectID": "prob-cont.html#standardization-and-z-scores",
    "href": "prob-cont.html#standardization-and-z-scores",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.3 Standardization and Z-Scores",
    "text": "10.3 Standardization and Z-Scores\n\nStandardization allows us to convert \\(N(\\mu, \\sigma^2)\\) to \\(N(0, 1)\\).\nWhy do we perform standardization?\n\nWe want to put data on a standardized scale, because it helps us make comparisons more easily!\n\nIf \\(x\\) is an observation from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the standardized value of \\(x\\) is its so-called \\(z\\)-score: \\[z = \\frac{x - \\mu}{\\sigma}\\]\nThe \\(z\\)-score tells us how many standard deviations \\(x\\) falls away from the mean and in which direction.\n\nObservations larger than the mean have positive \\(z\\)-scores.\nObservations smaller than the mean have negative \\(z\\)-scores.\nA \\(z\\)-score -1.2 means that \\(x\\) is 1.2 standard deviations to the left of or below the mean.\nA \\(z\\)-score 1.8 means that \\(x\\) is 1.8 standard deviations to the right of or above the mean.\n\n If \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma}\\) follows the standard normal distribution \\(Z \\sim N(0, 1)\\). \n\n\n Illustration \n\n\\(X - \\mu\\) shifts the mean from \\(\\mu\\) to 0.\n\n\n\n\n\n\nFigure¬†10.4: Standardization shifts mean from 3 to 0\n\n\n\n\n\n\\(\\frac{X - \\mu}{\\sigma}\\) scales the variation from 4 to 1.\n\n\n\n\n\n\nFigure¬†10.5: Standardization scales variance from 4 to 1\n\n\n\n\n\nA value of \\(x\\) that is 2 standard deviation below the mean, \\(\\mu\\), corresponds to \\(z = -2\\).\n\n\\(z = \\frac{x -\\mu}{\\sigma} \\iff x = \\mu + z\\sigma\\). If \\(z = -2\\), \\(x = \\mu - 2\\sigma\\).\n\nFigure¬†10.6 depicts how the values on the x-axis change when standardization is performed.\n\n\n\n\n\n\nFigure¬†10.6: Standardized Normal Distribution\n\n\n\n\n\n SAT and ACT Example \n\nStandardization can help us compare the performance of students on the SAT and ACT, which both have nearly normal distributions.\n\nThe table below lists the parameters for each distribution.\n\n\n\n\n\nMeasure\nSAT\nACT\n\n\n\n\nMean\n1100\n21\n\n\nSD\n200\n6\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to determine whether Anna or Tommy performed better on their respective tests.\n\nAnna scored a 1300 on her SAT and Tommy scored a 24 on his ACT.\n\n\n Standardization \n\n\\(z_{A} = \\frac{x_{A} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1300-1100}{200} = 1\\); \\(z_{T} = \\frac{x_{T} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24-21}{6} = 0.5\\).\n\n\n\n\n\n\nFigure¬†10.7: Standardization of Anna and Tommy‚Äôs scores\n\n\n\n\n\nThis standardization tells us that Anna scored 1 standard deviation above the mean and Tommy scored 0.5 standard deviations above the mean.\nFrom this information, we can conclude that Anna performed better on teh SAT than Tommy performed on the ACT."
  },
  {
    "objectID": "prob-cont.html#standardization-illustration",
    "href": "prob-cont.html#standardization-illustration",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.4 Standardization Illustration",
    "text": "10.4 Standardization Illustration\n\n\\(X - \\mu\\) shifts the mean from \\(\\mu\\) to 0\n\n\n\n\n\n\n\n\n\n\n\n\\(\\frac{X - \\mu}{\\sigma}\\) scales the variation from 4 to 1\n\n\n\n\n\n\n\n\n\n\n\nA value of \\(x\\) that is 2 standard deviation below \\(\\mu\\) corresponds to \\(z = -2\\).\n\\(z = \\frac{x -\\mu}{\\sigma} \\iff x = \\mu + z\\sigma\\). If \\(z = -2\\), \\(x = \\mu - 2\\sigma\\)."
  },
  {
    "objectID": "prob-cont.html#sat-and-act-example",
    "href": "prob-cont.html#sat-and-act-example",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.5 SAT and ACT Example",
    "text": "10.5 SAT and ACT Example\n\nWhat is the 95th percentile for SAT scores?\n Step 1: State the problem \n\n We want to find \\(x\\) s.t \\(P(X < x) = 0.95\\). \n\n Step 2: Draw a picture\n\n\n\n\n\n\nFigure¬†10.11: Picture for the 95th percentile of SAT scores\n\n\n\n\n\n\n\n\n\n\nFind an \\(x\\) value of the normal distribution, not an area (probability), which is 0.95.\n\n\n\n\n\n\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z < z) = 0.95\\) using qnorm():\n\n\n(z_95 <- qnorm(0.95))\n\n[1] 1.644854\n\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma\\). \n\n\n\n(x_95 <- 1100 + z_95 * 200)\n\n[1] 1428.971\n\n\n\nThe 95th percentile for SAT scores is 1429.\n\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1428.971\n\n\n\n Normal Percentiles"
  },
  {
    "objectID": "prob-cont.html#finding-tail-areas-px-x",
    "href": "prob-cont.html#finding-tail-areas-px-x",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.4 Finding Tail Areas \\(P(X < x)\\)",
    "text": "10.4 Finding Tail Areas \\(P(X < x)\\)\n\n\n\n\n\n\nWhat fraction of students have an SAT score below Anna‚Äôs score of 1300?\n\n\n\n\n\n\n\nThis is the same as the percentile Anna is at, which is the percentage of cases that have lower scores than Anna.\nNeed \\(P(X < 1300 \\mid \\mu = 1100, \\sigma = 200)\\) or \\(P(Z < 1 \\mid \\mu = 0, \\sigma = 1)\\).\n\n\n\n\n\n\n\n\n\n\n\n Calculation in R \n\nWith mean and sd representing the mean and standard deviation of a normal distribution\n\npnorm(q, mean, sd) to compute \\(P(X \\le q)\\)\npnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n\n\n\n\n\n\npnorm(1, mean = 0, sd = 1)\n\n[1] 0.8413447\n\npnorm(1300, mean = 1100, sd = 200)\n\n[1] 0.8413447\n\n\n\nThe shaded area represents the proportion 84.1% of SAT test takers who had z-score below 1."
  },
  {
    "objectID": "prob-cont.html#finding-tail-areas-px-x-in-r",
    "href": "prob-cont.html#finding-tail-areas-px-x-in-r",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.5 Finding Tail Areas \\(P(X < x)\\) in R",
    "text": "10.5 Finding Tail Areas \\(P(X < x)\\) in R\n\nWith mean and sd representing the mean and standard deviation of a normal distribution\n\npnorm(q, mean, sd) to compute \\(P(X \\le q)\\)\npnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n\n\n\n\n\npnorm(1, mean = 0, sd = 1)\n\n[1] 0.8413447\n\npnorm(1300, mean = 1100, sd = 200)\n\n[1] 0.8413447\n\n\n\n\n\nThe shaded area represents the proportion 84.1% of SAT test takers who had z-score below 1."
  },
  {
    "objectID": "prob-cont.html#sat-example-contd",
    "href": "prob-cont.html#sat-example-contd",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.7 SAT Example Cont‚Äôd",
    "text": "10.7 SAT Example Cont‚Äôd\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z < z) = 0.95\\) using qnorm():\n\n\n(z_95 <- qnorm(0.95))\n\n[1] 1.644854\n\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma\\). \n\n\n\n(x_95 <- 1100 + z_95 * 200)\n\n[1] 1428.971\n\n\n\nThe 95th percentile for SAT scores is 1429.\n\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1428.971"
  },
  {
    "objectID": "prob-cont.html#normal-percentiles-in-r",
    "href": "prob-cont.html#normal-percentiles-in-r",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.4 Normal Percentiles in R",
    "text": "10.4 Normal Percentiles in R\n\nTo get the \\(100p\\)-th percentile (or the \\(p\\) quantile \\(q\\) ), given probability \\(p\\), we use\n\nqnorm(p, mean, sd) to get a value of \\(X\\), \\(q\\), such that \\(P(X \\le q) = p\\)\nqnorm(p, mean, sd, lower.tail = FALSE) to get \\(q\\) such that \\(P(X \\ge q) = p\\)"
  },
  {
    "objectID": "prob-cont.html#sat-example",
    "href": "prob-cont.html#sat-example",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.6 SAT Example",
    "text": "10.6 SAT Example\n\nWhat is the 95th percentile for SAT scores?\n Step 1: State the problem \n\n We want to find \\(x\\) s.t \\(P(X < x) = 0.95\\). \n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind an \\(x\\) value of the normal distribution, not an area (probability), which is 0.95."
  },
  {
    "objectID": "prob-cont.html#sat-example-contd-1",
    "href": "prob-cont.html#sat-example-contd-1",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.7 SAT Example Cont‚Äôd",
    "text": "10.7 SAT Example Cont‚Äôd\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z < z) = 0.95\\) using qnorm():\n\n\n(z_95 <- qnorm(0.95))\n\n[1] 1.644854\n\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma\\). \n\n\n\n(x_95 <- 1100 + z_95 * 200)\n\n[1] 1428.971\n\n\n\nThe 95th percentile for SAT scores is 1429.\n\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1428.971"
  },
  {
    "objectID": "prob-cont.html#finding-probabilties",
    "href": "prob-cont.html#finding-probabilties",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.5 Finding Probabilties",
    "text": "10.5 Finding Probabilties\nüëâ ALWAYS draw and label the normal curve and shade the area of interest.\n\nüëâ Less than\n\n\\(\\small P(X < x) = P(Z < z)\\)\npnorm(z, mean = 0, sd = 1)\npnorm(x, mean = mu, sd = sigma)\n\nüëâ Greater than\n\n\\(\\small P(X > x) = P(Z > z) = 1 - P(Z \\le z)\\)\n1 - pnorm(z)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nStandardization is not a must.\nIf we don‚Äôt standardize, we must specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma).\n\n\n\n\nüëâ Between two numbers\n\n\\(\\small P(a < X < b) = P(z_a < Z < z_b) = P(Z < z_b) - P(Z < z_a)\\)\npnorm(z_b) - pnorm(z_a)\n\nüëâ Outside of two numbers \\((a < b)\\) \\[\\small \\begin{align} P(X < a \\text{ or } X > b) &= P(Z < z_a \\text{ or } Z > z_b) \\\\ &= P(Z < z_a) + P(Z > z_b) \\\\ &= P(Z < z_a) + 1 - P(Z < z_b) \\end{align}\\]\n\npnorm(z_a) + 1 - pnorm(z_b)\npnorm(z_a) + pnorm(z_b, lower.tail = FALSE)\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAny probability can be computed using the ‚Äúless than‚Äù form (lower or left tail).\nIf the calculation involves the ‚Äúgreater than‚Äù form, add lower.tail = FALSE in pnorm()."
  },
  {
    "objectID": "prob-cont.html#checking-normality-normal-quantile-plot",
    "href": "prob-cont.html#checking-normality-normal-quantile-plot",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.6 Checking Normality: Normal Quantile Plot",
    "text": "10.6 Checking Normality: Normal Quantile Plot\n\nMany statistical methods assume variables are normally distributed.\nTherefore, testing the appropriateness of the normal assumption is a key step.\nWe can check this normality assumption using a normal quantile plot (normal probability plot) or a Quantile-Quantile plot (QQ plot).\n\n\\(X\\)-axis: Quantiles of the ordered data if the data were normally distributed.\n\\(Y\\)-axis: Ordered data values\n\nIf the data are normally distributed, the points on the QQ plot will lie close to a straight line.\n\n\n QQ plot in R \n\nqqnorm(normal_sample, main = \"Normal data\", col = 4)\nqqline(normal_sample)\nqqnorm(right_skewed_sample, main = \"Right-skewed data\", col = 6)\nqqline(right_skewed_sample)\n\n\n\n\nFigure¬†10.12: QQ plots for normal and right-skewed data samples"
  },
  {
    "objectID": "prob-cont.html#qq-plot-in-r",
    "href": "prob-cont.html#qq-plot-in-r",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.8 QQ plot in R",
    "text": "10.8 QQ plot in R\n\nqqnorm(normal_sample, main = \"Normal data\", col = 4)\nqqline(normal_sample)\nqqnorm(right_skewed_sample, main = \"Right-skewed data\", col = 6)\nqqline(right_skewed_sample)"
  },
  {
    "objectID": "prob-samdist.html#parameter",
    "href": "prob-samdist.html#parameter",
    "title": "11¬† Sampling Distribution",
    "section": "11.1 Parameter",
    "text": "11.1 Parameter\n\nA parameter is a number that describes a probability distribution.\n\n Binomial: two parameters \\(n\\) and \\(\\pi\\) \n Poisson: one parameter \\(\\mu\\) \n Normal: two parameters \\(\\mu\\) and \\(\\sigma\\) \n\nIn statistics, we usually assume our target population follows some distribution, but its parameters are unknown to us.\n\n\n\n\n Human weight follows \\(N(\\mu, \\sigma^2)\\) \n\n\n # of snowstorms in one year follows \\(Poisson(\\lambda)\\)"
  },
  {
    "objectID": "prob-samdist.html#treat-each-data-point-as-a-random-variable",
    "href": "prob-samdist.html#treat-each-data-point-as-a-random-variable",
    "title": "11¬† Sampling Distribution",
    "section": "11.2 Treat Each Data Point as a Random Variable",
    "text": "11.2 Treat Each Data Point as a Random Variable\n\n\\(n\\) random variables: \\(X_1, X_2, \\dots, X_n\\).\nAssume \\(X_1, X_2, \\dots, X_n\\) follow the same distribution.\n\n\n\n\n\n\n\nView \\(X_i\\) as a data point to be drawn from a population with some distribution, say \\(N(\\mu, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssume that \\(X_1, X_2, \\dots, X_n\\) are independent, i.e., the distribution/value of \\(X_i\\) is not affected by any other \\(X_j\\).\nWith the same distribution, \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed (i.i.d.), for example,  \\(X_1, X_2, \\dots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) \n\\((X_1, X_2, \\dots, X_n)\\) is a random sample of size \\(n\\) from the population.\n\n \\(X_1, X_2, \\dots, X_{50}\\) are randomly selected SAT scores from the SAT score population that follows \\(N(1100, 200^2)\\) \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nBefore we actually collect the data, the data \\(X_1, X_2, \\dots, X_n\\) are random variables from the population distribution \\(N(\\mu, \\sigma^2)\\).\nOnce we collect the data, we know the realized value of these random variables: \\(x_1, x_2, \\dots, x_n\\)."
  },
  {
    "objectID": "prob-samdist.html#sampling-distribution",
    "href": "prob-samdist.html#sampling-distribution",
    "title": "11¬† Sampling Distribution",
    "section": "11.2 Sampling Distribution",
    "text": "11.2 Sampling Distribution\n\nAny value computed from a sample \\((X_1, X_2, \\dots, X_n)\\) is called a (sample) statistic.\n\n The sample mean \\(\\frac{1}{n}\\sum_{i=1}^n X_i\\) is a statistic. \n Sample variance \\(\\frac{\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2}{n-1}\\) is also a statistic. \n\nSince \\(X_1, X_2, \\dots, X_n\\) are random variables, any transformation or function of \\((X_1, X_2, \\dots, X_n)\\) or its statistics is also a random variable.\nThe probability distribution of a statistic is called the sampling distribution of that statistic.\n\nIt is the probability distribution of that statistic if we were to repeatedly draw samples of the same size from the population.\n\n\n\n\n\n\n\n\nDoes the sample mean \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) have a sampling distribution?\n\n\n\n\nYes!\n\n\n\n\n Sampling Distribution of Sample Mean \n\n\n\n\n\nFigure¬†11.2: Sampling distribution of sample means (Biostatistics for the Biological and Health Sciences p.241)\n\n\n\n\n\nSampling Distribution Applet\n\n\n\n\n\n\n\nWhat are the differences between the sampling distribution of \\(\\overline{X}\\) and the population distribution each individual random variable, \\(X_i\\), is drawn from?\n\n\n\n\nSample means \\((\\overline{X})\\) are  less variable  than individual observations \\(X_i\\).\nSample means \\((\\overline{X})\\) are  more normal  than individual observations \\(X_i\\).\n\n\n\n\nSuppose \\((X_1, \\dots, X_n)\\) is the random sample from a population distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\).\nThe mean of the sampling distribution of the sample mean, \\(\\overline{X} = \\frac{\\sum_{i=1}^nX_i}{n}\\), is  \\(\\mu_{\\overline{X}} = \\mu\\) .\nThe standard deviation of the sampling distribution of the sample mean, \\(\\overline{X}\\), is  \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) .\n\n\\(\\sigma_{\\overline{X}}\\) is also known as the standard error of \\(\\overline{X}\\).\n\nIf the population distribution is  \\(N(\\mu, \\sigma^2)\\) , the sampling distribution of \\(\\overline{X}\\) is  \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\) .\nFigure¬†11.3 depicts that sampling distributions are less variable and more normal than the population distribution.\n\n\n\n\n\n\nFigure¬†11.3: Comparison between sampling distributions and the population distribution\n\n\n\n\n Example: Rolling a Die \n\nRoll a fair die 3 times üé≤üé≤ üé≤ independently to obtain 3 values from the population \\(\\{1, 2, 3, 4, 5, 6\\}\\).\nRepeat the process 10,000 times and plot the histogram of the sampling mean.\nFigure¬†11.4 shows that the population distribution is not normal, but the sampling distribution of the sample mean is normal.\n\n\n\n\n\n\n\n\nFigure¬†11.4: Population distribution is Binomial\n\n\n\n\n\n\n\n\n\n\nFigure¬†11.5: Sampling distribution of sample means is normal"
  },
  {
    "objectID": "prob-samdist.html#example-sampling-distribution-of-the-sample-mean",
    "href": "prob-samdist.html#example-sampling-distribution-of-the-sample-mean",
    "title": "11¬† Sampling Distribution",
    "section": "11.3 Example: Sampling Distribution of the Sample Mean",
    "text": "11.3 Example: Sampling Distribution of the Sample Mean\n\nRoll a fair die 3 times üé≤üé≤ üé≤ independently to obtain 3 values from the population \\(\\{1, 2, 3, 4, 5, 6\\}\\).\nRepeat the process 10,000 times and plot the histogram of the sampling mean."
  },
  {
    "objectID": "prob-samdist.html#sampling-distribution-of-sample-mean",
    "href": "prob-samdist.html#sampling-distribution-of-sample-mean",
    "title": "11¬† Sampling Distribution",
    "section": "11.3 Sampling Distribution of Sample Mean",
    "text": "11.3 Sampling Distribution of Sample Mean\n\n\nSuppose \\((X_1, \\dots, X_n)\\) is the random sample from a population distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThe mean of the sampling distribution of the sample mean, \\(\\overline{X} = \\frac{\\sum_{i=1}^nX_i}{n}\\), is  \\(\\mu_{\\overline{X}} = \\mu\\) .\nThe standard deviation of the sampling distribution of the sample mean \\(\\overline{X}\\) is  \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) . \nIf the population distribution is  \\(N(\\mu, \\sigma^2)\\) , the sampling distribution of \\(\\overline{X}\\) is exactly  \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\) ."
  },
  {
    "objectID": "prob-samdist.html#standardization-of-sample-mean",
    "href": "prob-samdist.html#standardization-of-sample-mean",
    "title": "11¬† Sampling Distribution",
    "section": "11.3 Standardization of Sample Mean",
    "text": "11.3 Standardization of Sample Mean\n\nFor a single random variable \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\).\nFor the sample mean of \\(n\\) variables, \\(\\overline{X} \\sim N(\\mu_{\\overline{X}}, \\sigma^2_{\\overline{X}}) = N(\\mu, \\frac{\\sigma^2}{n})\\).\nHence,  \\[Z = \\frac{\\overline{X} - \\mu_{\\overline{X}}}{\\sigma_{\\overline{X}}} = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\]\n\n Example: Psychomotor Retardation \n\n\n\nPsychomotor retardation scores for a group of patients have a normal distribution with a mean of 930 and a standard deviation of 130.\nWhat is the probability that the mean retardation score of a random sample of 20 patients was between 900 and 960?\n\\(X_1, \\dots, X_{20} \\stackrel{iid}{\\sim} N(930, 130^2)\\), then \\(\\overline{X} = \\frac{\\sum_{i=1}^{20}X_i}{20} \\sim N\\left(930, \\frac{130^2}{20} \\right)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\small \\begin{align}\nP(900 < \\overline{X} < 960) &= P\\left( \\frac{900-930}{130/\\sqrt{20}} < \\frac{\\overline{X}-930}{130/\\sqrt{20}} < \\frac{960-930}{130/\\sqrt{20}}\\right)=P(-1.03 < Z < 1.03)\\\\\n&=P(Z < 1.03) - P(Z < -1.03)\n  \\end{align}\\]\n\npnorm(1.03) - pnorm(-1.03)\n\n[1] 0.69699\n\n\n\npnorm(960, mean = 930, sd = 130/sqrt(20)) - pnorm(900, mean = 930, sd = 130/sqrt(20))\n\n[1] 0.6979426\n\n\n\nThe probability that the mean psychomotor retardation score of a random sample of 20 patients is between 900 and 960 is about 70%."
  },
  {
    "objectID": "prob-samdist.html#example---psychomotor-retardation",
    "href": "prob-samdist.html#example---psychomotor-retardation",
    "title": "11¬† Sampling Distribution",
    "section": "11.4 Example - Psychomotor retardation",
    "text": "11.4 Example - Psychomotor retardation\n\n\n\n\nPsychomotor retardation scores for a group of patients have a normal distribution with a mean of 930 and a standard deviation of 130.\nWhat is the probability that the mean retardation score of a random sample of 20 patients was between 900 and 960?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X_1, \\dots, X_{20} \\stackrel{iid}{\\sim} N(930, 130^2)\\), then \\(\\overline{X} = \\frac{\\sum_{i=1}^{20}X_i}{20} \\sim N\\left(930, \\frac{130^2}{20} \\right)\\).\n\n\\[\\small \\begin{align}\nP(900 < \\overline{X} < 960) &= P\\left( \\frac{900-930}{130/\\sqrt{20}} < \\frac{\\overline{X}-930}{130/\\sqrt{20}} < \\frac{960-930}{130/\\sqrt{20}}\\right)=P(-1.03 < Z < 1.03)\\\\\n&=P(Z < 1.03) - P(Z < -1.03)\n  \\end{align}\\]\n\npnorm(1.03) - pnorm(-1.03)\n\n[1] 0.69699\n\n\n\npnorm(960, mean = 930, sd = 130/sqrt(20)) - pnorm(900, mean = 930, sd = 130/sqrt(20))\n\n[1] 0.6979426"
  },
  {
    "objectID": "prob-llnclt.html#why-use-normal-central-limit-theorem",
    "href": "prob-llnclt.html#why-use-normal-central-limit-theorem",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.2 Why Use Normal? Central Limit Theorem",
    "text": "12.2 Why Use Normal? Central Limit Theorem\n\nCentral Limit Theorem (CLT): Suppose \\(\\overline{X}\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma < \\infty\\). As \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\), regardless of the distribution from which we are sampling!\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png"
  },
  {
    "objectID": "prob-llnclt.html#clt-illustration-a-right-skewed-distribution",
    "href": "prob-llnclt.html#clt-illustration-a-right-skewed-distribution",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.3 CLT Illustration: A Right-Skewed Distribution",
    "text": "12.3 CLT Illustration: A Right-Skewed Distribution"
  },
  {
    "objectID": "prob-llnclt.html#clt-illustration-a-u-shaped-distribution",
    "href": "prob-llnclt.html#clt-illustration-a-u-shaped-distribution",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.4 CLT Illustration: A U-shaped Distribution",
    "text": "12.4 CLT Illustration: A U-shaped Distribution"
  },
  {
    "objectID": "prob-llnclt.html#why-clt-is-important",
    "href": "prob-llnclt.html#why-clt-is-important",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.2 Why CLT is Important?",
    "text": "12.2 Why CLT is Important?\n\n\nMany well-developed statistical methods are based on normal distribution assumption.\nWith CLT, we can use those methods even if we are sampling from a non-normal distribution, or we have no idea of the population distribution, provided that the sample size is large."
  },
  {
    "objectID": "prob-llnclt.html#clt-example",
    "href": "prob-llnclt.html#clt-example",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.3 CLT Example",
    "text": "12.3 CLT Example\n\n\n\n\nSuppose that selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000.\nIn 100 randomly selected sales, what is the probability the average selling price is more than $400,000?\nSince the sample size is fairly large \\((n = 100)\\), by CLT, the sampling distribution of the average selling price is approximately normal with mean 382,000 and SD \\(150,000 / \\sqrt{100}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(\\overline{X} > 400000) = P\\left(\\frac{\\overline{X} - 382000}{150000/\\sqrt{100}} > \\frac{400000 - 382000}{150000/\\sqrt{100}}\\right) \\approx P(Z > 1.2)\\) where \\(Z \\sim N(0, 1)\\).\n\n\n\npnorm(1.2, lower.tail = FALSE)\n\n[1] 0.1150697\n\npnorm(400000, mean = 382000, sd = 150000/sqrt(100), lower.tail = FALSE)\n\n[1] 0.1150697"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Introduction to Statistics",
    "section": "Welcome",
    "text": "Welcome\nThis is the website for my introductory statistics book. This book serves as a main reference book for my MATH 4720 Statistical Methods and MATH 4740 Biostatistical Methods at Marquette University.1 Some topics can also be discussed in an introductory data science course. You‚Äôll learn basic probability and statistical concepts as well as data analysis techniques such as linear regression using R computing software. The book balances the following aspects of statistics:\n\nmathematical derivation and statistical computation\ndistribution-based and simulation-based inferences\nmethodology and applications\nclassical/frequentist and Bayesian approaches\n\nCourse materials are borrowed from the following books:\n\nOpenIntro Statsitics (data oriented)\nIntroduction to Modern Statistics (computation oriented)\nAn Introduction to Statistical Methods & Data Analysis, 7th edition (mathematics oriented)"
  },
  {
    "objectID": "prob-rv.html#a-statistician-should-know",
    "href": "prob-rv.html#a-statistician-should-know",
    "title": "8¬† Random Variables",
    "section": "8.2 A Statistician Should Know",
    "text": "8.2 A Statistician Should Know\n\n\n\n\n\nhttps://github.com/rasmusab/distribution_diagrams"
  },
  {
    "objectID": "infer-ci.html#inference-framework",
    "href": "infer-ci.html#inference-framework",
    "title": "13¬† Confidence Interval",
    "section": "13.1 Inference Framework",
    "text": "13.1 Inference Framework\n\nInferential statistics uses sample data to learn about an unknown population.\nIdea: Assume the target population follows some distribution but with unknown parameters.\n\n Assume the population is normally distributed, but don‚Äôt know its mean and/or variance. Marquette students‚Äô mean GPA for example. \n\nGoal: Learning the unknown parameters of the assumed population distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo approaches in parameter learning: Estimation and Hypothesis testing."
  },
  {
    "objectID": "infer-ci.html#point-estimator",
    "href": "infer-ci.html#point-estimator",
    "title": "13¬† Confidence Interval",
    "section": "13.2 Point Estimator",
    "text": "13.2 Point Estimator\n\n\n\n\n\n\nIf you could only use a single number to guess the unknown population mean, \\(\\mu\\), what number would you like to use?\n\n\n\n\n\n\n\nThe single point used to estimate the unknown parameter is known as a point estimator.\nA point estimator is any function of data \\((X_1, X_2, \\dots, X_n)\\).\n\nAny statistic is considered a point estimator (before actually being collected).\n\nA point estimate is a value of a point estimator used to estimate a population parameter.\n\nThis is a value calculated from the collected data.\n\nThe sample mean, \\((\\overline{X})\\), is a statistic and a point estimator for the population mean, \\(\\mu\\).\n\n\n Sample Mean as an Point Estimator \n\nDraw 5 values from the population that follows \\(N(2, 1)\\) as sample data \\((x_1, x_2, x_3, x_4, x_5)\\).\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n\n\n1.64\n1.84\n1.23\n0.83\n1.68\n1.45\n\n\n\n\n\n\n\\(\\mu = 2\\), and we use the point estimate \\(\\overline{x}=\\) 1.45 to estimate it.\n\n\n\n\n\n\n\nWhy is \\(\\overline{x}\\) not equal to \\(\\mu\\)?\n\n\n\n\nDue to its randomness nature\n\n\n\n\n\n\n\n\nFigure¬†13.3: Sample mean has randomness associated with it\n\n\n\n\n\nIf another sample of size \\(5\\) is drawn from the same population,\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n\n\n1.65\n1.41\n0.41\n3.69\n2.56\n1.95\n\n\n\n\n\n\nThe second sample mean, \\(\\overline{x} =\\) 1.95, is different from the first one.\n\n\n\n\n\n\n\nWhy do the first sample and the second sample give us different sample means?\n\n\n\n\nA point estimator has its own sampling distribution.\n\n\n\n\n\n\n\n\nFigure¬†13.4: Sampling Distribution of Sampling Mean\n\n\n\n\n\n Why Point Estimates Are Not Enough \n\n\n\n\n\n\nIf you want to estimate \\(\\mu\\), would you prefer to report a range of values the parameter might be in or a single estimate like \\(\\overline{x}\\)?\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to catch a fish, would you prefer to use a spear or a net?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue to the variation of \\(\\overline{X}\\), if we report a point estimate \\(\\overline{x}\\), we probably won‚Äôt hit the exact \\(\\mu\\).\nIf we report a range of plausible values, we have a better shot at capturing the parameter!"
  },
  {
    "objectID": "infer-ci.html#sample-mean-as-an-point-estimator",
    "href": "infer-ci.html#sample-mean-as-an-point-estimator",
    "title": "13¬† Confidence Interval",
    "section": "13.2 Sample Mean as an Point Estimator",
    "text": "13.2 Sample Mean as an Point Estimator\n\nDraw 5 values from the population that follows \\(N(2, 1)\\) as sample data \\((x_1, x_2, x_3, x_4, x_5)\\).\n\n\n## Generate sample data x1, x2, x3, x4, x5, each from population distribution N(2, 1)\nx_data_1 <- rnorm(n = 5, mean = 2, sd = 1)\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n\n\n1.54\n1.64\n1.84\n1.23\n0.83\n1.42\n\n\n\n\n\n\n\\(\\mu = 2\\), and we use the point estimate \\(\\overline{x}=\\) 1.42 to estimate it.\n\n\n\n\n\n\n\nWhy \\(\\overline{x}\\) is not equal to \\(\\mu\\)?\n\n\n\n\n\n\n\nDue to its randomness nature:"
  },
  {
    "objectID": "infer-ci.html#variability-in-estimates",
    "href": "infer-ci.html#variability-in-estimates",
    "title": "13¬† Confidence Interval",
    "section": "13.2 Variability in Estimates",
    "text": "13.2 Variability in Estimates\n\nIf another sample of size \\(5\\) is drawn from the same population,\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n\n\n1.68\n1.65\n1.41\n0.41\n3.69\n1.77\n\n\n\n\n\n\nThe second sample mean \\(\\overline{x} =\\) 1.77 is different from the first one.\n\n\n\n\n\n\n\nWhy do the first sample and the second sample give us different sample means?\n\n\n\n\n\n\n\nA point estimator has its own sampling distribution."
  },
  {
    "objectID": "infer-ci.html#why-point-estimates-are-not-enough",
    "href": "infer-ci.html#why-point-estimates-are-not-enough",
    "title": "13¬† Confidence Interval",
    "section": "13.2 Why Point Estimates Are Not Enough",
    "text": "13.2 Why Point Estimates Are Not Enough\n\n\n\n\n\n\nIf you want to estimate \\(\\mu\\), do you prefer to report a range of values the parameter might be in, or a single estimate like \\(\\overline{x}\\)?\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to catch a fish, do you prefer a spear or a net?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue to variation of \\(\\overline{X}\\), if we report a point estimate \\(\\overline{x}\\), we probably won‚Äôt hit the exact \\(\\mu\\).\nIf we report a range of plausible values, we have a better shot at capturing the parameter!"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals",
    "href": "infer-ci.html#confidence-intervals",
    "title": "13¬† Confidence Interval",
    "section": "13.3 Confidence Intervals",
    "text": "13.3 Confidence Intervals\n\nA plausible range of values for \\(\\mu\\) is called a confidence interval (CI).\n\nThis range depends on how precise and reliable our statistic is as an estimate of the parameter.\n\nTo construct a CI for \\(\\mu\\), we first need to quantify the variability of our sample mean.\nQuantifying this uncertainty requires a measurement of how much we would expect the sample statistic to vary from sample to sample.\n\nThis is the variance of the sampling distribution of the sample mean!\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe larger the variation of \\(\\overline{X}\\) is, the wider the CI for \\(\\mu\\) will be given the same ‚Äúlevel of confidence‚Äù.\n\n\n\nDo we know the variance of \\(\\overline{X}\\)?\n\nBy CLT, \\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\) regardless of what the population distribution is.\n\n\n\n Precision vs.¬†Reliability \n\n\n\n\n\n\nIf we want to be very certain that we capture \\(\\mu\\), should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.5: Balance between precision and reliability\n\n\n\n\n\nWith a fixed sample size, precision and reliability have a trade-off relationship.\n\nNarrower intervals are more precise but less reliable, while wider intervals are more reliable but less precise.\n\n\n\n A Confidence Interval Is for a Parameter \n\nA confidence interval is for a parameter, NOT a statistic.\n\nFor example, we use the sample mean to form a confidence interval for the population mean.\n\nWe never say ‚ÄúThe confidence interval of the sample mean, \\(\\overline{X}\\), is ‚Ä¶‚Äù\nWe say ‚ÄúThe confidence interval for the true population mean, \\(\\mu\\), is ‚Ä¶‚Äù\nIn general, a confidence interval for \\(\\mu\\) has the form\n\n\n\\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\n\nThe \\(m\\) is called the margin of error.\n\\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval.\nThe point estimate, \\(\\overline{x}\\), and margin of error, \\(m\\), can be obtained from known quantities and our data once sampled.\n\n\n \\((1 - \\alpha)100\\%\\) Confidence Intervals \n\nThe confidence level \\(1-\\alpha\\): the proportion of times that the CI contains the population parameter, assuming that the estimation process is repeated a large number of times.\nCommon choices for the confidence level include\n\n90% \\((\\alpha = 0.10)\\)\n95% \\((\\alpha = 0.05)\\)\n99% \\((\\alpha = 0.01)\\)\n\n95% is the most common level because it has a good balance between precision (width of the CI) and reliability (confidence level).\n\n High reliability and Low precision: I am 100% confident that the mean height of Marquette students is between 3‚Äô0‚Äù and 8‚Äô0‚Äù. \n\nDuh‚Ä¶ü§∑\n\n Low reliability and High precision: I am 20% confident that mean height of Marquette students is between 5‚Äô6‚Äù and 5‚Äô7‚Äù. \n\nThis is far from the truth‚Ä¶ üôÖ\n\n\n\n\n \\(95\\%\\) Confidence Intervals for \\(\\mu\\) \n Z-score \n\n\n\n\\(\\alpha = 0.05\\)\nStart with the sampling distribution of \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\\(\\overline{x}\\) will be within 1.96 SDs of the population mean, \\(\\mu\\), \\(95\\%\\) of the time.\nThe \\(z\\)-score of 1.96 is associated with 2.5% area to the right and is called a critical value denoted as \\(z_{0.025}\\) .\n\n\n\n\n\n\n\n\n\n Probability \n\n\n\\[P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} < \\overline{X} < \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\]\n\n\n\n\n\n\nIs the interval \\(\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right)\\) our confidence interval?\n\n\n\n\n‚ùå No! We don‚Äôt know \\(\\mu\\), which is the quantity we want to estimate, but we‚Äôre almost there!\n\n\n\n\n\n\n\n\n\n\n\n Formula\n\n\n\\[\\begin{align}\n&P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} < \\overline{X} < \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\\\\n&P\\left( \\boxed{\\overline{X}-1.96\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}} \\right) = 0.95\n\\end{align}\\]\n\n With sample data of size \\(n\\), \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) is our \\(95\\%\\) CI for \\(\\mu\\) if \\(\\sigma\\) is known to us! \nThe margin of error \\(m = 1.96\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "infer-ci.html#precision-vs.-reliability",
    "href": "infer-ci.html#precision-vs.-reliability",
    "title": "13¬† Confidence Interval",
    "section": "13.4 Precision vs.¬†Reliability",
    "text": "13.4 Precision vs.¬†Reliability\n\n\n\n\n\n\nIf we want to be very certain that we capture \\(\\mu\\), should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the sample size fixed, precision and reliability have a trade-off relationship."
  },
  {
    "objectID": "infer-ci.html#confidence-interval-is-for-a-parameter",
    "href": "infer-ci.html#confidence-interval-is-for-a-parameter",
    "title": "13¬† Confidence Interval",
    "section": "13.4 Confidence Interval Is for a Parameter",
    "text": "13.4 Confidence Interval Is for a Parameter\n\nA confidence interval is for a parameter, NOT a statistic.\n\nUse the sample mean to form a confidence interval for the population mean.\n\nWe never say ‚ÄúThe confidence interval of the sample mean \\(\\overline{X}\\) is ‚Ä¶‚Äù\nWe say ‚ÄúThe confidence interval for the true population mean \\(\\mu\\) is ‚Ä¶‚Äù\nIn general, a confidence interval for \\(\\mu\\) has the form\n\n\n\\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\n\nThe \\(m\\) is called margin of error.\n\\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval.\nThe point estimate \\(\\overline{x}\\) and margin of error \\(m\\) can be obtained from known quantities and our data once sampled."
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-z-score",
    "href": "infer-ci.html#confidence-intervals-for-mu-z-score",
    "title": "13¬† Confidence Interval",
    "section": "13.4 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Z-score",
    "text": "13.4 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Z-score\n\n\n\n\n\\(\\alpha = 0.05\\)\nStart with the sampling distribution of \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\\(\\overline{x}\\) will be within 1.96 SDs of the population mean \\(\\mu\\) \\(95\\%\\) of the time.\nThe \\(z\\)-score of 1.96 is associated with 2.5% area to the right, and called a critical value denoted as \\(z_{0.025}\\) ."
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-probability",
    "href": "infer-ci.html#confidence-intervals-for-mu-probability",
    "title": "13¬† Confidence Interval",
    "section": "13.5 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Probability",
    "text": "13.5 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Probability\n\\[P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} < \\overline{X} < \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\]\n\n\n\n\n\n\nIs the interval \\(\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right)\\) our confidence interval?\n\n\n\n\n\n\n\n‚ùå No! We don‚Äôt know \\(\\mu\\), the quantity we like to estimate!\nBut we‚Äôre almost there!"
  },
  {
    "objectID": "infer-ci.html#alpha100-confidence-intervals",
    "href": "infer-ci.html#alpha100-confidence-intervals",
    "title": "13¬† Confidence Interval",
    "section": "13.4 \\((1 - \\alpha)100\\%\\) Confidence Intervals",
    "text": "13.4 \\((1 - \\alpha)100\\%\\) Confidence Intervals\n\nThe confidence level \\(1-\\alpha\\): the proportion of times that the CI contains the population parameter, assuming that the estimation process is repeated a large number of times.\nThe common choices for the confidence level are\n\n90% \\((\\alpha = 0.10)\\)\n95% \\((\\alpha = 0.05)\\)\n99% \\((\\alpha = 0.01)\\)\n\n95% is the most common level because of good balance between precision (width of the CI) and reliability (confidence level)\n\n High reliability and Low precision. I am 100% confident that the mean height of Marquette students is between 3‚Äô0‚Äù and 8‚Äô0‚Äù.  duh‚Ä¶ü§∑\n Low reliability and High precision. I am 20% confident that mean height of Marquette students is between 5‚Äô6‚Äù and 5‚Äô7‚Äù.  far from it‚Ä¶üôÖ"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-formula",
    "href": "infer-ci.html#confidence-intervals-for-mu-formula",
    "title": "13¬† Confidence Interval",
    "section": "13.4 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Formula",
    "text": "13.4 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Formula\n\\[\\begin{align}\n&P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} < \\overline{X} < \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\\\\n&P\\left( \\boxed{\\overline{X}-1.96\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}} \\right) = 0.95\n\\end{align}\\]\n\n With sample data of size \\(n\\), \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) is our \\(95\\%\\) CI for \\(\\mu\\) if \\(\\sigma\\) is known to us! \nThe margin of error \\(m = 1.96\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "title": "13¬† Confidence Interval",
    "section": "13.4 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "13.4 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\n\nRequirements for estimating \\(\\mu\\) when \\(\\sigma\\) is known:\n\nüëâ The sample should be a random sample, such that all data \\(X_i\\) are drawn from the same population and \\(X_i\\) and \\(X_j\\) are independent.\n\n Any methods in this course are based on the assumption of a random sample \n\nüëâ The population standard deviation, \\(\\sigma\\), is known.\nüëâ The population is either normally distributed, \\(n > 30\\) or both, i.e., \\(X_i \\sim N(\\mu, \\sigma^2)\\).\n\n \\(n > 30\\) allows the Central Limit Theorem to be applied and hence normality is satisfied. \n\n\n\n\n \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\) \n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\)   \\(\\left(\\overline{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) \n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) \n\n\n\n\n\nProcedures for constructing a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is known:\n\nCheck that the requirements are satisfied.\nDecide \\(\\alpha\\) or the confidence level \\((1 - \\alpha)\\).\nFind the critical value, \\(z_{\\alpha/2}\\).\nEvaluate margin of error, \\(m = z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\).\nConstruct the \\((1 - \\alpha)100\\%\\) CI for \\(\\mu\\) using the sample mean, \\(\\overline{x}\\), and margin of error, \\(m\\):\n\n\n\n \\[\\boxed{\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\text{  or  } \\left( \\overline{x} -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\, \\overline{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)}\\]\n\n\n Example \n\n\n\nSuppose we want to know the mean systolic blood pressure (SBP) of a population.\nAssume that the population distribution is normal and has a standard deviation of 5 mmHg.\nWe have a random sample of 16 subjects from this population with a mean of 121.5 mmHg.\nEstimate the mean SBP with a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\nRequirements:\n\n Normality is assumed, \\(\\sigma = 5\\) is known and a random sample is collected.\n\nDecide \\(\\alpha\\):\n\n \\(\\alpha = 0.05\\) \n\nFind the critical value \\(z_{\\alpha/2}\\):\n\n \\(z_{\\alpha/2} = z_{0.025} = 1.96\\) \n\nEvaluate margin of error \\(m = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\):\n\n \\(m = (1.96) \\frac{5}{\\sqrt{16}} = 2.45\\) \n\nConstruct the \\((1 - \\alpha)100\\%\\) CI:\n\n The 95% CI for the mean SBP is \\(\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = (121.5 -2.45, 121.5 + 2.45) = (119.05, 123.95)\\) \n\n\n Computation in R \n\nBelow is a demonstration of how to find the 95% CI for SBP using R.\n\n\n## save all information we have\nalpha <- 0.05\nn <- 16\nx_bar <- 121.5\nsig <- 5\n\n## 95% CI\n## z-critical value\n(cri_z <- qnorm(p = alpha / 2, lower.tail = FALSE))  \n\n[1] 1.959964\n\n## margin of error\n(m_z <- cri_z * (sig / sqrt(n)))  \n\n[1] 2.449955\n\n## 95% CI for mu when sigma is known\nx_bar + c(-1, 1) * m_z  \n\n[1] 119.05 123.95\n\n\n\n\n\n\n\n\nConstruct a 99% CI for the mean SBP. Do you expect it to have a wider or narrower interval than the 95% CI? Why?\n\n\n\n\n\n\n Interpreting the Confidence Interval\n\nWRONG ‚ùå ‚ÄúThere is a 95% chance/probability that the true population mean will fall between 119.1 mm and 123.9 mm.‚Äù\nWRONG ‚ùå ‚ÄúThe probability that the true population mean falls between 119.1 mm and 123.9 mm is 95%.‚Äù\n üëâ The sample mean is a random variable with a sampling distribution, so it makes sense to compute a probability of it being in some interval. \n üëâ The population mean is unknown and FIXED, so we cannot assign or compute any probability of it. \n\nIf we were using Bayesian inference, a different inference method, we could compute a probability associated with \\(\\mu\\) because \\(\\mu\\) is treated as a random variable.\n\nInstead we say,  ‚ÄúWe are 95% confident that the mean SBP lies between 119.1 mm and 123.9 mm.‚Äù \nThis means if we were able to collect our data many times and build the corresponding CIs, we would expect that about 95% of those intervals would contain the true population parameter, which, in this case, is the mean systolic blood pressure.\n Remember: \\(\\overline{x}\\) varies from sample to sample and so does its corresponding CI .\n\nThis idea is shown in Figure¬†13.6.\n\n\n\n\n\n\n\nFigure¬†13.6: Illustration of 100 95% confidence intervals\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe never know with certainty that 95% of the intervals, or any single interval for that matter, contains the true population parameter."
  },
  {
    "objectID": "infer-ci.html#alpha100-confidence-intervals-for-mu",
    "href": "infer-ci.html#alpha100-confidence-intervals-for-mu",
    "title": "13¬† Confidence Interval",
    "section": "13.5 \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\):",
    "text": "13.5 \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\)   \\(\\left(\\overline{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) \n\n\n \\(\\left(\\overline{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\)"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known-1",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known-1",
    "title": "13¬† Confidence Interval",
    "section": "13.5 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "13.5 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\nProcedures for constructing a confidence interval for \\(\\mu\\) when \\(\\sigma\\) known:\n\nCheck that the requirements are satisfied.\nDecide \\(\\alpha\\) or confidence level \\((1 - \\alpha)\\).\nFind the critical value \\(z_{\\alpha/2}\\).\nEvaluate margin of error \\(m = z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)\nConstruct the \\((1 - \\alpha)100\\%\\) CI for \\(\\mu\\) using sample mean \\(\\overline{x}\\) and margin of error \\(m\\):\n\n\n \\[\\boxed{\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\text{  or  } \\left( \\overline{x} -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\, \\overline{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)}\\]"
  },
  {
    "objectID": "infer-ci.html#example-ci-for-mu-when-sigma-is-known",
    "href": "infer-ci.html#example-ci-for-mu-when-sigma-is-known",
    "title": "13¬† Confidence Interval",
    "section": "13.5 Example: CI for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "13.5 Example: CI for \\(\\mu\\) When \\(\\sigma\\) is Known\n\n\n\n\nSuppose we want to know the mean systolic blood pressure (SBP) of a population.\n\nAssume that the population distribution is normal with the standard deviation of 5 mmHg.\nWe have a random sample of 16 subjects of this population with mean 121.5.\nEstimate the mean SBP with a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\nRequirements:  Normality is assumed, \\(\\sigma = 5\\) is known and a random sample is collected.\nDecide \\(\\alpha\\):  \\(\\alpha = 0.05\\) \nFind the critical value \\(z_{\\alpha/2}\\):  \\(z_{\\alpha/2} = z_{0.025} = 1.96\\) \nEvaluate margin of error \\(m = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\):  \\(m = (1.96) \\frac{5}{\\sqrt{16}} = 2.45\\) \nConstruct the \\((1 - \\alpha)100\\%\\) CI:  The 95% CI for the mean SBP is \\(\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = (121.5 -2.45, 121.5 + 2.45) = (119.05, 123.95)\\)"
  },
  {
    "objectID": "infer-ci.html#computation-in-r",
    "href": "infer-ci.html#computation-in-r",
    "title": "13¬† Confidence Interval",
    "section": "13.5 Computation in R",
    "text": "13.5 Computation in R\n\n## save all information we have\nalpha <- 0.05\nn <- 16\nx_bar <- 121.5\nsig <- 5\n\n## 95% CI\n## z-critical value\n(cri_z <- qnorm(p = alpha / 2, lower.tail = FALSE))  \n\n[1] 1.959964\n\n## margin of error\n(m_z <- cri_z * (sig / sqrt(n)))  \n\n[1] 2.449955\n\n## 95% CI for mu when sigma is known\nx_bar + c(-1, 1) * m_z  \n\n[1] 119.05 123.95\n\n\n\n\n\n\n\n\nConstruct a 99% CI for the mean SBP. Do you expect to have a wider or narrower interval? Why?"
  },
  {
    "objectID": "infer-ci.html#interpreting-a-confidence-interval",
    "href": "infer-ci.html#interpreting-a-confidence-interval",
    "title": "13¬† Confidence Interval",
    "section": "13.5 Interpreting a Confidence Interval",
    "text": "13.5 Interpreting a Confidence Interval\n\n ‚ÄúWe are 95% confident that the mean SBP lies between 119.1 mm and 123.9 mm.‚Äù \nSuppose we were able to collect our dataset many times and build the corresponding CIs.\nWe would expect about 95% of those intervals would contain the true population parameter, here the mean systolic blood pressure.\n\n Remember: \\(\\overline{x}\\) varies from sample to sample, so does its corresponding CI .\n\nWe never know if in fact 95% of them do, or whether any interval contains the true parameter!"
  },
  {
    "objectID": "infer-ci.html#generate-100-confidence-intervals-assuming-mu-120.",
    "href": "infer-ci.html#generate-100-confidence-intervals-assuming-mu-120.",
    "title": "13¬† Confidence Interval",
    "section": "13.5 Generate 100 Confidence Intervals Assuming \\(\\mu = 120\\).",
    "text": "13.5 Generate 100 Confidence Intervals Assuming \\(\\mu = 120\\)."
  },
  {
    "objectID": "infer-ci.html#interpreting-a-confidence-interval-do-not-say",
    "href": "infer-ci.html#interpreting-a-confidence-interval-do-not-say",
    "title": "13¬† Confidence Interval",
    "section": "13.5 Interpreting a Confidence Interval DO NOT SAY",
    "text": "13.5 Interpreting a Confidence Interval DO NOT SAY\n\nWRONG ‚ùå ‚ÄúThere is a 95% chance/probability that the true population mean will fall between 119.1 mm and 123.9 mm.‚Äù\nWRONG ‚ùå ‚ÄúThe probability that the true population mean falls between 119.1 mm and 123.9 mm is 95%.‚Äù\n üëâ The sample mean is a random variable with a sampling distribution, so it makes sense to compute a probability of it being in some interval. \n üëâ The population mean is unknown and FIXED. We cannot assign or compute any probability of it. \nAnother inference method, Bayesian inference, treats \\(\\mu\\) as a random variable and therefore we can compute any probability associated with it. (MATH 4790 Bayesian Statistics)"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "title": "13¬† Confidence Interval",
    "section": "13.5 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "13.5 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}\\), where \\(N\\) is the population size.\nIt‚Äôs rare that we don‚Äôt know \\(\\mu\\) but know \\(\\sigma\\), so what do we do if \\(\\sigma\\) is unknown?\n\nWe use the Student t distribution to construct a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown.\n\nTo construct these confidence intervals we still need\n\nA random sample\nA population that is normally distributed and/or \\(n > 30\\).\n\n\n\n\n\n\n\n\nWhat is a natural estimator for the unknown \\(\\sigma\\)?\n\n\n\n\n\n\n\nWhen \\(\\sigma\\) is unknown, we use the sample standard deviation, \\(S = \\sqrt{\\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{n-1}}\\), instead when constructing the CI.\n\n\n Student t Distribution \n\nIf the population is normally distributed or \\(n > 30\\),\n\n\\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\)\n\\(Z = \\frac{\\overline{X} - \\mu}{\\color{red}\\sigma/\\sqrt{n}} \\sim N(0, 1)\\)\n \\(T = \\frac{\\overline{X} - \\mu}{\\color{red}S/\\sqrt{n}} \\sim t_{n-1}\\) \n\\(t_{n-1}\\) denotes the Student t distribution with degrees of freedom (df) \\(n-1\\).\n\n\n Properties \n\nIt is symmetric about the mean 0 and bell-shaped like \\(N(0, 1)\\).\nIt has more variability than \\(N(0, 1)\\) (heavier tails and lower peak).\nThe variability is different for different sample sizes (degrees of freedom).\n\nAs \\(n \\rightarrow \\infty\\) \\((df \\rightarrow \\infty)\\), the Student t distribution approaches \\(N(0, 1)\\).\n\n\n\n\n\n\n\nFigure¬†13.7: Student t distributions with various degrees of freedom\n\n\n\n\n Critical Values of \\(t_{\\alpha/2, n-1}\\) \n\nWhen \\(\\sigma\\) is unknown, we use \\(t_{\\alpha/2, n-1}\\) as the critical value, instead of \\(z_{\\alpha/2}\\).\n\n\n\n\n\n\nFigure¬†13.8: Illustration of critical value for Student t distribution\n\n\n\n\n\n\n\n\n\n\nWith the same \\(\\alpha\\), is \\(t_{\\alpha, n-1}\\) or \\(z_{\\alpha}\\) larger?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58\n\n\n\n\n\n\n \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown \n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown is  \\[\\left(\\overline{x} - t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}, \\overline{x} + t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\right)\\] \nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} > z_{\\alpha/2}\\), which means the confidence interval for \\(\\mu\\) is wider when \\(\\sigma\\) is unknown.\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe are more ‚Äúuncertain‚Äù when doing inference about \\(\\mu\\) because don‚Äôt have information about \\(\\sigma\\) and replacing it with \\(s\\) adds additional uncertainty.\n\n\n\n Computation in R \n\nBack to the systolic blood pressure (SBP) example. We have \\(n=16\\) and \\(\\overline{x} = 121.5\\).\nEstimate the mean SBP with a 95% confidence interval with unknown \\(\\sigma\\) and \\(s = 5\\).\n\n\nalpha <- 0.05\nn <- 16\nx_bar <- 121.5\ns <- 5  ## sigma is unknown and s = 5\n\n## t-critical value\n(cri_t <- qt(p = alpha / 2, df = n - 1, lower.tail = FALSE)) \n\n[1] 2.13145\n\n## margin of error\n(m_t <- cri_t * (s / sqrt(n)))  \n\n[1] 2.664312\n\n## 95% CI for mu when sigma is unknown\nx_bar + c(-1, 1) * m_t  \n\n[1] 118.8357 124.1643"
  },
  {
    "objectID": "infer-ci.html#student-t-distribution",
    "href": "infer-ci.html#student-t-distribution",
    "title": "13¬† Confidence Interval",
    "section": "13.6 Student t Distribution",
    "text": "13.6 Student t Distribution\n\nIf the population is normally distributed or \\(n > 30\\),\n\n\\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\)\n\\(Z = \\frac{\\overline{X} - \\mu}{\\color{red}\\sigma/\\sqrt{n}} \\sim N(0, 1)\\)\n \\(T = \\frac{\\overline{X} - \\mu}{\\color{red}S/\\sqrt{n}} \\sim t_{n-1}\\) \n\\(t_{n-1}\\) denotes the Student t distribution with degrees of freedom (df) \\(n-1\\)."
  },
  {
    "objectID": "infer-ci.html#properties-of-student-t-distribution",
    "href": "infer-ci.html#properties-of-student-t-distribution",
    "title": "13¬† Confidence Interval",
    "section": "13.6 Properties of Student t Distribution",
    "text": "13.6 Properties of Student t Distribution\n\nSymmetric about the mean 0 and bell-shaped as \\(N(0, 1)\\).\nMore variability than \\(N(0, 1)\\) (heavier tails and lower peak).\nThe variability is different for different sample sizes (degrees of freedom).\nAs \\(n \\rightarrow \\infty\\) \\((df \\rightarrow \\infty)\\), the Student t distribution approaches to \\(N(0, 1)\\)."
  },
  {
    "objectID": "infer-ci.html#critical-values-of-t_alpha2-n-1",
    "href": "infer-ci.html#critical-values-of-t_alpha2-n-1",
    "title": "13¬† Confidence Interval",
    "section": "13.6 Critical Values of \\(t_{\\alpha/2, n-1}\\)",
    "text": "13.6 Critical Values of \\(t_{\\alpha/2, n-1}\\)\n\nWhen \\(\\sigma\\) is unknown, we use \\(t_{\\alpha/2, n-1}\\) as the critical value, instead of \\(z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the same \\(\\alpha\\), \\(t_{\\alpha, n-1}\\) or \\(z_{\\alpha}\\) is larger?\n\n\n\n\n\n\n\nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} > z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58"
  },
  {
    "objectID": "infer-ci.html#critical-values-of-t_alpha2-n-1-1",
    "href": "infer-ci.html#critical-values-of-t_alpha2-n-1-1",
    "title": "13¬† Confidence Interval",
    "section": "13.25 Critical Values of \\(t_{\\alpha/2, n-1}\\)",
    "text": "13.25 Critical Values of \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} > z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58"
  },
  {
    "objectID": "infer-ci.html#ci-for-mu-when-sigma-is-unknown",
    "href": "infer-ci.html#ci-for-mu-when-sigma-is-unknown",
    "title": "13¬† Confidence Interval",
    "section": "13.6 CI for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "13.6 CI for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown is  \\[\\left(\\overline{x} - t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}, \\overline{x} + t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\right)\\] \nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} > z_{\\alpha/2}\\).\n\n\n\n\n\n\n\nWe are more ‚Äúuncertain‚Äù when doing inference about \\(\\mu\\) because we also don‚Äôt have information about \\(\\sigma\\), and replacing it with \\(s\\) adds additional uncertainty."
  },
  {
    "objectID": "infer-ci.html#computation-in-r-t-interval",
    "href": "infer-ci.html#computation-in-r-t-interval",
    "title": "13¬† Confidence Interval",
    "section": "13.6 Computation in R (t interval)",
    "text": "13.6 Computation in R (t interval)\n\nBack to the systolic blood pressure (SBP) example. We have \\(n=16\\) and \\(\\overline{x} = 121.5\\).\nEstimate the mean SBP with a 95% confidence interval with unknown \\(\\sigma\\) and \\(s = 5\\).\n\n\nalpha <- 0.05\nn <- 16\nx_bar <- 121.5\ns <- 5  ## sigma is unknown and s = 5\n\n## t-critical value\n(cri_t <- qt(p = alpha / 2, df = n - 1, lower.tail = FALSE)) \n\n[1] 2.13145\n\n## margin of error\n(m_t <- cri_t * (s / sqrt(n)))  \n\n[1] 2.664312\n\n## 95% CI for mu when sigma is unknown\nx_bar + c(-1, 1) * m_t  \n\n[1] 118.8357 124.1643"
  },
  {
    "objectID": "infer-ci.html#summary",
    "href": "infer-ci.html#summary",
    "title": "13¬† Confidence Interval",
    "section": "13.6 Summary",
    "text": "13.6 Summary\n\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\) unknown\n\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\nPopulation Mean \\(\\mu\\)\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\n\nRemember to check if the population is normally distributed and/or \\(n>30\\).\nWhat if the population is not normal and \\(n \\le 30\\)?\n\nUse a so-called nonparametric method, for example bootstrapping."
  },
  {
    "objectID": "intro-data.html#randomly-select-40-males-and-40-females-to-see-the-difference-in-blood-pressure-levels-between-male-and-female.",
    "href": "intro-data.html#randomly-select-40-males-and-40-females-to-see-the-difference-in-blood-pressure-levels-between-male-and-female.",
    "title": "2¬† Data",
    "section": "2.8 - Randomly select 40 males and 40 females to see the difference in blood pressure levels between male and female. ",
    "text": "2.8 - Randomly select 40 males and 40 females to see the difference in blood pressure levels between male and female."
  },
  {
    "objectID": "intro-data.html#test-the-effects-of-a-new-drug-by-randomly-dividing-patients-into-3-groups-high-dosage-low-dosage-placebo.",
    "href": "intro-data.html#test-the-effects-of-a-new-drug-by-randomly-dividing-patients-into-3-groups-high-dosage-low-dosage-placebo.",
    "title": "2¬† Data",
    "section": "2.9 -  Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo). ",
    "text": "2.9 -  Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo)."
  },
  {
    "objectID": "intro-r.html#lab-time",
    "href": "intro-r.html#lab-time",
    "title": "3¬† Tool foR Data",
    "section": "3.2 Lab time!",
    "text": "3.2 Lab time!\n\nStep 1: In the Posit website https://posit.co/, choose Products > Posit Cloud as shown below."
  },
  {
    "objectID": "intro-r.html#welcome-to-the-r-world",
    "href": "intro-r.html#welcome-to-the-r-world",
    "title": "3¬† Tool foR Data",
    "section": "3.4 Welcome to the R World!",
    "text": "3.4 Welcome to the R World!\n\nNow you are ready to use R to do statistical computation.\nYou can use R like a calculator. After typing your formula, simply hit Enter, you get the answer! For example,\n\n\n1 + 2\n\n[1] 3\n\n30 * 42 / 3\n\n[1] 420\n\nlog(5) - exp(3) * sqrt(7)\n\n[1] -51.5319"
  },
  {
    "objectID": "intro-r.html#youve-seen-comments-a-lot-how-do-we-write-a-comment-in-r",
    "href": "intro-r.html#youve-seen-comments-a-lot-how-do-we-write-a-comment-in-r",
    "title": "3¬† Tool foR Data",
    "section": "3.16 You‚Äôve seen comments a lot! How do we write a comment in R?",
    "text": "3.16 You‚Äôve seen comments a lot! How do we write a comment in R?\n:::\n\nUse # to add a comment so that the text after # is not read as an R command.\nWriting (good) comments is highly recommended: help readers and more importantly yourself understand what the code is doing.\nComments should explain the why, not the what.\n\n\n\n\n\n\nhttps://www.reddit.com/r/ProgrammerHumor/comments/8w54mx/code_comments_be_like/"
  },
  {
    "objectID": "data-graphics.html#is-the-interest-rate-histogram-left-skewed-or-right-skewed",
    "href": "data-graphics.html#is-the-interest-rate-histogram-left-skewed-or-right-skewed",
    "title": "4¬† Data Visualization",
    "section": "4.6 Is the interest rate histogram left skewed or right skewed?",
    "text": "4.6 Is the interest rate histogram left skewed or right skewed?"
  },
  {
    "objectID": "data-numerics.html#larger-iqr-means-more-or-less-variation",
    "href": "data-numerics.html#larger-iqr-means-more-or-less-variation",
    "title": "5¬† Data Sample Statistics",
    "section": "5.7 Larger IQR means more or less variation?",
    "text": "5.7 Larger IQR means more or less variation?\n::: \n\n5.7.1 Variance and Standard Deviation\n\nThe distance of an observation from its mean, \\(x_i - \\overline{x}\\), its deviation.\nSample Variance is defined as \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1} \\]\nSample Standard Deviation (SD) is defined as the square root of the variance \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}} \\] \nVariance is the average of squared deviation from the sample mean \\(\\overline{x}\\) or the mean squared deviation from the mean.\nSD is the root mean squared deviation from the mean. It measures, on average, how far the data spread out around the average.\n\n\n\n5.7.2 Compute Variance and SD\n\nvar(int_rate)\n\n[1] 25.54942\n\nsqrt(var(int_rate))\n\n[1] 5.054644\n\nsd(int_rate)\n\n[1] 5.054644"
  },
  {
    "objectID": "prob-disc.html#if-we-toss-a-fair-coin-two-times-independently-and-let-x-of-heads-is-x-a-binomial-r.v.",
    "href": "prob-disc.html#if-we-toss-a-fair-coin-two-times-independently-and-let-x-of-heads-is-x-a-binomial-r.v.",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.7 If we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial r.v.?",
    "text": "9.7 If we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial r.v.?\n:::"
  },
  {
    "objectID": "prob-cont.html#we-have-to-specify-the-mean-and-sd-of-the-original-distribution-of-x-like-pnormx-mean-mu-sd-sigma.",
    "href": "prob-cont.html#we-have-to-specify-the-mean-and-sd-of-the-original-distribution-of-x-like-pnormx-mean-mu-sd-sigma.",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.16 üëâ We have to specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma).",
    "text": "10.16 üëâ We have to specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma)."
  },
  {
    "objectID": "infer-ci.html#large-overlinex-pm-m-overlinex---m-overlinex-m",
    "href": "infer-ci.html#large-overlinex-pm-m-overlinex---m-overlinex-m",
    "title": "13¬† Confidence Interval",
    "section": "13.9 \\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)",
    "text": "13.9 \\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\n\nThe \\(m\\) is called margin of error.\n\\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval.\nThe point estimate \\(\\overline{x}\\) and margin of error \\(m\\) can be obtained from known quantities and our data once sampled."
  },
  {
    "objectID": "infer-ht.html#what-is-a-hypothesis-testing",
    "href": "infer-ht.html#what-is-a-hypothesis-testing",
    "title": "15¬† Hypothesis Testing",
    "section": "15.1 What is a Hypothesis Testing?",
    "text": "15.1 What is a Hypothesis Testing?\n\n\nA hypothesis is a claim or statement about a property of a population, often the value of a population parameter.\n\n The mean body temperature of humans is less than \\(98.6^{\\circ}\\) F, or \\(\\mu < 98.6\\). \n Marquette students‚Äô IQ scores has standard deviation equal to 15, or \\(\\sigma = 15\\). \n\n\n\n\nNull hypothesis \\((H_0)\\): a statement that the value of a parameter is\n\nequal to some claim value\nthe negation of the alternative hypothesis\noften represents a skeptical perspective to be tested\n\nAlternative hypothesis \\((H_1\\) or \\(H_a)\\): a claim that the parameter is less than, greater than or not equal to some value.\n\nusually our research hypothesis of some new scientific theory or finding"
  },
  {
    "objectID": "infer-ht.html#null-and-alternative-hypothesis",
    "href": "infer-ht.html#null-and-alternative-hypothesis",
    "title": "15¬† Hypothesis Testing",
    "section": "15.2 Null and Alternative Hypothesis",
    "text": "15.2 Null and Alternative Hypothesis\n\n\n\n\n\n\nA \\(H_0\\) claim or \\(H_1\\) claim?\n\n\n\n\n\n\n\n The percentage of Marquette female students loving Japanese food is equal to 80%.\n On average, Marquette students consume less than 3 drinks per week. \nHypothesis testing 1 is a procedure to decide whether to reject \\(H_0\\) or not by how much evidence against \\(H_0\\).\n\n\n[1] Null Hypothesis Statistical Testing (NHST), statistical testing or test of significance."
  },
  {
    "objectID": "infer-ht.html#hypothesis-testing-example",
    "href": "infer-ht.html#hypothesis-testing-example",
    "title": "15¬† Hypothesis Testing",
    "section": "15.2 Hypothesis Testing Example",
    "text": "15.2 Hypothesis Testing Example\n\n\n\n\nA person is charged with a crime.\n\nA jury decide whether the person is guilty or not.\nThe accuse is assumed to be innocent until the jury declares otherwise.\nOnly if overwhelming evidence of the person‚Äôs guilt can be shown is the jury expected to declare the person guilty, otherwise the person is considered not guilty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat should \\(H_0\\) and \\(H_a\\) be?\n\n\n\n\n\n\n\n\\(H_0:\\) The person is  not guilty  üôÇ\n\\(H_1:\\) The person is  guilty  üòü\nEvidence:  Photos, videos, witness, fingerprint, DNA \nDecision Rule:  Jury‚Äôs voting \nConclusion: Verdict  ‚Äúguilty‚Äù  or  ‚ÄúNOT enough evidence to convict‚Äù"
  },
  {
    "objectID": "infer-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "href": "infer-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "title": "15¬† Hypothesis Testing",
    "section": "15.2 How to Formally Do a Statistical Hypothesis Testing",
    "text": "15.2 How to Formally Do a Statistical Hypothesis Testing\n\nStep 0: Check Method Assumptions\nStep 1: Set the \\(H_0\\) and \\(H_a\\) in Symbolic Form from a Claim\nStep 2: Set the Significance Level \\(\\alpha\\)\nStep 3: Calculate the Test Statistic (Evidence)\n\n\n\nDecision Rule I: Critical Value Method\n\n Step 4-c: Find the Critical Value \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n\nDecision Rule II: P-Value Method\n\n Step 4-p: Find the P-Value \n Step 5-p: Draw a Conclusion Using P-Value Method \n\n\n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\nüòé We will learn this step by step!\n\n\n Step 0: Check Method Assumptions \n\nThe testing methods are based on normality or approximate normality by CLT.\n\nRandom sample\nNormally distributed and/or \\(n > 30\\)\n\n\n\n\n\n\n\n\n\n\n\n\n Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \n\nüßë‚Äçüè´ The mean IQ score of statistics professors is higher than 120.\n\n \\(\\begin{align}&H_0: \\mu \\le 120 \\\\ &H_1: \\mu > 120 \\end{align}\\) \n\nüíµ The mean starting salary for Marquette graduates who didn‚Äôt take MATH 4720 is less than $60,000.\n\n \\(\\begin{align} &H_0: \\mu \\ge 60000 \\\\ &H_1: \\mu < 60000 \\end{align}\\) \n\nüì∫ The mean time between uses of a TV remote control by males during commercials equals 5 sec.¬†\n\n \\(\\begin{align} &H_0: \\mu = 5 \\\\ &H_1: \\mu \\ne 5 \\end{align}\\) \n\n\n\n Step 2: Set the Significance Level \\(\\alpha\\) \n\nThe significance level, \\(\\alpha\\), determines how rare or unlikely our evidence must be in order to represent sufficient evidence against \\(H_0\\).\nAn \\(\\alpha\\) level of 0.05 implies that evidence occurring with probability lower than 5% will be considered sufficient evidence to reject \\(H_0\\).\n\\(\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true})\\)\n\\(\\alpha = 0.05\\) means that we incorrectly reject \\(H_0\\) 5 out of every 100 times we collect a sample and run the test.\n\n\n\n\n\n\n\n\nFigure¬†15.1: Illustration of significance level, alpha\n\n\n\n\n\n\n\n\n\n\n\nRare Event Rule\n\n\n\nIf, under a given assumption, the probability of a particular observed event is exceptionally small, we conclude that the assumption is probably not correct.\n\n\n\n\n\n Step 3: Calculate the Test Statistic \n\nA test statistic is a statistical value used in making a decision about the \\(H_0\\).\nSuppose  \\(H_0: \\mu = \\mu_0\\) and \\(\\quad H_1: \\mu < \\mu_0\\) .\nWhen computing a test statistic, we assume \\(H_0\\) is true.\nWhen \\(\\sigma\\) is known, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\boxed{ z_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{\\sigma/\\sqrt{n}} }\\] - When \\(\\sigma\\) is unknown, the test statistic for testings about \\(\\mu\\) is\n\\[\\boxed{ t_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}} }\\]\n\n Step 4-c: Find the Critical Value \n\nThe critical value(s) separates the rejection region or critical region, where we reject \\(H_0\\), from the values of the test statistic that do not lead to the rejection of \\(H_0\\).\n\nThese depend on whether the test is a right-tailed, left-tailed or two-tailed.\n\n\n\n\n\n\n\nFigure¬†15.2: Rejection regions for the different types of hypothesis tests\n\n\n\n\n\n\\(z_{\\alpha}\\) is such that \\(P(Z > z_{\\alpha}) = \\alpha\\) and \\(Z \\sim N(0, 1)\\).\n\\(t_{\\alpha, n-1}\\) is such that \\(P(T > t_{\\alpha, n-1}) = \\alpha\\) and \\(T \\sim t_{n-1}\\).\n\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{\\alpha}\\)\n\\(-z_{\\alpha}\\)\n\\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\)\n\n\n\\(\\sigma\\) unknown\n\\(t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha/2, n-1}\\) and \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n\\(z_{0.025} =\\) 1.96, \\(z_{0.05} =\\) 1.64\n\\(z_{\\alpha}\\) and \\(t_{\\alpha, n-1}\\) are always positive.\n\n\n Step 5-c: Draw a Conclusion Using Critical Value \n\nIf the test statistic is\n\nin the rejection region, we reject \\(H_0\\).\nnot in the rejection region, we do not or fail to reject \\(H_0\\).\n\n\nReject \\(H_0\\) if\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{test} > z_{\\alpha}\\)\n\\(z_{test} < -z_{\\alpha}\\)\n\\(\\mid z_{test}\\mid \\, > z_{\\alpha/2}\\)\n\n\n\\(\\sigma\\) unknown\n\\(t_{test} > t_{\\alpha, n-1}\\)\n\\(t_{test} < -t_{\\alpha, n-1}\\)\n\\(\\mid t_{test}\\mid \\, > t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\n\n\nFigure¬†15.3: Test statistic inside of critical region\n\n\n\n\n\n Step 4-p: Find the P-Value \n\nThe \\(p\\)-value measures the strength of the evidence against \\(H_0\\) provided by the data.\n\nThe smaller the \\(p\\)-value, the greater the evidence against \\(H_0\\).\n\nThe \\(p\\)-value is the probability of getting a test statistic value that is at least as extreme as the one obtained from the data, assuming that \\(H_0\\) is true. \\((\\mu = \\mu_0)\\)\n\nFor example, \\(p\\)-value \\(= P(Z \\ge z_{test} \\mid H_0)\\) for a right-tailed test.\n\nWe are more likely to get a \\(p\\)-value near 0 when \\(H_0\\) is false than when \\(H_0\\) is true.\n\n P-Value Illustration \n\n\n\n\n\nFigure¬†15.4: Illustration of p-values for different types of hypothesis tests\n\n\n\n\n\n Step 5-p: Draw a Conclusion Using P-Value Method \n\nIf the \\(p\\)-value \\(\\le \\alpha\\) , we reject \\(H_0\\).\nIf the \\(p\\)-value \\(> \\alpha\\), we do not reject or fail to reject \\(H_0\\).\n\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(P(Z > z_{test} \\mid H_0)\\)\n\\(P(Z < z_{test} \\mid H_0)\\)\n\\(2P(Z > \\,\\mid z_{test} \\mid \\, \\mid H_0)\\)\n\n\n\\(\\sigma\\) unknown\n\\(P(T > t_{test} \\mid H_0)\\)\n\\(P(T < t_{test} \\mid H_0)\\)\n\\(2P(T > \\, \\mid t_{test} \\mid \\, \\mid H_0)\\)\n\n\n\n\n\n Both Methods Lead to the Same Conclusion \n\n\n\n\n\nFigure¬†15.5: The conclusion is the same regardless of the method used (Critical Value or P-Value).\n\n\n\n\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n\n\n\n\nFigure¬†15.6: Conclusions based on testing results (https://www.drdawnwright.com/category/statistics/)\n\n\n\n\n\nReminder‚Ä¶\n\n\n\n\n\n\nFigure¬†15.7: Meme about hypothesis testing conclusions (https://www.pinterest.com/pin/287878601159173631/)"
  },
  {
    "objectID": "infer-ht.html#the-new-treatment-is-effective",
    "href": "infer-ht.html#the-new-treatment-is-effective",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 The New Treatment is Effective?",
    "text": "15.3 The New Treatment is Effective?\n\n\n\n\nA population of hypertension group is normal and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is effective in reducing BP."
  },
  {
    "objectID": "infer-ht.html#step-0-check-method-assumptions",
    "href": "infer-ht.html#step-0-check-method-assumptions",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Step 0: Check Method Assumptions",
    "text": "15.3 Step 0: Check Method Assumptions\nThe testing methods are based on normality or approximate normality by CLT.\n\nRandom sample\nNormally distributed and/or \\(n > 30\\)"
  },
  {
    "objectID": "infer-ht.html#step-1-set-the-h_0-and-h_1-from-a-claim",
    "href": "infer-ht.html#step-1-set-the-h_0-and-h_1-from-a-claim",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim",
    "text": "15.3 Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim\n\nüßë‚Äçüè´ The mean IQ score of statistics professors is higher than 120.\n\n \\(\\begin{align}&H_0: \\mu \\le 120 \\\\ &H_1: \\mu > 120 \\end{align}\\) \n\nüíµ The mean starting salary for Marquette graduates who didn‚Äôt take MATH 4720 is less than $60,000.\n\n \\(\\begin{align} &H_0: \\mu \\ge 60000 \\\\ &H_1: \\mu < 60000 \\end{align}\\) \n\nüì∫ The mean time between uses of a TV remote control by males during commercials equals 5 sec.¬†\n\n \\(\\begin{align} &H_0: \\mu = 5 \\\\ &H_1: \\mu \\ne 5 \\end{align}\\) \n\nThe claim that the new treatment is effective in reducing BP means the mean BP is less than 150. ( \\(H_1\\) claim )\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu < 150 \\end{align}\\)"
  },
  {
    "objectID": "infer-ht.html#step-2-set-the-significance-level-alpha",
    "href": "infer-ht.html#step-2-set-the-significance-level-alpha",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Step 2: Set the Significance Level \\(\\alpha\\)",
    "text": "15.3 Step 2: Set the Significance Level \\(\\alpha\\)\n\nThe significant level \\(\\alpha\\) determines how rare or unlikely our evidence must be in order to represent sufficient evidence against \\(H_0\\).\nAn \\(\\alpha\\) level of 0.05 implies that evidence occurring with probability lower than 5% will be considered sufficient evidence against \\(H_0\\) (Reject \\(H_0\\)).\n\\(\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true})\\)\n\\(\\alpha = 0.05\\) means that we incorrectly reject \\(H_0\\) 5 out of every 100 times we collect a sample and run the test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRare Event Rule: If, under a given assumption, the probability of a particular observed event is exceptional small, we conclude that the assumption is probably not correct.\n\n\n\n\n\n\n\n\n\nLet‚Äôs set \\(\\alpha= 0.05\\).\nThis means we are asking, ‚ÄúIs there a sufficient evidence at \\(\\alpha= 0.05\\) that the new treatment is effective?‚Äù"
  },
  {
    "objectID": "infer-ht.html#step-3-calculate-the-test-statistic",
    "href": "infer-ht.html#step-3-calculate-the-test-statistic",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Step 3: Calculate the Test Statistic",
    "text": "15.3 Step 3: Calculate the Test Statistic\n\nA test statistic is a statistic value used in making a decision about the \\(H_0\\).\n\n\n\nSuppose  \\(H_0: \\mu = \\mu_0 \\quad H_1: \\mu < \\mu_0\\) \nWhen computing a test statistic, we assume \\(H_0\\) is true.\nWhen \\(\\sigma\\) is known, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\boxed{ z_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{\\sigma/\\sqrt{n}} }\\]\n\n\n\n\n\n\nGuess what test statistic we use when \\(\\sigma\\) is unknown!\n\n\n\n\n\n\n\nWhen \\(\\sigma\\) is unknown, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\boxed{ t_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}} }\\]\n\n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)"
  },
  {
    "objectID": "infer-ht.html#step-4-c-find-the-critical-value",
    "href": "infer-ht.html#step-4-c-find-the-critical-value",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Step 4-c: Find the Critical Value",
    "text": "15.3 Step 4-c: Find the Critical Value\n\nThe critical value(s) separates the rejection region or critical region (where we reject \\(H_0\\)) from the values of the test statistic that do not lead to rejection of \\(H_0\\).\n\nThey depend on whether the test is a right-tailed, left-tailed or two-tailed test.\n\n\n\n\n\n\n\n\n\n\n\n\nüëâ \\(z_{\\alpha}\\) is such that \\(P(Z > z_{\\alpha}) = \\alpha\\) and \\(Z \\sim N(0, 1)\\).\nüëâ \\(t_{\\alpha, n-1}\\) is such that \\(P(T > t_{\\alpha, n-1}) = \\alpha\\) and \\(T \\sim t_{n-1}\\).\n\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{\\alpha}\\)\n\\(-z_{\\alpha}\\)\n\\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\)\n\n\n\\(\\sigma\\) unknown\n\\(t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha/2, n-1}\\) and \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n\\(z_{0.025} =\\) 1.96, \\(z_{0.05} =\\) 1.64\n\n\n\n\\(z_{\\alpha}\\) and \\(t_{\\alpha, n-1}\\) are always positive.\n The critical value is \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\)"
  },
  {
    "objectID": "infer-ht.html#step-5-c-draw-a-conclusion-using-critical-value",
    "href": "infer-ht.html#step-5-c-draw-a-conclusion-using-critical-value",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Step 5-c: Draw a Conclusion Using Critical Value",
    "text": "15.3 Step 5-c: Draw a Conclusion Using Critical Value\n\nIf the test statistic is\n\nin the rejection region, we reject \\(H_0\\).\nnot in the rejection region, we do not or fail to reject \\(H_0\\).\n\nReject \\(H_0\\) if\n\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{test} > z_{\\alpha}\\)\n\\(z_{test} < -z_{\\alpha}\\)\n\\(\\mid z_{test}\\mid \\, > z_{\\alpha/2}\\)\n\n\n\\(\\sigma\\) unknown\n\\(t_{test} > t_{\\alpha, n-1}\\)\n\\(t_{test} < -t_{\\alpha, n-1}\\)\n\\(\\mid t_{test}\\mid \\, > t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\) \n \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) \n We reject \\(H_0\\) if \\(t_{test} < -t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = -2.55 < -1.711 = -t_{\\alpha, n-1}\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "infer-ht.html#step-4-p-find-the-p-value",
    "href": "infer-ht.html#step-4-p-find-the-p-value",
    "title": "15¬† Hypothesis Testing",
    "section": "15.4 Step 4-p: Find the P-Value",
    "text": "15.4 Step 4-p: Find the P-Value\n\nThe \\(p\\)-value measures the strength of the evidence against \\(H_0\\) provided by the data.\nThe smaller the \\(p\\)-value, the greater the evidence against \\(H_0\\).\nThe \\(p\\)-value is the probability of getting a test statistic value that is at least as extreme as the one obtained from the data, assuming that \\(H_0\\) is true. \\((\\mu = \\mu_0)\\)\n\nFor example, \\(p\\)-value \\(= P(Z \\ge z_{test} \\mid H_0)\\) for a right-tailed test."
  },
  {
    "objectID": "infer-ht.html#p-value-illustration",
    "href": "infer-ht.html#p-value-illustration",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 P-Value Illustration",
    "text": "15.3 P-Value Illustration\n\n\n\n\n\n\nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T < t_{test})=P(T < -2.55) =\\) 0.01"
  },
  {
    "objectID": "infer-ht.html#step-5-p-draw-a-conclusion-using-p-value-method",
    "href": "infer-ht.html#step-5-p-draw-a-conclusion-using-p-value-method",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Step 5-p: Draw a Conclusion Using P-Value Method",
    "text": "15.3 Step 5-p: Draw a Conclusion Using P-Value Method\n\nIf the \\(p\\)-value \\(\\le \\alpha\\) , we reject \\(H_0\\).\nIf the \\(p\\)-value \\(> \\alpha\\), we do not reject or fail to reject \\(H_0\\).\n\n\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(P(Z > z_{test} \\mid H_0)\\)\n\\(P(Z < z_{test} \\mid H_0)\\)\n\\(2P(Z > \\,\\mid z_{test} \\mid \\, \\mid H_0)\\)\n\n\n\\(\\sigma\\) unknown\n\\(P(T > t_{test} \\mid H_0)\\)\n\\(P(T < t_{test} \\mid H_0)\\)\n\\(2P(T > \\, \\mid t_{test} \\mid \\, \\mid H_0)\\)\n\n\n\n\n\n We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(= 0.01 < 0.05 = \\alpha\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "infer-ht.html#both-methods-lead-to-the-same-conclusion",
    "href": "infer-ht.html#both-methods-lead-to-the-same-conclusion",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Both Methods Lead to the Same Conclusion",
    "text": "15.3 Both Methods Lead to the Same Conclusion"
  },
  {
    "objectID": "infer-ht.html#step-6-restate-the-conclusion-in-nontechnical-terms-and-address-the-original-claim",
    "href": "infer-ht.html#step-6-restate-the-conclusion-in-nontechnical-terms-and-address-the-original-claim",
    "title": "15¬† Hypothesis Testing",
    "section": "15.4 Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim",
    "text": "15.4 Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\n\n\n\n\n\nhttps://www.drdawnwright.com/category/statistics/\n\n\n\n\n\n\n\n\n\nhttps://www.pinterest.com/pin/287878601159173631/\n\n\n\n\n\n There is sufficient evidence to support the claim that the new treatment is effective."
  },
  {
    "objectID": "infer-ht.html#example-calculation-in-r",
    "href": "infer-ht.html#example-calculation-in-r",
    "title": "15¬† Hypothesis Testing",
    "section": "15.4 Example Calculation in R",
    "text": "15.4 Example Calculation in R\n\n## create objects for any information we have\nalpha <- 0.05; mu_0 <- 150; \nx_bar <- 147.2; s <- 5.5; n <- 25\n\n## Test statistic\n(t_test <- (x_bar - mu_0) / (s / sqrt(n))) \n\n[1] -2.545455\n\n## Critical value\n(t_cri <- qt(alpha, df = n - 1, lower.tail = TRUE)) \n\n[1] -1.710882\n\n## p-value\n(p_val <- pt(t_test, df = n - 1, lower.tail = TRUE)) \n\n[1] 0.008878158"
  },
  {
    "objectID": "infer-ht.html#example-2-two-tailed-z-test",
    "href": "infer-ht.html#example-2-two-tailed-z-test",
    "title": "15¬† Hypothesis Testing",
    "section": "15.4 Example 2: Two-tailed z-test",
    "text": "15.4 Example 2: Two-tailed z-test\n\n\n\n\nThe milk price of a gallon of 2% milk is normally distributed with standard deviation of $0.10.\n\nLast week the mean milk price was 2.78. This week, based on a sample of size 25, the sample mean milk price \\(\\overline{x} = 2.80\\).\nUnder \\(\\alpha = 0.05\\), determine if this week the mean price is different.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: This is a \\(H_1\\) claim  \\(\\small \\begin{align}&H_0: \\mu = 2.78 \\\\ &H_1: \\mu \\ne 2.78 \\end{align}\\) \nStep 2:  \\(\\small \\alpha = 0.05\\) \nStep 3:  \\(\\small z_{test} = \\frac{\\overline{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{2.8 - 2.78}{0.1/\\sqrt{25}} = 1.00\\) \nStep 4-c:  \\(\\small z_{0.05/2} = 1.96\\). \nStep 5-c: This is a two-tailed test and we reject \\(H_0\\) if \\(|z_{test}| > z_{\\alpha/2}\\). Since \\(\\small |z_{test}| = 1 < 1.96 = z_{\\alpha/2}\\), we DO NOT reject \\(H_0\\).\nStep 4-p: This is a two-tailed test, and the test statistic is on the right \\((> 0)\\), so the \\(p\\)-value is \\(2P(Z > z_{test})=\\) 0.317 \nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(= 0.317 > 0.05 = \\alpha\\), we DO NOT reject \\(H_0\\).\nStep 6:  There is insufficient evidence to support the claim that this week the mean milk price is different from the price last week."
  },
  {
    "objectID": "infer-ht.html#example-2-calculation-in-r",
    "href": "infer-ht.html#example-2-calculation-in-r",
    "title": "15¬† Hypothesis Testing",
    "section": "15.5 Example 2 Calculation in R",
    "text": "15.5 Example 2 Calculation in R\n\n## create objects to be used\nalpha <- 0.05; mu_0 <- 2.78; \nx_bar <- 2.8; sigma <- 0.1; n <- 25\n\n## Test statistic\n(z_test <- (x_bar - mu_0) / (sigma / sqrt(n))) \n\n[1] 1\n\n## Critical value\n(z_crit <- qnorm(alpha/2, lower.tail = FALSE)) \n\n[1] 1.959964\n\n## p-value\n(p_val <- 2 * pnorm(z_test, lower.tail = FALSE)) \n\n[1] 0.3173105"
  },
  {
    "objectID": "infer-ht.html#testing-summary",
    "href": "infer-ht.html#testing-summary",
    "title": "15¬† Hypothesis Testing",
    "section": "15.5 Testing Summary",
    "text": "15.5 Testing Summary\n\nBelow is a table that summarizes what we have learned about Hypothesis Testing in this chapter.\n\n\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\)  unknown \n\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\nPopulation Mean \\(\\mu\\)\n\n\nTest Type\nOne sample \\(\\color{blue}{z}\\) test \\(H_0: \\mu = \\mu_0\\)\nOne sample \\(\\color{blue}{t}\\) test \\(H_0: \\mu = \\mu_0\\)\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{\\color{blue}{s}}{\\sqrt{n}}\\)\n\n\nTest Stat under \\(H_0\\) \n\\(z_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\\(t_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\color{blue}{s}}{\\sqrt{n}}}\\)\n\n\n\\(p\\)-value under \\(H_0\\)\n\\(H_1: \\mu < \\mu_0\\)  \\(p\\)-value \\(=P(Z \\le z_{test})\\)\n\\(H_1: \\mu < \\mu_0\\)  \\(p\\)-value \\(=P(T_{n-1} \\le t_{test})\\)\n\n\n\n\\(H_1: \\mu > \\mu_0\\)  \\(p\\)-value \\(=P(Z \\ge z_{test})\\)\n\\(H_1: \\mu < \\mu_0\\)  \\(p\\)-value \\(=P(T_{n-1} \\ge t_{test})\\)\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\)  \\(p\\)-value \\(=2P(Z \\ge \\, \\mid z_{test}\\mid)\\)\n\\(H_1: \\mu \\ne \\mu_0\\)  \\(p\\)-value \\(=2P(T_{n-1} \\ge \\, \\mid t_{test} \\mid)\\)"
  },
  {
    "objectID": "infer-ht.html#type-i-and-type-ii-errors",
    "href": "infer-ht.html#type-i-and-type-ii-errors",
    "title": "15¬† Hypothesis Testing",
    "section": "15.6 Type I and Type II Errors",
    "text": "15.6 Type I and Type II Errors\n\nIt is important to remember that hypothesis testing is not perfect.\nBecause of this, Type I and Type II errors are important to understand.\n\n\n\n\nDecision\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\nType I error\nCorrect decision\n\n\nDo not reject \\(H_0\\)\nCorrect decision\nType II error\n\n\n\n\nBack to the crime example:\n\n\\(H_0:\\) The person is  not guilty  v.s. \\(H_1:\\) The person is  guilty \n\n\n\n\n\n\n\n\n\n\nDecision\nTruth is the person innocent\nTruth is the person guilty\n\n\n\n\nJury decides the person guilty\nType I error\nCorrect decision\n\n\nJury decides the person innocent\nCorrect decision\nType II error\n\n\n\n\n\\(\\alpha = P(\\text{type I error}) = P(\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true})\\)\n\\(\\beta = P(\\text{type II error}) = P(\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false})\\)\n\n\n\n\n\n\nFigure¬†15.9: Example of Type I and Type II errors (https://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg)"
  },
  {
    "objectID": "infer-twomean.html#why-compare-two-populations",
    "href": "infer-twomean.html#why-compare-two-populations",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.1 Why Compare Two Populations?",
    "text": "16.1 Why Compare Two Populations?\n\nOften we are faced with a comparison of parameters from different populations.\n\n Comparing the mean annual income for Male and Female groups. \n Testing if a diet used for losing weight is effective from Placebo samples and New Diet samples. \n\nIf these two samples are drawn from populations with means \\(\\mu_1\\) and \\(\\mu_2\\) respectively, the testing problem can be formulated as  \\[\\begin{align}\n&H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 > \\mu_2\n\\end{align}\\] \n\n\\(\\mu_1\\): male mean annual income; \\(\\mu_2\\): female mean annual income\n\\(\\mu_1\\): mean weight loss from the New Diet group; \\(\\mu_2\\): mean weight loss from the Placebo group"
  },
  {
    "objectID": "infer-twomean.html#dependent-and-independent-samples",
    "href": "infer-twomean.html#dependent-and-independent-samples",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.2 Dependent and Independent Samples",
    "text": "16.2 Dependent and Independent Samples\n\nThe two samples collected can be independent or dependent.\n\n\n\n\n\nTwo samples are dependent or matched pairs if the sample values are matched, where the matching is based on some inherent relationship.\n\n Height data of fathers and daughters. The height of each dad is matched with the height of his daughter. \n Weights of subjects measure before and after some diet treatment. The subjects are the same before and after measurements."
  },
  {
    "objectID": "infer-twomean.html#dependent-samples-matched-pairs",
    "href": "infer-twomean.html#dependent-samples-matched-pairs",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.2 Dependent Samples (Matched Pairs)",
    "text": "16.2 Dependent Samples (Matched Pairs)\n\nSubject 1 may refer to\n\nthe same person with two measurements (before and after)\nthe first matched pair (dad-daughter).\n\n\n\n\n\n\n\n\nSubject\n(Dad) Before\n(Daughter) After\n\n\n\n\n1\n\\(x_{b1}\\)\n\\(x_{a1}\\)\n\n\n2\n\\(x_{b2}\\)\n\\(x_{a2}\\)\n\n\n3\n\\(x_{b3}\\)\n\\(x_{a3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{bn}\\)\n\\(x_{an}\\)"
  },
  {
    "objectID": "infer-twomean.html#independent-samples",
    "href": "infer-twomean.html#independent-samples",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.2 Independent Samples",
    "text": "16.2 Independent Samples\n\n\n\n\nTwo samples are independent if the sample values from one population are not related to the sample values from the other.\n\n Salary samples of men and women. Two samples are drawn independently from the male and female groups. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject 1 of the Group 1 has nothing to do with the Subject 1 of the Group 2.\n\n\n\n\n\n\n\n\n\n\nSubject of Group 1 (Male)\nMeasurement of Group 1\nSubject of Group 2 (Female)\nMeasurement of Group 2\n\n\n\n\n1\n\\(x_{11}\\)\n1\n\\(x_{21}\\)\n\n\n2\n\\(x_{12}\\)\n2\n\\(x_{22}\\)\n\n\n3\n\\(x_{13}\\)\n3\n\\(x_{23}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n_1\\)\n\\(x_{1n_1}\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\\(n_2\\)\n\\(x_{2n_2}\\)"
  },
  {
    "objectID": "infer-twomean.html#inference-from-two-samples",
    "href": "infer-twomean.html#inference-from-two-samples",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.2 Inference from Two Samples",
    "text": "16.2 Inference from Two Samples\n\n\n\n\nThe statistical methods are different for these two types of samples.\nGood news: The concepts of CI and HT for one population can be applied to two-population cases.\n\\(\\text{CI = point estimate} \\pm \\text{margin of error (E)}\\), e.g., \\(\\overline{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\nMargin of error = critical value \\(\\times\\) standard error of the point estimator\nThe 6 testing steps are the same, and both critical value and \\(p\\)-value method can be applied too, e.g., \\(t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}}\\)"
  },
  {
    "objectID": "infer-twomean.html#hypothesis-testing-for-dependent-samples",
    "href": "infer-twomean.html#hypothesis-testing-for-dependent-samples",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.2 Hypothesis Testing for Dependent Samples",
    "text": "16.2 Hypothesis Testing for Dependent Samples\n\n\n\n\n\n\nTo analyze a paired data set, simply analyze the differences!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(x_1\\)\n\\(x_2\\)\nDifference \\(d = x_1 - x_2\\)\n\n\n\n\n1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(\\color{red}{d_1}\\)\n\n\n2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(\\color{red}{d_2}\\)\n\n\n3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(\\color{red}{d_3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\color{red}{\\vdots}\\)\n\n\n\\(n\\)\n\\(x_{1n}\\)\n\\(x_{2n}\\)\n\\(\\color{red}{d_n}\\)\n\n\n\n\n\n\n\\(\\mu_d = \\mu_1 - \\mu_2\\)\n \\(\\begin{align} & H_0: \\mu_1 - \\mu_2 = 0 \\iff \\mu_d = 0 \\\\ & H_1: \\mu_1 - \\mu_2 > 0 \\iff \\mu_d > 0 \\\\ & H_1: \\mu_1 - \\mu_2 < 0 \\iff \\mu_d < 0 \\\\ & H_1: \\mu_1 - \\mu_2 \\ne 0 \\iff \\mu_d \\ne 0 \\end{align}\\) \n\n\n\n\n\n\n\n\n\n\nThe point estimate of \\(\\mu_1 - \\mu_2\\) is \\(\\overline{x}_1 - \\overline{x}_2 = \\overline{d}\\)."
  },
  {
    "objectID": "infer-twomean.html#inference-for-paired-data",
    "href": "infer-twomean.html#inference-for-paired-data",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.3 Inference for Paired Data",
    "text": "16.3 Inference for Paired Data\n\nRequirements: the sample differences \\(\\color{blue}{d_i}\\)s are\n\nrandom sample\nfrom a normal distribution and/or \\(n > 30\\) (tested by QQ-plot of \\(d_i\\)s)\n\nFollow the same procedure as the one-sample \\(t\\)-test!\nThe test statistic is \\(\\color{blue}{t_{test} = \\frac{\\overline{d}-0}{s_d/\\sqrt{n}}} \\sim T_{n-1}\\) under \\(H_0\\) where \\(\\overline{d}\\) and \\(s_d\\) are the mean and SD of the difference samples \\((d_1, d_2, \\dots, d_n)\\).\nThe critical value \\(t_{\\alpha, n-1}\\) and \\(t_{\\alpha/2, n-1}\\).\n\n\n\n\n\n\n\n\n\nPaired \\(t\\)-test\nTest Statistic\nConfidence Interval for \\(\\mu_d = \\mu_1 - \\mu_2\\)\n\n\n\n\n\\(\\sigma_d\\) is unknown\n\\(\\large t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}}\\)\n\\(\\large \\overline{d} \\pm t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\)\n\n\n\n\nThe test from matched pairs is called a paired \\(t\\)-test."
  },
  {
    "objectID": "infer-twomean.html#example",
    "href": "infer-twomean.html#example",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.3 Example",
    "text": "16.3 Example\n\nConsider a capsule used to reduce blood pressure (BP) for the hypertensive individuals. Sample of 10 hypertensive individuals take the medicine for 4 weeks.\nDoes the data provide sufficient evidence that the treatment is effective in reducing BP?\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nBefore \\((x_b)\\)\nAfter \\((x_a)\\)\nDifference \\(d = x_b - x_a\\)\n\n\n\n\n1\n143\n124\n19\n\n\n2\n153\n129\n24\n\n\n3\n142\n131\n11\n\n\n4\n139\n145\n-6\n\n\n5\n172\n152\n20\n\n\n6\n176\n150\n26\n\n\n7\n155\n125\n30\n\n\n8\n149\n142\n7\n\n\n9\n140\n145\n-5\n\n\n10\n169\n160\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\overline{d} = 13.5\\), \\(s_d= 12.48\\).\n\\(\\mu_1 =\\) Mean Before, \\(\\mu_2 =\\) Mean After, and \\(\\mu_d = \\mu_1 - \\mu_2\\).\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\iff \\mu_d = 0\\\\ &H_1: \\mu_1 > \\mu_2 \\iff \\mu_d > 0 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}} = \\frac{13.5}{12.48/\\sqrt{10}} = 3.42\\) \nStep 4-c:  \\(t_{\\alpha, n-1} = t_{0.05, 9} = 1.833\\).\nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} > t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = 3.42 > 1.833 = t_{\\alpha, n-1}\\), we reject \\(H_0\\). \n\n\n\nStep 6:  There is sufficient evidence to support the claim that the drug is effective in reducing blood pressure. \n\n\n\n\n\n\n\n\n\n\n\nThe 95% CI for \\(\\mu_d = \\mu_1 - \\mu_2\\) is \\[\\begin{align}\\overline{d} \\pm t_{\\alpha/2, df} \\frac{s_d}{\\sqrt{n}} &= 13.5 \\pm t_{0.025, 9}\\frac{12.48}{\\sqrt{10}}\\\\ &= 13.5 \\pm 8.927 \\\\ &= (4.573, 22.427).\\end{align}\\]\n95% confident that the mean difference in blood pressure is between 4.57 and 22.43.\nSince the interval does NOT include 0, it leads to the same conclusion as rejection of \\(H_0\\)."
  },
  {
    "objectID": "infer-twomean.html#two-sample-paired-test-in-r",
    "href": "infer-twomean.html#two-sample-paired-test-in-r",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.3 Two-Sample Paired Test in R",
    "text": "16.3 Two-Sample Paired Test in R\n\n\n\npair_data\n\n   before after\n1     143   124\n2     153   129\n3     142   131\n4     139   145\n5     172   152\n6     176   150\n7     155   125\n8     149   142\n9     140   145\n10    169   160\n\n(d <- pair_data$before - pair_data$after)\n\n [1] 19 24 11 -6 20 26 30  7 -5  9\n\n(d_bar <- mean(d))\n\n[1] 13.5\n\n\n\n(s_d <- sd(d))\n\n[1] 12.48332\n\n## t_test\n(t_test <- d_bar/(s_d/sqrt(length(d))))\n\n[1] 3.419823\n\n## t_cv\nqt(p = 0.95, df = length(d) - 1)\n\n[1] 1.833113\n\n## p_value\npt(q = t_test, df = length(d) - 1, \n   lower.tail = FALSE)\n\n[1] 0.003815036"
  },
  {
    "objectID": "infer-twomean.html#two-sample-paired-test-in-r-1",
    "href": "infer-twomean.html#two-sample-paired-test-in-r-1",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Two-Sample Paired Test in R",
    "text": "16.4 Two-Sample Paired Test in R\n\nd_bar + c(-1, 1) * qt(p = 0.975, df = length(d) - 1) * (s_d / sqrt(length(d))) ## CI\n\n[1]  4.569969 22.430031\n\n\n\n## t.test() function\nt.test(x = pair_data$before, y = pair_data$after,\n       alternative = \"greater\", mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pair_data$before and pair_data$after\nt = 3.4198, df = 9, p-value = 0.003815\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 6.263653      Inf\nsample estimates:\nmean difference \n           13.5 \n\n\n\nBe careful about the one-sided CI! We should use the two-sided CI!"
  },
  {
    "objectID": "infer-twomean.html#compare-population-means-independent-samples",
    "href": "infer-twomean.html#compare-population-means-independent-samples",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.3 Compare Population Means: Independent Samples",
    "text": "16.3 Compare Population Means: Independent Samples\n\nWhether stem cells can improve heart function.\nThe relationship between pregnant womens‚Äô smoking habits and newborns‚Äô weights.\nWhether one variation of an exam is harder than another variation."
  },
  {
    "objectID": "infer-twomean.html#inferences-from-two-independent-samples-overview",
    "href": "infer-twomean.html#inferences-from-two-independent-samples-overview",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Inferences from Two Independent Samples: Overview",
    "text": "16.4 Inferences from Two Independent Samples: Overview"
  },
  {
    "objectID": "infer-twomean.html#testing-for-independent-samples-sigma_1-ne-sigma_2",
    "href": "infer-twomean.html#testing-for-independent-samples-sigma_1-ne-sigma_2",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)",
    "text": "16.4 Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\nRequirements:\n\nThe two samples are independent.\nBoth samples are a random sample.\n\\(n_1 > 30\\), \\(n_2 > 30\\) and/or both samples are from a normally distributed population.\n\nInterested in whether the two population means ** \\(\\mu_1\\) and \\(\\mu_2\\) are equal or not **, or one is larger than the other.\n\\(H_0: \\mu_1 = \\mu_2\\)\nIt is equivalent to testing if their difference is zero.\n\\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\n\n\n\n\n\n\n\nWe start with finding a point estimate for \\(\\mu_1 - \\mu_2\\). What is the best point estimator for \\(\\mu_1 - \\mu_2\\)?\n\n\n\n\n\n\n\n\\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\)!"
  },
  {
    "objectID": "infer-twomean.html#sampling-distribution-of-overlinex_1---overlinex_2",
    "href": "infer-twomean.html#sampling-distribution-of-overlinex_1---overlinex_2",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\)",
    "text": "16.4 Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\)\n\nIf the two samples are from independent normally distributed populations or \\(n_1 > 30\\) and \\(n_2 > 30\\), \\[\\small \\overline{X}_1 \\sim N\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1} \\right), \\quad \\overline{X}_2 \\sim N\\left(\\mu_2,\n\\frac{\\sigma_2^2}{n_2} \\right)\\]\n\\(\\overline{X}_1 - \\overline{X}_2\\) has the sampling distribution \\[\\small \\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} \\color{red}{+} \\color{black}{\\frac{\\sigma_2^2}{n_2}} \\right) \\]\n\n\\[\\small Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2",
    "href": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)",
    "text": "16.4 Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, the test statistic becomes \\(t_{test}\\):\n\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} \\] \n\nThe critical value \\(t_{\\alpha, df}\\) (one-tailed) and \\(t_{\\alpha/2, df}\\) (two-tailed), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\] where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\).\n\n\n\nIf the \\(df\\) is not an integer, we round it down to an integer."
  },
  {
    "objectID": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2-1",
    "href": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2-1",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.6 Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)",
    "text": "16.6 Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, the test statistic becomes \\(t_{test}\\):\n\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} \\] \n\nThe critical value \\(t_{\\alpha, df}\\) (one-tailed) and \\(t_{\\alpha/2, df}\\) (two-tailed), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\] where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\).\n\n\n\nIf the \\(df\\) is not an integer, we round it down to an integer."
  },
  {
    "objectID": "infer-twomean.html#inference-from-independent-samples-sigma_1-ne-sigma_2",
    "href": "infer-twomean.html#inference-from-independent-samples-sigma_1-ne-sigma_2",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Inference from Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)",
    "text": "16.4 Inference from Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 \\ne \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}\\)\n\n\n\n\n\nUse \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\) to get the \\(p\\)-value, critical value, and CI.\nThe unequal-variance t-test is called Welch‚Äôs t-test."
  },
  {
    "objectID": "infer-twomean.html#example-two-sample-t-test",
    "href": "infer-twomean.html#example-two-sample-t-test",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Example: Two-Sample t-Test",
    "text": "16.4 Example: Two-Sample t-Test\n\n\n\n\nDoes an oversized tennis racket exert less stress/force on the elbow? The data show\n\nOversized: \\(n_1 = 33\\), \\(\\overline{x}_1 = 25.2\\), \\(s_1 = 8.6\\)\nConventional: \\(n_2 = 12\\), \\(\\overline{x}_2 = 33.9\\), \\(s_2 = 17.4\\)\nThe two populations are nearly normal.\nThe large difference in the sample SD suggests \\(\\sigma_1 \\ne \\sigma_2\\).\nForm a hypothesis test with \\(\\alpha = 0.05\\) and construct a 95% CI for the mean difference of force on the elbow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(25.2 - 33.9) - 0}{\\sqrt{\\frac{\\color{red}{8.6^2}}{33} + \\frac{\\color{red}{17.4^2}}{12}}} = -1.66\\)\n\\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\)\n\\(\\small A = \\dfrac{8.6^2}{33}\\), \\(\\small B = \\dfrac{17.4^2}{12}\\), \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01\\)\n\n\n\n\n\n\n\n\nIf the computed value of \\(df\\) is not an integer, always round down to the nearest integer.\n\n\n\n\n\n\n\nStep 4-c:  \\(-t_{0.05, 13} = -1.771\\). \nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} < -t_{\\alpha, df}\\). \\(\\small t_{test} = -1.66 > -1.771 = -t_{\\alpha, df}\\), we fail to reject \\(H_0\\). \nStep 6:  There is insufficient evidence to support the claim that the the oversized racket delivers less stress to the elbow. \nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is\n\n\\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}} &= (25.2 - 33.9) \\pm t_{0.025,13}\\sqrt{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}}\\\\&= -8.7 \\pm 11.32 = (-20.02, 2.62).\\end{align}\\]\n\nWe are 95% confident that the difference in the mean forces is between -20.02 and 2.62.\nSince the interval includes 0, it leads to the same conclusion as failing to reject \\(H_0\\)."
  },
  {
    "objectID": "infer-twomean.html#two-sample-t-test-in-r",
    "href": "infer-twomean.html#two-sample-t-test-in-r",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Two-Sample t-Test in R",
    "text": "16.4 Two-Sample t-Test in R\n\nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA <- s1^2 / n1; B <- s2^2 / n2\ndf <- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n(df <- floor(df))\n\n[1] 13\n\n## t_test\n(t_test <- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n\n[1] -1.659894\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.770933\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 0.06042575"
  },
  {
    "objectID": "infer-twomean.html#sampling-distribution-of-overlinex_1---overlinex_2-when-sigma_1-sigma_2-sigma",
    "href": "infer-twomean.html#sampling-distribution-of-overlinex_1---overlinex_2-when-sigma_1-sigma_2-sigma",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) when \\(\\sigma_1 = \\sigma_2 = \\sigma\\)",
    "text": "16.4 Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) when \\(\\sigma_1 = \\sigma_2 = \\sigma\\)\n\\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} \\right) \\] If \\(\\sigma_1 = \\sigma_2 = \\sigma\\), \\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\right) \\] \\[ Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma",
    "href": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)",
    "text": "16.4 Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, \\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nHere, the critical value \\(t_{\\alpha, df}\\) (for one-tailed tests) and \\(t_{\\alpha/2, df}\\) (for two-tailed tests), and the \\(t\\) distribution used to compute the \\(p\\)-value have the degrees of freedom \\[df = n_1 + n_2 - 2\\]"
  },
  {
    "objectID": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma-1",
    "href": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma-1",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.5 Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)",
    "text": "16.5 Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, \\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nHere, the critical value \\(t_{\\alpha, df}\\) (for one-tailed tests) and \\(t_{\\alpha/2, df}\\) (for two-tailed tests), and the \\(t\\) distribution used to compute the \\(p\\)-value have the degrees of freedom \\[df = n_1 + n_2 - 2\\]"
  },
  {
    "objectID": "infer-twomean.html#inference-from-independent-samples-sigma_1-sigma_2-sigma",
    "href": "infer-twomean.html#inference-from-independent-samples-sigma_1-sigma_2-sigma",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)",
    "text": "16.4 Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 = \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\color{red}{s_p}\\ \\color{black} sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\color{red}{s_p}\\ \\color{black} sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\n\n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\nUse \\(df = n_1+n_2-2\\) get the \\(p\\)-value, critical value and CI.\nThe test from two independent samples with \\(\\sigma_1 = \\sigma_2 = \\sigma\\) is usually called two-sample pooled \\(z\\)-test or two-sample pooled \\(t\\)-test."
  },
  {
    "objectID": "infer-twomean.html#example-weight-loss",
    "href": "infer-twomean.html#example-weight-loss",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Example: Weight Loss",
    "text": "16.4 Example: Weight Loss\n\n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\nIs there a sufficient evidence at \\(\\alpha = 0.05\\) to conclude that the program is effective?\nIf yes, construct a 95% CI for \\(\\mu_1 - \\mu_2\\) to show how much effective it is.\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\). \n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(10-1)0.5^2 + (10-1)0.7^2}{10+10-2}}=0.6083\\)\n \\(t_{test} = \\frac{(2.1 - 4.2) - 0}{0.6083\\sqrt{\\frac{1}{10} + \\frac{1}{10}}} = -7.72\\)\nStep 4-c:  \\(df = n_1 + n_2 - 2 = 10 + 10 - 2 = 18\\). So \\(-t_{0.05, df = 18} = -1.734\\). \nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} < -t_{\\alpha, df}\\). Since \\(\\small t_{test} = -7.72 < -1.734 = -t_{\\alpha, df}\\), we reject \\(H_0\\).\nStep 4-p:  The \\(p\\)-value is \\(P(T_{df=18} < t_{test}) \\approx 0\\) \nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(\\approx 0 < 0.05 = \\alpha\\), we reject \\(H_0\\).\nStep 6:  There is sufficient evidence to support the claim that the weight loss program is effective."
  },
  {
    "objectID": "infer-twomean.html#example-contd",
    "href": "infer-twomean.html#example-contd",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Example Cont‚Äôd",
    "text": "16.4 Example Cont‚Äôd\n\nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is \\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} &= (2.1 - 4.2) \\pm t_{0.025, 18} (0.6083)\\sqrt{\\frac{1}{10} + \\frac{1}{10}}\\\\ &= -2.1 \\pm 0.572 = (-2.672, -1.528) \\end{align}\\]\nWe are 95% confident that the difference in the mean weight is between -2.672 and -1.528.\nSince the interval does not include 0, it leads to the same conclusion as rejection of \\(H_0\\)."
  },
  {
    "objectID": "infer-twomean.html#two-sample-pooled-t-test-in-r",
    "href": "infer-twomean.html#two-sample-pooled-t-test-in-r",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Two-Sample Pooled t-Test in R",
    "text": "16.4 Two-Sample Pooled t-Test in R\n\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\nsp <- sqrt(((n1 - 1) * s1 ^ 2 + (n2 - 1) * s2 ^ 2) / (n1 + n2 - 2))\nsp\n\n[1] 0.6082763\n\ndf <- n1 + n2 - 2\n## t_test\n(t_test <- (x1_bar - x2_bar) / (sp * sqrt(1 / n1 + 1 / n2)))\n\n[1] -7.719754\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.734064\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 2.028505e-07"
  },
  {
    "objectID": "infer-ci.html#why-inference-for-population-variances",
    "href": "infer-ci.html#why-inference-for-population-variances",
    "title": "13¬† Confidence Interval",
    "section": "13.7 Why Inference for Population Variances?",
    "text": "13.7 Why Inference for Population Variances?\n\nWe want to know if \\(\\sigma_1 = \\sigma_2\\), so a correct or better method can be used.\n\n\n\n\n\n\n\nWhich test we learned needs \\(\\sigma_1 = \\sigma_2\\)?\n\n\n\n\n\n\n\nIn some situations, we care about variation!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n The variation in potency of drugs: affects patients‚Äô health\n\n\n The variance of stock prices : the higher the variance, the riskier the investment"
  },
  {
    "objectID": "infer-ci.html#inference-for-population-variances",
    "href": "infer-ci.html#inference-for-population-variances",
    "title": "13¬† Confidence Interval",
    "section": "13.8 Inference for Population Variances",
    "text": "13.8 Inference for Population Variances\n\nThe sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance \\(\\sigma^2\\).\n\n\\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\), i.e., \\(E(S^2) = \\sigma^2\\)\n\nThe inference methods for \\(\\sigma^2\\) needs the population to be normal.\n\n\n\n\n\n\n\n‚ùó The methods can work poorly if the normality is violated, even the sample is large."
  },
  {
    "objectID": "infer-ci.html#chi-square-chi2-distribution",
    "href": "infer-ci.html#chi-square-chi2-distribution",
    "title": "13¬† Confidence Interval",
    "section": "13.9 Chi-Square \\(\\chi^2\\) Distribution",
    "text": "13.9 Chi-Square \\(\\chi^2\\) Distribution\nThe inference for \\(\\sigma^2\\) involves the so called \\(\\chi^2\\) distribution.\n\n\n\n\nParameter: degrees of freedom \\(df\\)\nRight skewed distribution\nDefined over positive numbers\nMore symmetric as \\(df\\) gets larger\nChi-Square Distribution"
  },
  {
    "objectID": "infer-ci.html#upper-tail-and-lower-tail-of-chi-square",
    "href": "infer-ci.html#upper-tail-and-lower-tail-of-chi-square",
    "title": "13¬† Confidence Interval",
    "section": "13.10 Upper Tail and Lower Tail of Chi-Square",
    "text": "13.10 Upper Tail and Lower Tail of Chi-Square\n\n\n\\(\\chi^2_{\\frac{\\alpha}{2},\\, df}\\) has area to the right of \\(\\alpha/2\\).\n\\(\\chi^2_{1-\\frac{\\alpha}{2},\\, df}\\) has area to the left of \\(\\alpha/2\\).\nIn \\(N(0, 1)\\), \\(z_{1-\\frac{\\alpha}{2}} = -z_{\\frac{\\alpha}{2}}\\). But \\(\\chi^2_{1-\\frac{\\alpha}{2},\\,df} \\ne -\\chi^2_{\\frac{\\alpha}{2},\\,df}\\)."
  },
  {
    "objectID": "infer-ci.html#sampling-distribution",
    "href": "infer-ci.html#sampling-distribution",
    "title": "13¬† Confidence Interval",
    "section": "13.11 Sampling Distribution",
    "text": "13.11 Sampling Distribution\n\nWhen a random sample of size \\(n\\) is from \\(\\color{red}{N(\\mu, \\sigma^2)}\\), \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\]\n\n\n\nThe inference method for \\(\\sigma^2\\) introduced here can work poorly if the normality assumption is violated, even for large samples."
  },
  {
    "objectID": "infer-ci.html#alpha100-confidence-interval-for-sigma2",
    "href": "infer-ci.html#alpha100-confidence-interval-for-sigma2",
    "title": "13¬† Confidence Interval",
    "section": "13.12 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)",
    "text": "13.12 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} \\right)}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùó The CI for \\(\\sigma^2\\) cannot be expressed as \\((S^2-m, S^2+m)\\) anymore!"
  },
  {
    "objectID": "infer-ci.html#example-supermodel-heights",
    "href": "infer-ci.html#example-supermodel-heights",
    "title": "13¬† Confidence Interval",
    "section": "13.13 Example: Supermodel Heights",
    "text": "13.13 Example: Supermodel Heights\nListed below are heights (cm) for the simple random sample of 16 female supermodels:\n\nheights <- c(178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176)\n\n\n\n\n\nThe supermodels‚Äô height is normally distributed.\nConstruct a \\(95\\%\\) confidence interval for population standard deviation \\(\\sigma\\).\n\\(n = 16\\), \\(s^2 = 3.4\\), \\(\\alpha = 0.05\\).\n\\(\\chi^2_{\\alpha/2, n-1} = \\chi^2_{0.025, 15} = 27.49\\)\n\\(\\chi^2_{1-\\alpha/2, n-1} = \\chi^2_{0.975, 15} = 6.26\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(95\\%\\) CI for \\(\\sigma\\) is \\(\\small \\left( \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}}} \\right) = \\left( \\sqrt{\\frac{(16-1)(3.4)}{27.49}}, \\sqrt{\\frac{(16-1)(3.4)}{6.26}}\\right) = (1.36, 2.85)\\)"
  },
  {
    "objectID": "infer-ci.html#example-computation-in-r",
    "href": "infer-ci.html#example-computation-in-r",
    "title": "13¬† Confidence Interval",
    "section": "13.14 Example: Computation in R",
    "text": "13.14 Example: Computation in R\n\nn <- 16\ns2 <- var(heights)\nalpha <- 0.05\n\n## two chi-square critical values\nchi2_right <- qchisq(alpha/2, n - 1, lower.tail = FALSE)\nchi2_left <- qchisq(alpha/2, n - 1, lower.tail = TRUE)\n\n## two bounds of CI for sigma2\nci_sig2_lower <- (n - 1) * s2 / chi2_right\nci_sig2_upper <- (n - 1) * s2 / chi2_left\n\n## two bounds of CI for sigma\n(ci_sig_lower <- sqrt(ci_sig2_lower))\n\n[1] 1.362104\n\n(ci_sig_upper <- sqrt(ci_sig2_upper))\n\n[1] 2.853802"
  },
  {
    "objectID": "infer-ci.html#example-contd-testing",
    "href": "infer-ci.html#example-contd-testing",
    "title": "13¬† Confidence Interval",
    "section": "13.15 Example Cont‚Äôd: Testing",
    "text": "13.15 Example Cont‚Äôd: Testing\nUse \\(\\alpha = 0.05\\) to test the claim that ‚Äúsupermodels have heights with a standard deviation that is less than \\(\\sigma = 7.5\\) cm for the population of women‚Äù.\n\nStep 1: \\(H_0: \\sigma = \\sigma_0\\) vs.¬†\\(H_1: \\sigma < \\sigma_0\\). Here \\(\\sigma_0 = 7.5\\) cm\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Under \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), a statistic drawn from \\(\\chi^2_{n-1}\\).\n\n\n\n\n\nStep 4-c: This is a left-tailed test. The critical value is \\(\\chi_{1-\\alpha, df}^2 = \\chi_{0.95, 15}^2 = 7.26\\)\nStep-5-c: Reject \\(H_0\\) in favor of \\(H_1\\) if \\(\\chi_{test}^2 < \\chi_{1-\\alpha, df}^2\\). Since \\(0.91 < 7.26\\), we reject \\(H_0\\).\nStep 6: There is sufficient evidence to support the claim that supermodels have heights with a SD that is less than the SD for the population of women.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeights of supermodels vary less than heights of women in the general population."
  },
  {
    "objectID": "infer-ci.html#back-to-pooled-t-test",
    "href": "infer-ci.html#back-to-pooled-t-test",
    "title": "13¬† Confidence Interval",
    "section": "13.16 Back to Pooled t-Test",
    "text": "13.16 Back to Pooled t-Test\n\nIn a pooled t-test, we assume\n\n \\(n_1 \\ge 30\\) and \\(n_2 \\ge 30\\). If not, we assume that both samples are drawn from normal populations. \n \\(\\sigma_1 = \\sigma_2\\) \n\nUse QQ-plot (and normality tests, Anderson, Shapiro, etc) to check the assumption of normal distribution.\nWe learn to check the assumption \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "infer-ci.html#f-distribution",
    "href": "infer-ci.html#f-distribution",
    "title": "13¬† Confidence Interval",
    "section": "13.17 F Distribution",
    "text": "13.17 F Distribution\n\nWe use ** \\(F\\) distribution ** for the inference about two population variances.\n\n\n\n\nTwo parameters: \\(df_1\\), \\(df_2\\)\nRight skewed distribution\nDefined over positive numbers\nR Shiny app: F Distribution"
  },
  {
    "objectID": "infer-ci.html#upper-and-lower-tail-of-f-distribution",
    "href": "infer-ci.html#upper-and-lower-tail-of-f-distribution",
    "title": "13¬† Confidence Interval",
    "section": "13.18 Upper and Lower Tail of F Distribution",
    "text": "13.18 Upper and Lower Tail of F Distribution\n\nWe denote \\(F_{\\alpha, \\, df_1, \\, df_2}\\) as the \\(F\\) quantile so that \\(P(F_{df_1, df_2} > F_{\\alpha, \\, df_1, \\, df_2}) = \\alpha\\)."
  },
  {
    "objectID": "infer-ci.html#sampling-distribution-1",
    "href": "infer-ci.html#sampling-distribution-1",
    "title": "13¬† Confidence Interval",
    "section": "13.19 Sampling Distribution",
    "text": "13.19 Sampling Distribution\n\n\nWhen random samples of sizes \\(n_1\\) and \\(n_2\\) have been independently drawn from two normally distributed populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\), the ratio \\[\\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]"
  },
  {
    "objectID": "infer-ci.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "href": "infer-ci.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "title": "13¬† Confidence Interval",
    "section": "13.20 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)",
    "text": "13.20 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\color{blue}{\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, \\, n_1 - 1, \\, n_2 - 1}} \\right)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùó The CI for \\(\\sigma_1^2 / \\sigma_2^2\\) cannot be expressed as \\(\\left(\\frac{s_1^2}{s_2^2}-m, \\frac{s_1^2}{s_2^2} + m\\right)\\) anymore!"
  },
  {
    "objectID": "infer-ci.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "href": "infer-ci.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "title": "13¬† Confidence Interval",
    "section": "13.21 F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)",
    "text": "13.21 F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)\n\nStep 1: right-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 \\le \\sigma_2 \\\\ &H_1: \\sigma_1 > \\sigma_2 \\end{align}\\)  and two-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\) \nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Under \\(H_0\\), \\(\\sigma_1 = \\sigma_2\\), and the test statistic is \\[\\small F_{test} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} = \\frac{s_1^2}{s_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\nStep 4-c:\n\nRight-tailed:  \\(F_{\\alpha, \\, n_1-1, \\, n_2-1}\\) .\nTwo-tailed:  \\(F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\) \n\nStep 5-c:\n\nRight-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha, \\, n_1-1, \\, n_2-1}\\).\nTwo-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{test} \\le F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\)"
  },
  {
    "objectID": "infer-ci.html#back-to-the-weight-loss-example",
    "href": "infer-ci.html#back-to-the-weight-loss-example",
    "title": "13¬† Confidence Interval",
    "section": "13.22 Back to the Weight Loss Example",
    "text": "13.22 Back to the Weight Loss Example\n\n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\nAssumptions:\n\n \\(\\sigma_1 = \\sigma_2\\) \nThe weight loss for both groups are normally distributed."
  },
  {
    "objectID": "infer-ci.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "href": "infer-ci.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "title": "13¬† Confidence Interval",
    "section": "13.23 Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)",
    "text": "13.23 Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)\n\n\n\n\n\\(n_1 = 10\\), \\(s_1 = 0.5 \\, lb\\)\n\\(n_2 = 10\\), \\(s_2 = 0.7 \\, lb\\)\nStep 1: \\(\\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\)\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: The test statistic is \\(F_{test} = \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 4-c: This is a two-tailed test, the critical value is \\(F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\) or \\(F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\).\nStep 5-c: Is \\(F_{test} > 4.03\\) or \\(F_{test} < 0.25\\)? No.¬†\nStep 6: The evidence is not sufficient to reject the claim that \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "infer-ci.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "href": "infer-ci.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "title": "13¬† Confidence Interval",
    "section": "13.24 Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)",
    "text": "13.24 Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\n\\(\\small F_{\\alpha/2, \\, df_1, \\, df_2} = F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\)\n\\(\\small F_{1-\\alpha/2, \\, df_1, \\, df_2} = F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\)\n\\(\\small \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\)\nThe 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\small \\begin{align} &\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, df_1, \\, df_2}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, df_1, \\, df_2}} \\right) \\\\ &= \\left( \\frac{0.51}{4.03}, \\frac{0.51}{0.25} \\right) = \\left(0.127, 2.04\\right)\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are 95% confident that the ratio \\(\\sigma_1^2 / \\sigma_2^2\\) is between 0.127 and 2.04."
  },
  {
    "objectID": "infer-ci.html#implementing-f-test-in-r",
    "href": "infer-ci.html#implementing-f-test-in-r",
    "title": "13¬† Confidence Interval",
    "section": "13.25 Implementing F-test in R",
    "text": "13.25 Implementing F-test in R\n\n\n\n\nn1 <- 10; n2 <- 10\ns1 <- 0.5; s2 <- 0.7\nalpha <- 0.05\n\n## 95% CI for sigma_1^2 / sigma_2^2\nf_small <- qf(p = alpha / 2, \n              df1 = n1 - 1, df2 = n2 - 1, \n              lower.tail = TRUE)\nf_big <- qf(p = alpha / 2, \n            df1 = n1 - 1, df2 = n2 - 1, \n            lower.tail = FALSE)\n\n## lower bound\n(s1 ^ 2 / s2 ^ 2) / f_big\n\n[1] 0.1267275\n\n## upper bound\n(s1 ^ 2 / s2 ^ 2) / f_small\n\n[1] 2.054079\n\n\n\n## Testing sigma_1 = sigma_2\n(test_stats <- s1 ^ 2 / s2 ^ 2)\n\n[1] 0.5102041\n\n(cri_val_big <- qf(p = alpha/2, \n                   df1 = n1 - 1, \n                   df2 = n2 - 1, \n                   lower.tail = FALSE))\n\n[1] 4.025994\n\n(cri_val_small <- qf(p = alpha/2, \n                     df1 = n1 - 1, \n                     df2 = n2 - 1, \n                     lower.tail = TRUE))\n\n[1] 0.2483859\n\n# var.test(x, y, alternative = \"two.sided\")"
  },
  {
    "objectID": "model-anova.html#comparing-more-than-two-population-means",
    "href": "model-anova.html#comparing-more-than-two-population-means",
    "title": "23¬† Analysis of Variance",
    "section": "23.1 Comparing More Than Two Population Means",
    "text": "23.1 Comparing More Than Two Population Means\n\nIn many research settings, we‚Äôd like to compare 3 or more population means.\n\n\n\n\n 4 types of devices used to determine the pH of soil samples.   Determine whether there are differences in the mean readings of those 4 devices. \n\n\n Do different treatments (None, Fertilizer, Irrigation, Fertilizer and Irrigation) affect the mean weights of poplar trees?"
  },
  {
    "objectID": "model-anova.html#one-way-analysis-of-variance",
    "href": "model-anova.html#one-way-analysis-of-variance",
    "title": "23¬† Analysis of Variance",
    "section": "23.2 One-Way Analysis of Variance",
    "text": "23.2 One-Way Analysis of Variance\n\nA factor is a property or characteristic (categorical variable) that allows us to distinguish the different populations from one another.\n\nType of devices and treatment of trees are factors.\nOne-way ANOVA examines the effect of a categorical variable on the mean of a numerical variable (response).\nWe use analysis of  variance  to test the equality of 3 or more population  means. ü§î\nThe method is one-way because we use one single property (categorical variable) for categorizing the populations."
  },
  {
    "objectID": "model-anova.html#requirements-of-one-way-anova",
    "href": "model-anova.html#requirements-of-one-way-anova",
    "title": "23¬† Analysis of Variance",
    "section": "23.2 Requirements of One-Way ANOVA",
    "text": "23.2 Requirements of One-Way ANOVA\n\nThe populations of each category are normally distributed.\nThe populations have the same variance \\(\\sigma^2\\) (two sample pooled \\(t\\)-test).\nThe samples are random samples.\nThe samples are independent of each other. (not matched or paired in any way)"
  },
  {
    "objectID": "model-anova.html#rationale-for-anova",
    "href": "model-anova.html#rationale-for-anova",
    "title": "23¬† Analysis of Variance",
    "section": "23.2 Rationale for ANOVA",
    "text": "23.2 Rationale for ANOVA\n\nData 1 and Data 2 have the same group sample means \\(\\bar{y}_1\\), \\(\\bar{y}_2\\) and \\(\\bar{y}_3\\) denoted as red dots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich data you are more confident to say the population means \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not all the same?"
  },
  {
    "objectID": "model-anova.html#variation-between-samples-variation-within-samples",
    "href": "model-anova.html#variation-between-samples-variation-within-samples",
    "title": "23¬† Analysis of Variance",
    "section": "23.2 Variation Between Samples & Variation Within Samples",
    "text": "23.2 Variation Between Samples & Variation Within Samples\n\nData 1: Variability between samples is large in comparison to the variation within samples.\nData 2: Variation between samples is small relatively to the variation within samples.\n\n\n\n\n\n\n\nMore confident to conclude there is a difference in population means when variation between samples is relatively larger than variation within samples."
  },
  {
    "objectID": "model-anova.html#procedure-of-anova",
    "href": "model-anova.html#procedure-of-anova",
    "title": "23¬† Analysis of Variance",
    "section": "23.2 Procedure of ANOVA",
    "text": "23.2 Procedure of ANOVA\n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\\\ &H_1: \\text{Population means are not all equal} \\end{align}\\) \n\n\n\nStatistician Ronald Fisher found a way to define a variable that follows the \\(F\\) distribution: \\[\\frac{\\text{variance between samples}}{\\text{variance within samples}} \\sim F_{df_B,\\, df_W}\\]\nIf variance between samples is larger than variance within samples, i.e., \\(F_{test}\\) is much greater than 1, as Data 1, we reject \\(H_0\\).\n\n\n\n\n\n\n\nKey: Define variance between samples and variance within samples so that the ratio is \\(F\\) distributed."
  },
  {
    "objectID": "model-anova.html#variance-within-samples",
    "href": "model-anova.html#variance-within-samples",
    "title": "23¬† Analysis of Variance",
    "section": "23.3 Variance Within Samples",
    "text": "23.3 Variance Within Samples\n\nBack to two-sample pooled \\(t\\)-test with equal variance \\(\\sigma^2\\). We have \\[s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\\]\n\n\n\n\n\n\n\nHow about general \\(k\\) samples?\n\n\n\n\n\n\n\nANOVA assumes the populations have the same variance \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2 = \\sigma^2\\). \\[\\boxed{s_W^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2}{n_1 + n_2 + \\cdots + n_k - k}}\\] where \\(s_i^2\\), \\(i = 1, \\dots ,k\\), is the sample variance of group \\(i\\).\n\\(s_W^2\\) represents a combined estimate of the common variance \\(\\sigma^2\\). It measures variability of the observations within the \\(k\\) populations."
  },
  {
    "objectID": "model-anova.html#variance-between-samples",
    "href": "model-anova.html#variance-between-samples",
    "title": "23¬† Analysis of Variance",
    "section": "23.3 Variance Between Samples",
    "text": "23.3 Variance Between Samples\n\\[\\boxed{s^2_{B} = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^2}{k-1}}\\]\n\n\\(\\bar{y}_{i\\cdot}\\) is the \\(i\\)-th sample mean.\n\\(\\bar{y}_{\\cdot\\cdot}\\) is the grand sample mean with all data points in all groups combined.\n\\(s^2_{B}\\) is also an estimate of \\(\\sigma^2\\) and measures variability among sample means for the \\(k\\) groups.\nIf \\(H_0\\) is true \\((\\mu_1 = \\cdots = \\mu_k = \\mu)\\), any variation in the sample means is due to chance and randomness, and shouldn‚Äôt be too large.\n\n\\(\\bar{y}_{1\\cdot}, \\cdots, \\bar{y}_{k\\cdot}\\) should be close each other, and they are close to \\(\\bar{y}_{\\cdot \\cdot}\\)."
  },
  {
    "objectID": "model-anova.html#anova-table-sum-of-squares",
    "href": "model-anova.html#anova-table-sum-of-squares",
    "title": "23¬† Analysis of Variance",
    "section": "23.3 ANOVA Table: Sum of Squares",
    "text": "23.3 ANOVA Table: Sum of Squares\n\nTotal Sum of Squares (SST) measures total variation around \\(\\bar{y}_{\\cdot\\cdot}\\) in all of the sample data combined (ignoring the groups): \\[\\scriptsize{\\color{blue}{SST = \\sum_{j=1}^{n_i}\\sum_{i=1}^{k} \\left(y_{ij} - \\bar{y}_{\\cdot\\cdot}\\right)^2}}\\] where \\(y_{ij}\\) is the \\(j\\)-th data point in the \\(i\\)-th group.\nSum of Squares Between Samples (SSB) measures the variation between sample means: \\[\\scriptsize{ \\color{blue}{SSB = \\sum_{i=1}^{k}n_i \\left(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}\\right)^2}}\\]\nSum of Squares Within Samples (SSW) measures the variation of an value \\(y_{ij}\\) about its sample mean \\(\\bar{y}_{i\\cdot}\\): \\[\\scriptsize{ \\color{blue}{SSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\left(y_{ij} - \\bar{y}_{i\\cdot}\\right)^2 = \\sum_{i=1}^{k} (n_i - 1)s_i^2}}\\]"
  },
  {
    "objectID": "model-anova.html#sum-of-squares-identity",
    "href": "model-anova.html#sum-of-squares-identity",
    "title": "23¬† Analysis of Variance",
    "section": "23.3 Sum of Squares Identity",
    "text": "23.3 Sum of Squares Identity\n\n\n\n\\(SST = SSB + SSW\\)\n\\(df_{T} = df_{B} + df_{W} \\implies N - 1 = (k-1) + (N - k)\\) \n\\(\\text{Mean Square (MS)} = \\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\)\n\\(MSB = \\frac{SSB}{k-1} = s^2_{B}\\)\n\\(MSW = \\frac{SSW}{N-k} = s^2_{W}\\)\n\\(F_{test} = \\frac{MSB}{MSW}\\)\nUnder \\(H_0\\), \\(\\frac{S^2_{B}}{S_W^2} \\sim F_{k-1, \\, N-k}\\)\nReject \\(H_0\\) if\n\n\\(F_{test} > F_{\\alpha, \\, k - 1,\\, N-k}\\)\n\\(p\\)-value \\(P(F_{k - 1,\\, N-k} > F_{test}) < \\alpha\\)"
  },
  {
    "objectID": "model-anova.html#anova-table",
    "href": "model-anova.html#anova-table",
    "title": "23¬† Analysis of Variance",
    "section": "23.3 ANOVA Table",
    "text": "23.3 ANOVA Table"
  },
  {
    "objectID": "model-anova.html#example",
    "href": "model-anova.html#example",
    "title": "23¬† Analysis of Variance",
    "section": "23.3 Example",
    "text": "23.3 Example\n\nA hypothesis is that a nutrient ‚ÄúIsoflavones‚Äù varies among three types of food: (1) cereals and snacks, (2) energy bars, and (3) veggie burgers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sample of 5 each is taken and the amount of isoflavones is measured.\nIs there a sufficient evidence to conclude that the mean isoflavone levels vary among these food items? \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "model-anova.html#example---data",
    "href": "model-anova.html#example---data",
    "title": "23¬† Analysis of Variance",
    "section": "23.4 Example - Data",
    "text": "23.4 Example - Data\n\n\n\n\n\n\nWe prefer a data format like the one shown on the right.\n\n\n\ndata\n\n   1  2  3\n1  3 19 25\n2 17 10 15\n3 12  9 12\n4 10  7  9\n5  4  5  8\n\n\n\ndata_anova\n\n    y    food\n1   3 cereals\n2  17 cereals\n3  12 cereals\n4  10 cereals\n5   4 cereals\n6  19  energy\n7  10  energy\n8   9  energy\n9   7  energy\n10  5  energy\n11 25  veggie\n12 15  veggie\n13 12  veggie\n14  9  veggie\n15  8  veggie\n\n\n\n\n\n\n\n\n\n\nSo tell me what is the value of \\(y_{23}\\)!"
  },
  {
    "objectID": "model-anova.html#example---boxplot",
    "href": "model-anova.html#example---boxplot",
    "title": "23¬† Analysis of Variance",
    "section": "23.4 Example - Boxplot",
    "text": "23.4 Example - Boxplot"
  },
  {
    "objectID": "model-anova.html#example---test-assumptions",
    "href": "model-anova.html#example---test-assumptions",
    "title": "23¬† Analysis of Variance",
    "section": "23.4 Example - Test Assumptions",
    "text": "23.4 Example - Test Assumptions\n\nAssumptions:\n\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3\\) (I tested it)\nData are generated from a normal distribution for each type of food."
  },
  {
    "objectID": "model-anova.html#example---anova-testing",
    "href": "model-anova.html#example---anova-testing",
    "title": "23¬† Analysis of Variance",
    "section": "23.4 Example - ANOVA Testing",
    "text": "23.4 Example - ANOVA Testing\n\n\n\n\n\n \\(\\begin{align}&H_0: \\mu_1 = \\mu_2 = \\mu_3\\\\&H_1: \\mu_is \\text{ not all equal} \\end{align}\\) \n\n\nüòé Do all calculations and generate an ANOVA table using just one line of code! ü§ü ‚úåÔ∏è"
  },
  {
    "objectID": "model-anova.html#example---anova-table",
    "href": "model-anova.html#example---anova-table",
    "title": "23¬† Analysis of Variance",
    "section": "23.4 Example - ANOVA Table",
    "text": "23.4 Example - ANOVA Table\n\nanova(lm(y ~ food, data = data_anova))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value Pr(>F)\nfood       2   60.4   30.20   0.828   0.46\nResiduals 12  437.6   36.47"
  },
  {
    "objectID": "model-reg.html#relationship-between-2-numerical-variables",
    "href": "model-reg.html#relationship-between-2-numerical-variables",
    "title": "24¬† Linear Regression",
    "section": "24.1 Relationship Between 2 Numerical Variables",
    "text": "24.1 Relationship Between 2 Numerical Variables\n\nDepending on the situation, one of the variables is the explanatory variable and the other is the response variable. (Discussed in Regression)\nThere is not always an explanatory-response relationship.\nExamples:\n\n height and weight \n income and age \n SAT/ACT math score and verbal score \n amount of time spent studying for an exam and exam grade \n\n\n\n\n\n\n\n\nCan you provide an example that 2 variables are associated?"
  },
  {
    "objectID": "model-reg.html#scatterplots",
    "href": "model-reg.html#scatterplots",
    "title": "24¬† Linear Regression",
    "section": "24.2 Scatterplots",
    "text": "24.2 Scatterplots\n\n\n\n\n\n\n\n\n\n\nDescribe the overall pattern\n\nForm: linear or clusters\nDirection: positively associated or negatively associated\nStrength: how close the points lie to a line/curve"
  },
  {
    "objectID": "model-reg.html#linear-correlation-coefficient",
    "href": "model-reg.html#linear-correlation-coefficient",
    "title": "24¬† Linear Regression",
    "section": "24.2 Linear Correlation Coefficient",
    "text": "24.2 Linear Correlation Coefficient\n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right) = \\frac{\\sum_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\overline{x})^2\\sum_{i=1}^n(y_i-\\overline{y})^2}}\\]\n\n\n\n\n\n\\(-1 \\le r\\le 1\\)\n\\(r > 0\\): The larger value of \\(X\\) is, the larger value of \\(Y\\) tends toward.\n\\(r = 1\\): Perfect positive linear relationship.\n\\(r < 0\\): The larger value of \\(X\\) is, the smaller value of \\(Y\\) tends toward.\n\\(r = -1\\): Perfect negative linear relationship\n\\(r = 0\\): No linear relationship.\nIf explanatory and response are switched, \\(r\\) remains the same.\n\\(r\\) has no units of measurement, so scale changes do not affect \\(r\\)."
  },
  {
    "objectID": "model-reg.html#correlation-example",
    "href": "model-reg.html#correlation-example",
    "title": "24¬† Linear Regression",
    "section": "24.2 Correlation Example",
    "text": "24.2 Correlation Example\n\nIt is possible that there is a strong relationship between two variables and still have \\(r = 0\\).\n\n\n\n\n\n\nhttps://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg"
  },
  {
    "objectID": "model-reg.html#example-in-r",
    "href": "model-reg.html#example-in-r",
    "title": "24¬† Linear Regression",
    "section": "24.2 Example in R",
    "text": "24.2 Example in R\n\n\n\nplot(x = mtcars$wt, y = mtcars$mpg, \n     main = \"MPG vs. Weight\", \n     xlab = \"Car Weight\", \n     ylab = \"Miles Per Gallon\", \n     pch = 16, col = 4, las = 1)\n\n\ncor(x = mtcars$wt,\n    y = mtcars$mpg)\n\n[1] -0.8676594"
  },
  {
    "objectID": "model-reg.html#group-1",
    "href": "model-reg.html#group-1",
    "title": "24¬† Linear Regression",
    "section": "24.6 Group 1",
    "text": "24.6 Group 1\nplot(x = mtcars$wt, y = mtcars$mpg, \n     main = \"MPG vs. Weight\", \n     xlab = \"Car Weight\", \n     ylab = \"Miles Per Gallon\", \n     pch = 16, col = 4, las = 1)"
  },
  {
    "objectID": "model-reg.html#group-2",
    "href": "model-reg.html#group-2",
    "title": "24¬† Linear Regression",
    "section": "24.7 Group 2",
    "text": "24.7 Group 2\ncor(x = mtcars$wt,\n    y = mtcars$mpg)\n\n[1] -0.8676594"
  },
  {
    "objectID": "model-reg.html#what-is-regression",
    "href": "model-reg.html#what-is-regression",
    "title": "24¬† Linear Regression",
    "section": "24.2 What is Regression",
    "text": "24.2 What is Regression\n\nRegression models the relationship between one or more numerical/categorical response variables \\((Y)\\) and one or more numerical/categorical explanatory variables \\((X)\\).\nA regression function \\(f(X)\\) describes how a response variable \\(Y\\), on average, changes as an explanatory variable \\(X\\) changes.\n\n\n\n\n\nExamples:\n\n college GPA \\((Y)\\) vs.¬†ACT/SAT score \\((X)\\)\n sales \\((Y)\\) vs.¬†advertising expenditure \\((X)\\)\n crime rate \\((Y)\\) vs.¬†median income level \\((X)\\)"
  },
  {
    "objectID": "model-reg.html#unknown-regression-function",
    "href": "model-reg.html#unknown-regression-function",
    "title": "24¬† Linear Regression",
    "section": "24.3 Unknown Regression Function",
    "text": "24.3 Unknown Regression Function\n\nThe true relationship between \\(X\\) and the mean of \\(Y\\), the regression function \\(f(X)\\), is unknown.\nThe collected data \\((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\) is all we know and have.\n\n\n\n\n\n\n\n\n\n\n\nGoal: estimate \\(f(X)\\) from our data and use it to predict value of \\(Y\\) given a value of \\(X\\)."
  },
  {
    "objectID": "model-reg.html#simple-linear-regression",
    "href": "model-reg.html#simple-linear-regression",
    "title": "24¬† Linear Regression",
    "section": "24.3 Simple Linear Regression",
    "text": "24.3 Simple Linear Regression\n\nStart with simple linear regression:\n\nOnly one predictor \\(X\\) (known and constant) and one response variable \\(Y\\)\nthe regression function used for predicting \\(Y\\) is a linear function.\nuse a regression line in a X-Y plane to predict the value of \\(Y\\) for a given value of \\(X = x\\).\n\n\n\n\n\n\n\n\nMath review: A linear function \\(y = f(x) = \\beta_0 + \\beta_1 x\\) represents a straight line\n\n\\(\\beta_1\\): slope, the amount by which \\(y\\) changes when \\(x\\) increases by one unit.\n\\(\\beta_0\\): intercept, the value of \\(y\\) when \\(x = 0\\).\nThe linearity assumption: \\(\\beta_1\\) does not change as \\(x\\) changes."
  },
  {
    "objectID": "model-reg.html#sample-data-relationship-between-x-and-y",
    "href": "model-reg.html#sample-data-relationship-between-x-and-y",
    "title": "24¬† Linear Regression",
    "section": "24.3 Sample Data: Relationship Between X and Y",
    "text": "24.3 Sample Data: Relationship Between X and Y\n\nReal data \\((x_i, y_i), i = 1, 2, \\dots, n\\) do not form a perfect straight line!\n\\(y_i = \\beta_0+\\beta_1x_i + \\color{red}{\\epsilon_i}\\)\nWhen we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed being drawn from a normal distribution (for inference purpose).\nIts value varies around and will not be exactly equal to its mean \\(\\mu_y\\).\n\n\n\n\n\n\n\n\n\n\n\nThe mean of \\(Y\\) and \\(X\\) form a straight line."
  },
  {
    "objectID": "model-reg.html#simple-linear-regression-model-population",
    "href": "model-reg.html#simple-linear-regression-model-population",
    "title": "24¬† Linear Regression",
    "section": "24.3 Simple Linear Regression Model (Population)",
    "text": "24.3 Simple Linear Regression Model (Population)\nFor the \\(i\\)-th measurement in the target population, \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\n\n\\(Y_i\\): the \\(i\\)-th value of the response (random) variable.\n\\(X_i\\): the \\(i\\)-th known fixed value of the predictor.\n\\(\\epsilon_i\\): the \\(i\\)-th random error with assumption \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients.\n\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are fixed unknown parameters to be estimated from the sample data once we collect them."
  },
  {
    "objectID": "model-reg.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i",
    "href": "model-reg.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i",
    "title": "24¬† Linear Regression",
    "section": "24.3 Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)",
    "text": "24.3 Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\] For any fixed value of \\(X_i = x_i\\), the response \\(Y_i\\) varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nJob: Collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!"
  },
  {
    "objectID": "model-reg.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-1",
    "href": "model-reg.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-1",
    "title": "24¬† Linear Regression",
    "section": "24.4 Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)",
    "text": "24.4 Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\] For any fixed value of \\(X_i = x_i\\), the response \\(Y_i\\) varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nJob: Collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!"
  },
  {
    "objectID": "model-reg.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-2",
    "href": "model-reg.html#important-features-of-model-y_i-beta_0-beta_1x_i-epsilon_i-2",
    "title": "24¬† Linear Regression",
    "section": "24.5 Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)",
    "text": "24.5 Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\] For any fixed value of \\(X_i = x_i\\), the response \\(Y_i\\) varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nJob: Collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!"
  },
  {
    "objectID": "model-reg.html#fitting-a-regression-line-haty-b_0-b_1x",
    "href": "model-reg.html#fitting-a-regression-line-haty-b_0-b_1x",
    "title": "24¬† Linear Regression",
    "section": "24.3 Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)",
    "text": "24.3 Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n Idea of Fitting \n\nGiven the sample data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\)\n\nWhich sample regression line is the best?\nWhat are the best estimators, \\(b_0\\) and \\(b_1\\), for \\(\\beta_0\\) and \\(\\beta_1\\)?\n\n\n\n\n\n\n\nFigure¬†24.15: One data set can have multiple fitted regression lines\n\n\n\n\n\nWe are interested in \\(\\beta_0\\) and \\(\\beta_1\\) in the following sample regression model: \\[\\begin{align*}\ny_i = \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\\] or \\[E({y}_{i}) = \\mu_{y|x_i} = \\beta_0 + \\beta_1~x_{i}\\]\nWe use the sample statistics \\(b_0\\) and \\(b_1\\), which are computed from our sample data, to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\hat{y}_{i} = b_0 + b_1~x_{i}\\) is called the fitted value of \\(y_i\\) and is a point estimate of the mean, \\(\\mu_{y|x_i}\\), and \\(y_i\\) itself.\n\n\n Ordinary Least Squares (OLS) \n\nWhat does best mean?\n\nWe want to choose \\(b_0\\) and \\(b_1\\) or the sample regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals \\(SS_{res}\\).\n\nThe residual, \\(e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)\\), is a point estimate of \\(\\epsilon_i\\).\nThe sample regression line minimizes \\(SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2\\). \\[\\small{\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}}\\]\n\n Visualizing Residuals \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Least Squares Estimates (LSE) \n\nIn the least squares approach, we choose the \\(b_0\\) and \\(b_1\\) that minimize the \\(SS_{res}\\). \\[(b_0, b_1) = \\arg \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\]\nMATH 1450 ‚Ä¶\n\n\\[\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\]\n\\[\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = \\frac{S_{xy}}{S_{xx}} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},\\] where \\(S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2\\), \\(S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\), \\(S_{xy} = \\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})\\)\n\n\n\n\n\n\nWhat can we learn from the formula of \\(b_0\\) and \\(b_1\\)?\n\n\n\n\n\n\n\n Estimation for \\(\\sigma^2\\) \n\nWe can think of \\(\\sigma^2\\) as variance around the line or the mean square (prediction) error.\nThe estimate of \\(\\sigma^2\\) is the mean square residual, \\(MS_{res}\\): \\[\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}\\]\n\\(MS_{res}\\) is often shown in computer output as \\(\\texttt{MS(Error)}\\) or \\(\\texttt{MS(Residual)}\\).\n\\(E(MS_{res}) = \\sigma^2\\)\n\nTherefore, \\(\\hat{\\sigma}^2\\) is an unbiased estimator for \\(\\sigma^2\\) üëç."
  },
  {
    "objectID": "model-reg.html#idea-of-fitting",
    "href": "model-reg.html#idea-of-fitting",
    "title": "24¬† Linear Regression",
    "section": "24.3 Idea of Fitting",
    "text": "24.3 Idea of Fitting\n\nInterested in \\(\\beta_0\\) and \\(\\beta_1\\) in the following sample regression model: \\[\\begin{align*}\ny_i = \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\\] or \\[E({y}_{i}) = \\mu_{y|x_i} = \\beta_0 + \\beta_1~x_{i}\\]\nUse sample statistics \\(b_0\\) and \\(b_1\\) computed from our sample data to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\hat{y}_{i} = b_0 + b_1~x_{i}\\) is called fitted value of \\(y_i\\), a point estimate of the mean \\(\\mu_{y|x_i}\\) and \\(y_i\\) itself."
  },
  {
    "objectID": "model-reg.html#what-does-best-mean-ordinary-least-squares-ols",
    "href": "model-reg.html#what-does-best-mean-ordinary-least-squares-ols",
    "title": "24¬† Linear Regression",
    "section": "24.3 What does ‚Äúbest‚Äù mean? Ordinary Least Squares (OLS)",
    "text": "24.3 What does ‚Äúbest‚Äù mean? Ordinary Least Squares (OLS)\n\nChoose \\(b_0\\) and \\(b_1\\), or sample regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals \\(SS_{res}\\).\nThe residual \\(e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)\\) is a point estimate of \\(\\epsilon_i\\).\nThe sample regression line minimizes \\(SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2\\). \\[\\small{\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}}\\]"
  },
  {
    "objectID": "model-reg.html#visualizing-residuals",
    "href": "model-reg.html#visualizing-residuals",
    "title": "24¬† Linear Regression",
    "section": "24.4 Visualizing Residuals",
    "text": "24.4 Visualizing Residuals"
  },
  {
    "objectID": "model-reg.html#visualizing-residuals-cont.",
    "href": "model-reg.html#visualizing-residuals-cont.",
    "title": "24¬† Linear Regression",
    "section": "24.4 Visualizing Residuals (cont.)",
    "text": "24.4 Visualizing Residuals (cont.)"
  },
  {
    "objectID": "model-reg.html#visualizing-residuals-cont.-1",
    "href": "model-reg.html#visualizing-residuals-cont.-1",
    "title": "24¬† Linear Regression",
    "section": "24.5 Visualizing Residuals (cont.)",
    "text": "24.5 Visualizing Residuals (cont.)"
  },
  {
    "objectID": "model-reg.html#least-squares-estimates-lse",
    "href": "model-reg.html#least-squares-estimates-lse",
    "title": "24¬† Linear Regression",
    "section": "24.4 Least Squares Estimates (LSE)",
    "text": "24.4 Least Squares Estimates (LSE)\n\nThe least squares approach choose \\(b_0\\) and \\(b_1\\) to minimize the \\(SS_{res}\\), i.e., \\[(b_0, b_1) = \\arg \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\]\n\nMATH 1450 ‚Ä¶\n\\[\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\]\n\\[\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = \\frac{S_{xy}}{S_{xx}} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},\\] where \\(S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2\\), \\(S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\), \\(S_{xy} = \\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})\\)\n\n\n\n\n\n\nWhat can we learn from the formula of \\(b_0\\) and \\(b_1\\)?"
  },
  {
    "objectID": "model-reg.html#pinkr-lab-mpg-data",
    "href": "model-reg.html#pinkr-lab-mpg-data",
    "title": "24¬† Linear Regression",
    "section": "24.21 .pink[R Lab:] mpg Data",
    "text": "24.21 .pink[R Lab:] mpg Data\n\nlibrary(ggplot2)  ## use data mpg in ggplot2 package\nmpg\n\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ä¶ with 224 more rows"
  },
  {
    "objectID": "model-reg.html#pinkr-lab-highway-mpg-hwy-vs.-displacement-displ",
    "href": "model-reg.html#pinkr-lab-highway-mpg-hwy-vs.-displacement-displ",
    "title": "24¬† Linear Regression",
    "section": "24.22 .pink[R Lab:] Highway MPG hwy vs.¬†Displacement displ",
    "text": "24.22 .pink[R Lab:] Highway MPG hwy vs.¬†Displacement displ\n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")"
  },
  {
    "objectID": "model-reg.html#pinkr-lab-highway-mpg-hwy-vs.-displacement-displ-1",
    "href": "model-reg.html#pinkr-lab-highway-mpg-hwy-vs.-displacement-displ-1",
    "title": "24¬† Linear Regression",
    "section": "24.23 .pink[R Lab:] Highway MPG hwy vs.¬†Displacement displ",
    "text": "24.23 .pink[R Lab:] Highway MPG hwy vs.¬†Displacement displ\n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")"
  },
  {
    "objectID": "model-reg.html#pinkr-lab-fit-simple-linear-regression",
    "href": "model-reg.html#pinkr-lab-fit-simple-linear-regression",
    "title": "24¬† Linear Regression",
    "section": "24.24 .pink[R Lab:] Fit Simple Linear Regression",
    "text": "24.24 .pink[R Lab:] Fit Simple Linear Regression\n\n\n\nreg_fit <- lm(formula = hwy ~ displ, \n              data = mpg)\nreg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n     35.698       -3.531  \n\ntypeof(reg_fit)\n\n[1] \"list\"\n\n\n\n## all elements in reg_fit\nnames(reg_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n## use $ to extract an element of a list\nreg_fit$coefficients\n\n(Intercept)       displ \n  35.697651   -3.530589 \n\n\n\n\n‚Äì\n\n\\(\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i} = 35.7 - 3.5 \\times displ_{i}\\)\n\\(b_1\\): For one unit (litre) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.5."
  },
  {
    "objectID": "model-reg.html#r-lab-mpg-data",
    "href": "model-reg.html#r-lab-mpg-data",
    "title": "24¬† Linear Regression",
    "section": "24.4 R Lab: mpg Data",
    "text": "24.4 R Lab: mpg Data\n\nlibrary(ggplot2)  ## use data mpg in ggplot2 package\nmpg\n\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ä¶ with 224 more rows"
  },
  {
    "objectID": "model-reg.html#r-lab-highway-mpg-hwy-vs.-displacement-displ",
    "href": "model-reg.html#r-lab-highway-mpg-hwy-vs.-displacement-displ",
    "title": "24¬† Linear Regression",
    "section": "24.5 R Lab: Highway MPG hwy vs.¬†Displacement displ",
    "text": "24.5 R Lab: Highway MPG hwy vs.¬†Displacement displ\n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")"
  },
  {
    "objectID": "model-reg.html#r-lab-highway-mpg-hwy-vs.-displacement-displ-1",
    "href": "model-reg.html#r-lab-highway-mpg-hwy-vs.-displacement-displ-1",
    "title": "24¬† Linear Regression",
    "section": "24.23 R Lab: Highway MPG hwy vs.¬†Displacement displ",
    "text": "24.23 R Lab: Highway MPG hwy vs.¬†Displacement displ\n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")"
  },
  {
    "objectID": "model-reg.html#r-lab-fit-simple-linear-regression",
    "href": "model-reg.html#r-lab-fit-simple-linear-regression",
    "title": "24¬† Linear Regression",
    "section": "24.6 R Lab: Fit Simple Linear Regression",
    "text": "24.6 R Lab: Fit Simple Linear Regression\n\n\n\nreg_fit <- lm(formula = hwy ~ displ, \n              data = mpg)\nreg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n     35.698       -3.531  \n\ntypeof(reg_fit)\n\n[1] \"list\"\n\n\n\n## all elements in reg_fit\nnames(reg_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n## use $ to extract an element of a list\nreg_fit$coefficients\n\n(Intercept)       displ \n  35.697651   -3.530589 \n\n\n\n\n\n\\(\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i} = 35.7 - 3.5 \\times displ_{i}\\)\n\\(b_1\\): For one unit (litre) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.5."
  },
  {
    "objectID": "model-reg.html#r-lab-fitted-values-of-y",
    "href": "model-reg.html#r-lab-fitted-values-of-y",
    "title": "24¬† Linear Regression",
    "section": "24.5 R Lab: Fitted Values of \\(y\\)",
    "text": "24.5 R Lab: Fitted Values of \\(y\\)\n\n## the first 5 observed response value y\nmpg$hwy[1:5]\n\n[1] 29 29 31 30 26\n\n## the first 5 fitted value y_hat\nhead(reg_fit$fitted.values, 5)\n\n       1        2        3        4        5 \n29.34259 29.34259 28.63647 28.63647 25.81200 \n\n## the first 5 predictor value x\nmpg$displ[1:5]\n\n[1] 1.8 1.8 2.0 2.0 2.8\n\nlength(reg_fit$fitted.values)\n\n[1] 234"
  },
  {
    "objectID": "model-reg.html#r-lab-add-a-regression-line",
    "href": "model-reg.html#r-lab-add-a-regression-line",
    "title": "24¬† Linear Regression",
    "section": "24.5 R Lab: Add a Regression Line",
    "text": "24.5 R Lab: Add a Regression Line\n\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\nabline(reg_fit, col = \"#FFCC00\", lwd = 3)"
  },
  {
    "objectID": "model-reg.html#estimation-for-sigma2",
    "href": "model-reg.html#estimation-for-sigma2",
    "title": "24¬† Linear Regression",
    "section": "24.5 Estimation for \\(\\sigma^2\\)",
    "text": "24.5 Estimation for \\(\\sigma^2\\)\n\nWe can think of \\(\\sigma^2\\) as variance around the line or the mean square (prediction) error.\nThe estimate of \\(\\sigma^2\\) is the mean square residual \\(MS_{res}\\): \\[\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}\\]\n\\(MS_{res}\\) is often shown in computer output as \\(\\texttt{MS(Error)}\\) or \\(\\texttt{MS(Residual)}\\).\n\\(E(MS_{res}) = \\sigma^2\\), i.e., \\(\\hat{\\sigma}^2\\) is an unbiased estimator for \\(\\sigma^2\\). üëç"
  },
  {
    "objectID": "model-reg.html#r-lab-standard-error-of-regression",
    "href": "model-reg.html#r-lab-standard-error-of-regression",
    "title": "24¬† Linear Regression",
    "section": "24.5 R Lab: Standard Error of Regression",
    "text": "24.5 R Lab: Standard Error of Regression\n\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n\n[1] 3.835985\n\n# from reg_fit\nsqrt(sum(reg_fit$residuals^2) / reg_fit$df.residual)\n\n[1] 3.835985"
  },
  {
    "objectID": "model-reg.html#r-lab-standard-error-of-regression-1",
    "href": "model-reg.html#r-lab-standard-error-of-regression-1",
    "title": "24¬† Linear Regression",
    "section": "24.6 R Lab: Standard Error of Regression",
    "text": "24.6 R Lab: Standard Error of Regression\n\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n\n[1] 3.835985\n\n# from reg_fit\nsqrt(sum(reg_fit$residuals^2) / reg_fit$df.residual)\n\n[1] 3.835985"
  },
  {
    "objectID": "model-reg.html#confidence-interval-for-beta_0-and-beta_1",
    "href": "model-reg.html#confidence-interval-for-beta_0-and-beta_1",
    "title": "24¬† Linear Regression",
    "section": "24.5 Confidence Interval for \\(\\beta_0\\) and \\(\\beta_1\\)",
    "text": "24.5 Confidence Interval for \\(\\beta_0\\) and \\(\\beta_1\\)\n\n\\(\\frac{b_1 - \\beta_1}{\\sqrt{\\hat{\\sigma}^2/S_{xx}}} \\sim t_{n-2}\\); \\(\\quad \\frac{b_0 - \\beta_0}{\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}\\)\n\\((1-\\alpha)100\\%\\) CI for \\(\\beta_1\\) is \\(b_1 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2/S_{xx}}\\)\n\\((1-\\alpha)100\\%\\) CI for \\(\\beta_0\\) is \\(b_0 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}\\)\n\n\nconfint(reg_fit, level = 0.95)\n\n                2.5 %   97.5 %\n(Intercept) 34.278353 37.11695\ndispl       -3.913828 -3.14735"
  },
  {
    "objectID": "model-reg.html#hypothesis-testing-beta_1",
    "href": "model-reg.html#hypothesis-testing-beta_1",
    "title": "24¬† Linear Regression",
    "section": "24.6 Hypothesis Testing: \\(\\beta_1\\)",
    "text": "24.6 Hypothesis Testing: \\(\\beta_1\\)\n\n \\(H_0: \\beta_1 = \\beta_1^0 \\quad H_1: \\beta_1 \\ne \\beta_1^0\\)  \nTest statistic: Under \\(H_0\\), \\[t_{test} = \\frac{b_1 - \\color{red}{\\beta_1^0}}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\\]\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{p-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\)"
  },
  {
    "objectID": "model-reg.html#hypothesis-testing-beta_0",
    "href": "model-reg.html#hypothesis-testing-beta_0",
    "title": "24¬† Linear Regression",
    "section": "24.6 Hypothesis Testing: \\(\\beta_0\\)",
    "text": "24.6 Hypothesis Testing: \\(\\beta_0\\)\n\n \\(H_0: \\beta_0 = \\beta_0^0 \\quad H_1: \\beta_0 \\ne \\beta_0^0\\)  \nTest statistic: Under \\(H_0\\), \\[t_{test} = \\frac{b_0 - \\color{red}{\\beta_0^0}}{\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}\\]\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{p-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\)"
  },
  {
    "objectID": "model-reg.html#interpretation-of-testing-results",
    "href": "model-reg.html#interpretation-of-testing-results",
    "title": "24¬† Linear Regression",
    "section": "24.6 Interpretation of Testing Results",
    "text": "24.6 Interpretation of Testing Results\n\n \\(H_0: \\beta_1 = 0 \\quad H_1: \\beta_1 \\ne 0\\) \nFailing to reject \\(H_0: \\beta_1 = 0\\) implies there is no linear relationship between \\(Y\\) and \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf we reject \\(H_0: \\beta_1 = 0\\), does it mean \\(X\\) and \\(Y\\) are linearly related?"
  },
  {
    "objectID": "model-reg.html#r-lab-testing-on-beta_0-and-beta_1",
    "href": "model-reg.html#r-lab-testing-on-beta_0-and-beta_1",
    "title": "24¬† Linear Regression",
    "section": "24.6 R Lab: Testing on \\(\\beta_0\\) and \\(\\beta_1\\)",
    "text": "24.6 R Lab: Testing on \\(\\beta_0\\) and \\(\\beta_1\\)\n\nsumm_reg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\n\nsumm_reg_fit$coefficients\n\n             Estimate Std. Error   t value      Pr(>|t|)\n(Intercept) 35.697651  0.7203676  49.55477 2.123519e-125\ndispl       -3.530589  0.1945137 -18.15085  2.038974e-46\n\n\n\nTesting \\(H_0: \\beta_0 = 0\\) and \\(H_0: \\beta_1 = 0\\)"
  },
  {
    "objectID": "model-reg.html#test-of-significance-of-regression",
    "href": "model-reg.html#test-of-significance-of-regression",
    "title": "24¬† Linear Regression",
    "section": "24.6 Test of Significance of Regression",
    "text": "24.6 Test of Significance of Regression\n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate\nbetter results could be obtained with a more complicated model"
  },
  {
    "objectID": "model-reg.html#x---y-relationship-explains-some-deviation",
    "href": "model-reg.html#x---y-relationship-explains-some-deviation",
    "title": "24¬† Linear Regression",
    "section": "24.6 \\(X\\) - \\(Y\\) Relationship Explains Some Deviation",
    "text": "24.6 \\(X\\) - \\(Y\\) Relationship Explains Some Deviation\n\n\n\n\n\n\nSuppose we only have data \\(Y\\) and have no information about \\(X\\) and relationship between \\(X\\) and \\(Y\\). How do we predict a value of \\(Y\\)?\n\n\n\n\n\n\n\nOur best guess would be \\(\\overline{y}\\) if the data have no pattern, i.e., \\(\\hat{y}_i = \\overline{y}\\).\nTreat \\(X\\) and \\(Y\\) as uncorrelated.\nThe (total) deviation from the mean is \\((y_i - \\overline{y})\\)\nIf \\(X\\) and \\(Y\\) are linearly related, fitting a linear regression model helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided.\n\\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\).\nThe regression model explains some deviation of \\(y\\)."
  },
  {
    "objectID": "model-reg.html#partition-of-deviation",
    "href": "model-reg.html#partition-of-deviation",
    "title": "24¬† Linear Regression",
    "section": "24.7 Partition of Deviation",
    "text": "24.7 Partition of Deviation\n\nTotal deviation = Deviation explained by regression + Unexplained deviation\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)\n\\((19 - 9) = (13 - 9) + (19 - 13)\\)"
  },
  {
    "objectID": "model-reg.html#sum-of-squares-ss",
    "href": "model-reg.html#sum-of-squares-ss",
    "title": "24¬† Linear Regression",
    "section": "24.7 Sum of Squares (SS)",
    "text": "24.7 Sum of Squares (SS)\n\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\)\nTotal SS \\((SS_T)\\) = Regression SS \\((SS_R)\\) + Residual SS \\((SS_{res})\\)\n\\(df_T = df_R + df_{res}\\)\n\\(\\color{blue}{(n-1) = 1 +(n-2)}\\)"
  },
  {
    "objectID": "model-reg.html#anova-for-testing-significance-of-regression",
    "href": "model-reg.html#anova-for-testing-significance-of-regression",
    "title": "24¬† Linear Regression",
    "section": "24.7 ANOVA for Testing Significance of Regression",
    "text": "24.7 ANOVA for Testing Significance of Regression\n\n\n\n\n\n\n\n\n\n\nA larger value of \\(F_{test}\\) indicates that regression is significant.\nReject \\(H_0\\) if\n\n\\(F_{test} > F_{\\alpha, 1, n-2}\\)\n\\(\\text{p-value} = P(F_{1, n-2} > F_{test}) < \\alpha\\).\n\nThe ANOVA is designed to test \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, the \\(F\\)-test of ANOVA gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\)."
  },
  {
    "objectID": "model-reg.html#r-lab-anova-table",
    "href": "model-reg.html#r-lab-anova-table",
    "title": "24¬† Linear Regression",
    "section": "24.7 R Lab: ANOVA Table",
    "text": "24.7 R Lab: ANOVA Table\n\nFor \\(H_0: \\beta_1 = 0\\) in SLR, \\(t_{test}^2 = F_{test}\\).\n\n\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: hwy\n           Df Sum Sq Mean Sq F value    Pr(>F)    \ndispl       1 4847.8  4847.8  329.45 < 2.2e-16 ***\nResiduals 232 3413.8    14.7                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsumm_reg_fit$coefficients\n\n             Estimate Std. Error   t value      Pr(>|t|)\n(Intercept) 35.697651  0.7203676  49.55477 2.123519e-125\ndispl       -3.530589  0.1945137 -18.15085  2.038974e-46\n\nsumm_reg_fit$coefficients[2, 3] ^ 2\n\n[1] 329.4533"
  },
  {
    "objectID": "model-reg.html#coefficient-of-determination",
    "href": "model-reg.html#coefficient-of-determination",
    "title": "24¬† Linear Regression",
    "section": "24.7 Coefficient of Determination",
    "text": "24.7 Coefficient of Determination\n\nThe coefficient of determination \\((R^2)\\) is the proportion of the variation in \\(y\\) that is explained by the regression model. It is computed as \\[R^2 = \\frac{SS_R}{SS_T} =\\frac{SS_T - SS_{res}}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\]\n\\(R^2\\) as the proportionate reduction of total variation associated with the use of \\(X\\).\n(a) \\(\\hat{y}_i = y_i\\) and \\(\\small SS_{res} = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = 0\\). (b) \\(\\hat{y}_i = \\overline{y}\\) and \\(\\small SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 = 0\\)."
  },
  {
    "objectID": "model-reg.html#r-lab-r2",
    "href": "model-reg.html#r-lab-r2",
    "title": "24¬† Linear Regression",
    "section": "24.7 R Lab: \\(R^2\\)",
    "text": "24.7 R Lab: \\(R^2\\)\n\nsumm_reg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\n\nsumm_reg_fit$r.squared\n\n[1] 0.5867867"
  },
  {
    "objectID": "model-reg.html#predicting-the-mean-response",
    "href": "model-reg.html#predicting-the-mean-response",
    "title": "24¬† Linear Regression",
    "section": "24.7 Predicting the Mean Response",
    "text": "24.7 Predicting the Mean Response\n\nWith predictor value \\(x = x_0\\), we want to estimate the mean response \\(E(y\\mid x_0) = \\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\).\n\n The mean highway MPG \\(E(y \\mid x_0)\\) when displacement is \\(x = x_0 = 5.5\\). \n\nIf \\(x_0\\) is within the range of \\(x\\), an unbiased point estimator for \\(E(y\\mid x_0)\\) is \\[\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y\\mid x_0)\\) is \\(\\boxed{\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\).\n\n\n\n\n\n\n\nDoes the length of the CI for \\(E(y\\mid x_0)\\) stay the same at any location of \\(x_0\\)?"
  },
  {
    "objectID": "model-reg.html#predicting-new-observations",
    "href": "model-reg.html#predicting-new-observations",
    "title": "24¬† Linear Regression",
    "section": "24.8 Predicting New Observations",
    "text": "24.8 Predicting New Observations\n\nPredict the value of a new observation \\(y_0\\) with \\(x = x_0\\).\n\n The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\nAn unbiased point estimator for \\(y_0(x_0)\\) is \\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]\nThe \\((1-\\alpha)100\\%\\) prediction interval (PI) for \\(y_0(x_0)\\) is \\(\\small \\boxed{\\hat{y_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{1+ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\)\n\n\n\n\n\n\n\nWhat is the difference between CI for \\(E(y\\mid x_0)\\) and PI for \\(y_0(x_0)\\)?\n\n\n\n\n\n\n\nThe PI is wider as it includes the uncertainty about \\(b_0\\), \\(b_1\\) as well as \\(y_0\\) due to error \\(\\epsilon\\)."
  },
  {
    "objectID": "model-reg.html#r-lab-prediction",
    "href": "model-reg.html#r-lab-prediction",
    "title": "24¬† Linear Regression",
    "section": "24.8 R Lab: Prediction",
    "text": "24.8 R Lab: Prediction\n\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 16.27941 15.35839 17.20043\n\n## PI for the new observation\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"predict\", level = 0.95)\n\n       fit      lwr      upr\n1 16.27941 8.665682 23.89314"
  },
  {
    "objectID": "infer-prop.html#one-categorical-variable-with-two-categories",
    "href": "infer-prop.html#one-categorical-variable-with-two-categories",
    "title": "18¬† Inference About Proportions",
    "section": "18.1 One Categorical Variable with Two Categories",
    "text": "18.1 One Categorical Variable with Two Categories\n\nLet \\(X\\) be a categorical variable Gender having 2 categories Male and Female.\n\n\n\n\n\n\n\nSubject\nMale\nFemale\n\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne-way frequency/count table\n\n\n\nThe number of males can be viewed as a random variable because the count \\(y\\) varies from sample to sample.\n\n\n\n\n\n\n\n\n\\(X\\)\nCount\n\n\n\n\nMale\n\\(y\\)\n\n\nFemale\n\\(n-y\\)\n\n\n\n\n\n\n\n\n\n\nWhat probability distribution might be appropriate for the count \\(Y\\)?"
  },
  {
    "objectID": "infer-prop.html#probability-distribution-for-count-data-two-levels",
    "href": "infer-prop.html#probability-distribution-for-count-data-two-levels",
    "title": "18¬† Inference About Proportions",
    "section": "18.2 Probability Distribution for Count Data: Two Levels",
    "text": "18.2 Probability Distribution for Count Data: Two Levels\n\n\\(binomial(n, \\pi)\\) could be a good option for the count data with 2 categories.\n\nFixed number of trials.  (Fixed \\(n\\) subjects) \nEach trial results in one of two outcomes.  (Either \\(M\\) or \\(F\\)) \nTrials are independent.  (If sample subjects randomly) \n\nIf the proportion of being in category \\(M\\) is \\(\\pi\\), the number of \\(M\\), or the count \\(Y\\) has \\[P(Y = y \\mid n, \\pi) = \\frac{n!}{y!(n-y)!}\\pi^{y}(1-\\pi)^{n-y}\\]\nGoal: estimate or test the population proportion of category \\(M\\), or \\(\\pi\\)."
  },
  {
    "objectID": "infer-prop.html#inference-about-a-single-population-proportion",
    "href": "infer-prop.html#inference-about-a-single-population-proportion",
    "title": "18¬† Inference About Proportions",
    "section": "18.2 Inference About a Single Population Proportion",
    "text": "18.2 Inference About a Single Population Proportion\n\n\nExample: Exit Poll Suppose we collected data on 1,000 voters in an election with only two candidates: R and D.\n\n\n\n\n\n\n\nVoter\nR\nD\n\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the data, we want to predict who won the election."
  },
  {
    "objectID": "infer-prop.html#poll-example-contd",
    "href": "infer-prop.html#poll-example-contd",
    "title": "18¬† Inference About Proportions",
    "section": "18.3 Poll Example Cont‚Äôd",
    "text": "18.3 Poll Example Cont‚Äôd\n\nLet \\(Y\\) be the number of voters voted for R.\nAssume the count \\(Y\\) is sampled from \\(binomial(n = 1000, \\pi)\\).\n\\(\\pi = P(\\text{a voter voted for R}) =\\) (population) proportion of all voters voted for R: A unknown parameter to be estimated or tested.\nPredict whether or not R won the election.\n\n\n\n\n\n\n\nWhat are \\(H_0\\) and \\(H_1\\)?\n\n\n\n\n\n\n\n \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi > 1/2 \\text{ (more than half voted for R)} \\end{align}\\)"
  },
  {
    "objectID": "infer-prop.html#hypothesis-testing-for-pi-exact-binom-test",
    "href": "infer-prop.html#hypothesis-testing-for-pi-exact-binom-test",
    "title": "18¬† Inference About Proportions",
    "section": "18.3 Hypothesis Testing for \\(\\pi\\) (Exact Binom Test)",
    "text": "18.3 Hypothesis Testing for \\(\\pi\\) (Exact Binom Test)\n\nStep 0: Method Assumptions\n\n \\(n\\pi_0 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) (The larger, the better) \n\nStep 1: Set the Null and Alternative Hypothesis\n\n \\(\\begin{align} &H_0: \\pi = \\pi_0 \\\\ &H_1: \\pi > \\pi_0 \\text{ or } \\pi < \\pi_0 \\text{ or } \\pi \\ne \\pi_0 \\end{align}\\) \n\nStep 2: Set the Significance Level \\(\\alpha\\)\n\nStep 3: Calculate the Test Statistic\n\n Under \\(H_0\\), \\(z_{test} = \\dfrac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\) where \\(\\hat{\\pi} = \\frac{y}{n} =\\) sample proportion \n\n\n\n\n\n\n\n\nThe sampling distribution of \\(\\hat{\\pi}\\) is approximately normal with mean \\(\\pi\\) and standard error \\(\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\) if \\(y_i\\) are independent and the assumptions are satisfied.\n\n\n\n\n\n\n\nStep 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed)\nStep 5-c: Draw a Conclusion Using Critical Value Method \n\n \\(H_1: \\pi > \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z > z_{\\alpha}\\) \n \\(H_1: \\pi < \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z < -z_{\\alpha}\\) \n \\(H_1: \\pi \\ne \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| > z_{\\alpha/2}\\) \n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim"
  },
  {
    "objectID": "infer-prop.html#poll-example-contd-hypothesis-testing",
    "href": "infer-prop.html#poll-example-contd-hypothesis-testing",
    "title": "18¬† Inference About Proportions",
    "section": "18.3 Poll Example Cont‚Äôd (Hypothesis Testing)",
    "text": "18.3 Poll Example Cont‚Äôd (Hypothesis Testing)\n\nIn an exit poll of 1,000 voters, 520 voted for R, one of the two candidates.\nStep 0: \\(n\\pi_0 = 1000(1/2) = 500 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) \nStep 1:  \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi > 1/2 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(z_{test} = \\frac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} = \\frac{\\frac{520}{1000} - 0.5}{\\sqrt{\\frac{0.5(1-0.5)}{1000}}} = 1.26\\) \nStep 4:  \\(z_{\\alpha} = z_{0.05} = 1.645\\) \nStep 5:  Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} > z_{\\alpha}\\). Since \\(z_{test} < z_{\\alpha}\\), we do not reject \\(H_0\\). \nStep 6:  We do not have sufficient evidence to conclude that R won. \nWe make the same conclusion based on \\(p\\)-value.\n\n\\[ p\\text{-value} = P(Z > 1.26) = 0.1 > 0.05\\]"
  },
  {
    "objectID": "infer-prop.html#confidence-interval-for-pi",
    "href": "infer-prop.html#confidence-interval-for-pi",
    "title": "18¬† Inference About Proportions",
    "section": "18.3 Confidence Interval for \\(\\pi\\)",
    "text": "18.3 Confidence Interval for \\(\\pi\\)\n\nAssumptions: \\(n\\hat{\\pi} \\ge 5\\) and \\(n(1-\\hat{\\pi}) \\ge 5\\)\nA \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi\\) is \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] where \\(\\hat{\\pi} = y/n\\).\n\\(\\pi\\) is unknown and we use the estimate \\(\\hat{\\pi}\\) instead: \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}\\]\n\n\n\n\n\n\n\nüëâ No hypothesized value \\(\\pi_0\\) is involved in the confidence interval."
  },
  {
    "objectID": "infer-prop.html#poll-example-contd-confidence-interval",
    "href": "infer-prop.html#poll-example-contd-confidence-interval",
    "title": "18¬† Inference About Proportions",
    "section": "18.3 Poll Example Cont‚Äôd (Confidence Interval)",
    "text": "18.3 Poll Example Cont‚Äôd (Confidence Interval)\n\nAssumption: \\(n\\hat{\\pi} = 1000(0.52) = 520 \\ge 5\\) and \\(n(1-\\hat{\\pi}) = 480 \\ge 5\\).\nEstimate the proportion of all voters voted for R using 95% confidence interval:\n\n\\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}} = 0.52 \\pm z_{0.025}\\sqrt{\\frac{0.52(1-0.52)}{1000}} = (0.49, 0.55).\\]\n\n# Use alternative = \"two.sided\" to get CI\n# binom.test()\nprop.test(x = 520, n = 1000, p = 0.5, alternative = \"greater\", correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  520 out of 1000, null probability 0.5\nX-squared = 1.6, df = 1, p-value = 0.103\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.4939945 1.0000000\nsample estimates:\n   p \n0.52"
  },
  {
    "objectID": "infer-prop.html#inference-about-two-population-proportions",
    "href": "infer-prop.html#inference-about-two-population-proportions",
    "title": "18¬† Inference About Proportions",
    "section": "18.3 Inference About Two Population Proportions",
    "text": "18.3 Inference About Two Population Proportions\n\n\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\n\\(n_1\\) trials\n\\(n_2\\) trials\n\n\n\\(Y_1\\) number of successes\n\\(Y_2\\) number of successes\n\n\n\\(Y_1 \\sim binomial(n_1, \\pi_1)\\)\n\\(Y_2 \\sim binomial(n_2, \\pi_2)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\pi_1\\): Population proportion of success of Group 1\n\\(\\pi_2\\): Population proportion of success of Group 2\n\n\n\n\n\n\n\n\nIs male‚Äôs president approval rate \\(\\pi_1\\) higher than the female‚Äôs approval rate \\(\\pi_2\\)?"
  },
  {
    "objectID": "infer-prop.html#hypothesis-testing-for-pi_1-and-pi_2",
    "href": "infer-prop.html#hypothesis-testing-for-pi_1-and-pi_2",
    "title": "18¬† Inference About Proportions",
    "section": "18.4 Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)",
    "text": "18.4 Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\)\n\nStep 0: Check Method Assumptions\n\n \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\) \n\nStep 1: Set the Null and Alternative Hypothesis\n\n \\(\\begin{align} &H_0: \\pi_1 = \\pi_2 \\\\ &H_1: \\pi_1 > \\pi_2 \\text{ or } \\pi_1 < \\pi_2 \\text{ or } \\pi_1 \\ne \\pi_2 \\end{align}\\) \n\nStep 2: Set the Significance Level \\(\\alpha\\)\nStep 3: Calculate the Test Statistic\n\n \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})}\\), \\(\\bar{\\pi} = \\frac{y_1+y_2}{n_1+n_2}\\) is the pooled sample proportion estimating \\(\\pi\\) \n\nStep 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed)\nStep 5-c: Draw a Conclusion Using Critical Value Method\n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \n\n \\(H_1: \\pi_1 > \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z > z_{\\alpha}\\) \n \\(H_1: \\pi_1 < \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z < -z_{\\alpha}\\) \n \\(H_1: \\pi_1 \\ne \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| > z_{\\alpha/2}\\) \n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim"
  },
  {
    "objectID": "infer-prop.html#example-effectiveness-of-learning",
    "href": "infer-prop.html#example-effectiveness-of-learning",
    "title": "18¬† Inference About Proportions",
    "section": "18.4 Example: Effectiveness of Learning",
    "text": "18.4 Example: Effectiveness of Learning\n\n\n\n\nA study on 300 students to compare the effectiveness of learning Statistics via online and in-person classes.\nRandomly assign\n\n125 students to the online program\nthe remaining 175 to the in-person program.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam Results\nOnline Instruction\nIn-Person Instruction\n\n\n\n\nPass\n94\n113\n\n\nFail\n31\n62\n\n\nTotal\n125\n175\n\n\n\n\n\n\n\n\n\nIs there sufficient evidence to conclude that the online program is more effective than the traditional in-person program at \\(\\alpha=0.05\\)?\n\n\n\n\n\n\n\nStep 1:  \\(H_0: \\pi_1 = \\pi_2\\) vs.¬†\\(H_1: \\pi_1 > \\pi_2\\) \n\\(\\pi_1\\) \\((\\pi_2)\\) is the population proportion of students passing the exam under the online (in-person) program.\nStep 0: \\(\\hat{\\pi}_1 = 94/125 = 0.75\\) and \\(\\hat{\\pi}_2 = 113/175 = 0.65\\).\n\\(n_1\\hat{\\pi}_1 = 94 > 5\\), \\(n_1(1-\\hat{\\pi}_1) = 31 > 5\\), and \\(n_2\\hat{\\pi}_2 = 113 > 5\\), \\(n_2(1-\\hat{\\pi}_2) = 62 > 5\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(\\bar{\\pi} = \\frac{94+113}{125+175} = 0.69\\). \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})} = \\frac{0.75 - 0.65}{\\sqrt{0.69(1-0.69)(\\frac{1}{125} + \\frac{1}{175})}} = 1.96\\) \nStep 4:  \\(z_{\\alpha} = z_{0.05} = 1.645\\) \nStep 5:  Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} > z_{\\alpha}\\). Since \\(z_{test} > z_{\\alpha}\\), we reject \\(H_0\\). \nStep 6:  We have sufficient evidence to conclude that the online program is more effective."
  },
  {
    "objectID": "infer-prop.html#confidence-interval-for-pi_1---pi_2",
    "href": "infer-prop.html#confidence-interval-for-pi_1---pi_2",
    "title": "18¬† Inference About Proportions",
    "section": "18.5 Confidence Interval for \\(\\pi_1 - \\pi_2\\)",
    "text": "18.5 Confidence Interval for \\(\\pi_1 - \\pi_2\\)\n\nA \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi_1 - \\pi_2\\) is \\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\nRequirements: \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\)"
  },
  {
    "objectID": "infer-prop.html#example-effectiveness-of-learning-contd-ci",
    "href": "infer-prop.html#example-effectiveness-of-learning-contd-ci",
    "title": "18¬† Inference About Proportions",
    "section": "18.4 Example: Effectiveness of Learning Cont‚Äôd (CI)",
    "text": "18.4 Example: Effectiveness of Learning Cont‚Äôd (CI)\n\nWant to know how much effective is the online program.\nEstimate \\(\\pi_1 - \\pi_2\\) using a \\(95\\%\\) confidence interval:\n\n\\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\n\n\\(z_{0.05/2} = 1.96\\)\nThe 95% confidence interval is \\[0.75 - 0.65 \\pm 1.96\\sqrt{\\frac{(0.75)(1-0.75)}{125} + \\frac{(0.65)(1-0.65)}{175}}\\\\\n= (0.002, 0.210)\\]"
  },
  {
    "objectID": "infer-prop.html#implementation-in-r",
    "href": "infer-prop.html#implementation-in-r",
    "title": "18¬† Inference About Proportions",
    "section": "18.4 Implementation in R",
    "text": "18.4 Implementation in R\n\n# Use alternative = \"two.sided\" to get CI\nprop.test(x = c(94, 113), n = c(125, 175), alternative = \"greater\", correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(94, 113) out of c(125, 175)\nX-squared = 3.8509, df = 1, p-value = 0.02486\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01926052 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.7520000 0.6457143 \n\nprop_ci <- prop.test(x = c(94, 113), n = c(125, 175), alternative = \"two.sided\", correct = FALSE)\nprop_ci$conf.int\n\n[1] 0.002588801 0.209982628\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "infer-goodnessfit.html#categorical-variable-with-more-than-2-categories",
    "href": "infer-goodnessfit.html#categorical-variable-with-more-than-2-categories",
    "title": "19¬† Inference about Categorical Data",
    "section": "19.1 Categorical Variable with More Than 2 Categories",
    "text": "19.1 Categorical Variable with More Than 2 Categories\n\n\n\nA categorical variable has \\(k\\) categories \\(A_1, \\dots, A_k\\).\n\n\n\n\nSubject\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(A_k\\)\n\n\n\n\n1\nx\n\n\n\n\n\n\n2\n\nx\n\n\n\n\n\n3\n\n\n\n\nx\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\\(n\\)\n\n\n\nx\n\n\n\n\n\n\n\n\nWith the size \\(n\\), for categories \\(A_1, \\dots , A_k\\), their observed count is \\(O_1, \\dots, O_k\\), and \\(\\sum_{i=1}^kO_i = n\\).\nOne-way count table:\n\n\n\n\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(A_k\\)\nTotal\n\n\n\n\n\\(O_1\\)\n\\(O_2\\)\n\\(\\cdots\\)\n\\(O_k\\)\n\\(n\\)\n\n\n\n\n\n\n Example \n\n\nAre the selected jurors are racially representative of the population?\n\nIdea: If the jury is representative of the population, the proportions in the sample should reflect the population of eligible jurors, i.e., registered voters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nRepresentation in juries\n205\n26\n25\n19\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nRepresentation in juries\n0.745\n0.095\n0.091\n0.069\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\n\n\nAre the proportions of juries close enough to the proportions of registered voters, so that we are confident saying that the jurors really were randomly sampled from the registered voters?"
  },
  {
    "objectID": "infer-goodnessfit.html#example-more-than-2-categories",
    "href": "infer-goodnessfit.html#example-more-than-2-categories",
    "title": "19¬† Test of Goodness of Fit",
    "section": "19.2 Example: More Than 2 Categories",
    "text": "19.2 Example: More Than 2 Categories\n\n\n\n\nAre the selected jurors are racially representative of the population?\n\nIdea: If the jury is representative of the population, the proportions in the sample should reflect the population of eligible jurors, i.e., registered voters.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nRepresentation in juries\n205\n26\n25\n19\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nRepresentation in juries\n0.745\n0.095\n0.091\n0.069\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\n\n\nAre the proportions of juries close enough to the proportions of registered voters, so that we are confident saying that the jurors really were randomly sampled from the registered voters?"
  },
  {
    "objectID": "infer-goodnessfit.html#goodness-of-fit-test",
    "href": "infer-goodnessfit.html#goodness-of-fit-test",
    "title": "19¬† Inference about Categorical Data",
    "section": "19.2 Goodness-of-Fit Test",
    "text": "19.2 Goodness-of-Fit Test\n\nA goodness-of-fit test tests the hypothesis that the observed frequency distribution fits or conforms to some claim distribution.\n\n\n\n\n\n\n\nIn the jury example, what is our observed frequency distribution, and what is our claim distribution?\n\n\n\n\n\n\n\n\n\n\n\n\nIf the individuals are randomly selected to serve on a jury, about how many of the 275 people would we expect to be white? How about black?\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\nAbout \\(72\\%\\) of the population is white, so we would expect about \\(72\\%\\) of the jurors to be white: \\(0.72 \\times 275 = 198\\).\nWe expect about \\(7\\%\\) of the jurors to be black, which corresponds to about \\(0.07 \\times 275 = 19.25\\) black jurors.\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n205\n26\n25\n19\n\n\nExpected Count\n198\n19.25\n33\n24.75\n\n\nPopulation Proportion \\((H_0)\\)\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\nObserved Count and Expected Count are similar if no bias on juries.\nTest whether the differences are strong enough to provide convincing evidence that the jurors are not a random sample of registered voters.\n\n\n Example \n\n \\(\\begin{align} &H_0: \\text{No racial bias in who serves on a jury, and } \\\\ &H_1: \\text{There is racial bias in juror selection} \\end{align}\\) \n \\(\\begin{align} &H_0: \\pi_1 = \\pi_1^0, \\pi_2 = \\pi_2^0, \\dots, \\pi_k = \\pi_k^0\\\\ &H_1: \\pi_i \\ne \\pi_i^0 \\text{ for some } i \\end{align}\\) \n Under \\(H_0\\), \\(\\chi^2_{test} = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} + \\cdots + \\frac{(O_k - E_k)^2}{E_k}\\), \\(E_i = n\\pi_i^0, i = 1, \\dots, k\\) \nReject \\(H_0\\) if  \\(\\chi^2_{test} > \\chi^2_{\\alpha, df}\\), \\(df = k-1\\) \nRequire each \\(E_i \\ge 5\\), \\(i = 1, \\dots, k\\).\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\n\nProportion under \\(H_0\\)\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\n\n\n\nUnder \\(H_0\\), \\(\\chi^2_{test} = \\frac{(205 - 198)^2}{198} + \\frac{(26 - 19.25)^2}{19.25} + \\frac{(25 - 33)^2}{33} + \\frac{(19 - 24.75)^2}{24.75} = 5.89\\)\n\\(\\chi^2_{0.05, 3} = 7.81\\).\nDo not reject \\(H_0\\) in favor of \\(H_1\\).\nThe data do not provide convincing evidence of racial bias in the juror selection.\n\n\n Goodness-of-Fit Test in R \n\nobs <- c(205, 26, 25, 19)\npi_0 <- c(0.72, 0.07, 0.12, 0.09)\n\n## Use chisq.test() function\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 5.8896, df = 3, p-value = 0.1171"
  },
  {
    "objectID": "infer-goodnessfit.html#goodness-of-fit-test-statistic",
    "href": "infer-goodnessfit.html#goodness-of-fit-test-statistic",
    "title": "19¬† Test of Goodness of Fit",
    "section": "19.3 Goodness-of-Fit Test Statistic",
    "text": "19.3 Goodness-of-Fit Test Statistic\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\n\nProportion under \\(H_0\\)\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\n\n\n\n Under \\(H_0\\), \\(\\chi^2_{test} = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} + \\cdots + \\frac{(O_k - E_k)^2}{E_k}\\), \\(E_i = n\\pi_i^0, i = 1, \\dots, k\\) \nReject \\(H_0\\) if  \\(\\chi^2_{test} > \\chi^2_{\\alpha, df}\\), \\(df = k-1\\) \nRequire each \\(E_i \\ge 5\\), \\(i = 1, \\dots, k\\).\nUnder \\(H_0\\), \\(\\chi^2_{test} = \\frac{(205 - 198)^2}{198} + \\frac{(26 - 19.25)^2}{19.25} + \\frac{(25 - 33)^2}{33} + \\frac{(19 - 24.75)^2}{24.75} = 5.89\\)\n\\(\\chi^2_{0.05, 3} = 7.81\\).\nDo not reject \\(H_0\\) in favor of \\(H_1\\).\nThe data do not provide convincing evidence of racial bias in the juror selection."
  },
  {
    "objectID": "infer-goodnessfit.html#goodness-of-fit-test-in-r",
    "href": "infer-goodnessfit.html#goodness-of-fit-test-in-r",
    "title": "19¬† Test of Goodness of Fit",
    "section": "19.3 Goodness-of-Fit Test in R",
    "text": "19.3 Goodness-of-Fit Test in R\n\nobs <- c(205, 26, 25, 19)\npi_0 <- c(0.72, 0.07, 0.12, 0.09)\n\n## Use chisq.test() function\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 5.8896, df = 3, p-value = 0.1171"
  },
  {
    "objectID": "infer-indep.html#test-of-independence-contingency-table",
    "href": "infer-indep.html#test-of-independence-contingency-table",
    "title": "20¬† Test of Independence",
    "section": "20.1 Test of Independence (Contingency Table)",
    "text": "20.1 Test of Independence (Contingency Table)\n\nHave TWO categorical variables, and want to test whether or not the two variables are independent.\n\n Does the ‚ÄúOpinion on President‚Äôs Job Performance‚Äù depend on ‚ÄúGender‚Äù? \n\n\n\nJob performance: approve, disapprove, no opinion\nGender: male, female\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18\n22\n10\n\n\nFemale\n23\n20\n12"
  },
  {
    "objectID": "infer-indep.html#section",
    "href": "infer-indep.html#section",
    "title": "20¬† Test of Independence",
    "section": "20.2 ",
    "text": "20.2"
  },
  {
    "objectID": "infer-indep.html#test-of-independence-expected-count",
    "href": "infer-indep.html#test-of-independence-expected-count",
    "title": "20¬† Test of Independence",
    "section": "20.2 Test of Independence: Expected Count",
    "text": "20.2 Test of Independence: Expected Count\n\nCompute the expected count of each cell in the two-way table under the condition that the two variables were independent with each other.\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\nThe expected count for the \\(i\\)th row and \\(j\\)th column: \\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\]"
  },
  {
    "objectID": "infer-indep.html#test-of-independence-procedure",
    "href": "infer-indep.html#test-of-independence-procedure",
    "title": "20¬† Test of Independence",
    "section": "20.2 Test of Independence Procedure",
    "text": "20.2 Test of Independence Procedure\n\nRequire every \\(E_{ij} \\ge 5\\) in the contingency table.\n \\(\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\)\n\\(\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the table.\nReject \\(H_0\\) if \\(\\chi^2_{test} > \\chi^2_{\\alpha, \\, df}\\), \\(df = (r-1)(c-1)\\).\n\n\n Example \n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\n \\(\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\)\n\\(\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\)\n\\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\).\nSince \\(\\chi_{test}^2 < \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\).\nWe do not conclude that the Opinion on President‚Äôs Job Performance depends on Gender.\n\n\n Test of Independence in R \n\n(contingency_table <- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(indept_test <- chisq.test(contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.65019, df = 2, p-value = 0.7225\n\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  ## critical value\n\n[1] 5.991465"
  },
  {
    "objectID": "infer-indep.html#test-of-independence-example",
    "href": "infer-indep.html#test-of-independence-example",
    "title": "20¬† Test of Independence",
    "section": "20.3 Test of Independence Example",
    "text": "20.3 Test of Independence Example\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\n \\(\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\)\n\\(\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\)\n\\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\).\nSince \\(\\chi_{test}^2 < \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\).\nWe do not conclude that the Opinion on President‚Äôs Job Performance depends on Gender."
  },
  {
    "objectID": "infer-indep.html#test-of-independence-in-r",
    "href": "infer-indep.html#test-of-independence-in-r",
    "title": "20¬† Test of Independence",
    "section": "20.3 Test of Independence in R",
    "text": "20.3 Test of Independence in R\n\n(contingency_table <- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(indept_test <- chisq.test(contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.65019, df = 2, p-value = 0.7225\n\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  ## critical value\n\n[1] 5.991465"
  },
  {
    "objectID": "model-logistic.html#regression-vs.-classification",
    "href": "model-logistic.html#regression-vs.-classification",
    "title": "25¬† Logistic Regression",
    "section": "25.1 Regression vs.¬†Classification",
    "text": "25.1 Regression vs.¬†Classification\n\nLinear regression assumes that the response \\(Y\\) is numerical.\nIn many situations, \\(Y\\) is categorical.\n\n\n\nNormal vs.¬†COVID vs.¬†Smoker‚Äôs Lungs\n\n\n\n\n\n\n\n\n\n\n\n\nFake vs.¬†Fact\n\n\n\n\n\n\n\n\n\n\n\n\nThe process of predicting a categorical response is known as classification.\n\n\n Regression Function \\(f(x)\\) vs.¬†Classifier \\(C(x)\\) \n\n\n\n\n\n\n\n\n\n\nFigure¬†25.1: Difference between classification and regression (https://daviddalpiaz.github.io/r4sl/classification-overview.html)\n\n\n\n\n\n Classification Example \n\nPredict whether people will default on their credit card payment, where \\((Y)\\) is yes or no, based on their monthly credit card balance, \\((X)\\).\nWe use the sample data \\(\\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\) to build a classifier.\n\n\n\n\n\n\n\n\nFigure¬†25.2: Boxplot of Default vs.¬†Balance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Why Not Linear Regression? \n\\[Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}\\]\n\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), \\(\\, X =\\) credit card balance\n\n\n\n\n\n\n\nWhat is the problem with this dummy variable approach?\n\n\n\n\n\n\n\n\\(\\hat{Y} = b_0 + b_1X\\) estimates \\(P(Y = 1 \\mid X) = P(default = yes \\mid balance)\\)\n\n\n\n\n\n\nFigure¬†25.3: Graphical illustration of why a simple linear regression model won‚Äôt work for Default ~ Balance\n\n\n\n\n\nSome estimates might be outside \\([0, 1]\\), which doesn‚Äôt make sense given that \\(Y\\) is a probability.\n\n\n Why Logistic Regression? \n\nWe first predict the probability of each category of \\(Y\\).\nThen, we predict the probability of default using an S-shaped curve.\n\n\n\n\n\n\nFigure¬†25.4: Graphical illustration of why a logistic regression model works better for Default ~ Balance"
  },
  {
    "objectID": "model-logistic.html#regression-function-fx-vs.-classifier-cx",
    "href": "model-logistic.html#regression-function-fx-vs.-classifier-cx",
    "title": "25¬† Logistic Regression",
    "section": "25.2 Regression Function \\(f(x)\\) vs.¬†Classifier \\(C(x)\\)",
    "text": "25.2 Regression Function \\(f(x)\\) vs.¬†Classifier \\(C(x)\\)\n\n\n\n\n\n\n\n\n\n\nhttps://daviddalpiaz.github.io/r4sl/classification-overview.html"
  },
  {
    "objectID": "model-logistic.html#classification-example",
    "href": "model-logistic.html#classification-example",
    "title": "25¬† Logistic Regression",
    "section": "25.2 Classification Example",
    "text": "25.2 Classification Example\n\nPredict whether people will default on their credit card payment \\((Y)\\) yes or no, based on monthly credit card balance \\((X)\\).\nWe use the sample data \\(\\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\) to build a classifier."
  },
  {
    "objectID": "model-logistic.html#why-not-linear-regression",
    "href": "model-logistic.html#why-not-linear-regression",
    "title": "25¬† Logistic Regression",
    "section": "25.2 Why Not Linear Regression?",
    "text": "25.2 Why Not Linear Regression?\n\\[Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}\\]\n\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), \\(\\, X =\\) credit card balance\n\n\n\n\n\n\n\nWhat is the problem with this dummy variable approach?"
  },
  {
    "objectID": "model-logistic.html#why-not-linear-regression-1",
    "href": "model-logistic.html#why-not-linear-regression-1",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Why Not Linear Regression?",
    "text": "25.3 Why Not Linear Regression?\n\n\\(\\hat{Y} = b_0 + b_1X\\) estimates \\(P(Y = 1 \\mid X) = P(default = yes \\mid balance)\\) \n\n\n\n\n\n\n\n\n\n\n\nSome estimates might be outside \\([0, 1]\\).\nFirst predict the probability of each category of \\(Y\\).\nPredict probability of default using a S-shaped curve."
  },
  {
    "objectID": "model-logistic.html#framing-the-problem-binary-responses",
    "href": "model-logistic.html#framing-the-problem-binary-responses",
    "title": "25¬† Logistic Regression",
    "section": "25.2 Framing the Problem: Binary Responses",
    "text": "25.2 Framing the Problem: Binary Responses\n\nTreat each outcome (default \\((y = 1)\\) and not default \\((y = 0)\\)) as success and failure arising from separate Bernoulli trials.\n\n\n\n\n\n\n\nWhat is a Bernoulli trial?\n\n\n\n\n\n\n\nA Bernoulli trial is a special case of a binomial trial when the number of trials is \\(m = 1\\):\n\nexactly two possible outcomes, ‚Äúsuccess‚Äù and ‚Äúfailure‚Äù\nthe probability of success \\(\\pi\\) is constant\n\n\n\n\n\n\n\n\nIn the default credit card example,\n\n\n\n\nDo we have exactly two outcomes?\nDo we have constant probability? \\(P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?\\)"
  },
  {
    "objectID": "model-logistic.html#binary-responses-with-nonconstant-probability",
    "href": "model-logistic.html#binary-responses-with-nonconstant-probability",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Binary Responses with Nonconstant Probability",
    "text": "25.3 Binary Responses with Nonconstant Probability\n\n\n\n\nTwo outcomes: default \\((y = 1)\\) and not default \\((y = 0)\\)\nThe probability of success \\(\\pi\\) changes with the value of predictor \\(X\\)!\nWith a different value of \\(x_i\\), each Bernoulli trial outcome \\(y_i\\) has a different probability of success \\(\\pi_i\\):\n\n\n\n\n\n\n\n\n\n\\[ y_i \\mid x_i \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i)) = binomial(m=1,\\pi = \\pi(x_i)) \\]\n\n\\(X =\\) balance. \\(x_1 = 2000\\) has a larger \\(\\pi_1 = \\pi(2000)\\) than \\(\\pi_2 = \\pi(500)\\) with \\(x_2 = 500\\) because credit cards with a higher balance tend to be default."
  },
  {
    "objectID": "model-logistic.html#logistic-regression",
    "href": "model-logistic.html#logistic-regression",
    "title": "25¬† Logistic Regression",
    "section": "25.2 Logistic Regression",
    "text": "25.2 Logistic Regression\n Binary Responses \n\nTreat each outcome (default \\((y = 1)\\) and not default \\((y = 0)\\)) as success and failure arising from separate Bernoulli trials.\n\n\n\n\n\n\n\nWhat is a Bernoulli trial?\n\n\n\n\n\n\n\nA Bernoulli trial is a special case of a binomial trial when the number of trials is \\(m = 1\\):\n\nexactly two possible outcomes, ‚Äúsuccess‚Äù and ‚Äúfailure‚Äù\nthe probability of success \\(\\pi\\) is constant\n\n\n\n\n\n\n\n\nIn the default credit card example,\n\n\n\n\nDo we have exactly two outcomes?\nDo we have constant probability? \\(P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?\\)\n\n\n\n Nonconstant Probability\n\n\n\n\nTwo outcomes: default \\((y = 1)\\) and not default \\((y = 0)\\)\nThe probability of success \\(\\pi\\) changes with the value of predictor \\(X\\)!\nWith a different value of \\(x_i\\), each Bernoulli trial outcome \\(y_i\\) has a different probability of success \\(\\pi_i\\):\n\n\n\n\n\n\n\n\n\n\\[ y_i \\mid x_i \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i)) = binomial(m=1,\\pi = \\pi(x_i)) \\]\n\n\\(X =\\) balance. \\(x_1 = 2000\\) has a larger \\(\\pi_1 = \\pi(2000)\\) than \\(\\pi_2 = \\pi(500)\\) with \\(x_2 = 500\\) because credit cards with a higher balance tend to be default.\n\n\n Regression \n\nLogistic regression models a binary response \\((Y)\\) using predictors \\(X_1, \\dots, X_k\\).\n\n\\(k = 1\\): simple logistic regression\n\\(k > 1\\): multiple logistic regression\n\n\n\nInstead of predicting \\(y_i\\) directly, we use the predictors to model its probability of success, \\(\\pi_i\\).\n\n\nBut how?\n\n\nTransform \\(\\pi \\in (0, 1)\\) into another variable \\(\\eta \\in (-\\infty, \\infty)\\). Then fit a linear regression on \\(\\eta\\).\nLogit function: For \\(0 < \\pi < 1\\)\n\n\\[\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\]\n Logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) \n\n\n\n\n\n\n\n\n\n Logistic Function \\(\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\) \n\nThe logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\).\nLogistic function: \\[\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]\nThe logistic function takes a value \\(\\eta \\in (-\\infty, \\infty)\\) and maps it to a value \\(\\pi \\in (0, 1)\\).\nSo once \\(\\eta\\) is estimated by the linear regression, we use the logistic function to transform \\(\\eta\\) back to the probability."
  },
  {
    "objectID": "model-logistic.html#logit-function-eta-logitpi-lnleftfracpi1-piright",
    "href": "model-logistic.html#logit-function-eta-logitpi-lnleftfracpi1-piright",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\)",
    "text": "25.3 Logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\)"
  },
  {
    "objectID": "model-logistic.html#logistic-function",
    "href": "model-logistic.html#logistic-function",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Logistic Function",
    "text": "25.3 Logistic Function\n\nThe logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\).\nLogistic function: \\[\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]\nThe logistic function takes a value \\(\\eta \\in (-\\infty, \\infty)\\) and maps it to a value \\(\\pi \\in (0, 1)\\).\nSo once \\(\\eta\\) is estimated by the linear regression, we use the logistic function to transform \\(\\eta\\) back to the probability."
  },
  {
    "objectID": "model-logistic.html#logistic-function-pi-logisticeta-fracexpeta1expeta",
    "href": "model-logistic.html#logistic-function-pi-logisticeta-fracexpeta1expeta",
    "title": "25¬† Logistic Regression",
    "section": "25.4 Logistic Function \\(\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\)",
    "text": "25.4 Logistic Function \\(\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\)"
  },
  {
    "objectID": "model-logistic.html#simple-logistic-regression-model",
    "href": "model-logistic.html#simple-logistic-regression-model",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Simple Logistic Regression Model",
    "text": "25.3 Simple Logistic Regression Model\n\nFor \\(i = 1, \\dots, n\\) and with one predictor \\(X\\): \\[(Y_i \\mid X = x_i) \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i))\\] \\[\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi(x_i)}{1 - \\pi(x_i)} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i}\\]\n\n\\[\\small \\pi_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{i})}{1+\\exp(\\beta_0+\\beta_1 x_{i})} = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}\\]\n\\[\\small \\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i} )}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i})}\\]\n\n Probability Curve \n\n\n\nThe relationship between \\(\\pi(x)\\) and \\(x\\) is not linear! \\[\\pi(x) = \\frac{\\exp(\\beta_0+\\beta_1 x)}{1+\\exp(\\beta_0+\\beta_1 x)}\\]\nThe amount that \\(\\pi(x)\\) changes due to a one-unit change in \\(x\\) depends on the current value of \\(x\\).\nRegardless of the value of \\(x\\), if \\(\\beta_1 > 0\\), increasing \\(x\\) will increase \\(\\pi(x)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Interpretation of Coefficients \n\nThe ratio \\(\\frac{\\pi}{1-\\pi} \\in (0, \\infty)\\) is called the odds of some event.\nExample: If 1 in 5 people will default, the odds is 1/4 since \\(\\pi = 0.2\\) implies an odds of \\(0.2/(1‚àí0.2) = 1/4\\).\n\n\\[\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x\\]\n-Increasing \\(x\\) by one unit changes the log-odds by \\(\\beta_1\\), or it multiplies the odds by \\(e^{\\beta_1}\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\\(\\beta_1\\) does not correspond to the change in \\(\\pi(x)\\) associated with a one-unit increase in \\(x\\).\n\\(\\beta_1\\) is the change in log odds associated with one-unit increase in \\(x\\)."
  },
  {
    "objectID": "model-logistic.html#probability-curve",
    "href": "model-logistic.html#probability-curve",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Probability Curve",
    "text": "25.3 Probability Curve\n\n\n\n\n\n\nWhat is the probability of being male when HEIGHT is 160? What about HEIGHT 180?\n\n\n\n\n\n\n\npredict(logit_fit, newdata = data.frame(HEIGHT = c(160, 170, 180)), type = \"response\")\n\n        1         2         3 \n0.1334399 0.6333105 0.9509103 \n\n\n\n\n\n\n\n\n\n\n\n 160 cm, Pr(male) = 0.13\n 170 cm, Pr(male) = 0.63\n 180 cm, Pr(male) = 0.95"
  },
  {
    "objectID": "model-logistic.html#interpretation-of-coefficients",
    "href": "model-logistic.html#interpretation-of-coefficients",
    "title": "25¬† Logistic Regression",
    "section": "25.4 Interpretation of Coefficients",
    "text": "25.4 Interpretation of Coefficients\nThe ratio \\(\\frac{\\pi}{1-\\pi} \\in (0, \\infty)\\) is called the odds of some event. - Example: If 1 in 5 people will default, the odds is 1/4 since \\(\\pi = 0.2\\) implies an odds of \\(0.2/(1‚àí0.2) = 1/4\\).\n\\[\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x\\]\n\nIncreasing \\(x\\) by one unit changes the log-odds by \\(\\beta_1\\), or it multiplies the odds by \\(e^{\\beta_1}\\).\n\n\n\n\n\n\n\nNote\n\n\n\n\n\\(\\beta_1\\) does not correspond to the change in \\(\\pi(x)\\) associated with a one-unit increase in \\(x\\).\n\\(\\beta_1\\) is the change in log odds associated with one-unit increase in \\(x\\)."
  },
  {
    "objectID": "model-logistic.html#logistic-regression-in-r",
    "href": "model-logistic.html#logistic-regression-in-r",
    "title": "25¬† Logistic Regression",
    "section": "25.4 Logistic Regression in R",
    "text": "25.4 Logistic Regression in R\n\n\n\nGENDER = 1 if male\nGENDER = 0 if female\nUse HEIGHT (centimeter, 1 cm = 0.3937 in) to predict/classify GENDER: whether the person is male or female.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbody <- read.table(\"./data/01 - Body Data.txt\", header = TRUE)\nhead(body)\n\n  AGE GENDER PULSE SYSTOLIC DIASTOLIC HDL LDL WHITE  RED PLATE WEIGHT HEIGHT\n1  43      0    80      100        70  73  68   8.7 4.80   319   98.6  172.0\n2  57      1    84      112        70  35 116   4.9 4.73   187   96.9  186.0\n3  38      0    94      134        94  36 223   6.9 4.47   297  108.2  154.4\n4  80      1    74      126        64  37  83   7.5 4.32   170   73.1  160.5\n5  34      1    50      114        68  50 104   6.1 4.95   140   83.1  179.0\n6  77      1    60      134        60  55  75   5.7 3.95   192   86.5  166.7\n  WAIST ARM_CIRC  BMI\n1 120.4     40.7 33.3\n2 107.8     37.0 28.0\n3 120.3     44.3 45.4\n4  97.2     30.3 28.4\n5  95.1     34.0 25.9\n6 112.0     31.4 31.1\n\n\n\n Data Summary \n\n\n\ntable(body$GENDER)\n\n\n  0   1 \n147 153 \n\nsummary(body[body$GENDER == 1, ]$HEIGHT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  155.0   169.1   173.8   174.1   179.4   193.3 \n\nsummary(body[body$GENDER == 0, ]$HEIGHT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  134.5   156.5   162.2   161.7   166.8   181.4 \n\n\n\n\n\n\nboxplot(body$HEIGHT ~ body$GENDER)\n\n\n\n\n\n\n\n Model Fitting \n\nlogit_fit <- glm(GENDER ~ HEIGHT, data = body, family = \"binomial\")\n(summ_logit_fit <- summary(logit_fit))\n\n\nCall:\nglm(formula = GENDER ~ HEIGHT, family = \"binomial\", data = body)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5839  -0.6660   0.1078   0.6554   2.4998  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -40.54809    4.63084  -8.756   <2e-16 ***\nHEIGHT        0.24173    0.02758   8.764   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 415.77  on 299  degrees of freedom\nResidual deviance: 251.50  on 298  degrees of freedom\nAIC: 255.5\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nsumm_logit_fit$coefficients\n\n               Estimate Std. Error   z value     Pr(>|z|)\n(Intercept) -40.5480864 4.63083742 -8.756102 2.021182e-18\nHEIGHT        0.2417325 0.02758399  8.763507 1.892674e-18\n\n\n\n\\(\\hat{\\eta} = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -40.55 + 0.24 \\times \\text{HEIGHT}\\)\n\\(\\hat{\\eta}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\)\n\\(\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)\\)\n\\(\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x}) = \\ln \\left( \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} \\right)\\)\nA one centimeter increase in HEIGHT increases the log odds of being male by 0.24 units.\nThe odds ratio, \\(\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.24} = 1.273\\).\nThe odds of being male increases by 27.3% with an additional one centimeter of HEIGHT.\n\n\n Prediction \n Pr(GENDER = 1) When HEIGHT is 170 cm \n\n\\[ \\hat{\\pi}(x = 170) = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x)}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x)} = \\frac{\\exp(-40.55+0.24 \\times 170)}{1+\\exp(-40.55+0.24 \\times 170)} = 0.633 = 63.3\\%\\]\n\npi_hat <- predict(logit_fit, type = \"response\")\neta_hat <- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(HEIGHT = 170), type = \"response\")\n\n        1 \n0.6333105 \n\n\n Probability Curve \n\n\n\n\n\n\nWhat is the probability of being male when the HEIGHT is 160 cm? What about when the HEIGHTis 180 cm?\n\n\n\n\n\n\n\npredict(logit_fit, newdata = data.frame(HEIGHT = c(160, 170, 180)), type = \"response\")\n\n        1         2         3 \n0.1334399 0.6333105 0.9509103 \n\n\n\n\n\n\n\n\n\n\n\n 160 cm, Pr(male) = 0.13\n 170 cm, Pr(male) = 0.63\n 180 cm, Pr(male) = 0.95"
  },
  {
    "objectID": "model-logistic.html#sensitivity-and-specificity",
    "href": "model-logistic.html#sensitivity-and-specificity",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Sensitivity and Specificity",
    "text": "25.3 Sensitivity and Specificity\n\n\n\n\n\n\n\n\n\n1\n0\n\n\n\n\nLabeled 1\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nLabeled 0\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nSensitivity (True Positive Rate) \\(= P( \\text{Labeled 1} \\mid \\text{1}) = \\frac{TP}{TP+FN}\\)\nSpecificity (True Negative Rate) \\(= P( \\text{Labeled 0} \\mid \\text{0}) = \\frac{TN}{FP+TN}\\)\nAccuracy \\(= \\frac{TP + TN}{TP+FN+FP+TN}\\) \nMore on Wiki page"
  },
  {
    "objectID": "model-logistic.html#receiver-operating-characteristic-roc-curve",
    "href": "model-logistic.html#receiver-operating-characteristic-roc-curve",
    "title": "25¬† Logistic Regression",
    "section": "25.4 Receiver Operating Characteristic (ROC) Curve",
    "text": "25.4 Receiver Operating Characteristic (ROC) Curve\n\nReceiver operating characteristic (ROC) curve plots True Positive Rate (Sensitivity) vs.¬†False Positive Rate (1 - Specificity)\nR packages for ROC curves: ROCR and pROC, yardstick of Tidymodels"
  },
  {
    "objectID": "model-survival.html#is-the-cluster-of-deaths-significantly-high",
    "href": "model-survival.html#is-the-cluster-of-deaths-significantly-high",
    "title": "27¬† Survival Analysis",
    "section": "\n27.1 Is the Cluster of Deaths Significantly High?",
    "text": "27.1 Is the Cluster of Deaths Significantly High?\n\n\n\n\nA town in Wisconsin has 5000 people who reach their 16th birthday and 25 of them die before their 17th birthday.\n\nMany residents claim that the number is too high and suspect air pollution as a cause.\nOthers suggest that the number of deaths vary from year to year, so it is no cause for concern.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow do we objectively address this issue?\n\n\n\n\n\n\n\n\n\n\n\n\nOne essential piece of information is the death rate for people in this age group, which can be extracted from a life table."
  },
  {
    "objectID": "model-survival.html#life-table",
    "href": "model-survival.html#life-table",
    "title": "27¬† Survival Analysis",
    "section": "27.1 Life Table",
    "text": "27.1 Life Table\n\nA period life table describes mortality and longevity data for a hypothetical cohort.\nThe data is computed with the assumption that the conditions affecting mortality in a particular year remain the same throughout the lives of everyone in the hypothetical cohort.\n\nFor example, a 1-year-old toddler and an elderly 70-year-old live their entire life in a world with the same constant death rates that were present in a given year.\n\nAn example of a life table is shown in Figure¬†27.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†27.1: Life table for total population in the United States in 2018\n\n\n\n\n\nThe entire report can be downloaded at CDC Publications and Information Products.\nMortality experiences are different for various gender and race groups, so it is common to have tables for specific groups.\n\nFor example, Figure¬†27.2 below is a table for females in the United States.\n\n\n\n\n\n\n\nFigure¬†27.2: Life table for females in the United States in 2018\n\n\n\n\n\nThe basis year for the mortality rate in this table is 2018, as is highlighted in Figure¬†27.3.\nThis life table has data for a cohort of 100,000 hypothetical people.\n\n\n\n\n\n\nFigure¬†27.3: The life table lists its basis year and number of hypothetical individuals\n\n\n\n\n\nThe age ranges chosen for this life table include the following classes: \\([0, 1)\\), \\([1, 2)\\), \\([2, 3)\\), ‚Ä¶ \\([99, 100)\\), \\([100, \\infty)\\).\n\n\n\n\n\n\nFigure¬†27.4: The first column lists the age intervals of the individuals\n\n\n\n\n\nThe probabilities of dying during the age interval are listed in the 1st column of the life table.\nFor example, in Figure¬†27.5, there is a 0.000367 probability of someone dying between their 1st birthday and their 2nd birthday.\n\n\n\n\n\n\nFigure¬†27.5: The second column lists the probability of dying between two ages\n\n\n\n\n\nThe number of people alive at the beginning of the age interval is listed in column 2.\nAs Figure¬†27.6 displays, among the 100,000 hypothetical people who were born, 99,435 of them are alive on their 1st birthday.\n\n\n\n\n\n\nFigure¬†27.6: The third column lists the number of individuals alive at the beginning of the age interval\n\n\n\n\n\nThe number of people who died during the age interval is listed in column 3.\n\n\n\n\n\n\n\nHow is this column related to the previous two columns?\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†27.7: The fourth column lists the number of individuals who die during a given age interval\n\n\n\n\n\nThe total number of years lived during the age interval by those who were alive at the beginning of the age interval is listed in the fourth column.\nFor example, the 100,000 people who were present at age 0 lived a total of 99,505 years (Figure¬†27.8).\nIf none of those people had died, this entry would have been 100,000 years.\n\n\n\n\n\n\nFigure¬†27.8: The fifth column lists the total number of person-years lived within a given age interval\n\n\n\n\n\nThe sixth column is similar to the fifth, but lists the total number of years lived during the age interval and all of the following age intervals as well.\n\n\n\n\n\n\nFigure¬†27.9: The fifth column lists the total number of person-years lived above a given age\n\n\n\n\n\nThe final column lists the expected remaining lifetime in years, measured from the beginning of the age interval (Figure¬†27.10).\n\n\n\n\n\n\n\nWhy does the age interval of 1-2 have an expected remaining lifetime of 78.2 years?\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†27.10: The final column lists the expectation of life at a given age\n\n\n\n\n\n Example: Probability of Dying \n\nUse Figure¬†27.1 to find the probability of a person dying between age of 15 and 20.\n\n\\[\\begin{align*} Pr(\\text{die in } [15, 20)) &= Pr([15, 16) \\cup [16, 17) \\cup \\cdots \\cup [19, 20)) \\\\ &= Pr([15, 16) + Pr([16, 17)) + \\cdots + Pr([19, 20)) \\\\ &= 0.000214 + 0.000253 + 0.000292 + 0.000329 + 0.000365 = 0.001453 \\end{align*}\\]\n\\[\\begin{align*} Pr(\\text{surviving between 15th and 20th birthdays}) &= \\frac{\\text{Number of people alive on their 20th birthday}}{\\text{Number of people alive on their 15th birthday}} \\\\ &= \\frac{99,151}{99,296} \\\\ &= 0.99854 \\end{align*}\\]\n\\[Pr(\\text{die in } [15, 20)) = 1-Pr(\\text{survive in } [15, 20)) = 1 - 0.99854 = 0.00146\\]"
  },
  {
    "objectID": "model-survival.html#probability-of-dying",
    "href": "model-survival.html#probability-of-dying",
    "title": "27¬† Survival Analysis",
    "section": "27.2 Probability of Dying",
    "text": "27.2 Probability of Dying\nUse Figure¬†27.1 to\n\nFind the probability of a person dying between age of 15 and 20.\n\n\\[\\begin{align*} Pr(\\text{die in } [15, 20)) &= Pr([15, 16) \\cup [16, 17) \\cup \\cdots \\cup [19, 20)) \\\\ &= Pr([15, 16) + Pr([16, 17)) + \\cdots + Pr([19, 20)) \\\\ &= 0.000214 + 0.000253 + 0.000292 + 0.000329 + 0.000365 = 0.001453 \\end{align*}\\]\n\\[\\begin{align*} Pr(\\text{surviving between 15th and 20th birthdays}) &= \\frac{\\text{Number of people alive on their 20th birthday}}{\\text{Number of people alive on their 15th birthday}} \\\\ &= \\frac{99,151}{99,296} \\\\ &= 0.99854 \\end{align*}\\]\n\\[Pr(\\text{die in } [15, 20)) = 1-Pr(\\text{survive in } [15, 20)) = 1 - 0.99854 = 0.00146\\]"
  },
  {
    "objectID": "model-survival.html#applications-of-life-tables---social-security",
    "href": "model-survival.html#applications-of-life-tables---social-security",
    "title": "27¬† Survival Analysis",
    "section": "27.3 Applications of Life Tables - Social Security",
    "text": "27.3 Applications of Life Tables - Social Security\n\n\n\nThere were 3,600,000 births in the U.S. in 2020.\nIf the age for receiving full Social Security payment is 67, how many of those born in 2020 are expected to be alive on their 67 birthday? Check the report!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmong 100,000 people born, we expect 81,637 of them will survive to their 67th birthday.\nTherefore, we expect that \\(3,600,000 \\times 0.81637 = 2,938,932\\) people born in 2020 will receive their full Social Security payment."
  },
  {
    "objectID": "model-survival.html#applications-of-life-tables---hypothesis-testing",
    "href": "model-survival.html#applications-of-life-tables---hypothesis-testing",
    "title": "27¬† Survival Analysis",
    "section": "27.4 Applications of Life Tables - Hypothesis Testing",
    "text": "27.4 Applications of Life Tables - Hypothesis Testing\n\n\n\n\nFor one city there are 5000 people who reach their 16th birthday.\n25 of them die before their 17th birthday.\nDo we have sufficient evidence to conclude that this number of death is significantly high?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe probability of dying for the age interval of 16-17 is 0.000405.\nThis is a \\(H_1\\) claim.  \\(\\small \\begin{align} &H_0: \\pi = 0.000405 \\\\ &H_1: \\pi > 0.000405\\end{align}\\) \n\\(\\hat{\\pi} = 25/5000 = 0.005\\).\n\\(z = \\frac{0.005 - 0.000405}{\\sqrt{\\frac{(0.000405)(0.999595)}{5000}}} = 16.15\\)\n\\(P\\)-value \\(\\approx 0\\).\nThere is sufficient evidence to conclude that the proportion of deaths is significantly higher than the proportion that is usually expected for this age interval."
  },
  {
    "objectID": "model-survival.html#survival-analysis",
    "href": "model-survival.html#survival-analysis",
    "title": "27¬† Survival Analysis",
    "section": "27.3 Survival Analysis",
    "text": "27.3 Survival Analysis\n\n\n\n\nThe life table method is based on fixed time intervals.\nThe Kaplan-Meier method\n\nis based on intervals that vary according to the times of survival to some particular terminating event.\nis used to describe the probability of surviving for a specific period of time.\n\n What is the probability of surviving for 5 more years after cancer chemotherapy?"
  },
  {
    "objectID": "model-survival.html#survival-time",
    "href": "model-survival.html#survival-time",
    "title": "27¬† Survival Analysis",
    "section": "27.4 Survival Time",
    "text": "27.4 Survival Time\n\nThe time lapse from the beginning of observation to the time of terminating event is considered a survival time."
  },
  {
    "objectID": "model-survival.html#survivor",
    "href": "model-survival.html#survivor",
    "title": "27¬† Survival Analysis",
    "section": "27.4 Survivor",
    "text": "27.4 Survivor\n\nA survivor is a subject that successfully lasted throughout a particular time period.\n\n\n\n\n\n\n\nNote\n\n\n\n\nA survivor does not necessarily mean living.\n\nA patient trying to stop smoking is a survivor if smoking has not resumed.\nYour iPhone that worked for some particular length of time can be considered a survivor."
  },
  {
    "objectID": "model-survival.html#censored-data",
    "href": "model-survival.html#censored-data",
    "title": "27¬† Survival Analysis",
    "section": "27.4 Censored Data",
    "text": "27.4 Censored Data\n\nSurvival times are censored data if the subjects\n\nsurvive past the end of the study\nare dropped from the study for reasons not related to the terminating event being studied.\n\n\n\n\n\n\n\nhttps://unc.live/3K1ph8f"
  },
  {
    "objectID": "model-survival.html#medication-treatment-for-quitting-smoking",
    "href": "model-survival.html#medication-treatment-for-quitting-smoking",
    "title": "27¬† Survival Analysis",
    "section": "\n27.4 Medication Treatment for Quitting Smoking",
    "text": "27.4 Medication Treatment for Quitting Smoking\n\n\n\n\n\n\n\n\n\n\nDay\nStatus (0 = censored, 1 = Smoke Again)\nNumber of Patients\nPatients Not Smoking\nProportion Not Smoking\nCumulative Proportion Not Smoking\n\n\n\n1\n0\n\n\n\n\n\n\n3\n1\n4\n3\n3/4 = 0.75\n0.75\n\n\n4\n1\n3\n2\n2/3 = 0.67\n0.5\n\n\n7\n1\n2\n1\n1/2 = 0.5\n0.25\n\n\n21\n1\n1\n0\n0\n0\n\n\n\n\n\n\n‚ÄúSurviving‚Äù means the patient has NOT resumed smoking.\nThe 1st patient disliked the medication and dropped out of the study on day one.\n2nd row: A patient resumed smoking 3 days after the start of the program.\n3rd row: \\(0.5 = (3/4)(2/3)\\)\n\n4th row: \\(0.25 = (3/4)(2/3)(1/2)\\)\n\n5th row: \\(0 = (3/4)(2/3)(1/2)(0)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDay\nStatus  (0 = censored, 1 = Smoke Again)\nNumber of Patients\nPatients Not Smoking\nProportion Not Smoking\nCumulative Proportion Not Smoking\n\n\n\n2\n1\n10\n9\n9/10\n0.9\n\n\n4\n1\n9\n8\n8/9\n0.8\n\n\n5\n0\n\n\n\n\n\n\n8\n1\n7\n6\n6/7\n0.686\n\n\n9\n1\n6\n5\n5/6\n0.571\n\n\n12\n0\n\n\n\n\n\n\n14\n1\n4\n3\n3/4\n0.429\n\n\n22\n1\n3\n2\n2/3\n0.286\n\n\n24\n0\n\n\n\n\n\n\n28\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy the Cumulative Proportion on Day 8 is 0.686?\n\n\n\n\n\n\n\\[0.686 = (9/10)(8/9)(6/7)\\]\nIdea: On Day 5 a patient dropped out, and we don‚Äôt know whether he resumed smoking on Day 8 or not."
  },
  {
    "objectID": "model-survival.html#kaplan-meier-analysis",
    "href": "model-survival.html#kaplan-meier-analysis",
    "title": "27¬† Survival Analysis",
    "section": "27.4 Kaplan-Meier Analysis",
    "text": "27.4 Kaplan-Meier Analysis\n\n\n\n\n\n\nWhich treatment is better for quitting smoking?"
  },
  {
    "objectID": "intro-stats.html#what-is-statistics",
    "href": "intro-stats.html#what-is-statistics",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.1 What is Statistics?",
    "text": "1.1 What is Statistics?\n\nStatistics can be defined in a variety of ways, and there doesn‚Äôt seem to be one definition that describes it best.\nFor our purposes, statistics can be generally divided into two overarching categories.\n\nStatistics as a set of numeric records\nStatistics as a discipline\n\n\n Statistics as a Set of Numeric Records \n\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records.\nFor example, Figure¬†1.1 below shows Michael Jordan‚Äôs career statistics from his time in the NBA.\n\n\n\n\n\n\nFigure¬†1.1: Example of statistics as a set of numeric records (Source: https://www.nba.com/stats/player/893/career)\n\n\n\n\n\nHowever, this is just one way of defining statistics.\n\n\n Statistics as a Discipline \n\nAs previously stated, other definitions of statistics exist including the one shown in Figure¬†1.2.\n\n\n\n\n\n\nFigure¬†1.2: Statistics Shirt (Source: shorturl.at/vEMNS)\n\n\n\n\n\nThis definition emphasizes the idea that with the same data, different statistical methods may produce different results and lead to different conclusions.\nWiki lists a more formal definition of statistics in Figure¬†1.3 below.\n\n\n\n\n\n\nFigure¬†1.3: More formal definition of statistics (Source:https://en.wikipedia.org/wiki/Statistics)\n\n\n\n\n\nStatistics can also be described as a Science of Data that uses statistical thinking, methods and models.\n\n\nü§î But wait, if statistics is a science of data, then what is DATA SCIENCE‚ùì"
  },
  {
    "objectID": "intro-data.html#data-1",
    "href": "intro-data.html#data-1",
    "title": "2¬† Data",
    "section": "2.2 Data",
    "text": "2.2 Data\n\nWe usually store a data set in a matrix form that has rows and columns.\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns."
  },
  {
    "objectID": "intro-data.html#population-and-sample",
    "href": "intro-data.html#population-and-sample",
    "title": "2¬† Data",
    "section": "2.2 Population and Sample",
    "text": "2.2 Population and Sample\n Target Population \n\nThe first step in conducting a study is to identify questions to be investigated.\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\nOnce the research question is determined, it is important to identify the target population to be investigated.\nThe target population is the complete collection of data we‚Äôd like to make inference about.\n\n GPA Example \n\n\n\nResearch Question: What is the average GPA of currently enrolled Marquette undergraduate students?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Target Population: All Marquette undergraduate students that are currently enrolled.\nBecause all Marquette undergrads that are currently enrolled are the complete collection of data we‚Äôd like to make inference about, each currently enrolled Marquette undergrad is an object.\nAverage GPA is the variable or population property we would like to make an inference about.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nStudents who are not currently enrolled or students that have already graduated are not our interest, so they shouldn‚Äôt be a part of target population.\n\n\n\n Heart Disease Example \n\n\n\nDoes a new drug reduce mortality in patients with severe heart disease?\n\n\n\n\n\n\n\n\n\n\n\n\n Target Population: All people with severe heart disease. \nMortality is the variable or population property we would like to make an inference about.\n\n\n\n\n Sample Data \n\nIn some cases it‚Äôs possible to collect data of all the cases we are interested in.\nHowever, most of the time it is either expensive or too time consuming to collect data for every case in a population.\nWhat if we tried to collect data on the average GPA of all students in Illinois? The U.S.? The world? üò± üò± üò±\n\n\n\n\nThe solution to this problem is sampling.\nA sample is a subset of cases selected from a population.\nWe are not able to collect the average GPA of every member of the population, but we can collect a sample from that population which has fewer objects (Figure¬†13.1).\nWe can then compute the average GPA of the sample data.\n\n\n\n\n\n\n\n\n\nFigure¬†2.2: Sampling from the population reduces the number of objects from which to collect data.\n\n\n\n\n\n\n\nOur hope is that the average GPA of the sample is close to the average GPA of the population, which is our main interest.\nFor the sample‚Äôs average GPA to be close to population‚Äôs average GPA, we want the sample to look like the population such that they share similar attributes including GPA.\n\n\n Good Sample vs.¬†Bad Sample \n\n\n\n\n\n\nIs this 4720/5720 class a sample of the target population Marquette students?\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.3: Majors of students in this 4740/5740 class\n\n\n\n\n\n\n\n\n\n\nIs this 4720/5720 class a ‚Äúgood‚Äù sample of the target population?\n\n\n\n\n\n\n\nThe sample is convenient to be collected, but as Figure¬†2.3 shows, it is NOT representative of the population.\nBecause this class is primarily composed of STEM majors, it may not share the attributes necessary with the target population for the two to share a similar average GPA.\nTherefore, we call this a biased sample.\n\nThe average GPA of the class may differ greatly from the average GPA of all MU undergrads.\n\n\n\n\n\n\n\nFigure¬†2.4: Sampling from a class of mostly STEM students is not representative of the entire population.\n\n\n\n\n\n\n\nAs shown in Figure¬†2.5, the average GPA differs based on students‚Äô majors.\nBecause this class consists of mostly STEM majors, it is likely that the average GPA of its students is not the same as the average GPA of all MU undergraduates.\nFigure¬†2.4 depicts that sampling needs to be done appropriately to ensure the sample is representative of the population.\n\n\n\n\n\n\n\n\n\nFigure¬†2.5: UC Berkeley average GPAs by major\n\n\n\n\n\n\n\n How do we collect and why do we need a representative sample? \n\nWe always seek to randomly select a sample from a population.\nRandom sampling usually give us a representative sample, as long as the sample size, or the number of objects in the sample, is not too small.\nIt is important to collect samples this way, because many statistical methods are based on the randomness assumption."
  },
  {
    "objectID": "intro-r.html#lets-get-equipped-with-our-tools",
    "href": "intro-r.html#lets-get-equipped-with-our-tools",
    "title": "3¬† Tool foR Data",
    "section": "3.1 Let‚Äôs get equipped with our tools!",
    "text": "3.1 Let‚Äôs get equipped with our tools!\n Integrated Development Environment \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR and Python are programming languages.\nPosit Cloud offers two integrated development environments (IDE).\n\nRStudio\nJupyterLab/Jupyter Notebook\n\nThese IDEs are software for efficiently writing computer programs.\n\n R and Posit \n\n\n\n\n\n\n\n\n\n\n\n\nR: free open-source programming language üìà\nR is mainly for doing data science with strength in statistical modeling, computing and data visualization\n\n\n\n\n\n\n\n\n\n\n\n\nPosit: interface for R, Python, etc. called an IDE (integrated development environment), e.g.¬†‚ÄúI write R code in the RStudio IDE‚Äù.\nPosit is not a requirement for programming with R, but it‚Äôs commonly used by R developers, statisticians and data scientists.\n\n\n\n\n The R User Interface \n\nRStudio IDE includes\n\na viewable environment, a file browser, data viewer and a plotting pane. üëç\nalso features integrated help, syntax highlighting, context-aware tab completion and more! üòÑ\n\n\n\n\n\nR\n\n\n\n\n\n\nFigure¬†3.1: R Console\n\n\n\n\n\n\nRStudio\n\n\n\n\n\n\nFigure¬†3.2: RStudio Console\n\n\n\n\n\n\n\n ‚òÅÔ∏è Posit Cloud - Statistics w/o hardware hassles \n\nüòé You can implement R/Python programs without installing R/Python and the IDE on your laptop!\nüòé Posit Cloud lets you do, share and learn data science online for free!\n\n\n\nüòû Getting everything ready locally: Lots of friction\n\nDownload and install R/Python\nDownload and install IDE\nInstall wanted R/Python packages:\n\ntidymodels\ntidyverse\nNumPy\n‚Ä¶\n\nLoad these packages\nDownload and install tools like Git\n\n\nü§ì Posit Cloud: Much less friction\n\n\n\n\n\n\n\n\n\n\nGo to https://posit.cloud/\nLog in\n\n\n>hello R!\n\n\n\n\n Install Posit Cloud \n\n\n\n\n\n\nLab Time!\n\n\n\n\nStep 1: In the Posit website https://posit.co/, choose Products > Posit Cloud as shown below.\n\n\n\n\n\n\n\n\nFigure¬†3.3: Posit website\n\n\n\n\n\n\n\n\n\n\nLab Time!\n\n\n\n\nStep 2: Click GET STARTED.\nStep 3: Free1 > Sign Up. Please sign up with GitHub if you have one or use your Marquette email address.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n New Projects \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo create a new project, click New Project in the top right corner as shown above.\n\n\n Workspaces \n\nWhen you create an account on Posit Cloud, you get a workspace of your own.\nYou can add a new workspace (click + New Space in sidebar) and control its permissions.\n\n\n\n\n\n\n\n\n\n\n\n First R Code in Posit Cloud! \n\n\n\n\n\n\nLab Time!\n\n\n\n\nIn the bar, click the desired workspace.\nClick New Project > New RStudio Project to get into the IDE.\nClick Untitled Project and give your project a nice name, math-4720 for example.\nIn the Console pane, write your first R code: a string \"Hello WoRld!\" or math 2 + 4.\nChange the editor theme: Tools > Global Options > Appearance\n\n\n\n\n\n\n\n\nFigure¬†3.4: How to change the editor theme\n\n\n\n\n\n More Tips \n\nFor more help, read the Posit Cloud guide.\n\n\n\n\n\n\nFigure¬†3.5: Posit Cloud Guide"
  },
  {
    "objectID": "intro-r.html#install-r-and-r-studio-locally-to-your-computer",
    "href": "intro-r.html#install-r-and-r-studio-locally-to-your-computer",
    "title": "3¬† Tool foR Data",
    "section": "3.3 Install R and R Studio Locally to Your Computer",
    "text": "3.3 Install R and R Studio Locally to Your Computer\n Install R \n Step 1 \n\nGo to https://cloud.r-project.org.\nClick Download R for [your operating system].\n\n\n\n\nFigure¬†3.12: Downloading R\n\n\n Step 2 \n\nIf you are a Mac user, you should see the page shown below in Figure¬†3.13.\nYou are recommended to download and install the latest version of R (now R-4.2.1) if your OS version allows to do so.\nOtherwise, choose a previous version, such as R-3.6.3.\n\n\n\n\nFigure¬†3.13: Downloading R for Mac\n\n\n\nIf you are a Windows user, after clicking Download R for Windows, please choose base version and then click Download R-4.2.1 for Windows.\n\n Step 3 \n\nOnce you successfully install R, when you open R, you should be able to see the following R terminal or console:\n\n\n\n\nWindows\n\n\n\n\nFigure¬†3.14: Windows R Console\n\n\n\n\n\n\nMac\n\n\n\n\nFigure¬†3.15: Mac R Console\n\n\n\n\n Welcome to the R World! \n\nNow you are ready to use R for statistical computation.\nYou can use R like a calculator.\n\nAfter typing your formula, simply hit enter and you get the answer!\n\n\n\n1 + 2\n\n[1] 3\n\n30 * 42 / 3\n\n[1] 420\n\nlog(5) - exp(3) * sqrt(7)\n\n[1] -51.5319\n\n\n\n Install RStudio \n Step 1 \n\nIn the RStudio website, please choose Products > RStudio as shown in Figure¬†3.16.\n\n\n\n\nFigure¬†3.16: R Studio Website\n\n\n Step 2 \n\nChoose RStudio Desktop and click DOWNLOAD RSTUDIO DESKTOP for the free version.\n\n\n\n\n\n\nFigure¬†3.17: Downloading RStudio Desktop\n\n\n\n\n Step 3 \n\nClick DOWNLOAD RSTUDIO FOR [YOUR SYSTEM] (Figure¬†3.18).\nFollow the standard installation steps and you should get the software.\nMake sure that R is installed successfully on your computer before you download and install RStudio.\n\n\n\n\n\n\n\nNote\n\n\n\nThe latest version of RStudio is 2022.07.1+554.\n\n\n\n\n\n\n\nFigure¬†3.18: Latest version of RStudio\n\n\n\n\n\n RStudio Screen \n\nWhen you open RStudio, you should see something similar to Figure¬†3.19 below.\nIf you do, congratulations!\nYou can now do any statistical computation in R using RStudio locally on your computer.\n\n\n\n\n\n\nFigure¬†3.19: R Studio Screen"
  },
  {
    "objectID": "intro-r.html#operators",
    "href": "intro-r.html#operators",
    "title": "3¬† Tool foR Data",
    "section": "3.4 Operators",
    "text": "3.4 Operators\n R is a Calculator \n Arithmetic Operators \n\n\n\n\n\nFigure¬†3.20: Table of arithmetic operators\n\n\n\n\n Examples \n\n2 + 3 * 5 + 4\n\n[1] 21\n\n2 + 3 * (5 + 4)\n\n[1] 29\n\n\n\nWe have to do the operation in the parentheses first, as the PEMDAS rule describes in Figure¬†3.21 below.\n\n\n\n\n\n\nFigure¬†3.21: Order of operations\n\n\n\n\n\n R Does Comparisons \n Logical Operators \n\n\n\n\n\nFigure¬†3.22: Table of logical operators\n\n\n\n\n Examples \n\n\n\n5 <= 5\n\n[1] TRUE\n\n5 <= 4\n\n[1] FALSE\n\n# Is 5 is NOT equal to 5? FALSE\n5 != 5\n\n[1] FALSE\n\n\n\n\n\n\n## Is TRUE not equal to FALSE?\nTRUE != FALSE\n\n[1] TRUE\n\n## Is not TRUE equal to FALSE?\n!TRUE == FALSE\n\n[1] TRUE\n\n## TRUE if either one is TRUE or both are TRUE\nTRUE | FALSE\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\nWhat does TRUE & FALSE return?\n\n\n\n\n\n\n\n Built-in Functions \n\nR has lots of built-in functions, especially for mathematics, probability and statistics.\n\n\n\n\n\n\nFigure¬†3.23: R Built-in functions\n\n\n\n\n Examples \n\n\n\nsqrt(144)\n\n[1] 12\n\nexp(1)  ## Euler's number\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nabs(-7)\n\n[1] 7\n\n\n\n\n\n\nfactorial(5)\n\n[1] 120\n\n## without specifying base value\n## it is a natural log with base e\nlog(100)\n\n[1] 4.60517\n\n## log function and we specify base = 2\nlog(100, base = 10)\n\n[1] 2\n\n\n\n\n\n Commenting \n\n\n\n\n\n\nYou‚Äôve seen comments a lot! How do we write a comment in R?\n\n\n\n\n\n\n\nUse # to add a comment so that the text after # is not read as an R command.\nWriting (good) comments is highly recommended.\nComments help readers, and more importantly yourself, understand what the code is doing.\nThey should explain the why, not the what.\n\n\n\n\n\n\n\n\nhttps://www.reddit.com/r/ProgrammerHumor /comments/8w54mx/code_comments_be_like/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Objects and Funtions in R \n\n Everything that exists is an object. \n Everything that happens is a function call. \n‚Äì John Chambers, the creator of the S programming language.\n\n\nWe have made lots of things happen!\nEven arithmetic and logical operators are functions!\n\n\n`+`(x = 2, y = 3)\n\n[1] 5\n\n`&`(TRUE, FALSE)\n\n[1] FALSE\n\n\n\n Creating Variables \n\nA variable stores a value that can be changed according to our need.\nUse the <- operator to assign a value to the variable. (Highly recommendedüëç)\n\n\nx <- 5  ## we create an object, value 5, and call it x, which is a variable.\nx  ## type the variable name to see the value stored in the object x\n\n[1] 5\n\n\n\n\n(x <- x + 6)  # We can reassign any value to the variable we created\n\n[1] 11\n\nx == 5  # We can perform any operations on variables\n\n[1] FALSE\n\nlog(x) # Variables can also be used in any built-in functions\n\n[1] 2.397895\n\n\n\n\n\n\n\n\n\n\n\n\n Bad Naming \n\n‚ùå Unless you have a very good reason, don‚Äôt create a variable whose name is the same as any R built-in constant or function!\nüòü It causes lots of confusion when your code is long and when others read it.\n\n\n## THIS IS BAD CODING! DON'T DO THIS!\npi  ## pi is a built-in constant\n\n[1] 3.141593\n\n(pi <- 20)\n\n[1] 20\n\nabs ## abs is a built-in function\n\nfunction (x)  .Primitive(\"abs\")\n\n(abs <- abs(pi))\n\n[1] 20"
  },
  {
    "objectID": "intro-r.html#r-data-structures",
    "href": "intro-r.html#r-data-structures",
    "title": "3¬† Tool foR Data",
    "section": "3.6 R Data Structures",
    "text": "3.6 R Data Structures\n (Atomic) Vector \n\nTo create a vector, use c(), which is short for concatenate or combine.\nAll elements of a vector must be of the same type.\n\n\n\n\n(dbl_vec <- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(int_vec <- c(1L, 6L, 10L))\n\n[1]  1  6 10\n\n## TRUE and FALSE can be written as T and F\n(log_vec <- c(TRUE, FALSE, F))  \n\n[1]  TRUE FALSE FALSE\n\n(chr_vec <- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5\n\n\n\n\n Operations on Vectors \n\nWe can do the same operations on vectors that we do on a scalar variable (vector of length 1).\n\n\n\n\n# Create two vectors\nv1 <- c(3, 8)\nv2 <- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10\n\n\n\n\n Recycling of Vectors \n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\nv1 <- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 <- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16\n\n\n Subsetting Vectors \n\nTo extract element(s) in a vector, use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The first element\nv1[1] \n\n[1] 3\n\n## The second element\nv2[2]  \n\n[1] 100\n\n\n\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5\n\n\n\n\n\n Factor \n\nA vector of type factor can be ordered in a meaningful way.\nCreate a factor by factor().\nIt is a type of integer, not character. üò≤ üôÑ\n\n\nfac <- factor(c(\"med\", \"high\", \"low\"))\ntypeof(fac)\n\n[1] \"integer\"\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically.\n\n[1] \"high\" \"low\"  \"med\" \n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\norder_fac <- factor(c(\"med\", \"high\", \"low\"), levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1\n\n\n\n List (Generic Vectors) \n\nLists are different from vectors.\n\nElements can be of any type, including lists.\n\nConstruct a list by using list() instead of c().\n\n\n\n\n## a list of 3 elements of different types\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\nx_lst\n\n$idx\n[1] 1 2 3\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1]  TRUE FALSE\n\n\n\n\n\n\nstr(x_lst)\n\nList of 3\n $ idx: int [1:3] 1 2 3\n $    : chr \"a\"\n $    : logi [1:2] TRUE FALSE\n\nnames(x_lst)\n\n[1] \"idx\" \"\"    \"\"   \n\nlength(x_lst)\n\n[1] 3\n\n\n\n\n Subsetting a List \n\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\n\n\n\nReturn an  element  of a list\n\n\n## subset by name (a vector)\nx_lst$idx  \n\n[1] 1 2 3\n\n## subset by indexing (a vector)\nx_lst[[1]]  \n\n[1] 1 2 3\n\ntypeof(x_lst$idx)\n\n[1] \"integer\"\n\n\n\n\n\n\nReturn a  sub-list  of a list\n\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n\n$idx\n[1] 1 2 3\n\n## subset by indexing (still a list)\nx_lst[1]  \n\n$idx\n[1] 1 2 3\n\ntypeof(x_lst[\"idx\"])\n\n[1] \"list\"\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.24: Condiment analogy for subsetting lists\n\n\n\n\n\n\nIf list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\n‚Äî @RLangTip, https://twitter.com/RLangTip/status/268375867468681216\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.25: Train analogy for subsetting a list\n\n\n\n\n\n Matrix \n\nA matrix is a two-dimensional analog of a vector.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat <- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n Subsetting a Matrix \n\nTo extract a sub-matrix, use the same indexing approach as vectors.\nUse comma , to separate the row and column index.\n\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\n[1] 4 5 6\n\n\n\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\n[1] 2 5\n\n## The 1st and 3rd rows\nmat[c(1, 3), ] \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    3    6\n\n\n\n\n Binding Matrices \n\nWe can generalize c() used in vectors to cbind() (binding matrices by adding columns) and rbind() (binding matrices by adding rows) for matrices.\nWhen matrices are combined by columns, they should have the same number of rows.\nWhen matrices are combined by rows, they should have the same number of columns.\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nmat_c <- matrix(data = c(7, 0, 0, 8, 2, 6), \n                nrow = 3, ncol = 2)\n## should have the same number of rows\ncbind(mat, mat_c)  \n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7    8\n[2,]    2    5    0    2\n[3,]    3    6    0    6\n\n\n\n\n\n\nmat_r <- matrix(data = 1:4, \n                nrow = 2, \n                ncol = 2)\n## should have the same number of columns\nrbind(mat, mat_r)  \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n[4,]    1    3\n[5,]    2    4\n\n\n\n\n\n Data Frame: The Most Common Way of Storing Data \n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nIt is more general than a matrix.\n\nDifferent columns can have different types.\n\nTo create a data frame, use data.frame() that takes named vectors as input.\n\n\n\n\n## data frame w/ an dbl column named  \n## and char column named grade.\n(df <- data.frame(age = c(19,21,40), \n                  gender = c(\"m\",\"f\",\"m\")))\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n## a data frame has a list structure\nstr(df)  \n\n'data.frame':   3 obs. of  2 variables:\n $ age   : num  19 21 40\n $ gender: chr  \"m\" \"f\" \"m\"\n\n\n\n\n\n\n## must set column names\n## or they are ugly and non-recognizable\ndata.frame(c(19, 21, 40), c(\"m\",\"f\", \"m\")) \n\n  c.19..21..40. c..m....f....m..\n1            19                m\n2            21                f\n3            40                m\n\n\n\n\n Properties of Data Frames \n\nData frame has properties of matrix and list.\n\n\n\n\nnames(df)  ## df as a list\n\n[1] \"age\"    \"gender\"\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\"    \"gender\"\n\nlength(df) ## df as a list\n\n[1] 2\n\nncol(df) ## df as a matrix\n\n[1] 2\n\ndim(df) ## df as a matrix\n\n[1] 3 2\n\n\n\n\n\n\n## rbind() and cbind() can be used on df\n\ndf_r <- data.frame(age = 10, \n                   gender = \"f\")\nrbind(df, df_r)\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n4  10      f\n\ndf_c <- \n    data.frame(col = c(\"red\",\"blue\",\"gray\"))\n(df_new <- cbind(df, df_c))\n\n  age gender  col\n1  19      m  red\n2  21      f blue\n3  40      m gray\n\n\n\n\n Subsetting a Data Frame \n\nWhen we subset data frames, we can use either list or matrix subsetting methods.\n\n\n\n\ndf_new\n\n  age gender  col\n1  19      m  red\n2  21      f blue\n3  40      m gray\n\n## Subset rows\ndf_new[c(1, 3), ]\n\n  age gender  col\n1  19      m  red\n3  40      m gray\n\n## select the row where age == 21\ndf_new[df_new$age == 21, ]\n\n  age gender  col\n2  21      f blue\n\n\n\n\n\n\n## Subset columns\n## like a list\ndf_new$age\n\n[1] 19 21 40\n\ndf_new[c(\"age\", \"gender\")]\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n## like a matrix\ndf_new[, c(\"age\", \"gender\")]\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nCreate a vector object called x that has 5 elements 3, 6, 2, 9, 14.\nCompute the average of elements of x.\nSubset the mtcars data set by selecting variables mpg and disp.\nSelect the cars (rows) in mtcars that have 4 cylinders."
  },
  {
    "objectID": "data-graphics.html#descriptive-statistics",
    "href": "data-graphics.html#descriptive-statistics",
    "title": "4¬† Data Visualization",
    "section": "4.1 Descriptive Statistics",
    "text": "4.1 Descriptive Statistics\n\nBefore doing inferential statistics, let‚Äôs first learn to understand our data by describing or summarizing it using a table, graph, or some important values, so that appropriate methods can be performed for better inference results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Frequency Table for Categorical Variable \n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nFrequency table for categorical data with \\(n\\) the number of data values:\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\n\\(C_1\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(C_2\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(C_k\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\nExample: A categorical variable color that has three categories\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\nRed üî¥\n8\n8/50 = 0.16\n\n\nBlue üîµ\n26\n26/50 = 0.52\n\n\nBlack ‚ö´\n16\n16/50 = 0.32\n\n\n\n Categorical Frequency Table in R: loan50 [OI] \n\n# install.packages(\"openintro\")\nlibrary(openintro)\nstr(loan50)\n\ntibble [50 √ó 18] (S3: tbl_df/tbl/data.frame)\n $ state                  : Factor w/ 51 levels \"\",\"AK\",\"AL\",\"AR\",..: 32 6 41 6 36 16 35 25 11 11 ...\n $ emp_length             : num [1:50] 3 10 NA 0 4 6 2 10 6 3 ...\n $ term                   : num [1:50] 60 36 36 36 60 36 36 36 60 60 ...\n $ homeownership          : Factor w/ 3 levels \"rent\",\"mortgage\",..: 1 1 2 1 2 2 1 2 1 2 ...\n $ annual_income          : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 80000 ...\n $ verified_income        : Factor w/ 4 levels \"\",\"Not Verified\",..: 2 2 4 2 2 3 3 2 2 3 ...\n $ debt_to_income         : num [1:50] 0.558 1.306 1.056 0.574 0.238 ...\n $ total_credit_limit     : int [1:50] 95131 51929 301373 59890 422619 349825 15980 258439 87705 330394 ...\n $ total_credit_utilized  : int [1:50] 32894 78341 79221 43076 60490 72162 2872 28073 23715 32036 ...\n $ num_cc_carrying_balance: int [1:50] 8 2 14 10 2 4 1 3 10 4 ...\n $ loan_purpose           : Factor w/ 14 levels \"\",\"car\",\"credit_card\",..: 4 3 4 3 5 5 4 3 3 4 ...\n $ loan_amount            : int [1:50] 22000 6000 25000 6000 25000 6400 3000 14500 10000 18500 ...\n $ grade                  : Factor w/ 8 levels \"\",\"A\",\"B\",\"C\",..: 3 3 6 3 3 3 5 2 2 4 ...\n $ interest_rate          : num [1:50] 10.9 9.92 26.3 9.92 9.43 ...\n $ public_record_bankrupt : int [1:50] 0 1 0 0 0 0 0 0 0 1 ...\n $ loan_status            : Factor w/ 7 levels \"\",\"Charged Off\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ has_second_income      : logi [1:50] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ total_income           : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 192000 ...\n\n\n Categorical Frequency Table in R: homeownership \n\n# 50 values (rent, mortgage, own) of categorical homeownership in loan50 data\n(x <- loan50$homeownership)\n\n [1] rent     rent     mortgage rent     mortgage mortgage rent     mortgage\n [9] rent     mortgage rent     mortgage rent     mortgage rent     mortgage\n[17] rent     rent     rent     mortgage mortgage mortgage mortgage rent    \n[25] mortgage rent     mortgage own      mortgage mortgage rent     mortgage\n[33] mortgage rent     rent     own      mortgage rent     mortgage rent    \n[41] mortgage rent     rent     mortgage mortgage mortgage mortgage rent    \n[49] own      mortgage\nLevels: rent mortgage own\n\n## frequency table\ntable(x)\n\nx\n    rent mortgage      own \n      21       26        3 \n\n\n\n\n\n\n\n\n\nIf we want to create a frequency table shown in definition, which R data structure we can use?\n\n\n\n\n\n\n\nfreq <- table(x)\nrel_freq <- freq / sum(freq)\ncbind(freq, rel_freq)\n\n         freq rel_freq\nrent       21     0.42\nmortgage   26     0.52\nown         3     0.06\n\n\n Visualizing a Frequency Table: Bar Chart \n\nbarplot(height = table(x), main = \"Bar Chart\", xlab = \"Homeownership\")\n\n\n\n\n Visualizing a Frequency Table: Pie Chart \n\npie(x = table(x), main = \"Pie Chart\")\n\n\n\n\n\n Frequency Distribution for Numerical Variables \n\nDivide the data into \\(k\\) non-overlapping groups of intervals (classes).\nConvert the data into \\(k\\) categories with an associated class interval.\nCount the number of measurements falling in a given class interval (class frequency).\n\n\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\n\\(1\\)\n\\([a_1, a_2]\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(2\\)\n\\((a_2, a_3]\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(k\\)\n\\((a_k, a_{k+1}]\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\n\\((a_2 - a_1) = (a_3 - a_2) = \\cdots = (a_{k+1} - a_k)\\). All class widths are the same!\n\n\n\n\n\n\n\nCan our grade conversion be used for creating a frequency distribution?\n\n\n\n\n\n\n\n\n\n\n \n  \n    Grade \n    Percentage \n  \n \n\n  \n    A \n    [94, 100] \n  \n  \n    A- \n    [90, 94) \n  \n  \n    B+ \n    [87, 90) \n  \n  \n    B \n    [83, 87) \n  \n  \n    B- \n    [80, 83) \n  \n  \n    C+ \n    [77, 80) \n  \n  \n    C \n    [73, 77) \n  \n  \n    C- \n    [70, 73) \n  \n  \n    D+ \n    [65, 70) \n  \n  \n    D \n    [60, 65) \n  \n  \n    F \n    [0, 60) \n  \n\n\n\n\n\n Interest Rate in Data loan50 [OI] \n\n(int_rate <- round(loan50$interest_rate, 1))\n\n [1] 10.9  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n[16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n[31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n[46] 10.4 21.4 10.9  9.4  6.1\n\n\n\n\n\n\n\n\n\n\n\n Frequency Distribution of Interest Rate \n\n\n\n\n Class Class_Intvl Freq Rel_Freq\n     1     5%-7.5%   11     0.22\n     2    7.5%-10%   15     0.30\n     3   10%-12.5%    8     0.16\n     4   12.5%-15%    5     0.10\n     5   15%-17.5%    4     0.08\n     6   17.5%-20%    4     0.08\n     7   20%-22.5%    1     0.02\n     8   22.5%-25%    1     0.02\n     9   25%-27.5%    1     0.02\n\n\n\nrange(int_rate)\n\n[1]  5.3 26.3\n\n\n\n\nAll class widths are the same!\nNumber of classes should not be too big or too small.\nThe lower limit of the 1st class should not be greater than the minimum value of the data.\nThe upper limit of the last class should not be smaller than the maximum value of the data.\n\n\n\n\n\n\n\n\n\nWonder how we choose the number of classes or the class width?\n\n\n\nR decides the number for us when we visualize the frequency distribution by a histogram.\n\n\n Visualizing Frequency Distribution by a Histogram \n\n\nUse default breaks (no need to specify)\n\nhist(x = int_rate, \n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Defualt)\")\n\n\n\n\n\n\n\nUse customized breaks\n\nclass_boundary\n\n [1]  5.0  7.5 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5\n\nhist(x = int_rate, \n     breaks = class_boundary, #<<\n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Ours)\")\n\n\n\n\n\n\n\n Skewness \n\nKey characteristics of distributions includes shape, center and spread.\nSkewness provides a way to summarize the shape of a distribution.\n\n\n\n\n\n\n\n\n\n\n Remembering Skewness \n\n\n\n\n\n\n\n\nIs the interest rate histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.53\n\n\n\n\n\n\n\n Scatterplot for Two Numerical Variables \n\n\nA scatterplot provides a case-by-case view of data for two numerical variables.\n\n\nplot(x = loan50$total_income, y = loan50$loan_amount,\n     xlab = \"Total income\", ylab = \"Loan amount\",\n     pch = 16, col = 4)"
  },
  {
    "objectID": "data-graphics.html#frequency-table",
    "href": "data-graphics.html#frequency-table",
    "title": "4¬† Data Visualization",
    "section": "4.1 Frequency Table",
    "text": "4.1 Frequency Table\n Frequency Table for Categorical Variable \n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nFrequency table for categorical data with \\(n\\) the number of data values:\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\n\\(C_1\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(C_2\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(C_k\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\nExample: A categorical variable color that has three categories\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\nRed üî¥\n8\n8/50 = 0.16\n\n\nBlue üîµ\n26\n26/50 = 0.52\n\n\nBlack ‚ö´\n16\n16/50 = 0.32\n\n\n\n Categorical Frequency Table in R: loan50 [OI] \n\n# install.packages(\"openintro\")\nlibrary(openintro)\nstr(loan50)\n\ntibble [50 √ó 18] (S3: tbl_df/tbl/data.frame)\n $ state                  : Factor w/ 51 levels \"\",\"AK\",\"AL\",\"AR\",..: 32 6 41 6 36 16 35 25 11 11 ...\n $ emp_length             : num [1:50] 3 10 NA 0 4 6 2 10 6 3 ...\n $ term                   : num [1:50] 60 36 36 36 60 36 36 36 60 60 ...\n $ homeownership          : Factor w/ 3 levels \"rent\",\"mortgage\",..: 1 1 2 1 2 2 1 2 1 2 ...\n $ annual_income          : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 80000 ...\n $ verified_income        : Factor w/ 4 levels \"\",\"Not Verified\",..: 2 2 4 2 2 3 3 2 2 3 ...\n $ debt_to_income         : num [1:50] 0.558 1.306 1.056 0.574 0.238 ...\n $ total_credit_limit     : int [1:50] 95131 51929 301373 59890 422619 349825 15980 258439 87705 330394 ...\n $ total_credit_utilized  : int [1:50] 32894 78341 79221 43076 60490 72162 2872 28073 23715 32036 ...\n $ num_cc_carrying_balance: int [1:50] 8 2 14 10 2 4 1 3 10 4 ...\n $ loan_purpose           : Factor w/ 14 levels \"\",\"car\",\"credit_card\",..: 4 3 4 3 5 5 4 3 3 4 ...\n $ loan_amount            : int [1:50] 22000 6000 25000 6000 25000 6400 3000 14500 10000 18500 ...\n $ grade                  : Factor w/ 8 levels \"\",\"A\",\"B\",\"C\",..: 3 3 6 3 3 3 5 2 2 4 ...\n $ interest_rate          : num [1:50] 10.9 9.92 26.3 9.92 9.43 ...\n $ public_record_bankrupt : int [1:50] 0 1 0 0 0 0 0 0 0 1 ...\n $ loan_status            : Factor w/ 7 levels \"\",\"Charged Off\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ has_second_income      : logi [1:50] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ total_income           : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 192000 ...\n\n\n Categorical Frequency Table in R: homeownership \n\n# 50 values (rent, mortgage, own) of categorical homeownership in loan50 data\n(x <- loan50$homeownership)\n\n [1] rent     rent     mortgage rent     mortgage mortgage rent     mortgage\n [9] rent     mortgage rent     mortgage rent     mortgage rent     mortgage\n[17] rent     rent     rent     mortgage mortgage mortgage mortgage rent    \n[25] mortgage rent     mortgage own      mortgage mortgage rent     mortgage\n[33] mortgage rent     rent     own      mortgage rent     mortgage rent    \n[41] mortgage rent     rent     mortgage mortgage mortgage mortgage rent    \n[49] own      mortgage\nLevels: rent mortgage own\n\n## frequency table\ntable(x)\n\nx\n    rent mortgage      own \n      21       26        3 \n\n\n\n\n\n\n\n\n\nIf we want to create a frequency table shown in definition, which R data structure we can use?\n\n\n\n\n\n\n\nfreq <- table(x)\nrel_freq <- freq / sum(freq)\ncbind(freq, rel_freq)\n\n         freq rel_freq\nrent       21     0.42\nmortgage   26     0.52\nown         3     0.06\n\n\n Visualizing a Frequency Table: Bar Chart \n\nbarplot(height = table(x), main = \"Bar Chart\", xlab = \"Homeownership\")\n\n\n\n\n Visualizing a Frequency Table: Pie Chart \n\npie(x = table(x), main = \"Pie Chart\")\n\n\n\n\n\n\n\n\n\n\n Frequency Distribution for Numerical Variables \n\n\n- Divide the data into \\(k\\) non-overlapping groups of intervals (classes). - Convert the data into \\(k\\) categories with an associated class interval. - Count the number of measurements falling in a given class interval (class frequency).\n\n\n| Class | Class Interval | Frequency | Relative Frequency | |:‚Äî‚Äî‚Äî‚Äì:|:‚Äî‚Äî‚Äî‚Äî‚Äî:|:‚Äî‚Äî‚Äî‚Äî-:|:‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî:| | \\(1\\) | \\([a_1, a_2]\\) | \\(f_1\\) | \\(f_1/n\\) | | \\(2\\) | \\((a_2, a_3]\\) | \\(f_2\\) | \\(f_2/n\\) | | ‚Ä¶ | ‚Ä¶ | ‚Ä¶ | ‚Ä¶ | | \\(k\\) | \\((a_k, a_{k+1}]\\)| \\(f_k\\) | \\(f_k/n\\) |\n\n\n- \\((a_2 - a_1) = (a_3 - a_2) = \\cdots = (a_{k+1} - a_k)\\). All class widths are the same!\n\n\n:::{.callout-note icon=false} ## Can our grade conversion be used for creating a frequency distribution? :::\n\n\n::: {.cell layout-align=‚Äúcenter‚Äù} ::: {.cell-output-display}\n\n\n`````{=html}\n\n\n`````\n\n\n::: :::\n\n\n Interest Rate in Data loan50 [OI] \n\n\n::: {.cell}\n\n\n{.r .cell-code} (int_rate <- round(loan50$interest_rate, 1))\n\n\n::: {.cell-output .cell-output-stdout}\n\n\n::: {.cell layout-align=‚Äúcenter‚Äù} ::: {.cell-output-display}  ::: :::\n\n\n Frequency Distribution of Interest Rate \n\n\n::::{.columns} :::{.column width=‚Äú50%‚Äù}\n\n\n::: {.cell} ::: {.cell-output .cell-output-stdout}\n\n\n::: {.cell}\n\n\n{.r .cell-code} range(int_rate)\n\n\n::: {.cell-output .cell-output-stdout}\n\n\n:::\n\n\n:::{.column width=‚Äú50%‚Äù} - All class widths are the same! - Number of classes should not be too big or too small. - The lower limit of the 1st class should not be greater than the minimum value of the data. - The upper limit of the last class should not be smaller than the maximum value of the data. ::: ::::\n\n\n:::{.callout-note icon=false} ## Wonder how we choose the number of classes or the class width? R decides the number for us when we visualize the frequency distribution by a histogram. :::\n\n\n Visualizing Frequency Distribution by a Histogram \n\n\n::::{.columns} :::{.column width=‚Äú48%‚Äù} Use default breaks (no need to specify)\n\n\n::: {.cell}\n\n\n{.r .cell-code} hist(x = int_rate, xlab = \"Interest Rate (%)\", main = \"Hist. of Int. Rate (Defualt)\")\n\n\n::: {.cell-output-display}  ::: :::\n\n\n:::\n\n\n:::{.column width=‚Äú4%‚Äù}\n\n\n:::\n\n\n:::{.column width=‚Äú48%‚Äù} Use customized breaks\n\n\n::: {.cell}\n\n\n{.r .cell-code} class_boundary\n\n\n::: {.cell-output .cell-output-stdout}\n\n\n{.r .cell-code} hist(x = int_rate, breaks = class_boundary, #<< xlab = \"Interest Rate (%)\", main = \"Hist. of Int. Rate (Ours)\")\n\n\n::: {.cell-output-display}  ::: :::\n\n\n::: ::::\n\n\n\n Skewness \n\nKey characteristics of distributions includes shape, center and spread.\nSkewness provides a way to summarize the shape of a distribution.\n\n\n\n\n\n\n\n\n\n\n Remembering Skewness \n\n\n\n\n\n\n\n\nIs the interest rate histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.53\n\n\n\n\n\n\n\n Scatterplot for Two Numerical Variables \n\n\nA scatterplot provides a case-by-case view of data for two numerical variables.\n\n\nplot(x = loan50$total_income, y = loan50$loan_amount,\n     xlab = \"Total income\", ylab = \"Loan amount\",\n     pch = 16, col = 4)"
  },
  {
    "objectID": "data-numerics.html#measures-of-center",
    "href": "data-numerics.html#measures-of-center",
    "title": "5¬† Data Sample Statistics",
    "section": "5.1 Measures of Center",
    "text": "5.1 Measures of Center\n Mean \n\nThe (arithmetic) mean or average is calculated by adding up all of the values and then dividing by the total number of them.\nThe population mean is denoted as \\(\\mu\\).\nLet \\(x_1, x_2, \\dots, x_n\\) denote the measurements observed in a sample of size \\(n\\).\n\nThe sample mean is defined as\n\n\n\n\\[\\overline{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + x_2 + \\dots + x_n}{n}\\]\n\n\nFor the interest rate example,\n\n\n\\[\\overline{x} = \\frac{10.9\\% + 9.9\\% + \\cdots + 6.1\\%}{50} = 11.56\\%\\]\n\n Calculate Mean in R \n\n\n\n\nmean(int_rate)\n\n[1] 11.558\n\n\n Balancing Point \n\nThink of the mean as the balancing point of the distribution.\n\n\n\n\n\n\n\n\nFigure¬†5.1: Mean as a balancing point for interest rate example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Median \n\nThe median is the middle value when data values are sorted.\nHalf of the values are less than or equal to the median, and the other half are greater than the median.\nTo find the median, we first sort the values.\nIf \\(n\\) is odd, the median is located in the exact middle of the ordered values.\n\n Data: (0, 2, 10, 14, 8) \n Sorted Data: (0, 2, 8, 10, 14) \n The median is \\(8\\) .\n\nIf \\(n\\) is even, the median is the average of the two middle numbers.\n\n Data: (0, 2, 10, 14, 8, 12) \n Sorted Data: (0, 2, 8, 10, 12, 14) \n The median is \\(\\frac{8 + 10}{2} = 9\\) .\n\n\n Calculate Median in R \n\nThere are two ways to calculate the median in R.\n\n\nmedian(int_rate)  ## Compute the median using command median()\n\n[1] 9.9\n\n\n\n## Compute the median using definition\n(sort_rate <- sort(int_rate))  ## sort data\n\n [1]  5.3  5.3  5.3  6.1  6.1  6.1  6.7  6.7  7.3  7.3  7.3  8.0  8.0  8.0  8.0\n[16]  9.4  9.4  9.4  9.4  9.4  9.9  9.9  9.9  9.9  9.9  9.9 10.4 10.4 10.9 10.9\n[31] 10.9 10.9 10.9 12.0 12.6 12.6 12.6 14.1 15.0 16.0 17.1 17.1 17.1 18.1 18.4\n[46] 19.4 20.0 21.4 24.9 26.3\n\nlength(int_rate)  ## Check sample size is odd or even\n\n[1] 50\n\n(sort_rate[25] + sort_rate[26]) / 2  ## Verify the answer\n\n[1] 9.9\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nBe sure to sort the data first if computing the median using its definition.\n\n\n\n\n(int_rate[25] + int_rate[26]) / 2  ## Using un-sorted data leads to a wrong answer!!\n\n[1] 8.1\n\n\n\n Mode \n\nThe mode is the value that occurs most frequently.\nFor continuous numerical data, it is common for there not to be any observations that share the same value.\nA more practical definition is that a mode is represented by a prominent peak in the distribution.\n\n Calculate Mode in R \n\n## Create a frequency table \n(table_data <- table(int_rate))\n\nint_rate\n 5.3  6.1  6.7  7.3    8  9.4  9.9 10.4 10.9   12 12.6 14.1   15   16 17.1 18.1 \n   3    3    2    3    4    5    6    2    5    1    3    1    1    1    3    1 \n18.4 19.4   20 21.4 24.9 26.3 \n   1    1    1    1    1    1 \n\n\n\n## Sort the table to find the mode that occurs most frequently\n## the number that happens most frequently will be the first one\nsort(table_data, decreasing = TRUE)\n\nint_rate\n 9.9  9.4 10.9    8  5.3  6.1  7.3 12.6 17.1  6.7 10.4   12 14.1   15   16 18.1 \n   6    5    5    4    3    3    3    3    3    2    2    1    1    1    1    1 \n18.4 19.4   20 21.4 24.9 26.3 \n   1    1    1    1    1    1"
  },
  {
    "objectID": "data-numerics.html#visualizing-data-variation",
    "href": "data-numerics.html#visualizing-data-variation",
    "title": "5¬† Data Sample Statistics",
    "section": "5.4 Visualizing Data Variation",
    "text": "5.4 Visualizing Data Variation\n Boxplot \n\nWhen plotting the whiskers for a boxplot,\n\nthe minimum is the minimal value that is not a potential outlier.\nthe maximum is the maximal value that is not a potential outlier.\n\n\n\n\n\n\n\nFigure¬†5.5: Example of a boxplot (https://www.leansigmacorporation.com/box-plot-with-minitab/)\n\n\n\n\n Interest Rate Boxplot \n\nBelow is the boxplot for the interest rate data (Figure¬†5.6).\n\n\n\n\n\n\nFigure¬†5.6: Boxplot for interest rate example\n\n\n\n\n Boxplot in R \n\n\n\nboxplot(int_rate,ylab =\"Interest Rate (%)\")\n\n\n\n\n\n\n\n\nsort(int_rate, decreasing = TRUE)[1:5]\n\n[1] 26.3 24.9 21.4 20.0 19.4\n\nsort(int_rate)[1:5]\n\n[1] 5.3 5.3 5.3 6.1 6.1\n\nQ3 <- quantile(int_rate, probs = 0.75, \n               names = FALSE)\nQ1 <- quantile(int_rate, probs = 0.25, \n               names = FALSE)\nIQR <- Q3 - Q1\nQ1 - 1.5 * IQR\n\n[1] -0.5875\n\nQ3 + 1.5 * IQR\n\n[1] 22.3125"
  },
  {
    "objectID": "prob-define.html#language-of-uncertainty",
    "href": "prob-define.html#language-of-uncertainty",
    "title": "6¬† Definition of Probability",
    "section": "6.1 Language of Uncertainty",
    "text": "6.1 Language of Uncertainty\n Why Study Probability \n\nWe live in a world full of chances and uncertainty, and probability is the language of uncertainty!\nProbability is the language of statistical inference and prediction.\nBelow are a few real world examples of where probability plays a role.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Why Probability Before Statistics? \n\nFor probability, we know the process generating the data and are interested in properties of observations.\nFor statistics, we observe the data (sample) and are interested in determining what process is generating the data (population).\nThese principles are illustrated below in Figure¬†6.1.\n\n\n\n\n\n\nFigure¬†6.1: Relationship between probability and statistical inference"
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability",
    "href": "prob-define.html#interpretation-of-probability",
    "title": "6¬† Definition of Probability",
    "section": "6.2 Interpretation of Probability",
    "text": "6.2 Interpretation of Probability\n Relative Frequency \n\nThe probability that some outcome of a process will be obtained is interpreted as the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\nBelow is an example depicting the relative frequency of flipping a coin and getting heads or tails.\n\n\n\n\n      Frequency Relative Frequency\nHeads         4                0.4\nTails         6                0.6\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       513              0.513\nTails       487              0.487\nTotal      1000              1.000\n---------------------\n\n\n\n\nIf we flip the coin 10 times, the probability of obtaining a head is 40%.\nIf we flip it 1000 times, the probability is 51.3%.\n\n\n\n\n\n\n\nDo you see any issues with relative frequency probability?\n\n\n\n\n\n\n Issues with Relative Frequency \n\nüòï How large of a number is large enough?\nüòï What is the meaning of ‚Äúunder similar conditions‚Äù?\nüòï Is the relative frequency reliable under identical conditions?\nüëâ We can only obtain an approximation instead of exact value for the probability.\nüòÇ How do you compute the probability that Chicago Cubs win the World Series next year?\n\n\n\n\n\n\n\n\n\n\n\n Classical Approach \n\nFor the classical approach, the probability is based on the concept of equally likely outcomes.\nIf the outcome of some process must be one of \\(n\\) different outcomes, the probability of each outcome is \\(1/n\\).\nExample:\n\nIf you toss a fair coin (2 outcomes) ü™ô, the probability of each is 1/2.\nIf you roll a well-balanced die (6 outcomes) üé≤, the probability of each is 1/6.\nIf you draw one from a deck of cards (52 outcomes) üÉè, the probability of each is 1/52.\n\n\n\n\n\n\n\n\nDo you see any issues with classical probability?\n\n\n\n\n\n\n\nIt wouldn‚Äôt make sense to say that the probability that [you name it] wins the World Series next year is 1/30.\n\nEven thought there are 30 teams in the MLB, each team is not equally likely to win the World Series.\n\n\n\n Subjective Approach \n\nFor the subjective approach, the probability is assigned or estimated using people‚Äôs knowledge, beliefs and information about the data generating process.\nIn this case, it is a person‚Äôs subjective probability of an outcome, rather than the true probability of that outcome.\nFor example, I think ‚Äúthe probability that the Milwaukee Brewers win the World Series this year is 30%‚Äù.\n\nMy probability that the Milwaukee Brewers win the World Series this year is likely different from an ESPN analyst‚Äôs probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nProbability operations and rules do NOT depend on the interpretation of probability!"
  },
  {
    "objectID": "prob-rule.html#probability-operations-and-rules",
    "href": "prob-rule.html#probability-operations-and-rules",
    "title": "7¬† Probability Rules",
    "section": "\n7.1 Probability Operations and Rules",
    "text": "7.1 Probability Operations and Rules\n Experiments, Events and Sample Space \n\nAn experiment is any process in which the possible outcomes can be identified ahead of time.\nAn event is a set of possible outcomes of the experiment.\nThe sample space \\((\\mathcal{S})\\) of an experiment is the collection of ALL possible outcomes of the experiment.\nThe table below gives an example of the events and sample space that are associated with the experiment of flipping a coin.\n\n\n\n\n\n\n\n\n\nExperiment\nPossible Outcomes\nSome Events\nSample Space\n\n\n\nFlip a coin ü™ô\nHeads, Tails\n{Heads}, {Heads, Tails}, ‚Ä¶\n{Heads, Tails}\n\n\nRoll a die üé≤\n1, 2, 3, 4, 5, 6\n{1, 3, 5}, {2, 4, 6}, {2}, {3, 4, 5, 6}, ‚Ä¶\n{1, 2, 3, 4, 5, 6}\n\n\n\n\n\n\n\n\n\nIs the sample space also an event?\n\n\n\n\nYes, the sample space itself is an event because it is also a set of possible outcomes of the experiment.\n\n\n\n\n Set Concept: Example of Rolling a six-side balanced die\n\n\n\n\n\n\nTip\n\n\n\nDraw a Venn Diagram every time you get stuck!\n\n\n\n\n The complement  of an event (set) \\(A\\), is denoted  \\(A^c\\) .\n\nIt is the set of all outcomes (elements) of \\(\\mathcal{S}\\) in which \\(A\\) does not occur.\nFor the die example, let \\(A\\) be an event that a number greater than 2 is rolled. Then \\(A = \\{3, 4, 5, 6\\}\\) and \\(A^c = \\{1, 2\\}\\).\n\n\nThe  union \\((A \\cup B)\\)  is the set of all outcomes of \\(\\mathcal{S}\\) in \\(A\\) or \\(B\\).\n\n For the die example, let \\(B\\) be an event that an even number is rolled. Then \\(B = \\{2, 4, 6\\}\\) and \\(A \\cup B = \\{2, 3, 4, 5, 6\\}\\). \n\n\nThe  intersection \\((A \\cap B)\\)  is the set of all outcomes of \\(\\mathcal{S}\\) in both \\(A\\) and \\(B\\).\n\n For the die example, \\(A \\cap B = \\{4, 6\\}\\).\n\n\n\n\\(A\\) and \\(B\\) are disjoint or mutually exclusive if they have no outcomes in common \\((A \\cap B = \\emptyset)\\).\n\n\n\\(\\emptyset\\) means an empty set, \\(\\{\\}\\), i.e., no elements in the set.\n For the die example, let \\(C\\) be an event that an odd number is obtained. Then \\(C = \\{1, 3, 5\\}\\) and \\(B \\cap C = \\emptyset\\). \n\n\nThe containment \\((A \\subset B)\\) means every elements of \\(A\\) also belongs to \\(B\\).\n\nIf \\(A\\) occurs, then so does \\(B\\).\n For the die example, let \\(B\\) be the event that an even number is obtained and \\(D\\) be the event that a number greater than 1 is obtained. Then \\(B = \\{2, 4, 6\\}\\) and \\(D = \\{2, 3, 4, 5, 6\\}\\). \n\n\n\n\n\n\n\n\n\nIs \\(B \\subset D\\) or \\(D \\subset B\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Probability Rules \n\nWe denote the probability of an event \\(A\\) on a sample space \\(\\mathcal{S}\\) as \\(P(A)\\).\n\n\n\n\n\n\n\nTip\n\n\n\nTreat the probability of an event as the area of the event in the Venn diagram.\n\n\n\n\nAxioms\n\n\\(P(\\mathcal{S}) = 1\\)\nFor any event \\(A\\), \\(P(A) \\ge 0\\)\n\nIf \\(A\\) and \\(B\\) are disjoint or mutually exclusive, \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\n\n\nProperties\n\n\n\\(P(\\emptyset) = 0\\).\n\\(0 \\le P(A) \\le 1\\)\n\\(P(A^c) = 1 - P(A)\\)\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\nIf \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\n\n\n Venn Diagram Illustration \n\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\n\n\n\n\nFigure¬†7.1: Venn Diagram depiction of the addition rule\n\n\n\n\n\n\nDisjoint Case: \\(P(A \\cup B) = P(A) + P(B)\\) because \\(P(A \\cap B) = 0\\)!\n\n\n\n\n\nFigure¬†7.2: Venn Diagram depiction of the addition rule for the disjoint case\n\n\n\n\n Example: M&M Colors\n\nThe makers of the M&Ms report that their plain M&Ms are composed of\n\n15% Yellow, 10% Red, 20% Orange, 25% Blue, 15% Green and 15% Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\n\n\nIt is brown.\nIt is red or green.\nIt is not blue.\nIt is red and brown.\n\n\n\n\n\n\n\nSolution\n\n\\(P(\\mathrm{Brown}) = 0.15\\)\n\\(\\begin{align} P(\\mathrm{Red} \\cup \\mathrm{Green}) &= P(\\mathrm{Red}) + P(\\mathrm{Green}) - P(\\mathrm{Red} \\cap \\mathrm{Green}) \\\\ &= 0.10 + 0.15 - 0 = 0.25 \\end{align}\\)\n\\(P(\\text{Not Blue}) = 1 - P(\\text{Blue}) = 1 - 0.25 = 0.75\\)\n\\(P(\\text{Red and Brown}) = P(\\emptyset) = 0\\)\n\n\n\n\n\n\n\n\n\nWhich interpretation of probability is used in this question?"
  },
  {
    "objectID": "prob-rule.html#conditional-probability-and-independence",
    "href": "prob-rule.html#conditional-probability-and-independence",
    "title": "7¬† Probability Rules",
    "section": "\n7.2 Conditional Probability and Independence",
    "text": "7.2 Conditional Probability and Independence\n Conditional Probability \n\nAs shown in Figure¬†7.3 below, the conditional probability of \\(A\\) given \\(B\\) is\n\n\n\\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)}\\]\n\n\nIt is defined if \\(P(B) > 0\\) and undefined if \\(P(B) = 0\\).\n\n‚ÄúGiven \\(B\\)‚Äù means that event \\(B\\) has already occurred.\n\n\n\n\n\nFigure¬†7.3: Venn Diagram illustration of the conditional probability of A given B\n\n\n\n\n\n\nMultiplication Rule: \\(P(A \\cap B) = P(A \\mid B)P(B) = P(B \\mid A)P(A)\\)\n\nThis is a rearranged form of the equation for conditional probability.\n\n\n\n\\(P(A)\\) and \\(P(B)\\) are unconditional or marginal probabilities.\n\nThe marginal probability \\(P(A)\\) is the ratio of area of \\(A\\) to the area of the sample space.\nThe conditional probability \\(P(A|B)\\) is the ratio of area of \\(A \\cap B\\) to the area of \\(B\\).\n\n\nThe difference between marginal probabilities and conditional probabilities is depicted in Figure¬†7.4.\n\n\n\n\n\nFigure¬†7.4: Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)\n\n\n\n\n Example: Peanut Butter and Jelly\n\nSuppose 80% of people like peanut butter, 89% like jelly and 78% like both.\nGiven that a randomly sampled person likes peanut butter, what is the probability that she also likes jelly?\n\n\n\n\n\n\n\n\n\n\n\n\nWe want \\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)}\\).\nFrom the problem we have \\(P(PB) = 0.8\\), \\(P(J) = 0.89\\) and \\(P(PB \\cap J) = 0.78\\)\n\n\\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)} = \\frac{0.78}{0.8} = 0.975\\)\nIf we don‚Äôt know if the person loves peanut butter, the probability that he or she loves jelly is 89%.\nIf we do know she loves peanut butter, the probability that he or she loves jelly is going up to 97.5%.\n\n\n\n\n Independence \n\n\\(A\\) and \\(B\\) are independent if \\(\\begin{align} P(A \\mid B) &= P(A) \\text{ or }\\\\ P(B \\mid A) &= P(B) \\text{ or } \\\\P(A\\cap B) &= P(A)P(B)\\end{align}\\) \\(\\text{ if } P(A) > 0 \\text{ and } P(B) > 0\\)\nIntuitively, this means that knowing \\(B\\) occurs does not change the probability that \\(A\\) occurs and vice versa.\n\n\n\n\n\n\n\nCan we compute \\(P(A \\cap B)\\) if we only know \\(P(A)\\) and \\(P(B)\\)?\n\n\n\n\nNo, we cannot compute \\(P(A \\cap B)\\) because we don‚Äôt know if \\(A\\) and \\(B\\) are independent.\nWe can only do so if \\(A\\) and \\(B\\) are independent.\nIn general, we need the multiplication rule \\(P(A \\cap B) = P(A \\mid B)P(B)\\).\n\n\n\n\n\n\n\nFigure¬†7.5: Venn Diagram Explanation of Independence\n\n\n\n\n Independence Example \n\n\n\n\n\n\nAssuming that events \\(A\\) and \\(B\\) are independent and that \\(P(A) = 0.3\\) and \\(P(B) = 0.7\\).\n\n\n\n\n\n\\(P(A \\cap B)\\)?\n\n\\(P(A \\cup B)\\)?\n\n\\(P(A \\mid B)\\)?\n\n\n\nSolution\n\n\\(P(A \\cap B) = P(A)P(B)=0.21\\)\n\\(P(A \\cup B) = P(A)+P(B)-P(A\\cap B) = 0.3+0.7-0.21=0.79\\)\n\\(P(A \\mid B) = P(A) = 0.3\\)"
  },
  {
    "objectID": "prob-rv.html#discrete-probability-distributions",
    "href": "prob-rv.html#discrete-probability-distributions",
    "title": "8¬† Random Variables",
    "section": "8.2 Discrete Probability Distributions",
    "text": "8.2 Discrete Probability Distributions\n\nThe probability (mass) function of a discrete random variable (rv) \\(X\\) is a function \\(P(X = x)\\) (or \\(p(x)\\)) that assigns a probability for every possible number \\(x\\).\nThe probability distribution for a discrete r.v. \\(X\\) displays its probability function.\nThe display can be a table, graph, or mathematical formula of \\(P(X = x)\\).\n\n Example: ü™ôü™ô Toss a fair coin twice independently and \\(X\\) is the number of heads. \n\nThe probability distribution of \\(X\\) as a table is\n\n\n\n\n\n\n\n  \n    x \n    0 \n    1 \n    2 \n  \n  \n    P(X = x) \n    0.25 \n    0.5 \n    0.25 \n  \n\n\n\n\n\n\n\n\n\n\n\n\nüëâ \\(\\{X = x\\}\\) is an event corresponding to an event of some experiment.\n\n\n\n\nWhat is the event that \\(\\{X = 0\\}\\) corresponds to?\nHow do we get \\(P(X = 0)\\), \\(P(X=1)\\) and \\(P(X=2)\\) ?\n\n\n\n Graph \n\n\n\n\n\n\n\n\n\n\n\n\\(0 \\le P(X = x) \\le 1\\) for every value \\(x\\) of \\(X\\).\n\n \\(x = 0, 1, 2\\) \n\n\\(\\sum_{x}P(X=x) = 1\\), where \\(x\\) assumes all possible values.\n\n \\(P(X=0) + P(X = 1) + P(X = 2) = 1\\) \n\nThe probabilities for a discrete r.v. are additive because \\(\\{X = a\\}\\) and \\(\\{X = b\\}\\) are disjoint for any possible values \\(a \\ne b\\).\n\n \\(P(X = 1 \\text{ or } 2) = P(\\{X = 1\\} \\cup \\{X = 2\\}) = P(X = 1) + P(X = 2)\\). \n\n\n\n Mean \n\nSuppose \\(X\\) takes values \\(x_1, \\dots, x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\).\nThe mean or expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[E(X) := x_1 \\times P(X = x_1) + \\dots + x_k \\times P(X = x_k) = \\sum_{i=1}^kx_iP(X=x_i)\\]\nThe Greek letter \\(\\mu\\) may be used in place of the notation \\(E(X)\\).\n\n\n\n\n\n\n\nNote\n\n\n\nüëâ The mean of a discrete random variable \\(X\\) is the weighted average of possible values \\(x\\) weighted by their corresponding probability.\n\n\n\n\n\n\n\n\nWhat is the mean of \\(X\\) (the number of heads) in the previous example?\n\n\n\n\n\n\n\n Variance \n\nSuppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\).\nThe variance of \\(X\\), denoted by \\(Var(X)\\) or \\(\\sigma^2\\), is \\[\\ Var(X) := (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) = \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i)\\]\nThe standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance.\n\n\n\n\n\n\n\nNote\n\n\n\nüëâ The variance of a discrete random variable \\(X\\) is the weighted sum of squared deviation from the mean weighted by probability values.\n\n\n\n\n\n\n\n\nWhat is the variance of \\(X\\) (the number of heads) in the previous example?\n\n\n\n\n\n\n\n Binomial Distribution \n Binomial Experiment and Random Variable \n\nA binomial experiment is the one having the following properties:\n\nüëâ The experiment consists of a fixed number of identical trials \\(n\\).\nüëâ Each trial results in one of exactly two outcomes (success (S) and failure (F)).\nüëâ Trials are independent, meaning that the outcome of any trial does not affect the outcome of any other trial.\nüëâ The probability of success is constant for all trials.\n\nIf \\(X\\) is defined as  the number of successes observed in \\(n\\) trials , \\(X\\) is a binomial random variable.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe word success just means one of the two outcomes, and does not necessarily mean something good. \nüò≤ Can define Drug abuse as success and No drug abuse as failure.\n\n\n\n Distribution \n\nThe probability function \\(P(X = x)\\) of a binomial r.v. \\(X\\) can be fully determined by\n\nthe number of trials \\(n\\)\nprobability of success \\(\\pi\\)\n\nDifferent \\((n, \\pi)\\) pairs generate different binomial probability distributions.\n\\(X\\) is said to follow a binomial distribution with parameters \\(n\\) and \\(\\pi\\), written as \\(\\color{blue}{X \\sim binomial(n, \\pi)}\\).\nThe binomial probability function is \\[ \\color{blue}{P(X = x \\mid n, \\pi) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x}, \\quad x = 0, 1, 2, \\dots, n}\\] with mean \\(\\mu = E(X) = n\\pi\\) and variance \\(\\sigma^2 = Var(X) = n\\pi(1-\\pi)\\).\n\n\n\n\n\n\n\nTossing a fair coin two times independently. Let \\(X =\\) # of heads. Is \\(X\\) a binomial r.v.?\n\n\n\n\n\n\n Example \n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose it is a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit.\n\\(X \\sim binomial(15, 0.2)\\).\n\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, n}\\]\n\n\n\n\n\n\n\n\n\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\n\n\n\n\n\nTip\n\n\n\nNever do this by hand. We can compute them using R!\n\n\n Computation in R \n\n\nWith size the number of trials and prob the probability of success,\n\ndbinom(x, size, prob) to compute \\(P(X = x)\\)\npbinom(q, size, prob) to compute \\(P(X \\le q)\\)\npbinom(q, size, prob, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n## 1. P(X = 6)\ndbinom(x = 6, size = 15, prob = 0.2) \n\n[1] 0.04299262\n\n## 2. P(X >= 6) = 1 - P(X <= 5)\n1 - pbinom(q = 5, size = 15, prob = 0.2) \n\n[1] 0.06105143\n\n\n\n\n## 2. P(X >= 6) = P(X > 5)\npbinom(q = 5, size = 15, prob = 0.2, \n       lower.tail = FALSE)  \n\n[1] 0.06105143\n\n\n\n\n\nplot(x = 0:15, y = dbinom(0:15, size = 15, prob = 0.2), \n     type = 'h', xlab = \"x\", ylab = \"P(X = x)\", \n     lwd = 5, main = \"Binomial(15, 0.2)\")"
  },
  {
    "objectID": "prob-rv.html#binomial-distribution-example",
    "href": "prob-rv.html#binomial-distribution-example",
    "title": "8¬† Random Variables",
    "section": "8.3 Binomial Distribution Example",
    "text": "8.3 Binomial Distribution Example\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose it is a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit.\n\\(X \\sim binomial(15, 0.2)\\).\n\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, n}\\]"
  },
  {
    "objectID": "prob-rv.html#probability-distributions-a-statistician-should-know",
    "href": "prob-rv.html#probability-distributions-a-statistician-should-know",
    "title": "8¬† Random Variables",
    "section": "8.3 Probability Distributions A Statistician Should Know",
    "text": "8.3 Probability Distributions A Statistician Should Know\n\n\n\n\n\nFigure¬†8.1: Probability distributions a statisticain should know (https://github.com/rasmusab/distribution_diagrams)"
  },
  {
    "objectID": "prob-disc.html#introduction",
    "href": "prob-disc.html#introduction",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.1 Introduction",
    "text": "9.1 Introduction\n\nThe probability (mass) function of a discrete random variable (rv) \\(X\\) is a function \\(P(X = x)\\) (or \\(p(x)\\)) that assigns a probability to every possible number \\(x\\).\nThe probability distribution for a discrete r.v. \\(X\\) displays its probability function.\nThe display can be a table, graph or mathematical formula of \\(P(X = x)\\).\n\n Example:ü™ôü™ô Toss a fair coin twice independently where \\(X\\) is the number of heads. \n\nThe probability distribution of \\(X\\) as a table is\n\n\n\n\n\n\n\n  \n    x \n    0 \n    1 \n    2 \n  \n  \n    P(X = x) \n    0.25 \n    0.5 \n    0.25 \n  \n\n\n\n\n\n\n\n\n\n\n\n\nüëâ \\(\\{X = x\\}\\) corresponds to an event of some experiment.\n\n\n\n\nWhat is the event that \\(\\{X = 0\\}\\) corresponds to?\nHow do we determine \\(P(X = 0)\\), \\(P(X=1)\\) and \\(P(X=2)\\) ?\n\n\n\n\n\n\n\n\n\nFigure¬†9.1: Discrete probability distribution of two coin flips as a graph\n\n\n\n\n\n\\(0 \\le P(X = x) \\le 1\\) for every value \\(x\\) of \\(X\\).\n\n \\(x = 0, 1, 2\\) \n\n\\(\\sum_{x}P(X=x) = 1\\), where \\(x\\) assumes all possible values.\n\n \\(P(X=0) + P(X = 1) + P(X = 2) = 1\\) \n\nThe probabilities for a discrete r.v. are additive because \\(\\{X = a\\}\\) and \\(\\{X = b\\}\\) are disjoint for any possible values \\(a \\ne b\\).\n\n \\(P(X = 1 \\text{ or } 2) = P(\\{X = 1\\} \\cup \\{X = 2\\}) = P(X = 1) + P(X = 2)\\). \n\n\n\n Mean \n\nSuppose \\(X\\) takes values \\(x_1, \\dots, x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\).\nThe mean or expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[E(X) := x_1 \\times P(X = x_1) + \\dots + x_k \\times P(X = x_k) = \\sum_{i=1}^kx_iP(X=x_i)\\]\nThe Greek letter \\(\\mu\\) may also be used in place of the notation \\(E(X)\\).\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe mean of a discrete random variable \\(X\\) is a weighted average.\nThe possible values, \\(x\\), are weighted by their corresponding probability.\n\n\n\n\n\n\n\n\n\nWhat is the mean of \\(X\\) (the number of heads) in the previous example?\n\n\n\n\n\n\n\n Variance \n\nSuppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\).\nThe variance of \\(X\\), denoted by \\(Var(X)\\) or \\(\\sigma^2\\), is \\[\\small Var(X) := (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) = \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i)\\]\nThe standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe variance of a discrete random variable \\(X\\) is also weighted.\nIt is the sum of squared deviation from the mean weighted by probability values.\n\n\n\n\n\n\n\n\n\nWhat is the variance of \\(X\\) (the number of heads) in the previous example?"
  },
  {
    "objectID": "prob-disc.html#binomial-distribution-1",
    "href": "prob-disc.html#binomial-distribution-1",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.3 Binomial Distribution",
    "text": "9.3 Binomial Distribution\n\nThe probability function \\(P(X = x)\\) of a binomial r.v. \\(X\\) can be fully determined by\n\nthe number of trials \\(n\\)\nprobability of success \\(\\pi\\)\n\nDifferent \\((n, \\pi)\\) pairs generate different binomial probability distributions.\n\\(X\\) is said to follow a binomial distribution with parameters \\(n\\) and \\(\\pi\\), written as \\(\\color{blue}{X \\sim binomial(n, \\pi)}\\).\nThe binomial probability function is \\[ \\color{blue}{P(X = x \\mid n, \\pi) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x}, \\quad x = 0, 1, 2, \\dots, n}\\] with mean \\(\\mu = E(X) = n\\pi\\) and variance \\(\\sigma^2 = Var(X) = n\\pi(1-\\pi)\\).\n\n\n\n\n\n\n\nIf we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial r.v.?"
  },
  {
    "objectID": "prob-disc.html#poisson-distribution",
    "href": "prob-disc.html#poisson-distribution",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.3 Poisson Distribution",
    "text": "9.3 Poisson Distribution\n Poisson Random Variables \n\nIf we want to count the number of occurrences of some event over a unit of time or space (region) and observe its associated probability, we could consider the Poisson distribution.\nFor example,\n\nThe number of COVID patients arriving at ICU in one hour\nThe number of Marquette students logging onto D2L in one day\nThe number of dandelions per square meter on Marquette‚Äôs campus\n\nLet \\(X\\) be a Poisson random variable. Then \\(\\color{blue}{X \\sim Poisson(\\lambda)}\\), where \\(\\lambda\\) is the parameter representing the mean number of occurrences of the event in the interval. \\[\\color{blue}{P(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0, 1, 2, \\dots}\\]\nBoth the mean and the variance are equal to \\(\\lambda\\).\n\n\n Assumptions and Properties of Poisson Variables \n\nüëâ Events occur one at a time; two or more events do not occur at the same time or in the same space or spot.\nüëâ The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a nonoverlapping time period or region of space.\nüëâ \\(\\lambda\\) is constant for any period or region.\n\n\n\n\n\n\n\nWhat are the differences between Binomial and Poisson distributions?\n\n\n\n\nThe Poisson distribution\n\nis determined by one single parameter \\(\\lambda\\)\nhas possible values \\(x = 0, 1, 2, \\dots\\) with no upper limit (countable), while a binomial variable has possible values \\(0, 1, 2, \\dots, n\\) (finite)\n\n\n\n\n\n Example \n\n\n\nLast year there were 4200 births at the University of Wisconsin Hospital. Assume \\(X\\) be the number of births in a given day at the center, and \\(X \\sim Poisson(\\lambda)\\). Find\n\n\\(\\lambda\\), the mean number of births per day.\nthe probability that on a randomly selected day, there are exactly 10 births.\n\\(P(X > 10)\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\small \\lambda = \\frac{\\text{Number of birth in a year}}{\\text{Number of days}} = \\frac{4200}{365} = 11.5\\)\n\\(\\small P(X = 10 \\mid \\lambda = 11.5) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = \\frac{11.5^{10} e^{-11.5}}{10!} = 0.113\\)\n\\(\\small P(X > 10) = p(11) + p(12) + \\dots + p(20) + \\dots\\) (No end!) \\(\\small P(X > 10) = 1 - P(X \\le 10) = 1 - (p(1) + p(2) + \\dots + p(10))\\).\n\n\n Computation in R \n\n\n\nWith lambda being the mean of Poisson distribution,\n\nuse dpois(x, lambda) to compute \\(P(X = x)\\)\nuse ppois(q, lambda) to compute \\(P(X \\le q)\\)\nuse ppois(q, lambda, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n(lam <- 4200 / 365)\n\n[1] 11.50685\n\n## P(X = 10)\ndpois(x = 10, lambda = lam)  \n\n[1] 0.112834\n\n\n\n\n\n\n## P(X > 10) = 1 - P(X <= 10)\n1 - ppois(q = 10, lambda = lam)  \n\n[1] 0.5990436\n\n## P(X > 10)\nppois(q = 10, lambda = lam, \n      lower.tail = FALSE) \n\n[1] 0.5990436\n\n\n\n\n\nBelow is an example of how to generate the Poisson probability distribution as a graph.\n\n\nplot(0:24, dpois(0:24, lambda = lam), type = 'h', lwd = 5, \n     ylab = \"P(X = x)\", xlab = \"x\", main = \"Poisson(11.5)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\\(X\\) has no upper limit; the graph is truncated at \\(x = 24\\)."
  },
  {
    "objectID": "prob-cont.html#introduction",
    "href": "prob-cont.html#introduction",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.1 Introduction",
    "text": "10.1 Introduction\n\nA continuous random variable can take on any values from an interval of the real line.\nInstead of probability functions, a continuous random variable, \\(X\\), has the probability density function (pdf) \\(f(x)\\) such that for any real value \\(a < b\\), \\[P(a < X < b) = \\int_{a}^b f(x) dx\\]\nThe cumulative distribution function (cdf) of \\(X\\) is defined as \\[F(x) := P(X \\le x) = \\int_{-\\infty}^x f(t)dt\\]\nEvery probability density function must satisfy  (1) \\(f(x) \\ge 0\\) for all \\(x\\); (2) \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\) \nüòé Luckily, we don‚Äôt deal with integrals in this course.\n\n\n Density Curve \n\nA probability distribution function generates a graph called a density curve that shows the likelihood of a random variable at all possible values.\nThe area under the density curve between \\(a\\) and \\(b\\): \\(P(a < X < b) = \\int_{a}^b f(x) dx\\).\nThe total area under any density curve is equal to 1: \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\)\n\n\n\n\n\n\nFigure¬†10.1: Density curve for a random variable\n\n\n\n\n\n Commonly Used Continuous Distributions \n\nR Shiny app is a Continuous Distribution calculator.\nIn this course, we will touch on normal (Gaussian), student‚Äôs t, chi-square, F\nSome other common distributions include uniform, exponential, gamma, beta, inverse gamma, Cauchy, etc. (MATH 4700)"
  },
  {
    "objectID": "prob-cont.html#tail-areas-and-normal-percentiles",
    "href": "prob-cont.html#tail-areas-and-normal-percentiles",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.4 Tail Areas and Normal Percentiles",
    "text": "10.4 Tail Areas and Normal Percentiles\n Finding Tail Areas \\(P(X < x)\\) \n\nFinding tail areas allows us to determine the percentage of cases that are above or below a certain score.\nGoing back to the SAT and ACT example, this can help us determine the fraction of students have an SAT score below Anna‚Äôs score of 1300.\n\nThis is the same as determining what percentile Anna scored at, which is the percentage of cases that had lower scores than Anna.\n\nTherefore, we are looking for \\(P(X < 1300 \\mid \\mu = 1100, \\sigma = 200)\\) or \\(P(Z < 1 \\mid \\mu = 0, \\sigma = 1)\\).\n\nWe can calculate this value by using R.\n\n\n\n\n\n\n\nFigure¬†10.8: Tail area for scores below 1300\n\n\n\n\n Calculation in R \n\nWith mean and sd representing the mean and standard deviation of a normal distribution,\n\nuse pnorm(q, mean, sd) to compute \\(P(X \\le q)\\)\nuse pnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n\n\n\n\n\n\npnorm(1, mean = 0, sd = 1)\n\n[1] 0.8413447\n\npnorm(1300, mean = 1100, sd = 200)\n\n[1] 0.8413447\n\n\n\nThe shaded area represents the 84.1% of SAT test takers who had z-score below 1.\n\n\n\n Second ACT and SAT Example \n\n\n\n\n\n\nWhat is the probability Shannon SAT scores at least 1190?\n\n\n\n\nSAT score follows \\(N(1100, 200^2)\\).\nShannon is an SAT taker, and nothing else is known about her SAT aptitude.\n\n\n\n\n Step 1: State the problem \n\n We want to compute \\(P(X \\ge 1190)\\). \n\n Step 2: Draw a picture\n\n\n\n\n\n\nFigure¬†10.9: Tail area for scores greater than 1190\n\n\n\n\n\n\n\n\n\nFigure¬†10.10: Method to determine right tail areas\n\n\n\n\n\n Step 3: Find \\(z\\)-score \n\n \\(z = \\frac{1190 - 1100}{200} = 0.45\\) and we want to compute \\(P(X > 1190) = P\\left( \\frac{X - \\mu}{\\sigma} > \\frac{1190 - 1000}{200} \\right) = P(Z > 0.45) = 1 - P(Z \\le 0.45)\\) \n\n Step 4: Find the area using pnorm() \n\n\n1 - pnorm(0.45)\n\n[1] 0.3263552\n\n\n\n Normal Percentiles in R \n\nTo get the \\(100p\\)-th percentile (or the \\(p\\) quantile \\(q\\) ), given probability \\(p\\),\n\nuse qnorm(p, mean, sd) to get a value of \\(X\\), \\(q\\), such that \\(P(X \\le q) = p\\)\nuse qnorm(p, mean, sd, lower.tail = FALSE) to get \\(q\\) such that \\(P(X \\ge q) = p\\)\n\n\n SAT and ACT Example \n\nWhat is the 95th percentile for SAT scores?\n Step 1: State the problem \n\n We want to find \\(x\\) s.t \\(P(X < x) = 0.95\\). \n\n Step 2: Draw a picture\n\n\n\n\n\n\nFigure¬†10.11: Picture for the 95th percentile of SAT scores\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe want to find an \\(x\\) value of the normal distribution, which is greater than 95% of all other cases.\n\n\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z < z) = 0.95\\) using qnorm():\n\n\n(z_95 <- qnorm(0.95))\n\n[1] 1.644854\n\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma\\). \n\n\n\n(x_95 <- 1100 + z_95 * 200)\n\n[1] 1428.971\n\n\n\nThe 95th percentile for SAT scores is 1429.\n\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1428.971"
  },
  {
    "objectID": "prob-samdist.html#introduction",
    "href": "prob-samdist.html#introduction",
    "title": "11¬† Sampling Distribution",
    "section": "11.1 Introduction",
    "text": "11.1 Introduction\n Parameter \n\nA parameter is a number that describes a probability distribution.\n\n Binomial: two parameters, \\(n\\) and \\(\\pi\\) \n Poisson: one parameter, \\(\\lambda\\) \n Normal: two parameters, \\(\\mu\\) and \\(\\sigma\\) \n\nIn statistics, we usually assume our target population follows some distribution, but its parameters are unknown to us.\n\n\n\n\n Human weight follows \\(N(\\mu, \\sigma^2)\\) \n\n\n\n\n\n\n\n\n\n\n\n\n # of snowstorms in one year follows \\(Poisson(\\lambda)\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\n Treat Each Data Point as a Random Variable \n\n\\(n\\) random variables: \\(X_1, X_2, \\dots, X_n\\).\nAssume \\(X_1, X_2, \\dots, X_n\\) follow the same distribution.\nView \\(X_i\\) as a data point to be drawn from a population with some distribution, say \\(N(\\mu, \\sigma^2)\\).\n\n\n\n\n\n\nFigure¬†11.1: Illustration of sampling from a population\n\n\n\n\n\nAssume that \\(X_1, X_2, \\dots, X_n\\) are independent, meaning the distribution/value of \\(X_i\\) is not affected by any other \\(X_j\\).\nWith the same distribution, \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed (iid).\n\n \\(X_1, X_2, \\dots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) \n\n\\((X_1, X_2, \\dots, X_n)\\) is a random sample of size \\(n\\) from the population.\n\n Example: \\(X_1, X_2, \\dots, X_{50}\\) are randomly selected SAT scores from the SAT score population that follows \\(N(1100, 200^2)\\) \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nBefore we actually collect the data, the data \\(X_1, X_2, \\dots, X_n\\) are random variables from the population distribution \\(N(\\mu, \\sigma^2)\\).\nOnce we collect the data, we know the realized value of these random variables: \\(x_1, x_2, \\dots, x_n\\)."
  },
  {
    "objectID": "prob-llnclt.html#why-use-a-normal-distribution",
    "href": "prob-llnclt.html#why-use-a-normal-distribution",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.2 Why Use a Normal Distribution?",
    "text": "12.2 Why Use a Normal Distribution?\n\nCentral Limit Theorem (CLT): Suppose \\(\\overline{X}\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma < \\infty\\). As \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\), regardless of the distribution from which we are sampling!\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png\n\n\n\n\n CLT Illustration \n A Right-Skewed Distribution \n\n\n\n\n\n\n\n\n\n A U-shaped Distribution"
  },
  {
    "objectID": "prob-llnclt.html#why-is-the-central-limit-theorem-important",
    "href": "prob-llnclt.html#why-is-the-central-limit-theorem-important",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.2 Why is the Central Limit Theorem Important?",
    "text": "12.2 Why is the Central Limit Theorem Important?\n\n\nMany well-developed statistical methods are based on normal distribution assumption.\nWith CLT, we can use those methods even if we are sampling from a non-normal distribution, or we have no idea of the population distribution, provided that the sample size is large."
  },
  {
    "objectID": "prob-llnclt.html#central-limit-theorem-example",
    "href": "prob-llnclt.html#central-limit-theorem-example",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "12.2 Central Limit Theorem Example",
    "text": "12.2 Central Limit Theorem Example\n\n\n\nSuppose that the selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000.\nIn 100 randomly selected sales, what is the probability the average selling price is more than $400,000?\nSince the sample size is fairly large \\((n = 100)\\), by the Central Limit Theorem, the sampling distribution of the average selling price is approximately normal with a mean of $382,000 and a SD of \\(150,000 / \\sqrt{100}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(\\overline{X} > 400000) = P\\left(\\frac{\\overline{X} - 382000}{150000/\\sqrt{100}} > \\frac{400000 - 382000}{150000/\\sqrt{100}}\\right) \\approx P(Z > 1.2)\\) where \\(Z \\sim N(0, 1)\\).\n\n\n\npnorm(1.2, lower.tail = FALSE)\n\n[1] 0.1150697\n\npnorm(400000, mean = 382000, sd = 150000/sqrt(100), lower.tail = FALSE)\n\n[1] 0.1150697"
  },
  {
    "objectID": "infer-ci.html#foundations-for-inference",
    "href": "infer-ci.html#foundations-for-inference",
    "title": "13¬† Confidence Interval",
    "section": "13.1 Foundations for Inference",
    "text": "13.1 Foundations for Inference\n Inference Framework \n\nInferential statistics uses sample data to learn about an unknown population.\nIdea: Assume the target population follows some distribution but with unknown parameters.\n\n Assume the population is normally distributed but its mean and/or variance are unknown. \n\nGoal: Learn the unknown parameters of the assumed population distribution.\n\n\n\n\n\n\n\n\nFigure¬†13.1: Sampling from a population\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.2: Relationship between probabiltity and statistical inference\n\n\n\n\n\n\n\nThere are two approaches in parameter learning.\n\nEstimation\nHypothesis Testing"
  },
  {
    "objectID": "infer-var.html#inference-for-population-variances",
    "href": "infer-var.html#inference-for-population-variances",
    "title": "17¬† Inference About Variances",
    "section": "17.2 Inference for Population Variances",
    "text": "17.2 Inference for Population Variances\n\nThe sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance \\(\\sigma^2\\).\n\n\\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\), i.e., \\(E(S^2) = \\sigma^2\\)\n\nThe inference methods for \\(\\sigma^2\\) needs the population to be normal.\n\n\n\n\n\n\n\n‚ùó The methods can work poorly if the normality is violated, even the sample is large."
  },
  {
    "objectID": "infer-var.html#chi-square-chi2-distribution",
    "href": "infer-var.html#chi-square-chi2-distribution",
    "title": "17¬† Inference About Variances",
    "section": "17.2 Chi-Square \\(\\chi^2\\) Distribution",
    "text": "17.2 Chi-Square \\(\\chi^2\\) Distribution\nThe inference for \\(\\sigma^2\\) involves the so called \\(\\chi^2\\) distribution.\n\n\n\n\nParameter: degrees of freedom \\(df\\)\nRight skewed distribution\nDefined over positive numbers\nMore symmetric as \\(df\\) gets larger\nChi-Square Distribution"
  },
  {
    "objectID": "infer-var.html#upper-tail-and-lower-tail-of-chi-square",
    "href": "infer-var.html#upper-tail-and-lower-tail-of-chi-square",
    "title": "17¬† Inference About Variances",
    "section": "17.2 Upper Tail and Lower Tail of Chi-Square",
    "text": "17.2 Upper Tail and Lower Tail of Chi-Square\n\n\n\\(\\chi^2_{\\frac{\\alpha}{2},\\, df}\\) has area to the right of \\(\\alpha/2\\).\n\\(\\chi^2_{1-\\frac{\\alpha}{2},\\, df}\\) has area to the left of \\(\\alpha/2\\).\nIn \\(N(0, 1)\\), \\(z_{1-\\frac{\\alpha}{2}} = -z_{\\frac{\\alpha}{2}}\\). But \\(\\chi^2_{1-\\frac{\\alpha}{2},\\,df} \\ne -\\chi^2_{\\frac{\\alpha}{2},\\,df}\\)."
  },
  {
    "objectID": "infer-var.html#sampling-distribution",
    "href": "infer-var.html#sampling-distribution",
    "title": "17¬† Inference About Variances",
    "section": "17.3 Sampling Distribution",
    "text": "17.3 Sampling Distribution\n\n\nWhen random samples of sizes \\(n_1\\) and \\(n_2\\) have been independently drawn from two normally distributed populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\), the ratio \\[\\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]"
  },
  {
    "objectID": "infer-var.html#alpha100-confidence-interval-for-sigma2",
    "href": "infer-var.html#alpha100-confidence-interval-for-sigma2",
    "title": "17¬† Inference About Variances",
    "section": "17.2 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)",
    "text": "17.2 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} \\right)}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùó The CI for \\(\\sigma^2\\) cannot be expressed as \\((S^2-m, S^2+m)\\) anymore!"
  },
  {
    "objectID": "infer-var.html#example-supermodel-heights",
    "href": "infer-var.html#example-supermodel-heights",
    "title": "17¬† Inference About Variances",
    "section": "17.2 Example: Supermodel Heights",
    "text": "17.2 Example: Supermodel Heights\nListed below are heights (cm) for the simple random sample of 16 female supermodels:\n\nheights <- c(178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176)\n\n\n\n\n\nThe supermodels‚Äô height is normally distributed.\nConstruct a \\(95\\%\\) confidence interval for population standard deviation \\(\\sigma\\).\n\\(n = 16\\), \\(s^2 = 3.4\\), \\(\\alpha = 0.05\\).\n\\(\\chi^2_{\\alpha/2, n-1} = \\chi^2_{0.025, 15} = 27.49\\)\n\\(\\chi^2_{1-\\alpha/2, n-1} = \\chi^2_{0.975, 15} = 6.26\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(95\\%\\) CI for \\(\\sigma\\) is \\(\\small \\left( \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}}} \\right) = \\left( \\sqrt{\\frac{(16-1)(3.4)}{27.49}}, \\sqrt{\\frac{(16-1)(3.4)}{6.26}}\\right) = (1.36, 2.85)\\)"
  },
  {
    "objectID": "infer-var.html#example-computation-in-r",
    "href": "infer-var.html#example-computation-in-r",
    "title": "17¬† Inference About Variances",
    "section": "17.2 Example: Computation in R",
    "text": "17.2 Example: Computation in R\n\nn <- 16\ns2 <- var(heights)\nalpha <- 0.05\n\n## two chi-square critical values\nchi2_right <- qchisq(alpha/2, n - 1, lower.tail = FALSE)\nchi2_left <- qchisq(alpha/2, n - 1, lower.tail = TRUE)\n\n## two bounds of CI for sigma2\nci_sig2_lower <- (n - 1) * s2 / chi2_right\nci_sig2_upper <- (n - 1) * s2 / chi2_left\n\n## two bounds of CI for sigma\n(ci_sig_lower <- sqrt(ci_sig2_lower))\n\n[1] 1.362104\n\n(ci_sig_upper <- sqrt(ci_sig2_upper))\n\n[1] 2.853802"
  },
  {
    "objectID": "infer-var.html#example-contd-testing",
    "href": "infer-var.html#example-contd-testing",
    "title": "17¬† Inference About Variances",
    "section": "17.2 Example Cont‚Äôd: Testing",
    "text": "17.2 Example Cont‚Äôd: Testing\nUse \\(\\alpha = 0.05\\) to test the claim that ‚Äúsupermodels have heights with a standard deviation that is less than \\(\\sigma = 7.5\\) cm for the population of women‚Äù.\n\nStep 1: \\(H_0: \\sigma = \\sigma_0\\) vs.¬†\\(H_1: \\sigma < \\sigma_0\\). Here \\(\\sigma_0 = 7.5\\) cm\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Under \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), a statistic drawn from \\(\\chi^2_{n-1}\\).\n\n\n\n\n\nStep 4-c: This is a left-tailed test. The critical value is \\(\\chi_{1-\\alpha, df}^2 = \\chi_{0.95, 15}^2 = 7.26\\)\nStep-5-c: Reject \\(H_0\\) in favor of \\(H_1\\) if \\(\\chi_{test}^2 < \\chi_{1-\\alpha, df}^2\\). Since \\(0.91 < 7.26\\), we reject \\(H_0\\).\nStep 6: There is sufficient evidence to support the claim that supermodels have heights with a SD that is less than the SD for the population of women.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeights of supermodels vary less than heights of women in the general population."
  },
  {
    "objectID": "infer-var.html#back-to-pooled-t-test",
    "href": "infer-var.html#back-to-pooled-t-test",
    "title": "17¬† Inference About Variances",
    "section": "17.2 Back to Pooled t-Test",
    "text": "17.2 Back to Pooled t-Test\n\nIn a pooled t-test, we assume\n\n \\(n_1 \\ge 30\\) and \\(n_2 \\ge 30\\). If not, we assume that both samples are drawn from normal populations. \n \\(\\sigma_1 = \\sigma_2\\) \n\nUse QQ-plot (and normality tests, Anderson, Shapiro, etc) to check the assumption of normal distribution.\nWe learn to check the assumption \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "infer-var.html#f-distribution",
    "href": "infer-var.html#f-distribution",
    "title": "17¬† Inference About Variances",
    "section": "17.3 F Distribution",
    "text": "17.3 F Distribution\n\nWe use ** \\(F\\) distribution ** for the inference about two population variances.\n\n\n\n\nTwo parameters: \\(df_1\\), \\(df_2\\)\nRight skewed distribution\nDefined over positive numbers\nR Shiny app: F Distribution"
  },
  {
    "objectID": "infer-var.html#upper-and-lower-tail-of-f-distribution",
    "href": "infer-var.html#upper-and-lower-tail-of-f-distribution",
    "title": "17¬† Inference About Variances",
    "section": "17.3 Upper and Lower Tail of F Distribution",
    "text": "17.3 Upper and Lower Tail of F Distribution\n\nWe denote \\(F_{\\alpha, \\, df_1, \\, df_2}\\) as the \\(F\\) quantile so that \\(P(F_{df_1, df_2} > F_{\\alpha, \\, df_1, \\, df_2}) = \\alpha\\)."
  },
  {
    "objectID": "infer-var.html#sampling-distribution-1",
    "href": "infer-var.html#sampling-distribution-1",
    "title": "17¬† Inference About Variances",
    "section": "17.10 Sampling Distribution",
    "text": "17.10 Sampling Distribution\n\n\nWhen random samples of sizes \\(n_1\\) and \\(n_2\\) have been independently drawn from two normally distributed populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\), the ratio \\[\\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]"
  },
  {
    "objectID": "infer-var.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "href": "infer-var.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "title": "17¬† Inference About Variances",
    "section": "17.3 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)",
    "text": "17.3 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\color{blue}{\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, \\, n_1 - 1, \\, n_2 - 1}} \\right)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n‚ùó The CI for \\(\\sigma_1^2 / \\sigma_2^2\\) cannot be expressed as \\(\\left(\\frac{s_1^2}{s_2^2}-m, \\frac{s_1^2}{s_2^2} + m\\right)\\) anymore!"
  },
  {
    "objectID": "infer-var.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "href": "infer-var.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "title": "17¬† Inference About Variances",
    "section": "17.3 F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)",
    "text": "17.3 F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)\n\nStep 1: right-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 \\le \\sigma_2 \\\\ &H_1: \\sigma_1 > \\sigma_2 \\end{align}\\)  and two-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\) \nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Under \\(H_0\\), \\(\\sigma_1 = \\sigma_2\\), and the test statistic is \\[\\small F_{test} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} = \\frac{s_1^2}{s_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\nStep 4-c:\n\nRight-tailed:  \\(F_{\\alpha, \\, n_1-1, \\, n_2-1}\\) .\nTwo-tailed:  \\(F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\) \n\nStep 5-c:\n\nRight-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha, \\, n_1-1, \\, n_2-1}\\).\nTwo-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{test} \\le F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\)"
  },
  {
    "objectID": "infer-var.html#back-to-the-weight-loss-example",
    "href": "infer-var.html#back-to-the-weight-loss-example",
    "title": "17¬† Inference About Variances",
    "section": "17.3 Back to the Weight Loss Example",
    "text": "17.3 Back to the Weight Loss Example\n\n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\nAssumptions:\n\n \\(\\sigma_1 = \\sigma_2\\) \nThe weight loss for both groups are normally distributed."
  },
  {
    "objectID": "infer-var.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "href": "infer-var.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "title": "17¬† Inference About Variances",
    "section": "17.3 Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)",
    "text": "17.3 Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)\n\n\n\n\n\\(n_1 = 10\\), \\(s_1 = 0.5 \\, lb\\)\n\\(n_2 = 10\\), \\(s_2 = 0.7 \\, lb\\)\nStep 1: \\(\\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\)\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: The test statistic is \\(F_{test} = \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 4-c: This is a two-tailed test, the critical value is \\(F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\) or \\(F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\).\nStep 5-c: Is \\(F_{test} > 4.03\\) or \\(F_{test} < 0.25\\)? No.¬†\nStep 6: The evidence is not sufficient to reject the claim that \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "infer-var.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "href": "infer-var.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "title": "17¬† Inference About Variances",
    "section": "17.3 Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)",
    "text": "17.3 Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\n\\(\\small F_{\\alpha/2, \\, df_1, \\, df_2} = F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\)\n\\(\\small F_{1-\\alpha/2, \\, df_1, \\, df_2} = F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\)\n\\(\\small \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\)\nThe 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\small \\begin{align} &\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, df_1, \\, df_2}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, df_1, \\, df_2}} \\right) \\\\ &= \\left( \\frac{0.51}{4.03}, \\frac{0.51}{0.25} \\right) = \\left(0.127, 2.04\\right)\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are 95% confident that the ratio \\(\\sigma_1^2 / \\sigma_2^2\\) is between 0.127 and 2.04."
  },
  {
    "objectID": "infer-var.html#implementing-f-test-in-r",
    "href": "infer-var.html#implementing-f-test-in-r",
    "title": "17¬† Inference About Variances",
    "section": "17.3 Implementing F-test in R",
    "text": "17.3 Implementing F-test in R\n\n\n\n\nn1 <- 10; n2 <- 10\ns1 <- 0.5; s2 <- 0.7\nalpha <- 0.05\n\n## 95% CI for sigma_1^2 / sigma_2^2\nf_small <- qf(p = alpha / 2, \n              df1 = n1 - 1, df2 = n2 - 1, \n              lower.tail = TRUE)\nf_big <- qf(p = alpha / 2, \n            df1 = n1 - 1, df2 = n2 - 1, \n            lower.tail = FALSE)\n\n## lower bound\n(s1 ^ 2 / s2 ^ 2) / f_big\n\n[1] 0.1267275\n\n## upper bound\n(s1 ^ 2 / s2 ^ 2) / f_small\n\n[1] 2.054079\n\n\n\n## Testing sigma_1 = sigma_2\n(test_stats <- s1 ^ 2 / s2 ^ 2)\n\n[1] 0.5102041\n\n(cri_val_big <- qf(p = alpha/2, \n                   df1 = n1 - 1, \n                   df2 = n2 - 1, \n                   lower.tail = FALSE))\n\n[1] 4.025994\n\n(cri_val_small <- qf(p = alpha/2, \n                     df1 = n1 - 1, \n                     df2 = n2 - 1, \n                     lower.tail = TRUE))\n\n[1] 0.2483859\n\n# var.test(x, y, alternative = \"two.sided\")"
  },
  {
    "objectID": "infer-var.html#inference-for-one-population-variance",
    "href": "infer-var.html#inference-for-one-population-variance",
    "title": "17¬† Inference About Variances",
    "section": "17.1 Inference for One Population Variance",
    "text": "17.1 Inference for One Population Variance\n Why Inference for Population Variances? \n\nWe want to know if \\(\\sigma_1 = \\sigma_2\\), because the method we use depends on whether or not this is true.\n\n\n\n\n\n\n\nWhich test did we learn that needs \\(\\sigma_1 = \\sigma_2\\)?\n\n\n\n\nTwo-sample pooled test\n\n\n\n\nIn some situations, we care about variation!\n\n\n\n\n\n The variation in potency of drugs: affects patients‚Äô health\n\n\n\n\n\n\n\n\n\n\n\n\n The variance of stock prices : the higher the variance, the riskier the investment\n\n\n\n\n\n\n\n\n\n\n\n\n\n Inference for Population Variances \n\nThe sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance, \\(\\sigma^2\\).\n\\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\).\n\nThis means \\(E(S^2) = \\sigma^2\\).\n\nThe inference methods for \\(\\sigma^2\\) require the population to be normal.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe inference methods can work poorly if normality is violated, even if the sample is large.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Chi-Square \\(\\chi^2\\) Distribution \n\nThe inference for \\(\\sigma^2\\) involves the so called \\(\\chi^2\\) distribution.\n\n\n\n\n\nParameter: degrees of freedom \\(df\\)\nRight-skewed distribution\nDefined over positive numbers\nMore symmetric as \\(df\\) gets larger\nChi-Square Distribution\n\n\n\n\n\n\n\nFigure¬†17.1: Illustration of \\(\\chi^2\\) distributions with varying degrees of freedom\n\n\n\n\n\n\n Upper Tail and Lower Tail of Chi-Square \n\n\\(\\chi^2_{\\frac{\\alpha}{2},\\, df}\\) has area to the right of \\(\\alpha/2\\).\n\\(\\chi^2_{1-\\frac{\\alpha}{2},\\, df}\\) has area to the left of \\(\\alpha/2\\).\nIn \\(N(0, 1)\\), \\(z_{1-\\frac{\\alpha}{2}} = -z_{\\frac{\\alpha}{2}}\\), but \\(\\chi^2_{1-\\frac{\\alpha}{2},\\,df} \\ne -\\chi^2_{\\frac{\\alpha}{2},\\,df}\\).\n\n\n\n\n\n\nFigure¬†17.2: Illustration of \\(\\alpha/2\\) significance levels for \\(\\chi^2_{df}\\) distribution\n\n\n\n\n Sampling Distribution \n\nWhen a random sample of size \\(n\\) is from \\(\\color{red}{N(\\mu, \\sigma^2)}\\), \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\]\nReminder: The inference method for \\(\\sigma^2\\) introduced here can work poorly if the normality assumption is violated, even for large samples.\n\n \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\) \n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} \\right)}}\\]\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe CI for \\(\\sigma^2\\) cannot be expressed as \\((S^2-m, S^2+m)\\) anymore!\n\n\n\n\n Example: Supermodel Heights \n\n\n\nListed below are heights (cm) for the simple random sample of 16 female supermodels.\n\n\nheights <- c(178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176)\n\n\nThe supermodels‚Äô heights are normally distributed.\nConstruct a \\(95\\%\\) confidence interval for population standard deviation \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 16\\), \\(s^2 = 3.4\\), \\(\\alpha = 0.05\\).\n\\(\\chi^2_{\\alpha/2, n-1} = \\chi^2_{0.025, 15} = 27.49\\)\n\\(\\chi^2_{1-\\alpha/2, n-1} = \\chi^2_{0.975, 15} = 6.26\\)\nThe \\(95\\%\\) CI for \\(\\sigma\\) is \\(\\small \\left( \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}}} \\right) = \\left( \\sqrt{\\frac{(16-1)(3.4)}{27.49}}, \\sqrt{\\frac{(16-1)(3.4)}{6.26}}\\right) = (1.36, 2.85)\\)\n\n Computation in R \n\nn <- 16\ns2 <- var(heights)\nalpha <- 0.05\n\n## two chi-square critical values\nchi2_right <- qchisq(alpha/2, n - 1, lower.tail = FALSE)\nchi2_left <- qchisq(alpha/2, n - 1, lower.tail = TRUE)\n\n## two bounds of CI for sigma2\nci_sig2_lower <- (n - 1) * s2 / chi2_right\nci_sig2_upper <- (n - 1) * s2 / chi2_left\n\n## two bounds of CI for sigma\n(ci_sig_lower <- sqrt(ci_sig2_lower))\n\n[1] 1.362104\n\n(ci_sig_upper <- sqrt(ci_sig2_upper))\n\n[1] 2.853802\n\n\n Testing \n\nUse \\(\\alpha = 0.05\\) to test the claim that ‚Äúsupermodels have heights with a standard deviation that is less than the standard deviation, \\(\\sigma = 7.5\\) cm, for the population of women‚Äù.\n\n Step 1 \n\n\\(H_0: \\sigma = \\sigma_0\\) vs.¬†\\(H_1: \\sigma < \\sigma_0\\), where \\(\\sigma_0 = 7.5\\) cm.\n\n Step 2 \n\n\\(\\alpha = 0.05\\)\n\n Step 3 \n\nUnder \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), drawn from \\(\\chi^2_{n-1}\\).\n\n\n\n Step 4-c \n\nThis is a left-tailed test.\nThe critical value is \\(\\chi_{1-\\alpha, df}^2 = \\chi_{0.95, 15}^2 = 7.26\\)\n\n Step 5-c \n\nReject \\(H_0\\) in favor of \\(H_1\\) if \\(\\chi_{test}^2 < \\chi_{1-\\alpha, df}^2\\).\nSince \\(0.91 < 7.26\\), we reject \\(H_0\\).\n\n Step 6 \n\nThere is sufficient evidence to support the claim that supermodels have heights with a SD that is less than the SD for the population of all women.\n\n\n\n\n\n\n\n\nFigure¬†17.3: \\(\\chi_{0.95,15}^2\\) distribution for supermodel heights\n\n\n\n\n\n\n\nWe conclude that the heights of supermodels vary less than heights of women in the general population.\n\n\n Back to Pooled t-Test \n\nIn a pooled t-test, we assume\n\n \\(n_1 \\ge 30\\) and \\(n_2 \\ge 30\\) or that both samples are drawn from normal populations. \n \\(\\sigma_1 = \\sigma_2\\) \n\nWe can use a QQ-plot (and normality tests, Anderson, Shapiro, etc.) to check the assumption of a normal distribution.\nWe will now learn how to check the assumption \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "infer-var.html#inference-for-comparing-two-population-variances",
    "href": "infer-var.html#inference-for-comparing-two-population-variances",
    "title": "17¬† Inference About Variances",
    "section": "17.2 Inference for Comparing Two Population Variances",
    "text": "17.2 Inference for Comparing Two Population Variances\n F Distribution \n\nWe use the \\(F\\) distribution for inference about two population variances.\n\n\n\n\n\nTwo parameters: \\(df_1\\), \\(df_2\\)\nRight-skewed distribution\nDefined over positive numbers\nR Shiny app: F Distribution\n\n\n\n\n\n\n\nFigure¬†17.4: F distributions with different parameters\n\n\n\n\n\n\n Upper and Lower Tail of F Distribution \n\nWe denote \\(F_{\\alpha, \\, df_1, \\, df_2}\\) as the \\(F\\) quantile such that \\(P(F_{df_1, df_2} > F_{\\alpha, \\, df_1, \\, df_2}) = \\alpha\\).\n\n\n\n\n\n\nFigure¬†17.5: Illustration of \\(\\alpha/2\\) significance levels for \\(F_{df_1, df_2}\\) distribution\n\n\n\n\n Sampling Distribution \n\nWhen random samples of sizes \\(n_1\\) and \\(n_2\\) have been independently drawn from two normally distributed populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\), the ratio \\[\\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\n\n \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\) \n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, \\, n_1 - 1, \\, n_2 - 1}} \\right)}}\\]\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe CI for \\(\\sigma_1^2 / \\sigma_2^2\\) cannot be expressed as \\(\\left(\\frac{s_1^2}{s_2^2}-m, \\frac{s_1^2}{s_2^2} + m\\right)\\) anymore!\n\n\n\n\n F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) \n Step 1 \n\nRight-tailed:  \\(\\small \\begin{align} &H_0: \\sigma_1 \\le \\sigma_2 \\\\ &H_1: \\sigma_1 > \\sigma_2 \\end{align}\\) \nTwo-tailed:  \\(\\small \\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\) \n\n Step 2 \n\n\\(\\alpha = 0.05\\)\n\n Step 3 \n\nUnder \\(H_0\\), \\(\\sigma_1 = \\sigma_2\\), and the test statistic is \\[\\small F_{test} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} = \\frac{s_1^2}{s_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\n\n Step 4-c \n\nRight-tailed:  \\(F_{\\alpha, \\, n_1-1, \\, n_2-1}\\) .\nTwo-tailed:  \\(F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\) \n\n Step 5-c \n\nRight-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha, \\, n_1-1, \\, n_2-1}\\).\nTwo-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{test} \\le F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\)\n\n\n Example: Weight Loss \n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months.\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\nAssumptions:\n\n \\(\\sigma_1 = \\sigma_2\\) \nThe weight loss for both groups are normally distributed.\n\n\n Check if \\(\\sigma_1 = \\sigma_2\\) \n\n\\(n_1 = 10\\), \\(s_1 = 0.5 \\, lb\\)\n\\(n_2 = 10\\), \\(s_2 = 0.7 \\, lb\\)\n\n Step 1 \n\n\\(\\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\)\n\n Step 2 \n\n\\(\\alpha = 0.05\\)\n\n Step 3 \n\nThe test statistic is \\(F_{test} = \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\).\n\n Step 4-c \n\nThis is a two-tailed test.\nThe critical value is \\(F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\) or \\(F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\).\n\n\n\n\n\n\nFigure¬†17.6: F Distribution for Weight Loss Example\n\n\n\n\n Step 5-c \n\nIs \\(F_{test} > 4.03\\) or \\(F_{test} < 0.25\\)?\n\nNo.¬†\n\n\n Step 6 \n\nThe evidence is not sufficient to reject the claim that \\(\\sigma_1 = \\sigma_2\\).\n\n 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) \n\n\n\n\\(\\small F_{\\alpha/2, \\, df_1, \\, df_2} = F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\)\n\\(\\small F_{1-\\alpha/2, \\, df_1, \\, df_2} = F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\)\n\\(\\small \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\)\nThe 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\small \\begin{align} &\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, df_1, \\, df_2}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, df_1, \\, df_2}} \\right) \\\\ &= \\left( \\frac{0.51}{4.03}, \\frac{0.51}{0.25} \\right) = \\left(0.127, 2.04\\right)\\end{align}\\]\n\n\n\n\n\n\n\nFigure¬†17.7: F Distribution for significance level 0.05\n\n\n\n\n\n\n\nWe are 95% confident that the ratio \\(\\sigma_1^2 / \\sigma_2^2\\) is between 0.127 and 2.04.\nBecause 1 is included in this interval, it leads to the same conclusion as the F test.\n\n Implementing F-test in R \n\n\n\n\nn1 <- 10; n2 <- 10\ns1 <- 0.5; s2 <- 0.7\nalpha <- 0.05\n\n## 95% CI for sigma_1^2 / sigma_2^2\nf_small <- qf(p = alpha / 2, \n              df1 = n1 - 1, df2 = n2 - 1, \n              lower.tail = TRUE)\nf_big <- qf(p = alpha / 2, \n            df1 = n1 - 1, df2 = n2 - 1, \n            lower.tail = FALSE)\n\n## lower bound\n(s1 ^ 2 / s2 ^ 2) / f_big\n\n[1] 0.1267275\n\n## upper bound\n(s1 ^ 2 / s2 ^ 2) / f_small\n\n[1] 2.054079\n\n\n\n\n\n\n## Testing sigma_1 = sigma_2\n(test_stats <- s1 ^ 2 / s2 ^ 2)\n\n[1] 0.5102041\n\n(cri_val_big <- qf(p = alpha/2, \n                   df1 = n1 - 1, \n                   df2 = n2 - 1, \n                   lower.tail = FALSE))\n\n[1] 4.025994\n\n(cri_val_small <- qf(p = alpha/2, \n                     df1 = n1 - 1, \n                     df2 = n2 - 1, \n                     lower.tail = TRUE))\n\n[1] 0.2483859\n\n# var.test(x, y, alternative = \"two.sided\")"
  },
  {
    "objectID": "infer-ht.html#introduction",
    "href": "infer-ht.html#introduction",
    "title": "15¬† Hypothesis Testing",
    "section": "15.1 Introduction",
    "text": "15.1 Introduction\n What is Hypothesis Testing? \n\nA hypothesis is a claim or statement about a property of a population, often the value of a population parameter.\n\n The mean body temperature of humans is less than \\(98.6^{\\circ}\\) F, or \\(\\mu < 98.6\\). \n Marquette students‚Äô IQ scores has standard deviation equal to 15, or \\(\\sigma = 15\\). \n\n\n\n\nThe null hypothesis, \\((H_0)\\), is a statement that the value of a parameter is\n\nequal to some claim value\nthe negation of the alternative hypothesis\noften a skeptical perspective to be tested\n\nThe alternative hypothesis, \\((H_1\\) or \\(H_a)\\), is a claim that the parameter is less than, greater than or not equal to some value.\n\nIt is usually our research hypothesis of some new scientific theory or finding.\n\n\n\n\n\n\n\n\nAre these \\(H_0\\) or \\(H_1\\) claims?\n\n\n\n\n The percentage of Marquette female students loving Japanese food is equal to 80%.\n On average, Marquette students consume less than 3 drinks per week. \n\n\n\n\nHypothesis testing 1 is a procedure to decide whether or not to reject \\(H_0\\) based on how much evidence there is against \\(H_0\\).\n\nIf the evidence is strong enough, we reject \\(H_0\\) in favor of \\(H_1\\).\n\n\n[1] Null Hypothesis Statistical Testing (NHST), statistical testing or test of significance.\n\n Example \n\n\n\nA person is charged with a crime.\n\nA jury decides whether the person is guilty or not.\nThe accused is assumed to be innocent until the jury declares otherwise.\nIf overwhelming evidence of the person‚Äôs guilt can be shown, then the jury is expected to declare the person guilty; otherwise, the person is considered not guilty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat should \\(H_0\\) and \\(H_1\\) be for this example?\n\n\\(H_0:\\) The person is  not guilty  üôÇ\n\\(H_1:\\) The person is  guilty  üòü\n\nThe evidence is  photos, videos, witnesses, fingerprints, DNA, etc. \nThe decision rule is the  jury‚Äôs voting .\nThe conclusion is the verdict  ‚Äúguilty‚Äù  or  ‚ÄúNOT enough evidence to convict‚Äù ."
  },
  {
    "objectID": "infer-ht.html#example-is-the-new-treatment-effective",
    "href": "infer-ht.html#example-is-the-new-treatment-effective",
    "title": "15¬† Hypothesis Testing",
    "section": "15.3 Example: Is the New Treatment Effective?",
    "text": "15.3 Example: Is the New Treatment Effective?\n\n\n\nA population of patients with hypertension is normal and has mean blood pressure (BP) of 150.\nAfter 6 months of treatment, the BP of 25 patients from this population was recorded.\n\n\\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\n\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Step-by-Step \n Step 0: Check Method Assumptions \n\n A population of hypertension group is normal .\n\n Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \n\nThe claim that the new treatment is effective in reducing BP means the mean BP is less than 150, which is an \\(H_1\\) claim.\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu < 150 \\end{align}\\) \n\n\n Step 2: Set the Significance Level \\(\\alpha\\) \n\nLet‚Äôs set \\(\\alpha= 0.05\\).\nThis means we are asking, ‚ÄúIs there a sufficient evidence at \\(\\alpha= 0.05\\) that the new treatment is effective?‚Äù\n\n Step 3: Calculate the Test Statistic \n\n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)\n\n Step 4-c: Find the Critical Value \n\n The critical value is \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) \n\n Step 5-c: Draw a Conclusion Using Critical Value \n\n \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\) \n \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) \n We reject \\(H_0\\) if \\(t_{test} < -t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = -2.55 < -1.711 = -t_{\\alpha, n-1}\\), we reject \\(H_0\\).\n\n Step 4-p: Find the P-Value \n\nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T < t_{test})=P(T < -2.55) =\\) 0.01 \n\n Step 5-p: Draw a Conclusion Using P-Value Method \n\n We reject \\(H_0\\) if the \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(= 0.01 < 0.05 = \\alpha\\), we reject \\(H_0\\).\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n There is sufficient evidence to support the claim that the new treatment is effective. \n\n\n Example Calculation in R \n\nBelow is a demonstration of how to work through this example using R.\n\n\n## create objects for any information we have\nalpha <- 0.05; mu_0 <- 150; \nx_bar <- 147.2; s <- 5.5; n <- 25\n\n## Test statistic\n(t_test <- (x_bar - mu_0) / (s / sqrt(n))) \n\n[1] -2.545455\n\n## Critical value\n(t_cri <- qt(alpha, df = n - 1, lower.tail = TRUE)) \n\n[1] -1.710882\n\n## p-value\n(p_val <- pt(t_test, df = n - 1, lower.tail = TRUE)) \n\n[1] 0.008878158"
  },
  {
    "objectID": "infer-ht.html#example-two-tailed-z-test",
    "href": "infer-ht.html#example-two-tailed-z-test",
    "title": "15¬† Hypothesis Testing",
    "section": "15.4 Example: Two-tailed z-test",
    "text": "15.4 Example: Two-tailed z-test\n\n\n\nThe milk price of a gallon of 2% milk is normally distributed with standard deviation of $0.10.\nLast week the mean price of a gallon of milk was 2.78. This week, based on a sample of size 25, the sample mean price of a gallon of milk was \\(\\overline{x} = 2.80\\).\nUnder \\(\\alpha = 0.05\\), determine if the mean price is different this week.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Step-by-Step \n Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \n\nThis is an \\(H_1\\) claim:  \\(\\small \\begin{align}&H_0: \\mu = 2.78 \\\\ &H_1: \\mu \\ne 2.78 \\end{align}\\) \n\n Step 2: Set the Significance Level \\(\\alpha\\) \n\n \\(\\small \\alpha = 0.05\\) \n\n Step 3: Calculate the Test Statistic \n\n \\(\\small z_{test} = \\frac{\\overline{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{2.8 - 2.78}{0.1/\\sqrt{25}} = 1.00\\) \n\n Step 4-c: Find the Critical Value \n\n \\(\\small z_{0.05/2} = 1.96\\). \n\n Step 5-c: Draw a Conclusion Using Critical Value \n\nThis is a two-tailed test, and we reject \\(H_0\\) if \\(|z_{test}| > z_{\\alpha/2}\\). Since \\(\\small |z_{test}| = 1 < 1.96 = z_{\\alpha/2}\\), we DO NOT reject \\(H_0\\).\n\n Step 4-p: Find the P-Value \n\nThis is a two-tailed test, and the test statistic is on the right \\((> 0)\\), so the \\(p\\)-value is \\(2P(Z > z_{test})=\\) 0.317 .\n\n Step 5-p: Draw a Conclusion Using P-Value Method \n\n We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(= 0.317 > 0.05 = \\alpha\\), we DO NOT reject \\(H_0\\).\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n There is insufficient evidence to support the claim that this week the mean price of milk is different from the price last week. \n\n\n\n\n\n\nFigure¬†15.8: Illustration of Critical Value and P-Value methods\n\n\n\n\n\n Calculation in R \n\nBelow is an example of how to perform the two-tailed Z-test in R.\n\n\n## create objects to be used\nalpha <- 0.05; mu_0 <- 2.78; \nx_bar <- 2.8; sigma <- 0.1; n <- 25\n\n## Test statistic\n(z_test <- (x_bar - mu_0) / (sigma / sqrt(n))) \n\n[1] 1\n\n## Critical value\n(z_crit <- qnorm(alpha/2, lower.tail = FALSE)) \n\n[1] 1.959964\n\n## p-value\n(p_val <- 2 * pnorm(z_test, lower.tail = FALSE)) \n\n[1] 0.3173105"
  },
  {
    "objectID": "infer-twomean.html#introduction",
    "href": "infer-twomean.html#introduction",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.1 Introduction",
    "text": "16.1 Introduction\n Why Compare Two Populations? \n\nOften we are faced with a comparison of parameters from different populations.\n\n Comparing the mean annual income for Male and Female groups. \n Testing if a diet used for losing weight is effective from Placebo samples and New Diet samples. \n\nIf these two samples are drawn from populations with means, \\(\\mu_1\\) and \\(\\mu_2\\) respectively, the testing problem can be formulated as  \\[\\begin{align}\n&H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 > \\mu_2\n\\end{align}\\] \n\n\\(\\mu_1\\): male mean annual income; \\(\\mu_2\\): female mean annual income\n\\(\\mu_1\\): mean weight loss from the New Diet group; \\(\\mu_2\\): mean weight loss from the Placebo group\n\n\n\n Dependent and Independent Samples \n\nThe two samples collected can be independent or dependent.\n\n\n\n\nTwo samples are dependent or matched pairs if the sample values are matched, where the matching is based on some inherent relationship.\n\n Height data of fathers and daughters, where the height of each dad is matched with the height of his daughter. \n Weights of subjects measure before and after some diet treatment, where the subjects are the same both before and after measurements. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dependent Samples (Matched Pairs) \n\nSubject 1 may refer to\n\nthe same person with two measurements (before and after)\nthe first matched pair (dad-daughter)\n\n\n\n\n\n\n\nSubject\n(Dad) Before\n(Daughter) After\n\n\n\n\n1\n\\(x_{b1}\\)\n\\(x_{a1}\\)\n\n\n2\n\\(x_{b2}\\)\n\\(x_{a2}\\)\n\n\n3\n\\(x_{b3}\\)\n\\(x_{a3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{bn}\\)\n\\(x_{an}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Independent Samples \n\n\n\nTwo samples are independent if the sample values from one population are not related to the sample values from the other.\n\n Salary samples of men and women, where the two samples are drawn independently from the male and female groups. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject 1 of the Group 1 has nothing to do with the Subject 1 of the Group 2.\n\n\n\n\n\n\n\n\n\n\nSubject of Group 1 (Male)\nMeasurement of Group 1\nSubject of Group 2 (Female)\nMeasurement of Group 2\n\n\n\n\n1\n\\(x_{11}\\)\n1\n\\(x_{21}\\)\n\n\n2\n\\(x_{12}\\)\n2\n\\(x_{22}\\)\n\n\n3\n\\(x_{13}\\)\n3\n\\(x_{23}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n_1\\)\n\\(x_{1n_1}\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\\(n_2\\)\n\\(x_{2n_2}\\)\n\n\n\n\n Inference from Two Samples \n\nThe statistical methods are different for these two types of samples.\nThe good news is the concepts of confidence intervals and hypothesis testing for one population can be applied to two-population cases.\n\\(\\text{CI = point estimate} \\pm \\text{margin of error (E)}\\)\n\ne.g., \\(\\overline{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\nMargin of error = critical value \\(\\times\\) standard error of the point estimator\nThe 6 testing steps are the same, and both critical value and \\(p\\)-value method can be applied too\n\ne.g., \\(t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}}\\)"
  },
  {
    "objectID": "infer-twomean.html#inferences-about-two-means-dependent-samples-matched-pairs",
    "href": "infer-twomean.html#inferences-about-two-means-dependent-samples-matched-pairs",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.2 Inferences About Two Means: Dependent Samples (Matched Pairs)",
    "text": "16.2 Inferences About Two Means: Dependent Samples (Matched Pairs)\n Hypothesis Testing for Dependent Samples \n\n\n\n\n\n\nTo analyze a paired data set, we can simply analyze the differences!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(x_1\\)\n\\(x_2\\)\nDifference \\(d = x_1 - x_2\\)\n\n\n\n\n1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(\\color{red}{d_1}\\)\n\n\n2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(\\color{red}{d_2}\\)\n\n\n3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(\\color{red}{d_3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\color{red}{\\vdots}\\)\n\n\n\\(n\\)\n\\(x_{1n}\\)\n\\(x_{2n}\\)\n\\(\\color{red}{d_n}\\)\n\n\n\n\n\n\\(\\mu_d = \\mu_1 - \\mu_2\\)\n \\(\\begin{align} & H_0: \\mu_1 - \\mu_2 = 0 \\iff \\mu_d = 0 \\\\ & H_1: \\mu_1 - \\mu_2 > 0 \\iff \\mu_d > 0 \\\\ & H_1: \\mu_1 - \\mu_2 < 0 \\iff \\mu_d < 0 \\\\ & H_1: \\mu_1 - \\mu_2 \\ne 0 \\iff \\mu_d \\ne 0 \\end{align}\\) \n\n\n\n\n\n\n\n\n\nThe point estimate of \\(\\mu_1 - \\mu_2\\) is \\(\\overline{x}_1 - \\overline{x}_2 = \\overline{d}\\).\n\n\n\n\n\n\n\n Inference for Paired Data \n\nRequirements: the sample differences \\(\\color{blue}{d_i}\\)s are\n\nfrom a random sample\nfrom a normal distribution and/or \\(n > 30\\)\n\nThis can be confirmed using the QQ-plot of \\(d_i\\)s.\n\n\nFollow the same procedure as the one-sample \\(t\\)-test!\nThe test statistic is \\(\\color{blue}{t_{test} = \\frac{\\overline{d}-0}{s_d/\\sqrt{n}}} \\sim T_{n-1}\\) under \\(H_0\\) where \\(\\overline{d}\\) and \\(s_d\\) are the mean and SD of the difference samples \\((d_1, d_2, \\dots, d_n)\\).\nThe critical value is either \\(t_{\\alpha, n-1}\\) and \\(t_{\\alpha/2, n-1}\\) depending on if it is a one-tailed or two-tailed test.\nBelow is a table that summarizes information necessary to make inferences about paired data.\n\n\n\n\n\n\n\n\n\nPaired \\(t\\)-test\nTest Statistic\nConfidence Interval for \\(\\mu_d = \\mu_1 - \\mu_2\\)\n\n\n\n\n\\(\\sigma_d\\) is unknown\n\\(\\large t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}}\\)\n\\(\\large \\overline{d} \\pm t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\)\n\n\n\n\nThe test for matched pairs is called a paired \\(t\\)-test.\n\n\n Example \n\nConsider a capsule used to reduce blood pressure (BP) for individuals with hypertension.\nA sample of 10 individuals with hypertension takes the medicine for 4 weeks.\nDoes the data provide sufficient evidence that the treatment is effective in reducing BP?\n\n\n\n\n\n\n\n\n\n\nSubject\nBefore \\((x_b)\\)\nAfter \\((x_a)\\)\nDifference \\(d = x_b - x_a\\)\n\n\n\n\n1\n143\n124\n19\n\n\n2\n153\n129\n24\n\n\n3\n142\n131\n11\n\n\n4\n139\n145\n-6\n\n\n5\n172\n152\n20\n\n\n6\n176\n150\n26\n\n\n7\n155\n125\n30\n\n\n8\n149\n142\n7\n\n\n9\n140\n145\n-5\n\n\n10\n169\n160\n9\n\n\n\n\n\\(\\overline{d} = 13.5\\), \\(s_d= 12.48\\).\n\\(\\mu_1 =\\) Mean Before, \\(\\mu_2 =\\) Mean After, and \\(\\mu_d = \\mu_1 - \\mu_2\\).\n\n Step 1 \n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\iff \\mu_d = 0\\\\ &H_1: \\mu_1 > \\mu_2 \\iff \\mu_d > 0 \\end{align}\\) \n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}} = \\frac{13.5}{12.48/\\sqrt{10}} = 3.42\\) \n\n Step 4-c \n\n \\(t_{\\alpha, n-1} = t_{0.05, 9} = 1.833\\).\n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} > t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = 3.42 > 1.833 = t_{\\alpha, n-1}\\), we reject \\(H_0\\).\n\n Step 6 \n\n There is sufficient evidence to support the claim that the drug is effective in reducing blood pressure. \n\n\n\n\n\n\nFigure¬†16.1: Illustration of right-tailed test for blood pressure example\n\n\n\n\n\nThe 95% CI for \\(\\mu_d = \\mu_1 - \\mu_2\\) is \\[\\begin{align}\\overline{d} \\pm t_{\\alpha/2, df} \\frac{s_d}{\\sqrt{n}} &= 13.5 \\pm t_{0.025, 9}\\frac{12.48}{\\sqrt{10}}\\\\ &= 13.5 \\pm 8.927 \\\\ &= (4.573, 22.427).\\end{align}\\]\nWe are 95% confident that the mean difference in blood pressure is between 4.57 and 22.43.\nSince the interval does NOT include 0, it leads to the same conclusion as rejection of \\(H_0\\).\n\n\n Two-Sample Paired T-Test in R \n\nBelow is the same data as in the previous hypertension example.\nThese figures illustrate how to perform the hypothesis testing for paired data in R.\n\n\n\n\npair_data\n\n   before after\n1     143   124\n2     153   129\n3     142   131\n4     139   145\n5     172   152\n6     176   150\n7     155   125\n8     149   142\n9     140   145\n10    169   160\n\n(d <- pair_data$before - pair_data$after)\n\n [1] 19 24 11 -6 20 26 30  7 -5  9\n\n(d_bar <- mean(d))\n\n[1] 13.5\n\n\n\n\n\n\n(s_d <- sd(d))\n\n[1] 12.48332\n\n## t_test\n(t_test <- d_bar/(s_d/sqrt(length(d))))\n\n[1] 3.419823\n\n## t_cv\nqt(p = 0.95, df = length(d) - 1)\n\n[1] 1.833113\n\n## p_value\npt(q = t_test, df = length(d) - 1, \n   lower.tail = FALSE)\n\n[1] 0.003815036\n\n\n\n\n\nBelow is an example of how to calculate the confidence interval for the change in blood pressure.\n\n\nd_bar + c(-1, 1) * qt(p = 0.975, df = length(d) - 1) * (s_d / sqrt(length(d))) ## CI\n\n[1]  4.569969 22.430031\n\n\n\nWe can see that performing these calculations in R leads us to the same conclusions we previously made.\nThere is also a t-test function in R.\n\n\n## t.test() function\nt.test(x = pair_data$before, y = pair_data$after,\n       alternative = \"greater\", mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pair_data$before and pair_data$after\nt = 3.4198, df = 9, p-value = 0.003815\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 6.263653      Inf\nsample estimates:\nmean difference \n           13.5 \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nBe careful about the one-sided CI listed in the table above! We should use the two-sided CI!"
  },
  {
    "objectID": "infer-twomean.html#inference-about-two-means-independent-samples",
    "href": "infer-twomean.html#inference-about-two-means-independent-samples",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.3 Inference About Two Means: Independent Samples",
    "text": "16.3 Inference About Two Means: Independent Samples\n Compare Population Means: Independent Samples \n\nExamples of independent data:\n\nWhether stem cells can improve heart function.\nThe relationship between pregnant women‚Äôs smoking habits and newborns‚Äô weights.\nWhether one variation of an exam is harder than another variation.\n\n\n\n\n\n\n\n\n\nFigure¬†16.2: Boxplots for two variations of an exam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \n\nRequirements:\n\nThe two samples are independent.\nBoth samples are random samples.\n\\(n_1 > 30\\), \\(n_2 > 30\\) and/or both samples are from a normally distributed population.\n\nWe are interested in whether the two population means, \\(\\mu_1\\) and \\(\\mu_2\\), are equal or if one is larger than the other.\n\\(H_0: \\mu_1 = \\mu_2\\)\n\nThis is equivalent to testing if their difference is zero.\n\n\\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\n\n\n\n\n\n\nWe start by finding a point estimate for \\(\\mu_1 - \\mu_2\\). What is the best point estimator for \\(\\mu_1 - \\mu_2\\)?\n\n\n\n\n\\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\)!\n\n\n\n Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) \n\nIf the two samples are from independent normally distributed populations or \\(n_1 > 30\\) and \\(n_2 > 30\\), \\[\\small \\overline{X}_1 \\sim N\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1} \\right), \\quad \\overline{X}_2 \\sim N\\left(\\mu_2,\n\\frac{\\sigma_2^2}{n_2} \\right)\\]\n\\(\\overline{X}_1 - \\overline{X}_2\\) has the sampling distribution \\[\\small \\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} \\color{red}{+} \\color{black}{\\frac{\\sigma_2^2}{n_2}} \\right) \\]\n\n\\[\\small Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)\\]\n Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \n\nWith \\(D_0\\) being a hypothesized value (often 0), our testing problem is\n\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\le D_0\\\\ &H_1: \\mu_1 - \\mu_2 > D_0 \\end{align}\\)  (right-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\ge D_0\\\\ &H_1: \\mu_1 - \\mu_2 < D_0 \\end{align}\\)  (left-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 = D_0\\\\ &H_1: \\mu_1 - \\mu_2 \\ne D_0 \\end{align}\\)  (two-tailed)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\small \\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\]\nThen we find \\(z_{\\alpha}\\) or \\(z_{\\alpha/2}\\) and follow our testing steps!\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, the test statistic becomes \\(t_{test}\\):\n\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} \\]\n\nThe critical value is either \\(t_{\\alpha, df}\\) (one-tailed) pr \\(t_{\\alpha/2, df}\\) (two-tailed), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\] where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\).\nIf the \\(df\\) is not an integer, we round it down to the nearest integer.\n\n Inference About Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \n\nBelow is a table that summarizes ways to make inferences about independent samples when \\((\\sigma_1 \\ne \\sigma_2)\\).\n\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 \\ne \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}\\)\n\n\n\n\nUse \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\) to get the \\(p\\)-value, critical value and confidence interval.\nThe unequal-variance t-test is called Welch‚Äôs t-test.\n\n Example: Two-Sample t-Test \n\n\n\nDoes an over-sized tennis racket exert less stress/force on the elbow?\n\nOver-sized: \\(n_1 = 33\\), \\(\\overline{x}_1 = 25.2\\), \\(s_1 = 8.6\\)\nConventional: \\(n_2 = 12\\), \\(\\overline{x}_2 = 33.9\\), \\(s_2 = 17.4\\)\nThe two populations are nearly normal.\nThe large difference in the sample SD suggests \\(\\sigma_1 \\ne \\sigma_2\\).\n\nForm a hypothesis test with \\(\\alpha = 0.05\\), and construct a 95% CI for the mean difference of force on the elbow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Step 1 \n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}\\) \n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(25.2 - 33.9) - 0}{\\sqrt{\\frac{\\color{red}{8.6^2}}{33} + \\frac{\\color{red}{17.4^2}}{12}}} = -1.66\\)\n\\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\)\n\\(\\small A = \\dfrac{8.6^2}{33}\\), \\(\\small B = \\dfrac{17.4^2}{12}\\), \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01\\)\n\n\n\n\n\n\n\nNote\n\n\n\nIf the computed value of \\(df\\) is not an integer, always round down to the nearest integer.\n\n\n Step 4-c \n\n \\(-t_{0.05, 13} = -1.771\\). \n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} < -t_{\\alpha, df}\\). \\(\\small t_{test} = -1.66 > -1.771 = -t_{\\alpha, df}\\), we fail to reject \\(H_0\\). \n\n Step 6 \n\n There is insufficient evidence to support the claim that the the oversized racket delivers less stress to the elbow. \nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is\n\n\\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}} &= (25.2 - 33.9) \\pm t_{0.025,13}\\sqrt{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}}\\\\&= -8.7 \\pm 11.32 = (-20.02, 2.62).\\end{align}\\]\n\nWe are 95% confident that the difference in the mean forces is between -20.02 and 2.62.\nSince the interval includes 0, it leads to the same conclusion as failing to reject \\(H_0\\).\n\n Two-Sample t-Test in R \n\nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA <- s1^2 / n1; B <- s2^2 / n2\ndf <- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n(df <- floor(df))\n\n[1] 13\n\n## t_test\n(t_test <- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n\n[1] -1.659894\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.770933\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 0.06042575\n\n\n\n Testing for Independent Samples (\\(\\sigma_1 = \\sigma_2 = \\sigma\\)) \n Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) \n\\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} \\right) \\] If \\(\\sigma_1 = \\sigma_2 = \\sigma\\), \\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\right) \\] \\[ Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1)\\]\n Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\) \n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\]\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, we use \\(t_{test}\\) just like we would for the one-sample case.\nAs \\(\\sigma_1 = \\sigma_2 = \\sigma\\), we just need one sample SD to replace the population SD, \\(\\sigma\\).\nUse the pooled sample variance to estimate the common population variance, \\(\\sigma^2\\): \\[ s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} \\] which is the weighted average of \\(s_1^2\\) and \\(s_2^2\\).\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, \\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nHere, the critical value is either \\(t_{\\alpha, df}\\) (for one-tailed tests) or \\(t_{\\alpha/2, df}\\) (for two-tailed tests), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[df = n_1 + n_2 - 2\\]\n\n Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\) \n\nBelow is a table that summarizes ways to make inferences about independent samples when \\((\\sigma_1 = \\sigma_2)\\).\n\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 = \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\n\n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\nUse \\(df = n_1+n_2-2\\) to get the \\(p\\)-value, critical value and confidence interval.\nThe test from two independent samples with \\(\\sigma_1 = \\sigma_2 = \\sigma\\) is usually called two-sample pooled \\(z\\)-test or two-sample pooled \\(t\\)-test.\n\n Example: Weight Loss \n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\nIs there a sufficient evidence at \\(\\alpha = 0.05\\) to conclude that the program is effective?\n\nIf yes, construct a 95% CI for \\(\\mu_1 - \\mu_2\\) to show how much effective it is.\n\n\n Step 1 \n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}\\) \n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\). \n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(10-1)0.5^2 + (10-1)0.7^2}{10+10-2}}=0.6083\\)\n \\(t_{test} = \\frac{(2.1 - 4.2) - 0}{0.6083\\sqrt{\\frac{1}{10} + \\frac{1}{10}}} = -7.72\\)\n\n\n Step 4-c \n\n \\(df = n_1 + n_2 - 2 = 10 + 10 - 2 = 18\\). So \\(-t_{0.05, df = 18} = -1.734\\). \n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} < -t_{\\alpha, df}\\). Since \\(\\small t_{test} = -7.72 < -1.734 = -t_{\\alpha, df}\\), we reject \\(H_0\\).\n\n Step 4-p \n\n The \\(p\\)-value is \\(P(T_{df=18} < t_{test}) \\approx 0\\) \n\n Step 5-p \n\n We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(\\approx 0 < 0.05 = \\alpha\\), we reject \\(H_0\\).\n\n Step 6 \n\n There is sufficient evidence to support the claim that the weight loss program is effective. \nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is \\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} &= (2.1 - 4.2) \\pm t_{0.025, 18} (0.6083)\\sqrt{\\frac{1}{10} + \\frac{1}{10}}\\\\ &= -2.1 \\pm 0.572 = (-2.672, -1.528) \\end{align}\\]\nWe are 95% confident that the difference in the mean weight is between -2.672 and -1.528.\nSince the interval does not include 0, it leads to the same conclusion as rejection of \\(H_0\\).\n\n Two-Sample Pooled t-Test in R \n\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\nsp <- sqrt(((n1 - 1) * s1 ^ 2 + (n2 - 1) * s2 ^ 2) / (n1 + n2 - 2))\nsp\n\n[1] 0.6082763\n\ndf <- n1 + n2 - 2\n## t_test\n(t_test <- (x1_bar - x2_bar) / (sp * sqrt(1 / n1 + 1 / n2)))\n\n[1] -7.719754\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.734064\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 2.028505e-07"
  },
  {
    "objectID": "model-anova.html#anova-rationale",
    "href": "model-anova.html#anova-rationale",
    "title": "23¬† Analysis of Variance",
    "section": "23.1 ANOVA Rationale",
    "text": "23.1 ANOVA Rationale\n Comparing More Than Two Population Means \n\nIn many research settings, we want to compare 3 or more population means.\n\n\n\n\n Are there differences in the mean readings of 4 types of devices used to determine the pH of soil samples? \n\n\n\n\n\n\n\n\n\n\n\n\n Do different treatments (None, Fertilizer, Irrigation, Fertilizer and Irrigation) affect the mean weights of poplar trees? \n\n\n\n\n\n\n\n\n\n\n\n\n\n One-Way Analysis of Variance \n\nA factor is a property or characteristic (categorical variable) that allows us to distinguish the different populations from one another.\n\nType of device and treatment of trees are factors from the examples previously provided.\n\nOne-way ANOVA examines the effect of a categorical variable on the mean of a numerical variable (response).\n\nWe use analysis of  variance  to test the equality of 3 or more population  means. ü§î\nThe method is one-way because we use one single property (categorical variable) for categorizing the populations.\n\n\n Requirements \n\nThe populations of each category are normally distributed.\nThe populations have the same variance \\(\\sigma^2\\) (two sample pooled \\(t\\)-test).\nThe samples are random samples.\nThe samples are independent of each other (not matched or paired in any way).\n\n Rationale \n\nData 1 and Data 2 have the same group sample means \\(\\bar{y}_1\\), \\(\\bar{y}_2\\) and \\(\\bar{y}_3\\) denoted as red dots.\nHowever, they differ with regards to the variance within each group.\n\n\n\n\n\n\nFigure¬†23.1: Boxplots illustrating the variance within samples\n\n\n\n\n\n\n\n\n\n\nFor which data do you feel more confident in saying the population means \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not all the same?\n\n\n\n\n\n\n\n Variation Between Samples & Variation Within Samples \n\nData 1: Variability between samples is large in comparison to the variation within samples.\nData 2: Variation between samples is small relatively to the variation within samples.\n\n\n\n\n\n\n\nWe are more confident concluding there is a difference in population means when variation between samples is larger than variation within samples.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†23.2: Illustration of small and large variance within samples"
  },
  {
    "objectID": "model-anova.html#anova-procedures",
    "href": "model-anova.html#anova-procedures",
    "title": "23¬† Analysis of Variance",
    "section": "23.2 ANOVA Procedures",
    "text": "23.2 ANOVA Procedures\n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\\\ &H_1: \\text{Population means are not all equal} \\end{align}\\) \n\n\n\nStatistician Ronald Fisher found a way to define a variable that follows the \\(F\\) distribution: \\[\\frac{\\text{variance between samples}}{\\text{variance within samples}} \\sim F_{df_B,\\, df_W}\\]\nIf the variance between samples is larger than the variance within samples (\\(F_{test}\\) is much greater than 1), as in Data 1, we reject \\(H_0\\).\n\n\n\n\n\n\n\nKey\n\n\n\n\nDefine variance between samples and variance within samples so that the ratio is \\(F\\) distributed.\n\n\n\n\n Variance Within Samples \n\nBack to the two-sample pooled \\(t\\)-test with equal variance, \\(\\sigma^2\\). We have \\[s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\\]\n\n\n\n\n\n\n\nHow about the pooled sample variance for \\(k\\) samples?\n\n\n\n\n\n\n\nANOVA assumes the populations have the same variance such that \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2 = \\sigma^2\\). \\[\\boxed{s_W^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2}{n_1 + n_2 + \\cdots + n_k - k}}\\] where \\(s_i^2\\), \\(i = 1, \\dots ,k\\), is the sample variance of group \\(i\\).\n\\(s_W^2\\) represents a combined estimate of the common variance, \\(\\sigma^2\\).\n\nIt measures variability of the observations within the \\(k\\) populations.\n\n\n\n Variance Between Samples \n\\[\\boxed{s^2_{B} = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^2}{k-1}}\\]\n\n\\(\\bar{y}_{i\\cdot}\\) is the \\(i\\)-th sample mean.\n\\(\\bar{y}_{\\cdot\\cdot}\\) is the grand sample mean with all data points in all groups combined.\n\\(s^2_{B}\\) is also an estimate of \\(\\sigma^2\\) and measures variability among sample means for the \\(k\\) groups.\nIf \\(H_0\\) is true \\((\\mu_1 = \\cdots = \\mu_k = \\mu)\\), any variation in the sample means is due to chance and randomness, so it shouldn‚Äôt be too large.\n\n\\(\\bar{y}_{1\\cdot}, \\cdots, \\bar{y}_{k\\cdot}\\) should be close each other and should be close to \\(\\bar{y}_{\\cdot \\cdot}\\).\n\n\n\n ANOVA Table: Sum of Squares \n\nTotal Sum of Squares (SST) measures the total variation around \\(\\bar{y}_{\\cdot\\cdot}\\) in all of the sample data combined (ignoring the groups): \\[\\scriptsize{\\color{blue}{SST = \\sum_{j=1}^{n_i}\\sum_{i=1}^{k} \\left(y_{ij} - \\bar{y}_{\\cdot\\cdot}\\right)^2}}\\] where \\(y_{ij}\\) is the \\(j\\)-th data point in the \\(i\\)-th group.\nSum of Squares Between Samples (SSB) measures the variation between sample means: \\[\\scriptsize{ \\color{blue}{SSB = \\sum_{i=1}^{k}n_i \\left(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}\\right)^2}}\\]\nSum of Squares Within Samples (SSW) measures the variation of any value, \\(y_{ij}\\), about its sample mean, \\(\\bar{y}_{i\\cdot}\\): \\[\\scriptsize{ \\color{blue}{SSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\left(y_{ij} - \\bar{y}_{i\\cdot}\\right)^2 = \\sum_{i=1}^{k} (n_i - 1)s_i^2}}\\]\n\n Sum of Squares Identity \n\n\\(SST = SSB + SSW\\)\n\\(df_{T} = df_{B} + df_{W} \\implies N - 1 = (k-1) + (N - k)\\) \n\\(\\text{Mean Square (MS)} = \\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\)\n\\(MSB = \\frac{SSB}{k-1} = s^2_{B}\\)\n\\(MSW = \\frac{SSW}{N-k} = s^2_{W}\\)\n\\(F_{test} = \\frac{MSB}{MSW}\\)\nUnder \\(H_0\\), \\(\\frac{S^2_{B}}{S_W^2} \\sim F_{k-1, \\, N-k}\\)\nReject \\(H_0\\) if\n\n\\(F_{test} > F_{\\alpha, \\, k - 1,\\, N-k}\\)\n\\(p\\)-value \\(P(F_{k - 1,\\, N-k} > F_{test}) < \\alpha\\)\n\n\n ANOVA Table"
  },
  {
    "objectID": "model-anova.html#anova-example",
    "href": "model-anova.html#anova-example",
    "title": "23¬† Analysis of Variance",
    "section": "23.3 ANOVA Example",
    "text": "23.3 ANOVA Example\n\nWe hypothesize that a nutrient called ‚Äúisoflavones‚Äù varies among three types of food: (1) cereals and snacks, (2) energy bars and (3) veggie burgers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sample of 5 is taken from each type of food and the amount of isoflavones is measured.\nIs there a sufficient evidence to conclude that the mean isoflavone levels vary among these food items at \\(\\alpha = 0.05\\)?\n\n\n Data \n\n\n\n\nWe prefer a data format like the one shown on the right.\n\n\n\n\ndata\n\n   1  2  3\n1  3 19 25\n2 17 10 15\n3 12  9 12\n4 10  7  9\n5  4  5  8\n\n\n\n\n\n\n\n\nSo tell me what is the value of \\(y_{23}\\)!\n\n\n\n\n\n\n\n\n\n\ndata_anova\n\n    y    food\n1   3 cereals\n2  17 cereals\n3  12 cereals\n4  10 cereals\n5   4 cereals\n6  19  energy\n7  10  energy\n8   9  energy\n9   7  energy\n10  5  energy\n11 25  veggie\n12 15  veggie\n13 12  veggie\n14  9  veggie\n15  8  veggie\n\n\n\n\n\n\n\n\n\n\nFigure¬†23.3: Boxplot of the Isoflavone Content in 3 Types of Food\n\n\n\n\n\n Test Assumptions \n\nAssumptions:\n\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3\\) (I tested it).\nData are generated from a normal distribution for each type of food (QQ plots confirm this).\n\n\n\n\n\n\n\nFigure¬†23.4: QQ plots for each type of food\n\n\n\n\n ANOVA Testing \n\n \\(\\begin{align}&H_0: \\mu_1 = \\mu_2 = \\mu_3\\\\&H_1: \\mu_is \\text{ not all equal} \\end{align}\\) \n\n\n\n\n\n\n\n\n\n\n\nWe can do all the calculations and generate an ANOVA table using just one line of code, as shown below.\n\n\nanova(lm(y ~ food, data = data_anova))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value Pr(>F)\nfood       2   60.4   30.20   0.828   0.46\nResiduals 12  437.6   36.47"
  },
  {
    "objectID": "model-reg.html#correlation",
    "href": "model-reg.html#correlation",
    "title": "24¬† Linear Regression",
    "section": "24.1 Correlation",
    "text": "24.1 Correlation\n Relationship Between 2 Numerical Variables \n\nDepending on the situation, the two variables can be classified as the explanatory variable and the response variable. (Discussed in Regression)\nHowever, there is not always an explanatory-response relationship.\nExamples:\n\n height and weight \n income and age \n SAT/ACT math score and verbal score \n amount of time spent studying for an exam and exam grade \n\n\n\n\n\n\n\n\nCan you provide an example that 2 variables are associated?\n\n\n\n\n\n\n\n Scatterplots \n\n\n\n\n\nFigure¬†24.1: Examples of scatterplots\n\n\n\n\n\nThe overall pattern can be described in several ways.\n\nForm: linear or clusters\nDirection: positively associated or negatively associated\nStrength: how close the points lie to a line/curve\n\n\n\n Linear Correlation Coefficient \n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right) = \\frac{\\sum_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\overline{x})^2\\sum_{i=1}^n(y_i-\\overline{y})^2}}\\]\n\n\n\n\n\n\\(-1 \\le r\\le 1\\)\n\\(r > 0\\): The larger value of \\(X\\) is, the larger value of \\(Y\\) tends to be.\n\\(r = 1\\): Perfect positive linear relationship.\n\n\n\n\n\n\n\nFigure¬†24.2: Positive correlation between two variables\n\n\n\n\n\n\n\n\n\n\n\\(r < 0\\): The larger value of \\(X\\) is, the smaller value of \\(Y\\) tends to be.\n\\(r = -1\\): Perfect negative linear relationship\n\n\n\n\n\n\n\nFigure¬†24.3: Negative correlation between two variables\n\n\n\n\n\n\n\n\n\n\n\\(r = 0\\): No linear relationship.\nIf the explanatory and response variables are switched, \\(r\\) remains the same.\n\\(r\\) has no units of measurement, so scale changes do not affect \\(r\\).\n\n\n\n\n\n\n\nFigure¬†24.4: No correlation between two variables\n\n\n\n\n\n\n Example \n\nIt is possible that there is a strong relationship between two variables, but they still have \\(r = 0\\).\n\n\n\n\n\n\nFigure¬†24.5: Examples of relationships between two variables and their correlation coefficients (https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg)\n\n\n\n\n Example in R \n\n\n\nplot(x = mtcars$wt, y = mtcars$mpg, \n     main = \"MPG vs. Weight\", \n     xlab = \"Car Weight\", \n     ylab = \"Miles Per Gallon\", \n     pch = 16, col = 4, las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor(x = mtcars$wt,\n    y = mtcars$mpg)\n\n[1] -0.8676594"
  },
  {
    "objectID": "model-reg.html#regression",
    "href": "model-reg.html#regression",
    "title": "24¬† Linear Regression",
    "section": "24.2 Regression",
    "text": "24.2 Regression\n What is Regression? \n\nRegression models the relationship between one or more numerical/categorical response variables \\((Y)\\) and one or more numerical/categorical explanatory variables \\((X)\\).\nA regression function \\(f(X)\\) describes how a response variable \\(Y\\), on average, changes as an explanatory variable \\(X\\) changes.\n\n\n\nExamples:\n\n college GPA \\((Y)\\) vs.¬†ACT/SAT score \\((X)\\)\n sales \\((Y)\\) vs.¬†advertising expenditure \\((X)\\)\n crime rate \\((Y)\\) vs.¬†median income level \\((X)\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Unknown Regression Function \n\nThe true relationship between \\(X\\) and the mean of \\(Y\\), the regression function \\(f(X)\\), is unknown.\nThe collected data \\((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\) is all we know and have.\n\n\n\n\n\n\n\n\n\n\n\nGoal: estimate \\(f(X)\\) from our data and use it to predict value of \\(Y\\) given a value of \\(X\\).\n\n\n Simple Linear Regression \n\nStart with simple linear regression:\n\nOnly one predictor \\(X\\) (known and constant) and one response variable \\(Y\\)\nthe regression function used for predicting \\(Y\\) is a linear function.\nuse a regression line in a X-Y plane to predict the value of \\(Y\\) for a given value of \\(X = x\\).\n\n\n\n\n\n\nMath review: A linear function \\(y = f(x) = \\beta_0 + \\beta_1 x\\) represents a straight line\n\n\\(\\beta_1\\): slope, the amount by which \\(y\\) changes when \\(x\\) increases by one unit.\n\\(\\beta_0\\): intercept, the value of \\(y\\) when \\(x = 0\\).\nThe linearity assumption: \\(\\beta_1\\) does not change as \\(x\\) changes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Sample Data: Relationship Between X and Y \n\nReal data \\((x_i, y_i), i = 1, 2, \\dots, n\\) do not form a perfect straight line!\n\\(y_i = \\beta_0+\\beta_1x_i + \\color{red}{\\epsilon_i}\\)\n\nMissing Graph\n\nWhen we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed being drawn from a normal distribution (for inference purpose).\nIts value varies around and will not be exactly equal to its mean \\(\\mu_y\\).\n\n\n\n\n\n\n\n\n\n\n\nThe mean of \\(Y\\) and \\(X\\) form a straight line.\n\n\n\n\n\n\n\n\n\n\n\n Simple Linear Regression Model (Population) \nFor the \\(i\\)-th measurement in the target population, \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\n\n\\(Y_i\\): the \\(i\\)-th value of the response (random) variable.\n\\(X_i\\): the \\(i\\)-th known fixed value of the predictor.\n\\(\\epsilon_i\\): the \\(i\\)-th random error with assumption \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients.\n\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are fixed unknown parameters to be estimated from the sample data once we collect them.\n\n Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\n\\mu_{Y_i \\mid X_i} &= E(\\beta_0 + \\beta_1X_i + \\epsilon_i) \\\\\n&= \\beta_0 + \\beta_1X_i\n\\end{align*}\\] The mean response \\(\\mu_{Y\\mid X}\\) has a straight-line relationship with \\(X\\) given by a population regression line \\[\\mu_{Y\\mid X} = \\beta_0 + \\beta_1X\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\nVar(Y_i \\mid X_i) &= Var(\\epsilon_i) = \\sigma^2\n\\end{align*}\\] The variance of \\(Y\\) does not depend on \\(X\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\] For any fixed value of \\(X_i = x_i\\), the response \\(Y_i\\) varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nJob: Collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!\n\n\n Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\) \nGiven the sample data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\)\n\nWhich sample regression line is the best?\nWhat are the best estimators \\(b_0\\) and \\(b_1\\) for \\(\\beta_0\\) and \\(\\beta_1\\)?\n\nMissing Graph\n Idea of Fitting \n\nInterested in \\(\\beta_0\\) and \\(\\beta_1\\) in the following sample regression model: \\[\\begin{align*}\ny_i = \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\\] or \\[E({y}_{i}) = \\mu_{y|x_i} = \\beta_0 + \\beta_1~x_{i}\\]\nUse sample statistics \\(b_0\\) and \\(b_1\\) computed from our sample data to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\hat{y}_{i} = b_0 + b_1~x_{i}\\) is called fitted value of \\(y_i\\), a point estimate of the mean \\(\\mu_{y|x_i}\\) and \\(y_i\\) itself."
  },
  {
    "objectID": "model-reg.html#ordinary-least-squares-ols",
    "href": "model-reg.html#ordinary-least-squares-ols",
    "title": "24¬† Linear Regression",
    "section": "24.4 Ordinary Least Squares (OLS)",
    "text": "24.4 Ordinary Least Squares (OLS)\n\nChoose \\(b_0\\) and \\(b_1\\), or sample regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals \\(SS_{res}\\).\nThe residual \\(e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)\\) is a point estimate of \\(\\epsilon_i\\).\nThe sample regression line minimizes \\(SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2\\). \\[\\small{\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}}\\]\n\n\n Visualizing Residuals \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Least Squares Estimates (LSE) \n\nThe least squares approach choose \\(b_0\\) and \\(b_1\\) to minimize the \\(SS_{res}\\), i.e., \\[(b_0, b_1) = \\arg \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\]\n\nMATH 1450 ‚Ä¶\n\\[\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\]\n\\[\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = \\frac{S_{xy}}{S_{xx}} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},\\] where \\(S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2\\), \\(S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\), \\(S_{xy} = \\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})\\)\n\n\n\n\n\n\nWhat can we learn from the formula of \\(b_0\\) and \\(b_1\\)?"
  },
  {
    "objectID": "model-reg.html#r-lab",
    "href": "model-reg.html#r-lab",
    "title": "24¬† Linear Regression",
    "section": "24.7 R Lab",
    "text": "24.7 R Lab\n mpg Data \n\nlibrary(ggplot2)  ## use data mpg in ggplot2 package\nmpg\n\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ä¶ with 224 more rows\n\n\n\n Highway MPG hwy vs.¬†Displacement displ \n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\n\n\n\n\n\n\n\n\n\n Fit Simple Linear Regression \n\n\n\nreg_fit <- lm(formula = hwy ~ displ, \n              data = mpg)\nreg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n     35.698       -3.531  \n\ntypeof(reg_fit)\n\n[1] \"list\"\n\n\n\n\n\n\n## all elements in reg_fit\nnames(reg_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n## use $ to extract an element of a list\nreg_fit$coefficients\n\n(Intercept)       displ \n  35.697651   -3.530589 \n\n\n\n\n\n\\(\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i} = 35.7 - 3.5 \\times displ_{i}\\)\n\\(b_1\\): For a one unit (liter) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.5.\n\n\n Fitted Values of \\(y\\) \n\n## the first 5 observed response value y\nmpg$hwy[1:5]\n\n[1] 29 29 31 30 26\n\n## the first 5 fitted value y_hat\nhead(reg_fit$fitted.values, 5)\n\n       1        2        3        4        5 \n29.34259 29.34259 28.63647 28.63647 25.81200 \n\n## the first 5 predictor value x\nmpg$displ[1:5]\n\n[1] 1.8 1.8 2.0 2.0 2.8\n\nlength(reg_fit$fitted.values)\n\n[1] 234\n\n\n\n Add a Regression Line \n\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\nabline(reg_fit, col = \"#FFCC00\", lwd = 3)\n\n\n\n\n\n\n\n\n\n Standard Error of Regression \n\n(summ_reg_fit <- summary(reg_fit))\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\n\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n\n[1] 3.835985\n\n# from reg_fit\nsqrt(sum(reg_fit$residuals^2) / reg_fit$df.residual)\n\n[1] 3.835985\n\n\n\n Confidence Intervals and Testing for \\(\\beta_0\\) and \\(\\beta_1\\) \n\nconfint(reg_fit, level = 0.95)\n\n                2.5 %   97.5 %\n(Intercept) 34.278353 37.11695\ndispl       -3.913828 -3.14735\n\n\n\nsumm_reg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\n\nsumm_reg_fit$coefficients\n\n             Estimate Std. Error   t value      Pr(>|t|)\n(Intercept) 35.697651  0.7203676  49.55477 2.123519e-125\ndispl       -3.530589  0.1945137 -18.15085  2.038974e-46\n\n\n\n ANOVA Table \n\nFor \\(H_0: \\beta_1 = 0\\) in SLR, \\(t_{test}^2 = F_{test}\\).\n\n\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: hwy\n           Df Sum Sq Mean Sq F value    Pr(>F)    \ndispl       1 4847.8  4847.8  329.45 < 2.2e-16 ***\nResiduals 232 3413.8    14.7                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsumm_reg_fit$coefficients\n\n             Estimate Std. Error   t value      Pr(>|t|)\n(Intercept) 35.697651  0.7203676  49.55477 2.123519e-125\ndispl       -3.530589  0.1945137 -18.15085  2.038974e-46\n\nsumm_reg_fit$coefficients[2, 3] ^ 2\n\n[1] 329.4533\n\n\n\n \\(R^2\\) \n\nsumm_reg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\n\nsumm_reg_fit$r.squared\n\n[1] 0.5867867\n\n\n\n Prediction \n\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 16.27941 15.35839 17.20043\n\n## PI for the new observation\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"predict\", level = 0.95)\n\n       fit      lwr      upr\n1 16.27941 8.665682 23.89314"
  },
  {
    "objectID": "model-reg.html#confidence-intervals-and-hypothesis-testing-for-beta_0-and-beta_1",
    "href": "model-reg.html#confidence-intervals-and-hypothesis-testing-for-beta_0-and-beta_1",
    "title": "24¬† Linear Regression",
    "section": "24.4 Confidence Intervals and Hypothesis Testing for \\(\\beta_0\\) and \\(\\beta_1\\)",
    "text": "24.4 Confidence Intervals and Hypothesis Testing for \\(\\beta_0\\) and \\(\\beta_1\\)\n Confidence Intervals for \\(\\beta_0\\) and \\(\\beta_1\\) \n\n\\(\\frac{b_1 - \\beta_1}{\\sqrt{\\hat{\\sigma}^2/S_{xx}}} \\sim t_{n-2}\\); \\(\\quad \\frac{b_0 - \\beta_0}{\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}\\)\nThe \\((1-\\alpha)100\\%\\) CI for \\(\\beta_1\\) is \\(b_1 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2/S_{xx}}\\)\nThe \\((1-\\alpha)100\\%\\) CI for \\(\\beta_0\\) is \\(b_0 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}\\)\n\n\n Hypothesis Testing \n \\(\\beta_1\\) \n\n \\(H_0: \\beta_1 = \\beta_1^0 \\quad H_1: \\beta_1 \\ne \\beta_1^0\\)  \nTest statistic: Under \\(H_0\\), \\[t_{test} = \\frac{b_1 - \\color{red}{\\beta_1^0}}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\\]\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{$p$-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\)\n\n\n \\(\\beta_0\\) \n\n \\(H_0: \\beta_0 = \\beta_0^0 \\quad H_1: \\beta_0 \\ne \\beta_0^0\\)  \nTest statistic: Under \\(H_0\\), \\[t_{test} = \\frac{b_0 - \\color{red}{\\beta_0^0}}{\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}\\]\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{$p$-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\)\n\n\n Interpretation of Testing Results \n\n \\(H_0: \\beta_1 = 0 \\quad H_1: \\beta_1 \\ne 0\\) \nFailing to reject \\(H_0: \\beta_1 = 0\\) implies there is no linear relationship between \\(Y\\) and \\(X\\).\n\n\n\n\n\n\nFigure¬†24.16: Failing to reject \\(H_0\\) means there is no linear relationship between X and Y, but they could have some other type of relationship.\n\n\n\n\n\n\n\n\n\n\nIf we reject \\(H_0: \\beta_1 = 0\\), does it mean \\(X\\) and \\(Y\\) are linearly related?\n\n\n\n\n\n\n Test of Significance of Regression \n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate.\nbetter results could be obtained with a more complicated model.\n\n\n\n\n\n\n\nFigure¬†24.17: Rejecting \\(H_0\\) doesn‚Äôt necessarily mean that a linear model is the best model, just that it is adequate."
  },
  {
    "objectID": "model-reg.html#analysis-of-variance-anova-approach",
    "href": "model-reg.html#analysis-of-variance-anova-approach",
    "title": "24¬† Linear Regression",
    "section": "24.5 Analysis of Variance (ANOVA) Approach",
    "text": "24.5 Analysis of Variance (ANOVA) Approach\n \\(X\\) - \\(Y\\) Relationship Explains Some Deviation \n\n\n\n\n\n\nSuppose we only have data for \\(Y\\) and have no information about \\(X\\) or the relationship between \\(X\\) and \\(Y\\). How do we predict a value of \\(Y\\)?\n\n\n\n\n\n\n\nIf the data have no pattern, our best guess for \\(Y\\) would be \\(\\overline{y}\\) (i.e., \\(\\hat{y}_i = \\overline{y}\\)).\n\nWe would treat \\(X\\) and \\(Y\\) as uncorrelated.\n\nThe (total) deviation from the mean is \\((y_i - \\overline{y})\\)\nIf \\(X\\) and \\(Y\\) are linearly related, fitting a linear regression model helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided.\n\\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\).\nThe regression model explains some of the deviation of \\(y\\).\n\n\n Partition of Deviation \n\nTotal deviation = Deviation explained by regression + unexplained deviation\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)\n\\((19 - 9) = (13 - 9) + (19 - 13)\\)\n\n\n\n\n\n\nFigure¬†24.18: Explained vs.¬†unexplained deviation\n\n\n\n\n Sum of Squares (SS) \n\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\)\nTotal SS \\((SS_T)\\) = Regression SS \\((SS_R)\\) + Residual SS \\((SS_{res})\\)\n\\(df_T = df_R + df_{res}\\)\n\\(\\color{blue}{(n-1) = 1 +(n-2)}\\)\n\n\n ANOVA for Testing Significance of Regression \n\n\n\n\n\nFigure¬†24.19: Example of an ANOVA table\n\n\n\n\n\nA larger value of \\(F_{test}\\) indicates that the regression is significant.\nReject \\(H_0\\) if\n\n\\(F_{test} > F_{\\alpha, 1, n-2}\\)\n\\(\\text{$p$-value} = P(F_{1, n-2} > F_{test}) < \\alpha\\).\n\nANOVA is designed to test the \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, the \\(F\\)-test of ANOVA gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\).\n\n\n Coefficient of Determination \n\nThe coefficient of determination \\((R^2)\\) is the proportion of the variation in \\(y\\) that is explained by the regression model.\nIt is computed as \\[R^2 = \\frac{SS_R}{SS_T} =\\frac{SS_T - SS_{res}}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\]\n\\(R^2\\) is the proportionate reduction of total variation associated with the use of \\(X\\).\n(a) \\(\\hat{y}_i = y_i\\) and \\(\\small SS_{res} = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = 0\\). (b) \\(\\hat{y}_i = \\overline{y}\\) and \\(\\small SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 = 0\\).\n\n\n\n\n\n\nFigure¬†24.20: Examples of \\(R^2\\) being equal to 1 and 0"
  },
  {
    "objectID": "model-reg.html#prediction",
    "href": "model-reg.html#prediction",
    "title": "24¬† Linear Regression",
    "section": "24.6 Prediction",
    "text": "24.6 Prediction\n Predicting the Mean Response \n\nWith the predictor value \\(x = x_0\\), we want to estimate the mean response \\(E(y\\mid x_0) = \\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\).\n\n Example: The mean highway MPG \\(E(y \\mid x_0)\\) when displacement is \\(x = x_0 = 5.5\\). \n\nIf \\(x_0\\) is within the range of \\(x\\), an unbiased point estimator for \\(E(y\\mid x_0)\\) is \\[\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y\\mid x_0)\\) is \\(\\boxed{\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\).\n\n\n\n\n\n\n\nDoes the length of the CI for \\(E(y\\mid x_0)\\) stay the same at any location of \\(x_0\\)?\n\n\n\n\n\n\n\n Predicting New Observations \n\nPredict the value of a new observation, \\(y_0\\), with \\(x = x_0\\).\n\n Example: The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\nAn unbiased point estimator for \\(y_0(x_0)\\) is \\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]\nThe \\((1-\\alpha)100\\%\\) prediction interval (PI) for \\(y_0(x_0)\\) is \\(\\small \\boxed{\\hat{y_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{1+ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\)\n\n\n\n\n\n\n\nWhat is the difference between CI for \\(E(y\\mid x_0)\\) and PI for \\(y_0(x_0)\\)?\n\n\n\n\n\n\n\nThe PI is wider as it includes the uncertainty about \\(b_0\\), \\(b_1\\) as well as \\(y_0\\) due to error, \\(\\epsilon\\)."
  },
  {
    "objectID": "infer-prop.html#introduction",
    "href": "infer-prop.html#introduction",
    "title": "18¬† Inference About Proportions",
    "section": "18.1 Introduction",
    "text": "18.1 Introduction\n One Categorical Variable with Two Categories \n\nLet \\(X\\) be the categorical variable Gender with 2 categories, Male and Female.\n\n\n\n\n\n\nSubject\nMale\nFemale\n\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\nx\n\n\n\n\n\nOne-way frequency/count table\n\n\n\n\n\\(X\\)\nCount\n\n\n\n\nMale\n\\(y\\)\n\n\nFemale\n\\(n-y\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of males can be viewed as a random variable because the count, \\(y\\), varies from sample to sample.\n\n\n\n\n\n\n\nWhat probability distribution might be appropriate for the count, \\(Y\\)?\n\n\n\n\n\n\n\n\n\n Probability Distribution for Count Data: Two Levels \n\n\\(binomial(n, \\pi)\\) could be a good option for count data with 2 categories.\n\nFixed number of trials.  (Fixed \\(n\\) subjects) \nEach trial results in one of two outcomes.  (Either \\(M\\) or \\(F\\)) \nTrials are independent.  (If the subjects are randomly sampled) \n\nIf the proportion of being in category, \\(M\\), is \\(\\pi\\), the count, \\(Y\\), has \\[P(Y = y \\mid n, \\pi) = \\frac{n!}{y!(n-y)!}\\pi^{y}(1-\\pi)^{n-y}\\]\nGoal: Estimate or test the population proportion, \\(\\pi\\), of the category, \\(M\\)."
  },
  {
    "objectID": "infer-prop.html#inference-for-a-single-proportion",
    "href": "infer-prop.html#inference-for-a-single-proportion",
    "title": "18¬† Inference About Proportions",
    "section": "18.2 Inference for a Single Proportion",
    "text": "18.2 Inference for a Single Proportion\n Hypothesis Testing for \\(\\pi\\) (Exact Binom Test) \n Step 0: Method Assumptions \n\n \\(n\\pi_0 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) \n\nThe larger, the better\n\n\n Step 1: Set the Null and Alternative Hypothesis \n\n \\(\\begin{align} &H_0: \\pi = \\pi_0 \\\\ &H_1: \\pi > \\pi_0 \\text{ or } \\pi < \\pi_0 \\text{ or } \\pi \\ne \\pi_0 \\end{align}\\) \n\n Step 2: Set the Significance Level, \\(\\alpha\\) \n Step 3: Calculate the Test Statistic \n\n Under \\(H_0\\), \\(z_{test} = \\dfrac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\) where \\(\\hat{\\pi} = \\frac{y}{n} =\\) sample proportion \n\n\n\n\n\n\n\nNote\n\n\n\n\nThe sampling distribution of \\(\\hat{\\pi}\\) is approximately normal with mean, \\(\\pi\\), and standard error, \\(\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\), if \\(y_i\\) are independent and the assumptions are satisfied.\n\n\n\n Step 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed) \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n \\(H_1: \\pi > \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z > z_{\\alpha}\\) \n \\(H_1: \\pi < \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z < -z_{\\alpha}\\) \n \\(H_1: \\pi \\ne \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| > z_{\\alpha/2}\\) \n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n Confidence Interval for \\(\\pi\\) \n\nAssumptions:\n\n\\(n\\hat{\\pi} \\ge 5\\) and \\(n(1-\\hat{\\pi}) \\ge 5\\)\n\nThe \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi\\) is \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] where \\(\\hat{\\pi} = y/n\\).\n\\(\\pi\\) is unknown, so we use the estimate, \\(\\hat{\\pi}\\), instead: \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}\\]\n\n\n\n\n\n\n\nNote\n\n\n\n\nNo hypothesized value, \\(\\pi_0\\), is involved in the confidence interval.\n\n\n\n\n Example: Exit Poll \n\nSuppose we collect data on 1,000 voters in an election with only two candidates, R and D.\n\n\n\n\n\n\nVoter\nR\nD\n\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the data, we want to predict who won the election.\nLet \\(Y\\) be the number of voters that voted for R.\nAssume the count, \\(Y\\), is sampled from \\(binomial(n = 1000, \\pi)\\).\n\n\\(\\pi = P(\\text{a voter voted for R}) =\\) (population) proportion of all voters that for R\n\nThis is the unknown parameter to be estimated or tested.\n\n\nPredict whether or not R won the election.\n\n\n\n\n\n\n\nWhat are \\(H_0\\) and \\(H_1\\)?\n\n\n\n\n \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi > 1/2 \\text{ (more than half voted for R)} \\end{align}\\) \n\n\n\n Hypothesis Testing \n\nIn an exit poll of 1,000 voters, 520 voted for R, one of the two candidates.\n\n Step 0 \n\n \\(n\\pi_0 = 1000(1/2) = 500 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) \n\n Step 1 \n\n \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi > 1/2 \\end{align}\\) \n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(z_{test} = \\frac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} = \\frac{\\frac{520}{1000} - 0.5}{\\sqrt{\\frac{0.5(1-0.5)}{1000}}} = 1.26\\) \n\n Step 4-c \n\n \\(z_{\\alpha} = z_{0.05} = 1.645\\) \n\n Step 5-c \n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} > z_{\\alpha}\\). \n Since \\(z_{test} < z_{\\alpha}\\), we do not reject \\(H_0\\). \n\n Step 6 \n\n We do not have sufficient evidence to conclude that R won. \nWe make the same conclusion using the \\(p\\)-value method.\n\n\\[ p\\text{-value} = P(Z > 1.26) = 0.1 > 0.05\\]\n Confidence Interval \n\nAssumptions:\n\n\\(n\\hat{\\pi} = 1000(0.52) = 520 \\ge 5\\) and \\(n(1-\\hat{\\pi}) = 480 \\ge 5\\).\n\nEstimate the proportion of all voters that voted for R using a 95% confidence interval.\n\n\\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}} = 0.52 \\pm z_{0.025}\\sqrt{\\frac{0.52(1-0.52)}{1000}} = (0.49, 0.55).\\]\n\nBelow is a demonstration of how to perform a binomial test in R.\n\n\n# Use alternative = \"two.sided\" to get CI\n# binom.test()\nprop.test(x = 520, n = 1000, p = 0.5, alternative = \"greater\", correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  520 out of 1000, null probability 0.5\nX-squared = 1.6, df = 1, p-value = 0.103\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.4939945 1.0000000\nsample estimates:\n   p \n0.52"
  },
  {
    "objectID": "infer-prop.html#inference-for-two-proportions",
    "href": "infer-prop.html#inference-for-two-proportions",
    "title": "18¬† Inference About Proportions",
    "section": "18.3 Inference for Two Proportions",
    "text": "18.3 Inference for Two Proportions\n\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\n\\(n_1\\) trials\n\\(n_2\\) trials\n\n\n\\(Y_1\\) number of successes\n\\(Y_2\\) number of successes\n\n\n\\(Y_1 \\sim binomial(n_1, \\pi_1)\\)\n\\(Y_2 \\sim binomial(n_2, \\pi_2)\\)\n\n\n\n\n\\(\\pi_1\\): Population proportion of success of Group 1\n\\(\\pi_2\\): Population proportion of success of Group 2\n\n\n\n\n\nIs the male presidential approval rate, \\(\\pi_1\\), higher than the female approval rate, \\(\\pi_2\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\) \n Step 0: Check Method Assumptions \n\n \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\) \n\n Step 1: Set the Null and Alternative Hypothesis \n\n \\(\\begin{align} &H_0: \\pi_1 = \\pi_2 \\\\ &H_1: \\pi_1 > \\pi_2 \\text{ or } \\pi_1 < \\pi_2 \\text{ or } \\pi_1 \\ne \\pi_2 \\end{align}\\) \n\n Step 2: Set the Significance Level, \\(\\alpha\\) \n Step 3: Calculate the Test Statistic \n\n \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})}\\), \\(\\bar{\\pi} = \\frac{y_1+y_2}{n_1+n_2}\\) is the pooled sample proportion estimating \\(\\pi\\) \n\n Step 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed) \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \n\n \\(H_1: \\pi_1 > \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z > z_{\\alpha}\\) \n \\(H_1: \\pi_1 < \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z < -z_{\\alpha}\\) \n \\(H_1: \\pi_1 \\ne \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| > z_{\\alpha/2}\\) \n\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n Confidence Interval for \\(\\pi_1 - \\pi_2\\)\n\nThe \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi_1 - \\pi_2\\) is \\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\nRequirements:\n\n\\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\)\n\n\n\n Example: Effectiveness of Learning \n\n\n\nSuppose we do a study on 300 students to compare the effectiveness of learning statistics in online vs.¬†in-person programs.\nRandomly assign\n\n125 students to the online program\nthe remaining 175 to the in-person program\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam Results\nOnline Instruction\nIn-Person Instruction\n\n\n\n\nPass\n94\n113\n\n\nFail\n31\n62\n\n\nTotal\n125\n175\n\n\n\n\n\n\n\n\n\nIs there sufficient evidence to conclude that the online program is more effective than the traditional in-person program at \\(\\alpha=0.05\\)?\n\n\n\n\n\n\n Hypothesis Testing \n Step 0 \n\n \\(\\hat{\\pi}_1 = 94/125 = 0.75\\) and \\(\\hat{\\pi}_2 = 113/175 = 0.65\\). \n \\(n_1\\hat{\\pi}_1 = 94 > 5\\), \\(n_1(1-\\hat{\\pi}_1) = 31 > 5\\), and \\(n_2\\hat{\\pi}_2 = 113 > 5\\), \\(n_2(1-\\hat{\\pi}_2) = 62 > 5\\) \n\n Step 1 \n\n \\(H_0: \\pi_1 = \\pi_2\\) vs.¬†\\(H_1: \\pi_1 > \\pi_2\\) \n\n\\(\\pi_1\\) \\((\\pi_2)\\) is the population proportion of students passing the exam in the online (in-person) program.\n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(\\bar{\\pi} = \\frac{94+113}{125+175} = 0.69\\) \n \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})} = \\frac{0.75 - 0.65}{\\sqrt{0.69(1-0.69)(\\frac{1}{125} + \\frac{1}{175})}} = 1.96\\) \n\n Step 4-c \n\n \\(z_{\\alpha} = z_{0.05} = 1.645\\) \n\n Step 5-c \n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} > z_{\\alpha}\\). \n Since \\(z_{test} > z_{\\alpha}\\), we reject \\(H_0\\). \n\n Step 6 \n\n We have sufficient evidence to conclude that the online program is more effective. \n\n Confidence Interval \n\nWe want to know how effective the online program is.\nEstimate \\(\\pi_1 - \\pi_2\\) using a \\(95\\%\\) confidence interval. \\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\n\\(z_{0.05/2} = 1.96\\)\nThe 95% confidence interval is \\[0.75 - 0.65 \\pm 1.96\\sqrt{\\frac{(0.75)(1-0.75)}{125} + \\frac{(0.65)(1-0.65)}{175}}\\\\\n= (0.002, 0.210)\\]\nBecause 0 is not included in this interval, we reach the same conclusion as the hypothesis testing.\n\n Implementation in R \n\nBelow is a demonstration of how to make inferences about two proportions in R.\n\n\n# Use alternative = \"two.sided\" to get CI\nprop.test(x = c(94, 113), n = c(125, 175), alternative = \"greater\", correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(94, 113) out of c(125, 175)\nX-squared = 3.8509, df = 1, p-value = 0.02486\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01926052 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.7520000 0.6457143 \n\nprop_ci <- prop.test(x = c(94, 113), n = c(125, 175), alternative = \"two.sided\", correct = FALSE)\nprop_ci$conf.int\n\n[1] 0.002588801 0.209982628\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "infer-indep.html#contingency-table-and-expected-count",
    "href": "infer-indep.html#contingency-table-and-expected-count",
    "title": "20¬† Test of Independence",
    "section": "20.1 Contingency Table and Expected Count",
    "text": "20.1 Contingency Table and Expected Count\n Contingency Table \n\nHave TWO categorical variables, and want to test whether or not the two variables are independent.\n\n Does the ‚ÄúOpinion on President‚Äôs Job Performance‚Äù depend on ‚ÄúGender‚Äù? \n\n\n\nJob performance: approve, disapprove, no opinion\nGender: male, female\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18\n22\n10\n\n\nFemale\n23\n20\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Expected Count \n\nCompute the expected count of each cell in the two-way table under the condition that the two variables were independent with each other.\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\nThe expected count for the \\(i\\)th row and \\(j\\)th column: \\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\]"
  },
  {
    "objectID": "infer-indep.html#procedure",
    "href": "infer-indep.html#procedure",
    "title": "20¬† Test of Independence",
    "section": "20.2 Procedure",
    "text": "20.2 Procedure\n\nRequire every \\(E_{ij} \\ge 5\\) in the contingency table.\n \\(\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\)\n\\(\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the table.\nReject \\(H_0\\) if \\(\\chi^2_{test} > \\chi^2_{\\alpha, \\, df}\\), \\(df = (r-1)(c-1)\\)."
  },
  {
    "objectID": "model-logistic.html#classification",
    "href": "model-logistic.html#classification",
    "title": "25¬† Logistic Regression",
    "section": "25.1 Classification",
    "text": "25.1 Classification\n Regression vs.¬†Classification \n\nLinear regression assumes that the response \\(Y\\) is numerical.\nIn many situations, \\(Y\\) is categorical.\n\n\n\nNormal vs.¬†COVID vs.¬†Smoking\n\n\n\n\n\n\n\n\n\n\n\n\nFake news vs.¬†True news\n\n\n\n\n\n\n\n\n\n\n\n\nA process of predicting categorical response is known as classification.\n\n\n Regression Function \\(f(x)\\) vs.¬†Classifier \\(C(x)\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://daviddalpiaz.github.io/r4sl/classification-overview.html\n\n\n Example \n\nPredict whether people will default on their credit card payment \\((Y)\\) yes or no, based on monthly credit card balance \\((X)\\).\nWe use the sample data \\(\\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\) to build a classifier.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Why Not Linear Regression? \n\\[Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}\\]\n\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), \\(\\, X =\\) credit card balance\n\n\n\n\n\n\n\nWhat is the problem with this dummy variable approach?\n\n\n\n\n\n\n Simple Linear Regression \n\n\\(\\hat{Y} = b_0 + b_1X\\) estimates \\(P(Y = 1 \\mid X) = P(default = yes \\mid balance)\\) \n\n\n\n\n\n\n\n\n\n\n\nSome estimates might be outside \\([0, 1]\\).\n\n Simple Logistic Regression \n\nFirst predict the probability of each category of \\(Y\\).\nPredict probability of default using an S-shaped curve."
  },
  {
    "objectID": "model-logistic.html#logistic-regression-1",
    "href": "model-logistic.html#logistic-regression-1",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Logistic Regression",
    "text": "25.3 Logistic Regression\n\nLogistic regression models a binary response \\((Y)\\) using predictors \\(X_1, \\dots, X_k\\).\n\n\\(k = 1\\): simple logistic regression\n\\(k > 1\\): multiple logistic regression\n\n\n\nInstead of predicting \\(y_i\\) directly, we use the predictors to model its probability of success, \\(\\pi_i\\).\n\n\nBut how?\n\n\nTransform \\(\\pi \\in (0, 1)\\) into another variable \\(\\eta \\in (-\\infty, \\infty)\\). Then fit a linear regression on \\(\\eta\\).\nLogit function: For \\(0 < \\pi < 1\\)\n\n\\[\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\]"
  },
  {
    "objectID": "prob-llnclt.html#central-limit-theorem",
    "href": "prob-llnclt.html#central-limit-theorem",
    "title": "12¬† Law of Large Numbers and Central Limit Theorem",
    "section": "\n12.1 Central Limit Theorem",
    "text": "12.1 Central Limit Theorem\n\nWe know if \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) , then \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\).\n What if the population distribution is NOT normal? \n\n\nThe central limit theorem (CLT) gives us the answer!\n\n\n\n\nCentral Limit Theorem (CLT): Suppose \\(\\overline{X}\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma < \\infty\\). As \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\), regardless of the distribution from which we are sampling!\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png\n\n\n\n\n Illustration \n A Right-Skewed Distribution \n\n\n\n\n\n\n\n\n A U-shaped Distribution \n\n\n\n\n\n\n Why is the Central Limit Theorem Important? \n\n\nMany well-developed statistical methods are based on normal distribution assumption.\nWith CLT, we can use those methods even if we are sampling from a non-normal distribution, or we have no idea of the population distribution, provided that the sample size is large."
  },
  {
    "objectID": "model-logistic.html#evaluation-metrics",
    "href": "model-logistic.html#evaluation-metrics",
    "title": "25¬† Logistic Regression",
    "section": "25.5 Evaluation Metrics",
    "text": "25.5 Evaluation Metrics\n Sensitivity and Specificity \n\n\n\n\n\n\n\n\n\n1\n0\n\n\n\n\nLabeled 1\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nLabeled 0\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\nSensitivity (True Positive Rate) \\(= P( \\text{Labeled 1} \\mid \\text{1}) = \\frac{TP}{TP+FN}\\)\nSpecificity (True Negative Rate) \\(= P( \\text{Labeled 0} \\mid \\text{0}) = \\frac{TN}{FP+TN}\\)\nAccuracy \\(= \\frac{TP + TN}{TP+FN+FP+TN}\\) \nMore on Wiki page\n\n\n Confusion Matrix \n\nprob <- predict(logit_fit, type = \"response\")\n\n## true observations\ngender_true <- body$GENDER\n\n## predicted labels\ngender_predict <- (prob > 0.5) * 1\n\n## confusion matrix\ntable(gender_predict, gender_true)\n\n              gender_true\ngender_predict   0   1\n             0 118  29\n             1  29 124\n\n\n\n Receiver Operating Characteristic (ROC) Curve \n\nReceiver operating characteristic (ROC) curve\n\nPlots True Positive Rate (Sensitivity) vs.¬†False Positive Rate (1 - Specificity)\n\nR packages for ROC curves: ROCR and pROC, yardstick of Tidymodels\n\n\n\n\n\n\nFigure¬†25.7: ROC curve for Gender ~ Height"
  },
  {
    "objectID": "model-logistic.html#introduction-to-logistic-regression",
    "href": "model-logistic.html#introduction-to-logistic-regression",
    "title": "25¬† Logistic Regression",
    "section": "25.2 Introduction to Logistic Regression",
    "text": "25.2 Introduction to Logistic Regression\n Binary Responses \n\nTreat each outcome, default \\((y = 1)\\) and not default \\((y = 0)\\), as success and failure arising from separate Bernoulli trials.\n\n\n\n\n\n\n\nWhat is a Bernoulli trial?\n\n\n\n\nA Bernoulli trial is a special case of a binomial trial when the number of trials is \\(m = 1\\).\n\nThere are exactly two possible outcomes, ‚Äúsuccess‚Äù and ‚Äúfailure‚Äù.\nThe probability of success, \\(\\pi\\), is constant.\n\n\n\n\n\n\n\n\n\n\nIn the default credit card example,\n\n\n\n\nDo we have exactly two outcomes?\nDo we have constant probability? \\(P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?\\)\n\n\n\n Nonconstant Probability\n\n\n\nTwo outcomes: Default \\((y = 1)\\) and Not Default \\((y = 0)\\)\nThe probability of success, \\(\\pi\\), changes with the value of predictor, \\(X\\)!\nWith a different value of \\(x_i\\), each Bernoulli trial outcome, \\(y_i\\), has a different probability of success, \\(\\pi_i\\).\n\n\n\n\n\n\n\n\n\n\n\n\\[ y_i \\mid x_i \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i)) = binomial(m=1,\\pi = \\pi(x_i)) \\]\n\n\\(X =\\) balance. \\(x_1 = 2000\\) has a larger \\(\\pi_1 = \\pi(2000)\\) than \\(\\pi_2 = \\pi(500)\\) with \\(x_2 = 500\\) because credit cards with a higher balance are more likely to default.\n\n\n Logistic Regression \n\nLogistic regression models a binary response \\((Y)\\) using predictors \\(X_1, \\dots, X_k\\).\n\n\\(k = 1\\): simple logistic regression\n\\(k > 1\\): multiple logistic regression\n\nInstead of predicting \\(y_i\\) directly, we use the predictors to model its probability of success, \\(\\pi_i\\).\n\n\nBut how?\n\n\n Logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) \n\nTransform \\(\\pi \\in (0, 1)\\) into another variable \\(\\eta \\in (-\\infty, \\infty)\\). Then fit a linear regression on \\(\\eta\\).\nLogit function: For \\(0 < \\pi < 1\\)\n\n\\[\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\]\n\n\n\n\n\nFigure¬†25.5: Graphical illustration of the logit function\n\n\n\n\n Logistic Function \\(\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\) \n\nThe logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\).\nLogistic function: \\[\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]\nThe logistic function takes a value \\(\\eta \\in (-\\infty, \\infty)\\) and maps it to a value \\(\\pi \\in (0, 1)\\).\nSo once \\(\\eta\\) is estimated by the linear regression, we use the logistic function to transform \\(\\eta\\) back to the probability.\n\n\n\n\n\n\nFigure¬†25.6: Graphical illustration of the logistic function"
  },
  {
    "objectID": "model-logistic.html#logistic-regression-model",
    "href": "model-logistic.html#logistic-regression-model",
    "title": "25¬† Logistic Regression",
    "section": "25.3 Logistic Regression Model",
    "text": "25.3 Logistic Regression Model\n Simple Model \nFor \\(i = 1, \\dots, n\\) and with one predictor \\(X\\): \\[(Y_i \\mid X = x_i) \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i))\\] \\[\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi(x_i)}{1 - \\pi(x_i)} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i}\\] \n\\[\\small \\pi_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{i})}{1+\\exp(\\beta_0+\\beta_1 x_{i})} = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}\\]\n\\[\\small \\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i} )}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i})}\\]\n\n Probability Curve \n\n\n\n\nThe relationship between \\(\\pi(x)\\) and \\(x\\) is not linear! \\[\\pi(x) = \\frac{\\exp(\\beta_0+\\beta_1 x)}{1+\\exp(\\beta_0+\\beta_1 x)}\\]\nThe amount that \\(\\pi(x)\\) changes due to a one-unit change in \\(x\\) depends on the current value of \\(x\\).\nRegardless of the value of \\(x\\), if \\(\\beta_1 > 0\\), increasing \\(x\\) will be increasing \\(\\pi(x)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Interpretation of Coefficients \nThe ratio \\(\\frac{\\pi}{1-\\pi} \\in (0, \\infty)\\) is called the odds of some event. - Example: If 1 in 5 people will default, the odds is 1/4 since \\(\\pi = 0.2\\) implies an odds of \\(0.2/(1‚àí0.2) = 1/4\\).\n\\[\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x\\]\n\nIncreasing \\(x\\) by one unit changes the log-odds by \\(\\beta_1\\), or it multiplies the odds by \\(e^{\\beta_1}\\).\n\n\n\n\n\n\n\nNote\n\n\n\n\n\\(\\beta_1\\) does not correspond to the change in \\(\\pi(x)\\) associated with a one-unit increase in \\(x\\).\n\\(\\beta_1\\) is the change in log odds associated with one-unit increase in \\(x\\).\n\n\n\n\n Logistic Regression in R \n\n\n\n\nGENDER = 1 if male\nGENDER = 0 if female\nUse HEIGHT (centimeter, 1 cm = 0.3937 in) to predict/classify GENDER: whether the person is male or female.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbody <- read.table(\"./data/01 - Body Data.txt\", header = TRUE)\nhead(body)\n\n  AGE GENDER PULSE SYSTOLIC DIASTOLIC HDL LDL WHITE  RED PLATE WEIGHT HEIGHT\n1  43      0    80      100        70  73  68   8.7 4.80   319   98.6  172.0\n2  57      1    84      112        70  35 116   4.9 4.73   187   96.9  186.0\n3  38      0    94      134        94  36 223   6.9 4.47   297  108.2  154.4\n4  80      1    74      126        64  37  83   7.5 4.32   170   73.1  160.5\n5  34      1    50      114        68  50 104   6.1 4.95   140   83.1  179.0\n6  77      1    60      134        60  55  75   5.7 3.95   192   86.5  166.7\n  WAIST ARM_CIRC  BMI\n1 120.4     40.7 33.3\n2 107.8     37.0 28.0\n3 120.3     44.3 45.4\n4  97.2     30.3 28.4\n5  95.1     34.0 25.9\n6 112.0     31.4 31.1\n\n\n Data Summary \n\n\n\ntable(body$GENDER)\n\n\n  0   1 \n147 153 \n\nsummary(body[body$GENDER == 1, ]$HEIGHT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  155.0   169.1   173.8   174.1   179.4   193.3 \n\nsummary(body[body$GENDER == 0, ]$HEIGHT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  134.5   156.5   162.2   161.7   166.8   181.4 \n\n\n\n\n\n\nboxplot(body$HEIGHT ~ body$GENDER)\n\n\n\n\n\n\n Model Fitting \n\nlogit_fit <- glm(GENDER ~ HEIGHT, data = body, family = \"binomial\")\n(summ_logit_fit <- summary(logit_fit))\n\n\nCall:\nglm(formula = GENDER ~ HEIGHT, family = \"binomial\", data = body)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.5839  -0.6660   0.1078   0.6554   2.4998  \n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -40.54809    4.63084  -8.756   <2e-16 ***\nHEIGHT        0.24173    0.02758   8.764   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 415.77  on 299  degrees of freedom\nResidual deviance: 251.50  on 298  degrees of freedom\nAIC: 255.5\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nsumm_logit_fit$coefficients\n\n               Estimate Std. Error   z value     Pr(>|z|)\n(Intercept) -40.5480864 4.63083742 -8.756102 2.021182e-18\nHEIGHT        0.2417325 0.02758399  8.763507 1.892674e-18\n\n\n\n\\(\\hat{\\eta} = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -40.55 + 0.24 \\times \\text{HEIGHT}\\)\n\\(\\hat{\\eta}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\)\n\\(\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)\\)\n\\(\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x}) = \\ln \\left( \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} \\right)\\)\nOne cm increase in HEIGHT increases the log odds of being male by 0.24 units.\nThe odds ratio, \\(\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.24} = 1.273\\).\nThe odds of being male increases by 27.3% with additional one cm of HEIGHT.\n\n\n Prediction \n Pr(GENDER = 1) When HEIGHT is 170 cm \n\n\\[ \\hat{\\pi}(x = 170) = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x)}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x)} = \\frac{\\exp(-40.55+0.24 \\times 170)}{1+\\exp(-40.55+0.24 \\times 170)} = 0.633 = 63.3\\%\\]\n\npi_hat <- predict(logit_fit, type = \"response\")\neta_hat <- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(HEIGHT = 170), type = \"response\")\n\n        1 \n0.6333105 \n\n\n Probability Curve \n\n\n\n\n\n\nWhat is the probability of being male when HEIGHT is 160? What about HEIGHT 180?\n\n\n\n\n\n\n\npredict(logit_fit, newdata = data.frame(HEIGHT = c(160, 170, 180)), type = \"response\")\n\n        1         2         3 \n0.1334399 0.6333105 0.9509103 \n\n\n\n\n\n\n\n\n\n\n\n 160 cm, Pr(male) = 0.13\n 170 cm, Pr(male) = 0.63\n 180 cm, Pr(male) = 0.95"
  },
  {
    "objectID": "infer-goodnessfit.html#contingency-table-and-expected-count",
    "href": "infer-goodnessfit.html#contingency-table-and-expected-count",
    "title": "19¬† Inference about Categorical Data",
    "section": "19.2 Contingency Table and Expected Count",
    "text": "19.2 Contingency Table and Expected Count\n Contingency Table \n\nHave TWO categorical variables, and want to test whether or not the two variables are independent.\n\n Does the ‚ÄúOpinion on President‚Äôs Job Performance‚Äù depend on ‚ÄúGender‚Äù? \n\n\n\nJob performance: approve, disapprove, no opinion\nGender: male, female\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18\n22\n10\n\n\nFemale\n23\n20\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Expected Count \n\nCompute the expected count of each cell in the two-way table under the condition that the two variables were independent with each other.\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\nThe expected count for the \\(i\\)th row and \\(j\\)th column: \\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\]"
  },
  {
    "objectID": "infer-goodnessfit.html#test-of-independence-procedure",
    "href": "infer-goodnessfit.html#test-of-independence-procedure",
    "title": "19¬† Inference about Categorical Data",
    "section": "19.3 Test of Independence Procedure",
    "text": "19.3 Test of Independence Procedure\n\nRequire every \\(E_{ij} \\ge 5\\) in the contingency table.\n \\(\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\)\n\\(\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the table.\nReject \\(H_0\\) if \\(\\chi^2_{test} > \\chi^2_{\\alpha, \\, df}\\), \\(df = (r-1)(c-1)\\).\n\n\n Example \n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\n \\(\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\)\n\\(\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\)\n\\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\).\nSince \\(\\chi_{test}^2 < \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\).\nWe do not conclude that the Opinion on President‚Äôs Job Performance depends on Gender.\n\n\n Test of Independence in R \n\n(contingency_table <- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(indept_test <- chisq.test(contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.65019, df = 2, p-value = 0.7225\n\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  ## critical value\n\n[1] 5.991465"
  },
  {
    "objectID": "infer-goodnessfit.html#test-of-independence",
    "href": "infer-goodnessfit.html#test-of-independence",
    "title": "19¬† Inference about Categorical Data",
    "section": "19.2 Test of Independence",
    "text": "19.2 Test of Independence\n Contingency Table and Expected Count\n Contingency Table \n\nWe have TWO categorical variables, and we want to test whether or not the two variables are independent.\n Does the opinion of the President‚Äôs job performance depend on gender? \n\n\n\n\nJob performance: approve, disapprove, no opinion\nGender: male, female\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18\n22\n10\n\n\nFemale\n23\n20\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Expected Count \n\nCompute the expected count of each cell in the two-way table under the condition that the two variables are independent of each other.\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\nThe expected count for the \\(i\\)-th row and \\(j\\)-th column, which is listed in parentheses in the table above, is: \\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\]\n\n\n Test of Independence Procedure \n\nRequirements:\n\nEvery \\(E_{ij} \\ge 5\\) in the contingency table.\n\n \\(\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\)\n\\(\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the table.\nReject \\(H_0\\) if \\(\\chi^2_{test} > \\chi^2_{\\alpha, \\, df}\\), \\(df = (r-1)(c-1)\\).\n\n Example \n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\n \\(\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\)\n\\(\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\)\n\\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\).\nSince \\(\\chi_{test}^2 < \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\).\nWe fail to conclude that the opinion of the President‚Äôs job performance depends on gender.\n\n Test of Independence in R \n\nBelow is an example of how to perform the test of independence using R.\n\n\n(contingency_table <- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(indept_test <- chisq.test(contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.65019, df = 2, p-value = 0.7225\n\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  ## critical value\n\n[1] 5.991465"
  },
  {
    "objectID": "model-reg.html#introduction-to-regression",
    "href": "model-reg.html#introduction-to-regression",
    "title": "24¬† Linear Regression",
    "section": "24.2 Introduction to Regression",
    "text": "24.2 Introduction to Regression\n What is Regression? \n\nRegression models the relationship between one or more numerical/categorical response variables \\((Y)\\) and one or more numerical/categorical explanatory variables \\((X)\\).\nA regression function, \\(f(X)\\), describes how a response variable, \\(Y\\), on average, changes as an explanatory variable, \\(X\\), changes.\n\n\n\n\nExamples:\n\n College GPA \\((Y)\\) vs.¬†ACT/SAT score \\((X)\\)\n Sales \\((Y)\\) vs.¬†Advertising Expenditure \\((X)\\)\n Crime Rate \\((Y)\\) vs.¬†Median Income Level \\((X)\\) \n\n\n\n\n\n\n\n\nFigure¬†24.6: Example of a Regression Function\n\n\n\n\n\n\n\n Unknown Regression Function \n\nThe true relationship between \\(X\\) and the mean of \\(Y\\), the regression function \\(f(X)\\), is unknown.\nThe collected data \\((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\) are all we know and have.\n\n\n\n\n\n\nFigure¬†24.7: Data with an unknown regression function\n\n\n\n\n\nGoal: Estimate \\(f(X)\\) from our data, and use it to predict the value of \\(Y\\) given a value of \\(X\\).\n\n\n Simple Linear Regression \n\nStart with simple linear regression.\n\nThere is only one predictor, \\(X\\) (known and constant), and one response variable, \\(Y\\).\nThe regression function used for predicting \\(Y\\) is a linear function.\nUse a regression line in an X-Y plane to predict the value of \\(Y\\) for a given value of \\(X = x\\).\n\n\n\n\n\nMath review: A linear function \\(y = f(x) = \\beta_0 + \\beta_1 x\\) represents a straight line.\n\n\\(\\beta_1\\): slope, the amount by which \\(y\\) changes when \\(x\\) increases by one unit\n\\(\\beta_0\\): intercept, the value of \\(y\\) when \\(x = 0\\)\nThe linearity assumption: \\(\\beta_1\\) does not change as \\(x\\) changes.\n\n\n\n\n\n\n\n\nFigure¬†24.8: Example of a regression line\n\n\n\n\n\n\n Sample Data: Relationship Between X and Y \n\nReal data \\((x_i, y_i), i = 1, 2, \\dots, n\\) do not form a perfect straight line!\n\\(y_i = \\beta_0+\\beta_1x_i + \\color{red}{\\epsilon_i}\\)\n\n\n\n\n\n\nFigure¬†24.9: Actual relationship between X and Y isn‚Äôt perfectly linear\n\n\n\n\n\nWhen we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed to be drawn from a normal distribution (for inference purpose).\nIts value varies and will not be exactly equal to its mean, \\(\\mu_y\\).\n\n\n\n\n\n\nFigure¬†24.10: Illustration that the responses, y, follow a normal distribution\n\n\n\n\n\nThe mean of \\(Y\\) and \\(X\\) form a straight line.\n\n\n\n\n\n\nFigure¬†24.11: Illustration that the regression line is formed from the mean of Y and X\n\n\n\n\n\n Simple Linear Regression Model (Population) \n\nFor the \\(i\\)-th measurement in the target population, \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\n\n\\(Y_i\\): the \\(i\\)-th value of the response (random) variable.\n\\(X_i\\): the \\(i\\)-th fixed known value of the predictor.\n\\(\\epsilon_i\\): the \\(i\\)-th random error with the assumption \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients.\n\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are fixed unknown parameters to be estimated from the sample data once we collect them.\n\n\n Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\n\\mu_{Y_i \\mid X_i} &= E(\\beta_0 + \\beta_1X_i + \\epsilon_i) \\\\\n&= \\beta_0 + \\beta_1X_i\n\\end{align*}\\]\n\nThe mean response, \\(\\mu_{Y\\mid X}\\), has a straight-line relationship with \\(X\\) given by a population regression line \\[\\mu_{Y\\mid X} = \\beta_0 + \\beta_1X\\]\n\n\n\n\n\n\n\nFigure¬†24.12: Illustration of the straight line relationship between \\(\\mu_{Y\\mid X}\\) and \\(X\\)\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\nVar(Y_i \\mid X_i) &= Var(\\epsilon_i) = \\sigma^2\n\\end{align*}\\]\n\nThe variance of \\(Y\\) does not depend on \\(X\\).\n\n\n\n\n\n\n\nFigure¬†24.13: \\(Y\\) has a constant variance regardless of \\(X\\).\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\]\n\nFor any fixed value of \\(X_i = x_i\\), the response, \\(Y_i\\), varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\).\n\n\n\n\n\n\n\nFigure¬†24.14: The response \\(Y_i\\) follows a normal distribution.\n\n\n\n\n\n\n\nJob: Collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!"
  },
  {
    "objectID": "model-survival.html#applications-of-life-tables",
    "href": "model-survival.html#applications-of-life-tables",
    "title": "27¬† Survival Analysis",
    "section": "27.2 Applications of Life Tables",
    "text": "27.2 Applications of Life Tables\n Social Security \n\n\n\nThere were 3,600,000 births in the U.S. in 2020.\nIf the age for receiving full Social Security payment is 67, how many of those born in 2020 are expected to be alive on their 67th birthday? Check the report!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmong 100,000 people born, we expect 81,637 of them will survive to their 67th birthday.\nTherefore, we expect that \\(3,600,000 \\times 0.81637 = 2,938,932\\) people born in 2020 will receive their full Social Security payment.\n\n\n Hypothesis Testing \n\n\n\nFor one city, there are 5000 people who reach their 16th birthday.\n25 of them die before their 17th birthday.\nDo we have sufficient evidence to conclude that this number of deaths is significantly high?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe probability of dying for the age interval of 16-17 is 0.000405.\nThis is a \\(H_1\\) claim.  \\(\\small \\begin{align} &H_0: \\pi = 0.000405 \\\\ &H_1: \\pi > 0.000405\\end{align}\\) \n\\(\\hat{\\pi} = 25/5000 = 0.005\\).\n\\(z = \\frac{0.005 - 0.000405}{\\sqrt{\\frac{(0.000405)(0.999595)}{5000}}} = 16.15\\)\n\\(P\\)-value \\(\\approx 0\\).\nThere is sufficient evidence to conclude that the proportion of deaths is significantly higher than the proportion that is usually expected for this age interval."
  },
  {
    "objectID": "model-survival.html#kaplan-meier-survival-analysis",
    "href": "model-survival.html#kaplan-meier-survival-analysis",
    "title": "27¬† Survival Analysis",
    "section": "27.3 Kaplan-Meier Survival Analysis",
    "text": "27.3 Kaplan-Meier Survival Analysis\n Survival Analysis \n\n\n\nThe life table method is based on fixed time intervals.\nThe Kaplan-Meier method\n\nis based on intervals that vary according to the times of survival to some particular terminating event.\nis used to describe the probability of surviving for a specific period of time.\n\n What is the probability of surviving for 5 more years after cancer chemotherapy? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Survival Time \n\nThe time lapse from the beginning of observation to the time of terminating event is considered the survival time (Figure¬†27.11).\n\n\n\n\n\n\nFigure¬†27.11: Graph of survival time\n\n\n\n\n\n Survivor \n\nA survivor is a subject that successfully lasted throughout a particular time period.\n\n\n\n\n\n\n\nNote\n\n\n\n\nA survivor does not necessarily mean living.\n\nA patient trying to stop smoking is a survivor if smoking has not resumed.\nYour iPhone that worked for some particular length of time can be considered a survivor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Censored Data \n\nSurvival times are censored data if the subjects\n\nsurvive past the end of the study\nare dropped from the study for reasons not related to the terminating event being studied.\n\n\n\n\n\n\n\nFigure¬†27.12: Illustration of censored data (https://unc.live/3K1ph8f)\n\n\n\n\n\n Example: Medication Treatment for Quitting Smoking \n\n\n\n\n\n\n\n\n\n\n\nDay\nStatus (0 = censored, 1 = Smoke Again)\nNumber of Patients\nPatients Not Smoking\nProportion Not Smoking\nCumulative Proportion Not Smoking\n\n\n\n\n1\n0\n\n\n\n\n\n\n3\n1\n4\n3\n3/4 = 0.75\n0.75\n\n\n4\n1\n3\n2\n2/3 = 0.67\n0.5\n\n\n7\n1\n2\n1\n1/2 = 0.5\n0.25\n\n\n21\n1\n1\n0\n0\n0\n\n\n\n\n\n\n‚ÄúSurviving‚Äù means the patient has NOT resumed smoking.\nAs shown in Figure¬†27.13, the Subject 1 disliked the medication and dropped out of the study on day one.\nThe table above also provides information regarding the study.\n\n2nd row: Subject 2 resumed smoking 3 days after the start of the program.\n3rd row: \\(0.5 = (3/4)(2/3)\\)\n4th row: \\(0.25 = (3/4)(2/3)(1/2)\\)\n5th row: \\(0 = (3/4)(2/3)(1/2)(0)\\)\n\n\n\n\n\n\n\n\n\n\nFigure¬†27.13: Survival time for five subjects receiving the medication treatment\n\n\n\n\n\n\n\n Example: Counseling Treatment for Quitting Smoking \n\n\n\n\n\n\n\n\n\n\n\nDay\nStatus  (0 = censored, 1 = Smoke Again)\nNumber of Patients\nPatients Not Smoking\nProportion Not Smoking\nCumulative Proportion Not Smoking\n\n\n\n\n2\n1\n10\n9\n9/10\n0.9\n\n\n4\n1\n9\n8\n8/9\n0.8\n\n\n5\n0\n\n\n\n\n\n\n8\n1\n7\n6\n6/7\n0.686\n\n\n9\n1\n6\n5\n5/6\n0.571\n\n\n12\n0\n\n\n\n\n\n\n14\n1\n4\n3\n3/4\n0.429\n\n\n22\n1\n3\n2\n2/3\n0.286\n\n\n24\n0\n\n\n\n\n\n\n28\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is the cumulative proportion on Day 8 0.686?\n\n\n\n\\[0.686 = (9/10)(8/9)(6/7)\\]\n\nOn Day 5 a patient dropped out, so we don‚Äôt know whether he resumed smoking on Day 8 or not.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†27.14: Survival time for ten subjects receiving the counseling treatment\n\n\n\n\n\n\n\n Kaplan-Meier Analysis \n\n\n\n\n\n\nWhich treatment is better for quitting smoking?\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†27.15: Data from the medication treatment group and the counseling treatment group are compared using a Kaplan-Meier plot"
  },
  {
    "objectID": "intro-stats.html#what-will-we-learn-in-this-course",
    "href": "intro-stats.html#what-will-we-learn-in-this-course",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.3 What Will We Learn In this Course?",
    "text": "1.3 What Will We Learn In this Course?\n\nBelow, the main topics of this course, as well as brief descriptions, are listed in the order in which they will be covered. .\n\n\n\n\n\n\n\n\n\n\n\nWe will spend most of our time talking about probability and the statistical inference methods that are circled on the list above.\nThis course will also include a focus on the statistical methods for analyzing data.\nIn summary, we will learn useful information\n\nabout the population we are interested in\nfrom our sample data\nthrough statistical inferential methods, including estimation and testing\n\n\n\n\n\n\n\nFigure¬†1.8: Illustration of obtaining sample data from a population"
  },
  {
    "objectID": "intro-r.html#lab-time-2",
    "href": "intro-r.html#lab-time-2",
    "title": "3¬† Tool foR Data",
    "section": "3.2 Lab Time!",
    "text": "3.2 Lab Time!\n\nIn the bar, click the desired workspace.\nClick New Project > New RStudio Project to get into the IDE.\nClick Untitled Project and give your project a nice name, math-4720 for example.\nIn the Console pane, write your first R code: a string \"Hello WoRld!\" or math 2 + 4.\nChange the editor theme: Tools > Global Options > Appearance"
  },
  {
    "objectID": "intro-r.html#working-in-posit-cloud",
    "href": "intro-r.html#working-in-posit-cloud",
    "title": "3¬† Tool foR Data",
    "section": "3.2 Working in Posit Cloud",
    "text": "3.2 Working in Posit Cloud\n Panes \n\n\n\n\n\nFigure¬†3.6: RStudio Panes\n\n\n\n\n\n Source Pane \n R Script \n\nAn R script is a .R file that contains R code.\nTo create an R script, go to File > New > R Script, or click the green-plus icon on the top left corner and select R Script.\n\n\n\n\n\n\nFigure¬†3.7: Creating an R script\n\n\n\n\n Python Script \n\nA Python script is a .py file that contains Python code.\nTo create a Python script, go to File > New > Python Script, or click the green-plus icon on the topleft corner and select Python Script.\n\n\n\n\n\n\nFigure¬†3.8: Creating a Python script\n\n\n\n\n Run Code \n\n Run : run the current line or selection of code.\n\nctrl + enter (Win) or cmd + enter (Mac)\n\n Icon to the right of Run : re-run the previous code.\n\nalt + ctrl + p (Win) or option + cmd + p (Mac)\n\n Source : run all the code in the R script.\n\nshift + ctrl + s (Win) or shift + cmd + s (Mac)\n\n Source with Echo : run all the code in the R script with the code printed in the console.\n\nshift + ctrl + enter (Win) or shift + cmd + enter (Mac)\n\n\n\n\n\n\n\nFigure¬†3.9: Running R Code\n\n\n\n\n Run Python Code \n\nRunning Python code may require you to update some packages. Please say YES!\nWhen you run the Python code in the R console, the console will switch from R to Python.\nType quit in the Python console to switch back to the R console.\n\n\n\n\n\n\n\n\n\n\n\n Environment Tab \n\nThe (global) environment is where we are currently working.\nAnything created or imported into the current R/Python session is stored in our environment and shown in the Environment tab.\nAfter we run the R script from Figure¬†3.7, the following objects are stored in the environment:\n\nData set mtcars\nObject x storing integer values 1 to 10.\nObject y storing three numeric values 3, 5, 9.\n\n\n\n\n\n\n\nFigure¬†3.10: Environment Pane\n\n\n\n\n\nAfter we run the Python script from Figure¬†3.8, the following object is stored in the environment:\n\nObject b storing a string Hello World!\n\n\n\n\n\n\n\n\n\n\n\n\n History Tab \n\nThe History tab keeps a record of all previous commands.\n\nSave icon: save all history to a file\nTo Console: send the selected commands to the console.\nTo Source : inserted the selected commands into the current script.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn the console pane, use ‚¨ÜÔ∏è to show the previous commands.\n\n\n\n R Packages üì¶ \n\nWhen we start an R session, only built-in packages like base, stats, graphics, etc. are available.\nInstalling packages is an easy way to get access to other data and functions.\n\n\n!\n\n\n Installing R Packages \n\n\n\nTo install a package, such as the ggplot2 package, we use the command\n\n\ninstall.packages(\"ggplot2\")\n\n\nA different option in the right-bottom pane is Packages > Install.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Loading R Packages üì¶ \n\n\n\nTo use any function or data in ggplot2, we write ggplot2:: followed by the name of the function or data.\n\n\nggplot2::ggplot(ggplot2::mpg, \n                ggplot2::aes(\n                    x = displ, \n                    y = hwy, \n                    colour = class)\n                ) + \n    ggplot2::geom_point()\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens when you run the code shown below?\n\nggplot(mpg, aes(x = displ, \n                y = hwy, \n                colour = class)) + \n    geom_point()\n\n\n\n\n\n\n\nWe can load the package into our R session using library().\nWith library(ggplot2), R knows the function and data are from the ggplot2 package.\n\n\nlibrary(ggplot2)\nggplot(mpg, aes(x = displ, \n                y = hwy, \n                colour = class)) + \n    geom_point()\n\n\n\n\n Help \n\nWhat if you don‚Äôt know how a function works or what a data set is about ‚ùì\n\nüëâ Simply type ? followed by the data name or function name to get more information.\n\n\n\n?mean\n?mtcars\n\n\n\n\n\n\n\nWhat does the function mean() do? What is the size of mpg?\n\n\n\n\n\n\n\nA document will show up in the Help tab, teaching you how to use the function or explaining the data set.\n\nAn example is shown for the mpg data set in Figure¬†3.11 below.\n\n\n\n\n\n\n\nFigure¬†3.11: Document explaining the mpg data set in the Help tab\n\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nWhat is the size of mtcars data?\nType mtcars and hit enter in the console to see the data set.\nDiscuss the data type of each variable.\nType mtcars[, 1] and hit enter in the console and discuss what you see."
  },
  {
    "objectID": "intro-stats.html#exercises",
    "href": "intro-stats.html#exercises",
    "title": "1¬† Science of Data and Data Science",
    "section": "1.4 Exercises",
    "text": "1.4 Exercises"
  },
  {
    "objectID": "intro-data.html#exercises",
    "href": "intro-data.html#exercises",
    "title": "2¬† Data",
    "section": "2.5 Exercises",
    "text": "2.5 Exercises\n\nData Type: Identify each of the following as numerical or categorical data.\n\nThe names of the companies that manufacture paper towels\nThe colors of cars\nThe heights of football players\n\nLevel of Measurements: Identify the level of measurement used in each of the following.\n\nThe weights of people in a sample of people living in Milwaukee.\nA physician‚Äôs descriptions of ‚Äúabstains from smoking, light smoker, moderate smoker, heavy smoker.‚Äù\nFlower classifications of ‚Äúrose, tulip, daisy.‚Äù\nSuzy measures time in days, with 0 corresponding to her birth date. The day before her birth is -1, the day after her birth is +1, and so on. Suzy has converted the dates of major historical events to her numbering system. What is the level of measurement of these numbers?\n\nDiscrete vs Continuous: Determine whether the data are discrete or continuous.\n\nThe length of stay (in days) for each COVID patient in Wisconsin.\nSeveral subjects are randomly selected and their heights are recorded.\nFrom a data set, we see that a male had an arm circumference of 31.28 cm.\nA sample of married couples is randomly selected and the number of animals in each family is recorded.\n\nSampling Method: Identify which of these types of sampling is used: random, stratified, or cluster.\n\nDr.¬†Yu surveys his statistics class by identifying groups of males and females, then randomly selecting 7 students from each of those two groups.\nDr.¬†Yu conducts a survey by randomly selecting 5 different sports teams at Marquette and surveying all of the student-athletes on those teams.\n427 subjects were randomly assigned to (1) meditation or (2) no mediation group to study the effectiveness of this mindfulness activity on lowering blood pressure.\n\nStudy Type: Determine whether the study is an experiment or an observational study, then identify a major problem with this study.\n\nIn a survey conducted by USA Today, 998 Internet users chose to respond to the question:‚ÄúHow often do you seek medical advice online?‚Äù 42% of the respondents said ‚Äúfrequently.‚Äù\nThe Physicians‚Äô Health Study involved 21,045 female physicians. Based on random selections, 11,224 of them were treated with aspirin and other other 9,821 were given placebos. The study was stopped early because it became clear that aspirin did not reduce the risk of myocardial infarctions by a substantial amount."
  },
  {
    "objectID": "intro-r.html#exercises",
    "href": "intro-r.html#exercises",
    "title": "3¬† Tool foR Data",
    "section": "3.7 Exercises",
    "text": "3.7 Exercises\n\n# ==============================================================================\n## Vector\n# ==============================================================================\npoker_vec <- c(170, -20, 50, -140, 210)\nroulette_vec <- c(-30, -40, 70, -340, 20)\ndays_vec <- c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\")\nnames(poker_vec) <- days_vec\nnames(roulette_vec) <- days_vec\n\n\nVector\n\nThe code above shows a Marquette student poker and roulette winnings from Monday to Friday. Copy and paste them into your R and complete problem 1.\n\nAssign to the variable total_daily how much you won or lost on each day in total (poker and roulette combined).\nCalculate the winnings overall total_week. Print it out.\n\n\n\n# ==============================================================================\n## Factor\n# ==============================================================================\n# Create speed_vector\nspeed_vec <- c(\"medium\", \"low\", \"low\", \"medium\", \"high\")\n\n\nFactor\n\n\nspeed_vec above should be converted to an ordinal factor since its categories have a natural ordering. Create an ordered factor vector speed_fac by completing the code below. Set ordered to TRUE, and set levels to c(\"low\", \"medium\", \"high\"). Print speed_fac.\n\n\n_________ <- factor(________, ordered = ______, \n                    levels = ______________________)\n\n\n\n# ==============================================================================\n## Data frame\n# ==============================================================================\n# Definition of vectors\nname <- c(\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \n          \"Uranus\", \"Neptune\")\ntype <- c(\"Terrestrial planet\", \"Terrestrial planet\", \"Terrestrial planet\", \n          \"Terrestrial planet\", \"Gas giant\", \"Gas giant\", \n          \"Gas giant\", \"Gas giant\")\ndiameter <- c(0.375, 0.947, 1, 0.537, 11.219, 9.349, 4.018, 3.843)\nrotation <- c(57.63, -242.03, 1, 1.05, 0.42, 0.44, -0.73, 0.65)\nrings <- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\n\n\nData Frame\n\nData frames have properties of lists and matrices, so we skip lists and matrices and focus on data frames. You want to construct a data frame that describes the main characteristics of eight planets in our solar system. You feel confident enough to create the necessary vectors: name, type, diameter, rotation and rings that have already been coded up as above. The first element in each of these vectors corresponds to the first observation.\n\nUse the function data.frame() to construct a data frame. Pass the vectors name, type, diameter, rotation and rings as arguments to data.frame(), in this order. Call the resulting data frame planets_df.\n\n\n________ <- data.frame(______, ______, ______, ______, ______)\n\n\nUse str() to investigate the structure of the new planets_df variable. Which are categorical (qualitative) variables and which are numerical (quantitative) variables? For those that are categorical, are they nominal or ordinal? For those numerical variables, are they interval or ratio level? discrete or continuous?\nFrom planets_df, select the diameter of Mercury: this is the value at the first row and the third column. Simply print out the result.\nFrom planets_df, select all data on Mars (the fourth row). Simply print out the result.\nSelect and print out the first 5 values in the diameter column of planets_df.\nUse $ to select the rings variable from planets_df.\nUse (f) to select all columns for planets that have rings."
  },
  {
    "objectID": "data-graphics.html#exercises",
    "href": "data-graphics.html#exercises",
    "title": "4¬† Data Visualization",
    "section": "4.3 Exercises",
    "text": "4.3 Exercises\nIn the following, we will be using the data set mtcars to do some data summary and graphics. First load the data set into your R session by the command data(mtcars). The data set is like\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nPlease see ?mtcars for the description of the data set.\n\nUse the function pie() to create a pie chart for the number of carburetors (carb). What the number of carburetors has the most frequencies in the data?\nUse the function barplot() to create a bar chart for the number of cylinders (cyl). What the number of cylinders has the most frequencies in the data?\nUse the function hist() to generate a histogram of the gross horsepower (hp). Is it right or left-skewed?\nUse the function plot() to create a scatter plot of weight (wt) vs.¬†miles per gallon (mpg). As the weight increases, does the miles per gallon tend to increase or decrease?"
  },
  {
    "objectID": "data-numerics.html#exercises",
    "href": "data-numerics.html#exercises",
    "title": "5¬† Data Sample Statistics",
    "section": "5.5 Exercises",
    "text": "5.5 Exercises\n\nIn the following, we will be using the data set mtcars to do some data summary and graphics. First load the data set into your R session by the command data(mtcars). The data set is like\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nPlease see ?mtcars for the description of the data set.\n\nUse the function boxplot() to generate a boxplot of 1/4 mile time (qsec). Are there any outliers?\nCompute the mean, median and standard deviation of displacement (disp).\n\n\nMean and standard deviation (SD): For each part, compare data (1) and (2) based on their mean and SDs. You don‚Äôt need to calculate these statistics, but compare (1) and (2) by stating which one has a larger mean/SD or they have the same mean/SD. Explain your reasoning.\n\n\n-30, 0, 0, 0, 15, 25, 25\n-50, 0, 0, 0, 15, 20, 25\n\n\n\n0, 1, 3, 5, 7\n21, 23, 25, 27, 29\n\n\n\n100, 200, 300, 400, 500\n0, 50, 350, 500, 600\n\n\nSkewness: Facebook data indicate that \\(50\\%\\) of Facebook users have 130 or more friends, and that the average friend count of users is 115. What do these findings suggest about the shape (right-skewed, left-skewed, symmetric) of the distribution of number of friends of Facebook users? Please explain."
  },
  {
    "objectID": "prob-define.html#exercises",
    "href": "prob-define.html#exercises",
    "title": "6¬† Definition of Probability",
    "section": "6.3 Exercises",
    "text": "6.3 Exercises"
  },
  {
    "objectID": "prob-rule.html#exercises",
    "href": "prob-rule.html#exercises",
    "title": "7¬† Probability Rules",
    "section": "\n7.4 Exercises",
    "text": "7.4 Exercises\n\nA Pew Research survey asked 2,422 randomly sampled registered voters their political affiliation (Republican, Democrat, or Independent) and whether or not they identify as swing voters. 38% of respondents identified as Independent, 25% identified as swing voters, and 13% identified as both.\n\nAre being Independent and being a swing voter disjoint, i.e.¬†mutually exclusive?\nWhat percent of voters are Independent but not swing voters?\nWhat percent of voters are Independent or swing voters?\nWhat percent of voters are neither Independent nor swing voters?\nIs the event that someone is a swing voter independent of the event that someone is a political Independent?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarth is warming\nNot warming\nDon‚Äôt Know/Refuse\nTotal\n\n\n\nConservative Republican\n0.11\n0.20\n0.02\n0.33\n\n\nMod/Lib Republican\n0.06\n0.06\n0.01\n0.13\n\n\nMod/Cons Democrat\n0.25\n0.07\n0.02\n0.34\n\n\nLiberal Democrat\n0.18\n0.01\n0.01\n0.20\n\n\nTotal\n0.60\n0.34\n0.06\n1.00\n\n\n\n\nA Pew Research poll asked 1,423 Americans, ‚ÄúFrom what you‚Äôve read and heard, is there solid evidence that the average temperature on earth has been getting warmer over the past few decades, or not?‚Äù. The table above shows the distribution of responses by party and ideology, where the counts have been replaced with relative frequencies.\n\nAre believing that the earth is warming and being a liberal Democrat mutually exclusive?\nWhat is the probability that a randomly chosen respondent believes the earth is warming or is a Mod/Cons Democrat?\nWhat is the probability that a randomly chosen respondent believes the earth is warming given that he is a Mod/Cons Democrat?\nWhat is the probability that a randomly chosen respondent believes the earth is warming given that he is a Mod/Lib Republican?\nDoes it appear that whether or not a respondent believes the earth is warming is independent of their party and ideology? Explain your reasoning.\n\n\nAfter an MATH 4740/MSSC 5740 course, 73% of students could successfully construct scatter plots using R. Of those who could construct scatter plots, 84% passed, while only 62% of those students who could not construct scatter plots passed. Calculate the probability that a student is able to construct a scatter plot if it is known that she passed."
  },
  {
    "objectID": "prob-rv.html#recap",
    "href": "prob-rv.html#recap",
    "title": "8¬† Random Variables",
    "section": "8.1 Recap",
    "text": "8.1 Recap\n\nA variable in a data set is a characteristic that varies from one object to another.\n\nA variable can be either categorical or numerical.\nNumerical variables can be either discrete or continuous.\n\nA random variable, usually written as \\(X\\) 1, is a variable whose possible values are numerical outcomes determined by the chance or randomness of a procedure or experiment.\n\n \\(X\\) = # of heads after flipping a coin twice. \n \\(X\\) = # of accidents in W. Wisconsin Ave. per day.\n\nA random variable has a probability distribution associated with it, accounting for its randomness.\n\n[1] Usually in statistics, a capital \\(X\\) represents a random variable and a small \\(x\\) represents a realized value of \\(X\\)."
  },
  {
    "objectID": "prob-disc.html#exercises",
    "href": "prob-disc.html#exercises",
    "title": "9¬† Discrete Probability Distributions",
    "section": "9.4 Exercises",
    "text": "9.4 Exercises\n\nData collected by the Substance Abuse and Mental Health Services Administration (SAMSHA) suggests that 65% of 18-20 year olds consumed alcoholic beverages in any given year.\n\nSuppose a random sample of twelve 18-20 year olds is taken. When does it make sense to use binomial distribution for calculating the probability that exactly five consumed alcoholic beverages?\nWhat is the probability that exactly five out of twelve 18-20 year olds have consumed an alcoholic beverage?\nWhat is the probability that at most 3 out of 7 randomly sampled 18-20 year olds have consumed alcoholic beverages?\n\nA Dunkin‚Äô Donuts in Milwaukee serves an average of 65 customers per hour during the morning rush.\n\nWhich distribution have we studied that is most appropriate for calculating the probability of a given number of customers arriving within one hour during this time of day?\nWhat are the mean and the standard deviation of the number of customers this Starbucks serves in one hour during this time of day?\nCalculate the probability that this Dunkin‚Äô Donuts serves 55 customers in one hour during this time of day."
  },
  {
    "objectID": "prob-cont.html#exercises",
    "href": "prob-cont.html#exercises",
    "title": "10¬† Continuous Probability Distributions",
    "section": "10.7 Exercises",
    "text": "10.7 Exercises\n\nWhat percentage of data that follow a standard normal distribution \\(N(\\mu=0, \\sigma=1)\\) is found in each region? Drawing a normal graph may help.\n\n\\(Z < -1.75\\)\n\\(-0.7 < Z < 1.3\\)\n\\(|Z| > 1\\)\n\nThe average daily high temperature in June in Chicago is 74\\(^{\\circ}\\)F with a standard deviation of 4\\(^{\\circ}\\)F. Suppose that the temperatures in June closely follows a normal distribution.\n\nWhat is the probability of observing an 81\\(^{\\circ}\\) F temperature or higher in Chigcago during a randomly chosen day in June?\nHow cool are the coldest 15% of the days (days with lowest average high temperature) during June in Chicago?\n\nHead lengths of Virginia opossums follow a normal distribution with mean 104 mm and standard deviation 6 mm.\n\nCompute the \\(z\\)-scores for opossums with head lengths of 97 mm and 108 mm.\nWhich observation (97 mm or 108 mm) is more unusual or less likely to happen than another observation? Why?"
  },
  {
    "objectID": "prob-samdist.html#exercises",
    "href": "prob-samdist.html#exercises",
    "title": "11¬† Sampling Distribution",
    "section": "11.4 Exercises",
    "text": "11.4 Exercises\n\nHead lengths of Virginia possums follow a normal distribution with mean 104 mm and standard deviation 6 mm.\n\nWhat is the sampling distribution of the sample mean of the head length when the sample size \\(n = 18\\)?\n\nAssume that females have pulse rates that are normally distributed with a mean of 76.0 beats per minute and a standard deviation of 11.5 beats per minute.\n\nIf 1 adult female is randomly selected, find the probability that her pulse rate is less than 81 beats per minute.\nIf 18 adult female are randomly selected, find the probability that their mean pulse rate is less than 81 beats per minute."
  },
  {
    "objectID": "infer-ci.html#exercises",
    "href": "infer-ci.html#exercises",
    "title": "13¬† Confidence Interval",
    "section": "13.7 Exercises",
    "text": "13.7 Exercises\n\nHere are summary statistics for randomly selected weights of newborn boys: \\(n =207\\), \\(\\bar{x} = 30.2\\)hg (1hg = 100 grams), \\(s = 7.3\\)hg.\n\nCompute a 95% confidence interval for \\(\\mu\\), the mean weight of newborn boys.\nIs the result in (a) very different from the 95% confidence interval if \\(\\sigma = 7.3\\)?\n\nA 95% confidence interval for a population mean \\(\\mu\\) is given as (18.635, 21.125). This confidence interval is based on a simple random sample of 32 observations. Calculate the sample mean and standard deviation. Assume that all conditions necessary for inference are satisfied. Use the t-distribution in any calculations.\nA market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation of savings is $95. He wants to collect data such that he can get a margin of error of no more than $12 at a 95% confidence level. How large of a sample should he collect?\nThe 95% confidence interval for the mean rent of one bedroom apartments in Chicago was calculated as ($2400, $3200).\n\nInterpret the meaning of the 95% interval.\nFind the sample mean rent from the interval."
  },
  {
    "objectID": "infer-ht.html#exercises",
    "href": "infer-ht.html#exercises",
    "title": "15¬† Hypothesis Testing",
    "section": "15.7 Exercises",
    "text": "15.7 Exercises\n\nHere are summary statistics for randomly selected weights of newborn boys: \\(n =207\\), \\(\\bar{x} = 30.2\\)hg (1hg = 100 grams), \\(s = 7.3\\)hg.\n\nWith significance level 0.01, use the critical value method to test the claim that the population mean of birth weights of females is greater than 30hg.\nDo the test in (c) by using the p-value method.\n\nYou are given the following hypotheses: \\[\\begin{align*}\nH_0&: \\mu = 45 \\\\\nH_A&: \\mu \\neq 45\n\\end{align*}\\] We know that the sample standard deviation is 5 and the sample size is 24. For what sample mean would the p-value be equal to 0.05? Assume that all conditions necessary for inference are satisfied.\nOur one sample \\(z\\) test is \\(H_0: \\mu = \\mu_0 \\quad H_1: \\mu < \\mu_0\\) with a significance level \\(\\alpha\\).\n\nDescribe how we reject \\(H_0\\) using the critical-value method and the \\(p\\)-value method.\nWhy do the two methods lead to the same conclusion?"
  },
  {
    "objectID": "infer-twomean.html#exercises",
    "href": "infer-twomean.html#exercises",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.4 Exercises",
    "text": "16.4 Exercises\n\nA study was conducted to assess the effects that occur when children are expected to cocaine before birth. Children were tested at age 4 for object assembly skill, which was described as ‚Äúa task requiring visual-spatial skills related to mathematical competence.‚Äù The 187 children born to cocaine users had a mean of 7.1 and a standard deviation of 2.5. The 183 children not exposed to cocaine had a mean score of 8.4 and a standard deviation of 2.5.\n\nWith \\(\\alpha = 0.05\\), use the critical-value method and p-value method to perform a 2-sample t-test on the claim that prenatal cocaine exposure is associated with lower scores of 4-year-old children on the test of object assembly.\nTest the claim in part (a) by using a confidence interval.\n\nListed below are heights (in.) of mothers and their first daughters.\n\nUse \\(\\alpha = 0.05\\) to test the claim that there is no difference in heights between mothers and their first daughters.\nTest the claim in part (a) by using a confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeight of Mother\n66\n62\n62\n63.5\n67\n64\n69\n65\n62.5\n67\n\n\nHeight of Daughter\n67.5\n60\n63.5\n66.5\n68\n65.5\n69\n68\n65.5\n64"
  },
  {
    "objectID": "infer-twomean.html#height-of-mother-66-62-62-63.5-67-64-69-65-62.5-67",
    "href": "infer-twomean.html#height-of-mother-66-62-62-63.5-67-64-69-65-62.5-67",
    "title": "16¬† Comparing Two Population Means",
    "section": "16.5 | Height of Mother | 66 | 62 | 62 | 63.5 | 67 | 64 | 69 | 65 | 62.5 | 67",
    "text": "16.5 | Height of Mother | 66 | 62 | 62 | 63.5 | 67 | 64 | 69 | 65 | 62.5 | 67\nHeight of Daughter| 67.5 | 60 | 63.5 | 66.5 | 68 | 65.5 | 69 | 68 | 65.5 | 64"
  },
  {
    "objectID": "infer-var.html#exercises",
    "href": "infer-var.html#exercises",
    "title": "17¬† Inference About Variances",
    "section": "17.3 Exercises",
    "text": "17.3 Exercises\n\nThe data about male and female pulse rates are summarized below.\n\nConstruct a 95% CI for \\(\\sigma_{male}\\) of pulse rates for males.\nConstruct a 95% CI for \\(\\sigma_{male}/\\sigma_{female}\\).\nDoes it appear that the population standard deviations for males and females are different? Why or why not?\n\n\n\n\n\n\n\nMale\nFemale\n\n\n\n\n\\(\\overline{x}\\)\n71\n75\n\n\n\\(s\\)\n9\n12\n\n\n\\(n\\)\n14\n12"
  },
  {
    "objectID": "infer-prop.html#exercises",
    "href": "infer-prop.html#exercises",
    "title": "18¬† Inference About Proportions",
    "section": "18.4 Exercises",
    "text": "18.4 Exercises\n\nLipitor (atorvastatin) is a drug used to control cholesterol. In clinical trials of Lipitor, 98 subjects were treated with Lipitor and 245 subjects were given a placebo. Among those treated with Lipitor, 6 developed infections. Among those given a placebo, 24 developed infections. Use a 0.05 significance level to test the claim that the rate of inflections was the same for those treated with Lipitor and those given a placebo.\n\nTest the claim using the critical-value and p-value methods.\nTest the claim by constructing a confidence interval."
  },
  {
    "objectID": "infer-goodnessfit.html#exercises",
    "href": "infer-goodnessfit.html#exercises",
    "title": "19¬† Inference about Categorical Data",
    "section": "19.3 Exercises",
    "text": "19.3 Exercises\n\nA researcher has developed a model for predicting eye color. After examining a random sample of parents, she predicts the eye color of the first child. The table below lists the eye colors of offspring. On the basis of her theory, she predicted that 87% of the offspring would have brown eyes, 8% would have blue eyes, and 5% would have green eyes. Use 0.05 significance level to test the claim that the actual frequencies correspond to her predicted distribution.\n\n\n\n\nEye Color\nBrown\nBlue\nGreen\n\n\nFrequency\n127\n21\n5\n\n\n\n\nIn a study of high school students at least 16 years of age, researchers obtained survey results summarized in the accompanying table. Use a 0.05 significance level to test the claim of independence between texting while driving and driving when drinking alcohol. Are these two risky behaviors independent of one another?\n\n\n\n\n\nDrove after drinking alcohol?\n\n\n\n\n\n\nYes\nNo\n\n\nTexted while driving\n720\n3027\n\n\nDid not text while driving\n145\n4472"
  },
  {
    "objectID": "model-reg.html#exercises",
    "href": "model-reg.html#exercises",
    "title": "24¬† Linear Regression",
    "section": "24.8 Exercises",
    "text": "24.8 Exercises\nUse the data in the table below to answer questions 1-7.\n\n\n\nTar\n24\n28\n21\n23\n20\n22\n20\n25\n\n\n\n\nNicotine\n1.6\n1.7\n1.2\n1.4\n1.1\n1.0\n1.3\n1.2\n\n\n\n\n\n\nConstruct a scatterplot using tar for the \\(x\\) axis and nicotine for the \\(y\\) axis. Does the scatterplot suggest a linear relationship between the two variables? Are they positively or negatively related?\nLet \\(y\\) be the amount of nicotine and let \\(x\\) be the amount of tar. Fit a simple linear regression to the data and identify the sample regression equation.\nWhat percentage of the variation in nicotine can be explained by the linear correlation between nicotine and tar?\nThe Raleigh brand king size cigarette is not included in the table, and it has 21 mg of tar. What is the best predicted amount of nicotine? How does the predicted amount compare to the actual amount of 1.2 mg of nicotine? What is the value of residual?\nPerform the test \\(H_0: \\beta_1 = 0\\) vs.¬†\\(H_1: \\beta_1 \\ne 0\\).\nProvide 95% confidence interval for \\(\\beta_1\\).\nGenerate the ANOVA table for the linear regression.\n\n\n\nCorrelation (30 points): Match each correlation to the corresponding scatterplot.\n\n\\(R = -0.65\\)\n\\(R = 0.47\\)\n\\(R = 0.03\\)\n\\(R = 0.93\\)\n\n\n\n\n\n\n\n\n\n\n\n\nRegression (30 points): The following regression output is for predicting the heart weight (in g) of cats from their body weight (in kg).\n\nWrite out the simple linear regression equation.\nWhich one is the response variable and which one is the predictor (explanatory variable)?\nInterpret the slope and intercept.\n\n\n\n\n\n(Intercept)\n-0.346\n\n\nbody wt\n3.953"
  },
  {
    "objectID": "model-logistic.html#exercises",
    "href": "model-logistic.html#exercises",
    "title": "25¬† Logistic Regression",
    "section": "25.6 Exercises",
    "text": "25.6 Exercises\n\nThe following logistic regression equation is used for predicting whether a bear is male or female. The value of \\(\\pi\\) is the probability that the bear is male. \\[\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = 2.3 - 0.0573 (\\text{Length}) + 0.00842(\\text{Weight})\\]\n\nIdentify the predictor and response variables. Which of these are dummy variables?\nGiven that the variable Lengthis in the model, does a heavier weight increase or decrease the probability that the bear is a male? Please explain.\nThe given regression equation has an overall p-value of 0.218. What does that suggest about the quality of predictions made using the regression equation?\nUse a length of 60 in. and a weight of 300 lb to find the probability that the bear is a male. Also, what is the probability that the bear is a female?"
  }
]