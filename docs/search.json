[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistics",
    "section": "",
    "text": "This is the website for my introductory statistics book. This book serves as a main reference book for my MATH 4720 Statistical Methods and MATH 4740 Biostatistical Methods at Marquette University.1 Some topics can also be discussed in an introductory data science course. You’ll learn basic probability and statistical concepts as well as data analysis techniques such as linear regression using R computing software. The book balances the following aspects of statistics:\n\nmathematical derivation and statistical computation\ndistribution-based and simulation-based inferences\nmethodology and applications\nclassical/frequentist and Bayesian approaches\n\nCourse materials are borrowed from the following books:\n\nOpenIntro Statsitics (data oriented)\nIntroduction to Modern Statistics (computation oriented)\nAn Introduction to Statistical Methods & Data Analysis, 7th edition (mathematics oriented)"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Statistics and Data",
    "section": "",
    "text": "What is Statistics? What is Data Science? What are data?"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "13  Summary",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Statistics",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License. If you’d like to give back, please consider reporting a typo or leaving a pull request at github.com/chenghanyustats/introstatsbook."
  },
  {
    "objectID": "intro-stats.html",
    "href": "intro-stats.html",
    "title": "1  Science of Data and Data Science",
    "section": "",
    "text": "In ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records. as shown in Figure 1.1\nInterestingly someone defines statistics as the only field where two experts, using identical data, may come to completely opposite conclusions Figure 1.2, which is true in some sense. And we’ll see why later in this course. With the same data, different statistical methods may produce different results and lead to difference conclusions."
  },
  {
    "objectID": "intro-stats.html#the-r-user-interface",
    "href": "intro-stats.html#the-r-user-interface",
    "title": "1  What is Statistics",
    "section": "1.1 The R User Interface",
    "text": "1.1 The R User Interface\nBefore you can ask your computer to save some numbers, you’ll need to know how to talk to it. That’s where R and RStudio come in. RStudio gives you a way to talk to your computer. R gives you a language to speak in. To get started, open RStudio just as you would open any other application on your computer. When you do, a window should appear in your screen like the one shown in ?fig-console.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you do not yet have R and RStudio installed on your computer–or do not know what I am talking about–visit Appendix A. The appendix will give you an overview of the two free tools and tell you how to download them.\n\n\nThe RStudio interface is simple. You type R code into the bottom line of the RStudio console pane and then click Enter to run it. The code you type is called a command, because it will command your computer to do something for you. The line you type it into is called the command line.\nWhen you type a command at the prompt and hit Enter, your computer executes the command and shows you the results. Then RStudio displays a fresh prompt for your next command. For example, if you type 1 + 1 and hit Enter, RStudio will display:\n> 1 + 1\n[1] 2\n>\nYou’ll notice that a [1] appears next to your result. R is just letting you know that this line begins with the first value in your result. Some commands return more than one value, and their results may fill up multiple lines. For example, the command 100:130 returns 31 values; it creates a sequence of integers from 100 to 130. Notice that new bracketed numbers appear at the start of the second and third lines of output. These numbers just mean that the second line begins with the 14th value in the result, and the third line begins with the 25th value. You can mostly ignore the numbers that appear in brackets:\n> 100:130\n [1] 100 101 102 103 104 105 106 107 108 109 110 111 112\n[14] 113 114 115 116 117 118 119 120 121 122 123 124 125\n[25] 126 127 128 129 130\n\n\n\n\n\n\nTip\n\n\n\nThe colon operator (:) returns every integer between two integers. It is an easy way to create a sequence of numbers.\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou may hear me speak of R in the third person. For example, I might say, “Tell R to do this” or “Tell R to do that”, but of course R can’t do anything; it is just a language. This way of speaking is shorthand for saying, “Tell your computer to do this by writing a command in the R language at the command line of your RStudio console.” Your computer, and not R, does the actual work.\nIs this shorthand confusing and slightly lazy to use? Yes. Do a lot of people use it? Everyone I know–probably because it is so convenient.\n\n\n\n\n\n\n\n\nNote\n\n\n\nIn some languages, like C, Java, and FORTRAN, you have to compile your human-readable code into machine-readable code (often 1s and 0s) before you can run it. If you’ve programmed in such a language before, you may wonder whether you have to compile your R code before you can use it. The answer is no. R is a dynamic programming language, which means R automatically interprets your code as you run it.\n\n\nIf you type an incomplete command and press Enter, R will display a + prompt, which means R is waiting for you to type the rest of your command. Either finish the command or hit Escape to start over:\n> 5 -\n+\n+ 1\n[1] 4\nIf you type a command that R doesn’t recognize, R will return an error message. If you ever see an error message, don’t panic. R is just telling you that your computer couldn’t understand or do what you asked it to do. You can then try a different command at the next prompt:\n> 3 % 5\nError: unexpected input in \"3 % 5\"\n>\nOnce you get the hang of the command line, you can easily do anything in R that you would do with a calculator. For example, you could do some basic arithmetic:\n2 * 3   \n## 6\n\n4 - 1   \n## 3\n\n6 / (4 - 1)   \n## 2\nDid you notice something different about this code? I’ve left out the >’s and [1]’s. This will make the code easier to copy and paste if you want to put it in your own console.\nR treats the hashtag character, #, in a special way; R will not run anything that follows a hashtag on a line. This makes hashtags very useful for adding comments and annotations to your code. Humans will be able to read the comments, but your computer will pass over them. The hashtag is known as the commenting symbol in R.\nFor the remainder of the book, I’ll use hashtags to display the output of R code. I’ll use a single hashtag to add my own comments and a double hashtag, ##, to display the results of code. I’ll avoid showing >s and [1]s unless I want you to look at them.\n\n\n\n\n\n\nImportant\n\n\n\nSome R commands may take a long time to run. You can cancel a command once it has begun by pressing ctrl + c. Note that it may also take R a long time to cancel the command.\n\n\n\n\n\n\n\n\nExercise: Magic with Numbers\n\n\n\nThat’s the basic interface for executing R code in RStudio. Think you have it? If so, try doing these simple tasks. If you execute everything correctly, you should end up with the same number that you started with:\n\nChoose any number and add 2 to it.\nMultiply the result by 3.\nSubtract 6 from the answer.\nDivide what you get by 3.\n\n\n\nThroughout the book, I’ll put exercises in chunks, like the one above. I’ll follow each exercise with a model answer, like the one below.\nYou could start with the number 10, and then do the following steps:\n10 + 2\n## 12\n\n12 * 3\n## 36\n\n36 - 6\n## 30\n\n30 / 3\n## 10"
  },
  {
    "objectID": "intro-data.html",
    "href": "intro-data.html",
    "title": "2  Data",
    "section": "",
    "text": "Data: A set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\n\n\n\n\n\n\n\n\n\n\n\nAll right. Statistics is a Science of Data, so What is Data?\nLet’s define Data.\nA data set is a set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\nFor example, the data set right here is a set of Marquette basketball players.\nSo objects are individuals or players in the data.\nAnd each player has several characteristics or attributes shown in columns associated with him.\nFor example, his #, class, position, height, weight, hometown, and high school.\nThese characteristics are called variables because they vary form one to another. Clear?"
  },
  {
    "objectID": "intro-data.html#the-r-user-interface",
    "href": "intro-data.html#the-r-user-interface",
    "title": "2  Data",
    "section": "2.1 The R User Interface",
    "text": "2.1 The R User Interface"
  },
  {
    "objectID": "intro-r.html",
    "href": "intro-r.html",
    "title": "3  Tool foR Data",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "intro-r.html#the-r-user-interface",
    "href": "intro-r.html#the-r-user-interface",
    "title": "3  Tool foR Data",
    "section": "3.2 The R User Interface",
    "text": "3.2 The R User Interface\n\nRStudio IDE includes\n\na viewable environment, a file browser, data viewer, and a plotting pane. 👍\nalso features integrated help, syntax highlighting, context-aware tab completion and more! 😄\n\n\n\n\n\nR\n\n\nR Studio"
  },
  {
    "objectID": "data-graphics.html",
    "href": "data-graphics.html",
    "title": "4  Data Visualization",
    "section": "",
    "text": "Data Graphics"
  },
  {
    "objectID": "data-graphics.html#the-r-user-interface",
    "href": "data-graphics.html#the-r-user-interface",
    "title": "4  Data Visualization",
    "section": "4.1 The R User Interface",
    "text": "4.1 The R User Interface"
  },
  {
    "objectID": "data-numerics.html",
    "href": "data-numerics.html",
    "title": "5  Data Sample Statistics",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "data-numerics.html#the-r-user-interface",
    "href": "data-numerics.html#the-r-user-interface",
    "title": "5  Data Sample Statistics",
    "section": "5.1 The R User Interface",
    "text": "5.1 The R User Interface"
  },
  {
    "objectID": "datasummary.html",
    "href": "datasummary.html",
    "title": "Summarizing Data",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "datasummary.html#the-r-user-interface",
    "href": "datasummary.html#the-r-user-interface",
    "title": "Summarizing Data",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "intro-stats.html#objects",
    "href": "intro-stats.html#objects",
    "title": "1  What is Statistics",
    "section": "1.2 Objects",
    "text": "1.2 Objects\nNow that you know how to use R, let’s use it to make a virtual die. The : operator from a couple of pages ago gives you a nice way to create a group of numbers from one to six. The : operator returns its results as a vector, a one-dimensional set of numbers:\n1:6\n## 1 2 3 4 5 6\nThat’s all there is to how a virtual die looks! But you are not done yet. Running 1:6 generated a vector of numbers for you to see, but it didn’t save that vector anywhere in your computer’s memory. What you are looking at is basically the footprints of six numbers that existed briefly and then melted back into your computer’s RAM. If you want to use those numbers again, you’ll have to ask your computer to save them somewhere. You can do that by creating an R object.\nR lets you save data by storing it inside an R object. What is an object? Just a name that you can use to call up stored data. For example, you can save data into an object like a or b. Wherever R encounters the object, it will replace it with the data saved inside, like so:\na <- 1\na\n## 1\n\na + 2\n## 3\n\n\n\n\n\n\nNote\n\n\n\n\nTo create an R object, choose a name and then use the less-than symbol, <, followed by a minus sign, -, to save data into it. This combination looks like an arrow, <-. R will make an object, give it your name, and store in it whatever follows the arrow. So a <- 1 stores 1 in an object named a.\nWhen you ask R what’s in a, R tells you on the next line.\nYou can use your object in new R commands, too. Since a previously stored the value of 1, you’re now adding 1 to 2.\n\n\n\nSo, for another example, the following code would create an object named die that contains the numbers one through six. To see what is stored in an object, just type the object’s name by itself:\ndie <- 1:6\n\ndie\n## 1 2 3 4 5 6\nWhen you create an object, the object will appear in the environment pane of RStudio, as shown in ?fig-environment. This pane will show you all of the objects you’ve created since opening RStudio.\n\nYou can name an object in R almost anything you want, but there are a few rules. First, a name cannot start with a number. Second, a name cannot use some special symbols, like ^, !, $, @, +, -, /, or *:\n\n\n\nGood names\nNames that cause errors\n\n\n\n\na\n1trial\n\n\nb\n$\n\n\nFOO\n^mean\n\n\nmy_var\n2nd\n\n\n.day\n!bad\n\n\n\n\n\n\n\n\n\nCapitalization\n\n\n\nR is case-sensitive, so name and Name will refer to different objects:\nName <- 1\nname <- 0  \n  \nName + 1  \n## 2  \n\n\nFinally, R will overwrite any previous information stored in an object without asking you for permission. So, it is a good idea to not use names that are already taken:\nmy_number <- 1\nmy_number \n## 1\n\nmy_number <- 999\nmy_number\n## 999\nYou can see which object names you have already used with the function ls:\nls()\n## \"a\"         \"die\"       \"my_number\" \"name\"     \"Name\"     \nYou can also see which names you have used by examining RStudio’s environment pane.\nYou now have a virtual die that is stored in your computer’s memory. You can access it whenever you like by typing the word die. So what can you do with this die? Quite a lot. R will replace an object with its contents whenever the object’s name appears in a command. So, for example, you can do all sorts of math with the die. Math isn’t so helpful for rolling dice, but manipulating sets of numbers will be your stock and trade as a data scientist. So let’s take a look at how to do that:\ndie - 1\n## 0 1 2 3 4 5\n\ndie / 2\n## 0.5 1.0 1.5 2.0 2.5 3.0\n\ndie * die\n## 1  4  9 16 25 36\nIf you are a big fan of linear algebra (and who isn’t?), you may notice that R does not always follow the rules of matrix multiplication. Instead, R uses element-wise execution. When you manipulate a set of numbers, R will apply the same operation to each element in the set. So for example, when you run die - 1, R subtracts one from each element of die.\nWhen you use two or more vectors in an operation, R will line up the vectors and perform a sequence of individual operations. For example, when you run die * die, R lines up the two die vectors and then multiplies the first element of vector 1 by the first element of vector 2. R then multiplies the second element of vector 1 by the second element of vector 2, and so on, until every element has been multiplied. The result will be a new vector the same length as the first two, as shown in ?fig-elementwise.\n\nIf you give R two vectors of unequal lengths, R will repeat the shorter vector until it is as long as the longer vector, and then do the math, as shown in Figure 1.4. This isn’t a permanent change–the shorter vector will be its original size after R does the math. If the length of the short vector does not divide evenly into the length of the long vector, R will return a warning message. This behavior is known as vector recycling, and it helps R do element-wise operations:\n1:2\n## 1 2\n\n1:4\n## 1 2 3 4\n\ndie\n## 1 2 3 4 5 6\n\ndie + 1:2\n## 2 4 4 6 6 8\n\ndie + 1:4\n## 2 4 6 8 6 8\nWarning message:\nIn die + 1:4 :\n  longer object length is not a multiple of shorter object length\n\n\n\nFigure 1.4: R will repeat a short vector to do element-wise operations with two vectors of uneven lengths.\n\n\nElement-wise operations are a very useful feature in R because they manipulate groups of values in an orderly way. When you start working with data sets, element-wise operations will ensure that values from one observation or case are only paired with values from the same observation or case. Element-wise operations also make it easier to write your own programs and functions in R.\nBut don’t think that R has given up on traditional matrix multiplication. You just have to ask for it when you want it. You can do inner multiplication with the %*% operator and outer multiplication with the %o% operator:\ndie %*% die\n## 91\n\ndie %o% die\n##      [,1] [,2] [,3] [,4] [,5] [,6]\n## [1,]    1    2    3    4    5    6\n## [2,]    2    4    6    8   10   12\n## [3,]    3    6    9   12   15   18\n## [4,]    4    8   12   16   20   24\n## [5,]    5   10   15   20   25   30\n## [6,]    6   12   18   24   30   36\nYou can also do things like transpose a matrix with t and take its determinant with det.\nDon’t worry if you’re not familiar with these operations. They are easy to look up, and you won’t need them for this book.\nNow that you can do math with your die object, let’s look at how you could “roll” it. Rolling your die will require something more sophisticated than basic arithmetic; you’ll need to randomly select one of the die’s values. And for that, you will need a function."
  },
  {
    "objectID": "intro-stats.html#functions",
    "href": "intro-stats.html#functions",
    "title": "1  What is Statistics",
    "section": "1.3 Functions",
    "text": "1.3 Functions\nR comes with many functions that you can use to do sophisticated tasks like random sampling. For example, you can round a number with the round function, or calculate its factorial with the factorial function. Using a function is pretty simple. Just write the name of the function and then the data you want the function to operate on in parentheses:\nround(3.1415)\n## 3\n\nfactorial(3)\n## 6\nThe data that you pass into the function is called the function’s argument. The argument can be raw data, an R object, or even the results of another R function. In this last case, R will work from the innermost function to the outermost, as in ?fig-pemdas.\nmean(1:6)\n## 3.5\n\nmean(die)\n## 3.5\n\nround(mean(die))\n## 4\n\nLucky for us, there is an R function that can help “roll” the die. You can simulate a roll of the die with R’s sample function. sample takes two arguments: a vector named x and a number named size. sample will return size elements from the vector:\nsample(x = 1:4, size = 2)\n## 3 2\nTo roll your die and get a number back, set x to die and sample one element from it. You’ll get a new (maybe different) number each time you roll it:\nsample(x = die, size = 1)\n## 2\n\nsample(x = die, size = 1)\n## 1\n\nsample(x = die, size = 1)\n## 6\nMany R functions take multiple arguments that help them do their job. You can give a function as many arguments as you like as long as you separate each argument with a comma.\nYou may have noticed that I set die and 1 equal to the names of the arguments in sample, x and size. Every argument in every R function has a name. You can specify which data should be assigned to which argument by setting a name equal to data, as in the preceding code. This becomes important as you begin to pass multiple arguments to the same function; names help you avoid passing the wrong data to the wrong argument. However, using names is optional. You will notice that R users do not often use the name of the first argument in a function. So you might see the previous code written as:\nsample(die, size = 1)\n## 2\nOften, the name of the first argument is not very descriptive, and it is usually obvious what the first piece of data refers to anyways.\nBut how do you know which argument names to use? If you try to use a name that a function does not expect, you will likely get an error:\nround(3.1415, corners = 2)\n## Error in round(3.1415, corners = 2) : unused argument(s) (corners = 2)\nIf you’re not sure which names to use with a function, you can look up the function’s arguments with args. To do this, place the name of the function in the parentheses behind args. For example, you can see that the round function takes two arguments, one named x and one named digits:\nargs(round)\n## function (x, digits = 0) \n## NULL\nDid you notice that args shows that the digits argument of round is already set to 0? Frequently, an R function will take optional arguments like digits. These arguments are considered optional because they come with a default value. You can pass a new value to an optional argument if you want, and R will use the default value if you do not. For example, round will round your number to 0 digits past the decimal point by default. To override the default, supply your own value for digits:\nround(3.1415)\n## 3\n\nround(3.1415, digits = 2)\n## 3.14\nYou should write out the names of each argument after the first one or two when you call a function with multiple arguments. Why? First, this will help you and others understand your code. It is usually obvious which argument your first input refers to (and sometimes the second input as well). However, you’d need a large memory to remember the third and fourth arguments of every R function. Second, and more importantly, writing out argument names prevents errors.\nIf you do not write out the names of your arguments, R will match your values to the arguments in your function by order. For example, in the following code, the first value, die, will be matched to the first argument of sample, which is named x. The next value, 1, will be matched to the next argument, size:\nsample(die, 1)\n## 2\nAs you provide more arguments, it becomes more likely that your order and R’s order may not align. As a result, values may get passed to the wrong argument. Argument names prevent this. R will always match a value to its argument name, no matter where it appears in the order of arguments:\nsample(size = 1, x = die)\n## 2\n\n1.3.1 Sample with Replacement\nIf you set size = 2, you can almost simulate a pair of dice. Before we run that code, think for a minute why that might be the case. sample will return two numbers, one for each die:\nsample(die, size = 2)\n## 3 4\nI said this “almost” works because this method does something funny. If you use it many times, you’ll notice that the second die never has the same value as the first die, which means you’ll never roll something like a pair of threes or snake eyes. What is going on?\nBy default, sample builds a sample without replacement. To see what this means, imagine that sample places all of the values of die in a jar or urn. Then imagine that sample reaches into the jar and pulls out values one by one to build its sample. Once a value has been drawn from the jar, sample sets it aside. The value doesn’t go back into the jar, so it cannot be drawn again. So if sample selects a six on its first draw, it will not be able to select a six on the second draw; six is no longer in the jar to be selected. Although sample creates its sample electronically, it follows this seemingly physical behavior.\nOne side effect of this behavior is that each draw depends on the draws that come before it. In the real world, however, when you roll a pair of dice, each die is independent of the other. If the first die comes up six, it does not prevent the second die from coming up six. In fact, it doesn’t influence the second die in any way whatsoever. You can recreate this behavior in sample by adding the argument replace = TRUE:\nsample(die, size = 2, replace = TRUE)\n## 5 5\nThe argument replace = TRUE causes sample to sample with replacement. Our jar example provides a good way to understand the difference between sampling with replacement and without. When sample uses replacement, it draws a value from the jar and records the value. Then it puts the value back into the jar. In other words, sample replaces each value after each draw. As a result, sample may select the same value on the second draw. Each value has a chance of being selected each time. It is as if every draw were the first draw.\nSampling with replacement is an easy way to create independent random samples. Each value in your sample will be a sample of size one that is independent of the other values. This is the correct way to simulate a pair of dice:\nsample(die, size = 2, replace = TRUE)\n## 2 4\nCongratulate yourself; you’ve just run your first simulation in R! You now have a method for simulating the result of rolling a pair of dice. If you want to add up the dice, you can feed your result straight into the sum function:\ndice <- sample(die, size = 2, replace = TRUE)\ndice\n## 2 4\n\nsum(dice)\n## 6\nWhat would happen if you call dice multiple times? Would R generate a new pair of dice values each time? Let’s give it a try:\ndice\n## 2 4\n\ndice\n## 2 4\n\ndice\n## 2 4\nNope. Each time you call dice, R will show you the result of that one time you called sample and saved the output to dice. R won’t rerun sample(die, 2, replace = TRUE) to create a new roll of the dice. This is a relief in a way. Once you save a set of results to an R object, those results do not change. Programming would be quite hard if the values of your objects changed each time you called them.\nHowever, it would be convenient to have an object that can re-roll the dice whenever you call it. You can make such an object by writing your own R function."
  },
  {
    "objectID": "intro-stats.html#sec-write-functions",
    "href": "intro-stats.html#sec-write-functions",
    "title": "1  What is Statistics",
    "section": "1.4 Writing Your Own Functions",
    "text": "1.4 Writing Your Own Functions\nTo recap, you already have working R code that simulates rolling a pair of dice:\ndie <- 1:6\ndice <- sample(die, size = 2, replace = TRUE)\nsum(dice)\nYou can retype this code into the console anytime you want to re-roll your dice. However, this is an awkward way to work with the code. It would be easier to use your code if you wrapped it into its own function, which is exactly what we’ll do now. We’re going to write a function named roll that you can use to roll your virtual dice. When you’re finished, the function will work like this: each time you call roll(), R will return the sum of rolling two dice:\nroll()\n## 8 \n\nroll()\n## 3\n\nroll()\n## 7\nFunctions may seem mysterious or fancy, but they are just another type of R object. Instead of containing data, they contain code. This code is stored in a special format that makes it easy to reuse the code in new situations. You can write your own functions by recreating this format.\n\n1.4.1 The Function Constructor\nEvery function in R has three basic parts: a name, a body of code, and a set of arguments. To make your own function, you need to replicate these parts and store them in an R object, which you can do with the function function. To do this, call function() and follow it with a pair of braces, {}:\nmy_function <- function() {}\nfunction will build a function out of whatever R code you place between the braces. For example, you can turn your dice code into a function by calling:\nroll <- function() {\n  die <- 1:6\n  dice <- sample(die, size = 2, replace = TRUE)\n  sum(dice)\n}\n\n\n\n\n\n\nNote\n\n\n\nNotice that I indented each line of code between the braces. This makes the code easier for you and me to read but has no impact on how the code runs. R ignores spaces and line breaks and executes one complete expression at a time.\n\n\nJust hit the Enter key between each line after the first brace, {. R will wait for you to type the last brace, }, before it responds.\nDon’t forget to save the output of function to an R object. This object will become your new function. To use it, write the object’s name followed by an open and closed parenthesis:\nroll()\n## 9\nYou can think of the parentheses as the “trigger” that causes R to run the function. If you type in a function’s name without the parentheses, R will show you the code that is stored inside the function. If you type in the name with the parentheses, R will run that code:\nroll\n## function() {\n##   die <- 1:6\n##   dice <- sample(die, size = 2, replace = TRUE)\n##   sum(dice)\n## }\n\nroll()\n## 6\nThe code that you place inside your function is known as the body of the function. When you run a function in R, R will execute all of the code in the body and then return the result of the last line of code. If the last line of code doesn’t return a value, neither will your function, so you want to ensure that your final line of code returns a value. One way to check this is to think about what would happen if you ran the body of code line by line in the command line. Would R display a result after the last line, or would it not?\nHere’s some code that would display a result:\ndice\n1 + 1\nsqrt(2)\nAnd here’s some code that would not:\ndice <- sample(die, size = 2, replace = TRUE)\ntwo <- 1 + 1\na <- sqrt(2)\nDo you notice the pattern? These lines of code do not return a value to the command line; they save a value to an object."
  },
  {
    "objectID": "intro-stats.html#arguments",
    "href": "intro-stats.html#arguments",
    "title": "1  What is Statistics",
    "section": "1.5 Arguments",
    "text": "1.5 Arguments\nWhat if we removed one line of code from our function and changed the name die to bones, like this?\nroll2 <- function() {\n  dice <- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow I’ll get an error when I run the function. The function needs the object bones to do its job, but there is no object named bones to be found:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   object 'bones' not found\nYou can supply bones when you call roll2 if you make bones an argument of the function. To do this, put the name bones in the parentheses that follow function when you define roll2:\nroll2 <- function(bones) {\n  dice <- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow roll2 will work as long as you supply bones when you call the function. You can take advantage of this to roll different types of dice each time you call roll2. Dungeons and Dragons, here we come!\nRemember, we’re rolling pairs of dice:\nroll2(bones = 1:4)\n##  3\n\nroll2(bones = 1:6)\n## 10\n\nroll2(1:20)\n## 31\nNotice that roll2 will still give an error if you do not supply a value for the bones argument when you call roll2:\nroll2()\n## Error in sample(bones, size = 2, replace = TRUE) : \n##   argument \"bones\" is missing, with no default\nYou can prevent this error by giving the bones argument a default value. To do this, set bones equal to a value when you define roll2:\nroll2 <- function(bones = 1:6) {\n  dice <- sample(bones, size = 2, replace = TRUE)\n  sum(dice)\n}\nNow you can supply a new value for bones if you like, and roll2 will use the default if you do not:\nroll2()\n## 9\nYou can give your functions as many arguments as you like. Just list their names, separated by commas, in the parentheses that follow function. When the function is run, R will replace each argument name in the function body with the value that the user supplies for the argument. If the user does not supply a value, R will replace the argument name with the argument’s default value (if you defined one).\nTo summarize, function helps you construct your own R functions. You create a body of code for your function to run by writing code between the braces that follow function. You create arguments for your function to use by supplying their names in the parentheses that follow function. Finally, you give your function a name by saving its output to an R object, as shown in ?fig-functions.\nOnce you’ve created your function, R will treat it like every other function in R. Think about how useful this is. Have you ever tried to create a new Excel option and add it to Microsoft’s menu bar? Or a new slide animation and add it to Powerpoint’s options? When you work with a programming language, you can do these types of things. As you learn to program in R, you will be able to create new, customized, reproducible tools for yourself whenever you like. Project 3: Slot Machine will teach you much more about writing functions in R."
  },
  {
    "objectID": "intro-stats.html#scripts",
    "href": "intro-stats.html#scripts",
    "title": "1  What is Statistics",
    "section": "1.6 Scripts",
    "text": "1.6 Scripts\nWhat if you want to edit roll2 again? You could go back and retype each line of code in roll2, but it would be so much easier if you had a draft of the code to start from. You can create a draft of your code as you go by using an R script. An R script is just a plain text file that you save R code in. You can open an R script in RStudio by going to File > New File > R script in the menu bar. RStudio will then open a fresh script above your console pane, as shown in ?fig-script.\nI strongly encourage you to write and edit all of your R code in a script before you run it in the console. Why? This habit creates a reproducible record of your work. When you’re finished for the day, you can save your script and then use it to rerun your entire analysis the next day. Scripts are also very handy for editing and proofreading your code, and they make a nice copy of your work to share with others. To save a script, click the scripts pane, and then go to File > Save As in the menu bar.\n\nRStudio comes with many built-in features that make it easy to work with scripts. First, you can automatically execute a line of code in a script by clicking the Run button, as shown in ?fig-run.\nR will run whichever line of code your cursor is on. If you have a whole section highlighted, R will run the highlighted code. Alternatively, you can run the entire script by clicking the Source button. Don’t like clicking buttons? You can use Control + Return as a shortcut for the Run button. On Macs, that would be Command + Return.\n\nIf you’re not convinced about scripts, you soon will be. It becomes a pain to write multi-line code in the console’s single-line command line. Let’s avoid that headache and open your first script now before we move to the next chapter.\n\n\n\n\n\n\nExtract function\n\n\n\nRStudio comes with a tool that can help you build functions. To use it, highlight the lines of code in your R script that you want to turn into a function. Then click Code > Extract Function in the menu bar. RStudio will ask you for a function name to use and then wrap your code in a function call. It will scan the code for undefined variables and use these as arguments.\nYou may want to double-check RStudio’s work. It assumes that your code is correct, so if it does something surprising, you may have a problem in your code."
  },
  {
    "objectID": "intro-stats.html#summary",
    "href": "intro-stats.html#summary",
    "title": "1  What is Statistics",
    "section": "1.7 Summary",
    "text": "1.7 Summary\nYou’ve covered a lot of ground already. You now have a virtual die stored in your computer’s memory, as well as your own R function that rolls a pair of dice. You’ve also begun speaking the R language.\nAs you’ve seen, R is a language that you can use to talk to your computer. You write commands in R and run them at the command line for your computer to read. Your computer will sometimes talk back–for example, when you commit an error–but it usually just does what you ask and then displays the result.\nThe two most important components of the R language are objects, which store data, and functions, which manipulate data. R also uses a host of operators like +, -, *, /, and <- to do basic tasks. As a data scientist, you will use R objects to store data in your computer’s memory, and you will use functions to automate tasks and do complicated calculations. We will examine objects in more depth later in Project 2: Playing Cards and dig further into functions in Project 3: Slot Machine. The vocabulary you have developed here will make each of those projects easier to understand. However, we’re not done with your dice yet.\nIn Packages and Help Pages, you’ll run some simulations on your dice and build your first graphs in R. You’ll also look at two of the most useful components of the R language: R packages, which are collections of functions writted by R’s talented community of developers, and R documentation, which is a collection of help pages built into R that explains every function and data set in the language."
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Probability",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "prob.html#the-r-user-interface",
    "href": "prob.html#the-r-user-interface",
    "title": "Probability",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "prob-define.html",
    "href": "prob-define.html",
    "title": "6  Definition of Probability",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "prob-define.html#the-r-user-interface",
    "href": "prob-define.html#the-r-user-interface",
    "title": "6  Definition of Probability",
    "section": "6.1 The R User Interface",
    "text": "6.1 The R User Interface"
  },
  {
    "objectID": "prob-rule.html",
    "href": "prob-rule.html",
    "title": "7  Probability Rules",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "prob-rule.html#the-r-user-interface",
    "href": "prob-rule.html#the-r-user-interface",
    "title": "7  Probability Rules",
    "section": "7.1 The R User Interface",
    "text": "7.1 The R User Interface"
  },
  {
    "objectID": "prob-rv.html",
    "href": "prob-rv.html",
    "title": "8  Random Variables",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "prob-rv.html#the-r-user-interface",
    "href": "prob-rv.html#the-r-user-interface",
    "title": "8  Random Variables",
    "section": "8.1 The R User Interface",
    "text": "8.1 The R User Interface"
  },
  {
    "objectID": "prob-disc.html",
    "href": "prob-disc.html",
    "title": "9  Discrete Probability Distributions",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "prob-disc.html#the-r-user-interface",
    "href": "prob-disc.html#the-r-user-interface",
    "title": "9  Discrete Probability Distributions",
    "section": "9.1 The R User Interface",
    "text": "9.1 The R User Interface"
  },
  {
    "objectID": "prob-cont.html",
    "href": "prob-cont.html",
    "title": "10  Continuous Probability Distributions",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "prob-cont.html#the-r-user-interface",
    "href": "prob-cont.html#the-r-user-interface",
    "title": "10  Continuous Probability Distributions",
    "section": "10.1 The R User Interface",
    "text": "10.1 The R User Interface"
  },
  {
    "objectID": "prob-samdist.html",
    "href": "prob-samdist.html",
    "title": "11  Sampling Distribution",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "prob-samdist.html#the-r-user-interface",
    "href": "prob-samdist.html#the-r-user-interface",
    "title": "11  Sampling Distribution",
    "section": "11.1 The R User Interface",
    "text": "11.1 The R User Interface"
  },
  {
    "objectID": "prob-llnclt.html",
    "href": "prob-llnclt.html",
    "title": "12  Law of Large Numbers and Central Limit Theorem",
    "section": "",
    "text": "We know if \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) , then \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\).\n What if the population distribution is NOT normal? \n\n\nThe central limit theorem (CLT) gives us the answer!"
  },
  {
    "objectID": "prob-llnclt.html#the-r-user-interface",
    "href": "prob-llnclt.html#the-r-user-interface",
    "title": "12  Law of Large Numbers and Central Limit Theorem",
    "section": "12.1 The R User Interface",
    "text": "12.1 The R User Interface"
  },
  {
    "objectID": "infer.html",
    "href": "infer.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer.html#the-r-user-interface",
    "href": "infer.html#the-r-user-interface",
    "title": "Statistical Inference",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "infer-ci.html",
    "href": "infer-ci.html",
    "title": "13  Confidence Interval",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-ci.html#the-r-user-interface",
    "href": "infer-ci.html#the-r-user-interface",
    "title": "13  Confidence Interval",
    "section": "13.1 The R User Interface",
    "text": "13.1 The R User Interface"
  },
  {
    "objectID": "infer-bt.html",
    "href": "infer-bt.html",
    "title": "14  Bootstrapping",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-bt.html#the-r-user-interface",
    "href": "infer-bt.html#the-r-user-interface",
    "title": "14  Bootstrapping",
    "section": "14.1 The R User Interface",
    "text": "14.1 The R User Interface"
  },
  {
    "objectID": "infer-ht.html",
    "href": "infer-ht.html",
    "title": "15  Hypothesis Testing",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-ht.html#the-r-user-interface",
    "href": "infer-ht.html#the-r-user-interface",
    "title": "15  Hypothesis Testing",
    "section": "15.1 The R User Interface",
    "text": "15.1 The R User Interface"
  },
  {
    "objectID": "infer-twomean.html",
    "href": "infer-twomean.html",
    "title": "16  Comparing Two Population Means",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-twomean.html#the-r-user-interface",
    "href": "infer-twomean.html#the-r-user-interface",
    "title": "16  Comparing Two Population Means",
    "section": "16.1 The R User Interface",
    "text": "16.1 The R User Interface"
  },
  {
    "objectID": "infer-var.html",
    "href": "infer-var.html",
    "title": "17  Inference About Variances",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-var.html#the-r-user-interface",
    "href": "infer-var.html#the-r-user-interface",
    "title": "17  Inference About Variances",
    "section": "17.1 The R User Interface",
    "text": "17.1 The R User Interface"
  },
  {
    "objectID": "infer-prop.html",
    "href": "infer-prop.html",
    "title": "18  Inference About Proportions",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-prop.html#the-r-user-interface",
    "href": "infer-prop.html#the-r-user-interface",
    "title": "18  Inference About Proportions",
    "section": "18.1 The R User Interface",
    "text": "18.1 The R User Interface"
  },
  {
    "objectID": "infer-goodnessfit.html",
    "href": "infer-goodnessfit.html",
    "title": "19  Test of Goodness of Fit",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-goodnessfit.html#the-r-user-interface",
    "href": "infer-goodnessfit.html#the-r-user-interface",
    "title": "19  Test of Goodness of Fit",
    "section": "19.1 The R User Interface",
    "text": "19.1 The R User Interface"
  },
  {
    "objectID": "infer-indep.html",
    "href": "infer-indep.html",
    "title": "20  Test of Independence",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-indep.html#the-r-user-interface",
    "href": "infer-indep.html#the-r-user-interface",
    "title": "20  Test of Independence",
    "section": "20.1 The R User Interface",
    "text": "20.1 The R User Interface"
  },
  {
    "objectID": "infer-bayes.html",
    "href": "infer-bayes.html",
    "title": "21  Bayesian Inference",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-bayes.html#the-r-user-interface",
    "href": "infer-bayes.html#the-r-user-interface",
    "title": "21  Bayesian Inference",
    "section": "21.1 The R User Interface",
    "text": "21.1 The R User Interface"
  },
  {
    "objectID": "infer-nonpar.html",
    "href": "infer-nonpar.html",
    "title": "22  Nonparametric Inference",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "infer-nonpar.html#the-r-user-interface",
    "href": "infer-nonpar.html#the-r-user-interface",
    "title": "22  Nonparametric Inference",
    "section": "22.1 The R User Interface",
    "text": "22.1 The R User Interface"
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Statistical Models",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "model.html#the-r-user-interface",
    "href": "model.html#the-r-user-interface",
    "title": "Statistical Models",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "model-anova.html",
    "href": "model-anova.html",
    "title": "23  Analysis of Variance",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "model-anova.html#the-r-user-interface",
    "href": "model-anova.html#the-r-user-interface",
    "title": "23  Analysis of Variance",
    "section": "23.1 The R User Interface",
    "text": "23.1 The R User Interface"
  },
  {
    "objectID": "model-reg.html",
    "href": "model-reg.html",
    "title": "24  Linear Regression",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "model-reg.html#the-r-user-interface",
    "href": "model-reg.html#the-r-user-interface",
    "title": "24  Linear Regression",
    "section": "24.1 The R User Interface",
    "text": "24.1 The R User Interface"
  },
  {
    "objectID": "model-logistic.html",
    "href": "model-logistic.html",
    "title": "25  Logistic Regression",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "model-logistic.html#the-r-user-interface",
    "href": "model-logistic.html#the-r-user-interface",
    "title": "25  Logistic Regression",
    "section": "25.1 The R User Interface",
    "text": "25.1 The R User Interface"
  },
  {
    "objectID": "model-bayes.html",
    "href": "model-bayes.html",
    "title": "26  Bayesian Linear Regression",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "model-bayes.html#the-r-user-interface",
    "href": "model-bayes.html#the-r-user-interface",
    "title": "26  Bayesian Linear Regression",
    "section": "26.1 The R User Interface",
    "text": "26.1 The R User Interface"
  },
  {
    "objectID": "model-survival.html",
    "href": "model-survival.html",
    "title": "27  Survival Analysis",
    "section": "",
    "text": "This chapter provides a broad overview of the R language that will get you programming right away. In it, you will build a pair of virtual dice that you can use to generate random numbers. Don’t worry if you’ve never programmed before; the chapter will teach you everything you need to know.\nTo simulate a pair of dice, you will have to distill each die into its essential features. You cannot place a physical object, like a die, into a computer (well, not without unscrewing some screws), but you can save information about the object in your computer’s memory.\nWhich information should you save? In general, a die has six important pieces of information: when you roll a die, it can only result in one of six numbers: 1, 2, 3, 4, 5, and 6. You can capture the essential characteristics of a die by saving the numbers 1, 2, 3, 4, 5, and 6 as a group of values in your computer’s memory.\nLet’s work on saving these numbers first, and then consider a method for “rolling” our die."
  },
  {
    "objectID": "model-survival.html#the-r-user-interface",
    "href": "model-survival.html#the-r-user-interface",
    "title": "27  Survival Analysis",
    "section": "27.1 The R User Interface",
    "text": "27.1 The R User Interface"
  },
  {
    "objectID": "intro-stats.html#statistics-as-a-discipline",
    "href": "intro-stats.html#statistics-as-a-discipline",
    "title": "1  Science of Data and Data Science",
    "section": "1.2 Statistics as a Discipline",
    "text": "1.2 Statistics as a Discipline\n\nForget about that useless definition.\n\n\n\n\n\n\nFigure 1.3: Wiki definition of statistics (Source: https://en.wikipedia.org/wiki/Statistics)\n\n\n\n\n\nFrom Wiki, Statistics is formally defined as the discipline that concerns the collection, organization, analysis, interpretation and presentation of data.\nStatistics is a Science of Data.\nA science of data using statistical thinking, methods and models.\nThere might be another science of data. I’m not saying statistics is THE science of data.\n\n\n\n\n\n🤔 But wait, then what is DATA SCIENCE ❓"
  },
  {
    "objectID": "intro-stats.html#difference-between-statistics-and-data-science",
    "href": "intro-stats.html#difference-between-statistics-and-data-science",
    "title": "1  Science of Data and Data Science",
    "section": "1.3 Difference between Statistics and Data Science",
    "text": "1.3 Difference between Statistics and Data Science\n\n\n\n\n\n\nFigure 1.4: Tweets about what data science and data scientists are\n\n\n\nInvestopedia defines Data Science as a field of applied mathematics and statistics that provides useful information based on large amounts of complex data or big data.\n\n\n\n\n\n\n\nFigure 1.5: Professor Ariely on big data\n\n\n\nBut Dan Ariely, a famous behavioral economist at Duke once asked: So what on earth is DS?\nMaybe nobody has the exactly correct answer."
  },
  {
    "objectID": "intro-stats.html#uc-santa-cruz-department-of-statistics-courses",
    "href": "intro-stats.html#uc-santa-cruz-department-of-statistics-courses",
    "title": "1  Science of Data and Data Science",
    "section": "1.4 UC Santa Cruz Department of Statistics Courses",
    "text": "1.4 UC Santa Cruz Department of Statistics Courses\n\n\n\n\n\nFigure 1.6: Course offering of department of statistics at UC Santa Cruz. (Source: https://courses.soe.ucsc.edu/)\n\n\n\n\n\nThis shows statistics courses offered by UC Santa Cruz, the university I graduated from.\nYou can see that statistics focuses much more on data analysis, methods and models.\nThe stats department doesn’t talk a lot about data collection, organization, data presentation or data visualization."
  },
  {
    "objectID": "intro-stats.html#data-science-is-now-a-broader-view-of-statistics",
    "href": "intro-stats.html#data-science-is-now-a-broader-view-of-statistics",
    "title": "1  Science of Data and Data Science",
    "section": "1.5 Data Science Is Now a Broader View of Statistics",
    "text": "1.5 Data Science Is Now a Broader View of Statistics\n\nCollection, organization, analysis, interpretation and presentation of data.\n\n\n\n\n\n\nFigure 1.7: The Data Science process. Created at Harvard by Joe Blitzstein and Hanspeter Pfister.\n\n\n\n\n\nData science is more like a broader view of statistics.\nBecause again, in typical statistics departments, we don’t really teach or do much research on data collection, cleaning, storage, database management, and data visualization, which all are now a part of DS.\nStatistics focuses very much on data analysis and modeling.\nAnyway, please don’t worry about the names.\nThe important thing is you learn useful methods to help you analyze your data no matter what it is called, statistics or data science."
  },
  {
    "objectID": "intro-stats.html#what-do-we-learn-in-this-course",
    "href": "intro-stats.html#what-do-we-learn-in-this-course",
    "title": "1  Science of Data and Data Science",
    "section": "1.6 What Do We Learn In this Course",
    "text": "1.6 What Do We Learn In this Course\n\n\n\n\n\n\n\n\n\n\n\nIn particular, we will spend most of the time talking about probability and statistical inference methods."
  },
  {
    "objectID": "intro-stats.html#we-focus-on-statistical-inference",
    "href": "intro-stats.html#we-focus-on-statistical-inference",
    "title": "1  Science of Data and Data Science",
    "section": "1.7 We Focus On Statistical Inference",
    "text": "1.7 We Focus On Statistical Inference\n\nWe spend most of time on various statistical methods for analyzing data.\nLearn useful information\n\nabout the population we are interested\nfrom our sample data\nthrough statistical inferential methods, including estimation and testing\n\nDon’t worry if you have no idea what these terms are.\nThese are what we will discuss throughout the course, and I’ll explain each term in detail later in class."
  },
  {
    "objectID": "intro-data.html#data-matrix",
    "href": "intro-data.html#data-matrix",
    "title": "2  Data",
    "section": "2.2 Data Matrix",
    "text": "2.2 Data Matrix\n\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns.\n\n\n\n\n\n\n\n\n\n\n\nAnd we usually store a data set in a matrix form that has rows and columns.\nEach row corresponds to a unique case or observational unit, or the object.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns.\nThe first step in conducting a study is to identify questions to be investigated.\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\nTarget Population: The complete collection of data we’d like to make inference about.\nWhat is the average GPA of currently enrolled Marquette students?\n\n\n\n\n\n\n\n\n\n\n\nTarget Population: The complete collection of data we’d like to make inference about.\nSo the population is a set of all objects which we are interested in studying from.\nBecause All Marquette undergrads that are currently enrolled is the complete collection of data we’d like to make inference about.\nEach currently enrolled Marquette undergrad is an object.\nNote that students who are not currently enrolled or students that are already graduated are not our interest, and they shouldn’t be a part of target population.\nCan anybody tell me what variable associated with Marquette undergrads is our interest?\nSo average GPA is the variable or population property we like to make inference about."
  },
  {
    "objectID": "intro-data.html#sample-data",
    "href": "intro-data.html#sample-data",
    "title": "2  Data",
    "section": "2.4 Sample Data",
    "text": "2.4 Sample Data\n\nSometimes, it’s possible to collect data of all cases we are interested in.\nMost of the time, it is too expensive to collect data for every case in a population.\nWhat about the average GPA of all students in Illinois? The U.S.? The world? 😱 😱 😱\n\n\n\n\n\n\n\nSampling is our solution to it.\nA sample is a subset of cases selected from a population.\nThe idea is that we are not able to compute the average GPA of a population, but we can collect a sample from that population which has way less objects than the population.\nThen we compute the average GPA of the sample data.\nWe hope the sample average GPA can be close to the population average GPA because the population GPA is our main interest, not the sample GPA.\nTo have sample average GPA close to population GPA, we want the sample to look like the population so that the sample and the population share similar attributes including GPA."
  },
  {
    "objectID": "intro-data.html#statistics-is-a-science-of-data-so-what-is-data",
    "href": "intro-data.html#statistics-is-a-science-of-data-so-what-is-data",
    "title": "2  Data",
    "section": "2.1 Statistics is a Science of Data, so What is Data?",
    "text": "2.1 Statistics is a Science of Data, so What is Data?\n\nData: A set of objects on which we observe or measure one or more characteristics.\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is called a variable because it varies from one to another.\nFor example, the data set right here is a set of Marquette basketball players.\n\n\n\n\n\n\n\n\n\n\n\nThe objects are individuals or players in the data.\nAnd each player has several characteristics or attributes shown in columns associated with him.\n\nFor example, his #, class, position, height, weight, hometown, and high school.\n\nThese characteristics are called variables because they vary from one to another."
  },
  {
    "objectID": "intro-data.html#good-sample-vs.-bad-sample",
    "href": "intro-data.html#good-sample-vs.-bad-sample",
    "title": "2  Data",
    "section": "2.5 Good Sample vs. Bad Sample",
    "text": "2.5 Good Sample vs. Bad Sample\n\n\n\n\n\n\nIs this 4740/5740 class a sample of the target population Marquette students?\n\n\n\n\n\n\n\n\n\n\n\n\nIs this 4740/5740 class a “good” sample of the target population?\n\n\n\n\n\n\n\nDoes this 4740 class look like the target population?\nThe sample is convenient to be collected, but it is NOT representative of the population.\nYou are mostly STEM majors, and so with a high chance, your average GPA is not the same as the GPA of humanities or business students.\nBiased sample: The average GPA of the class may be far from that of all MU undergrads.\nSample data must be collected in an appropriate way. If not GIGO.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2.1: Average GPAs for different majors (https://www.dailycal.org/2018/04/19/integrative-biology-computer-science-sociology-majors-lowest-gpa/)"
  },
  {
    "objectID": "intro-data.html#how-and-why-a-representative-sample",
    "href": "intro-data.html#how-and-why-a-representative-sample",
    "title": "2  Data",
    "section": "2.6 How and Why a Representative Sample?",
    "text": "2.6 How and Why a Representative Sample?\n\nWe always seek to randomly select a sample from a population.\nRandom sampling usually give us a representative Sample, as long as the sample size, or the number of objects in the sample, is not too small.\nLots of statistical methods are based on randomness assumption."
  },
  {
    "objectID": "intro-data.html#data-collection",
    "href": "intro-data.html#data-collection",
    "title": "2  Data",
    "section": "2.6 Data Collection",
    "text": "2.6 Data Collection\n\nObservational Study: Observe and measure characteristics/variables, and do NOT attempt to modify or intervene with the subjects being studied.\n\n Sample from 1️⃣ the heart disease and 2️⃣ heart disease-free populations. Then record the fat content of the diets for the two groups. \n\nExperimental Study: Apply some treatment(s) and then proceed to observe its responses or effects on the individuals (experimental units).\n\nAssign volunteers to one of several diets with different levels of dietary fat (treatments). Then compare the treatments with respect to the incidence of hear disease after a period of time. \n\n\nObservational or Experimental? - Randomly select 40 males and 40 females to test the difference in blood pressure levels between male and female.  -  Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo). \n\nConfounder: A variable NOT included in a study but affects the variables in the study.\nObserve past data show that increases in ice cream sales are associated with increases in drownings, and we conclude that ice cream causes drownings. 😱 😕 ⁉️\n\nWhat is the confounder that is not in the data, but affects ice cream sales and the number of drownings?\nTemperature: as temperature increases, ice cream sales increase and the number of drownings goes up because more people swim.\nAs temperature increases (season), ice cream sales increase and the number of drownings goes up because more people swim."
  },
  {
    "objectID": "intro-data.html#causal-relationship",
    "href": "intro-data.html#causal-relationship",
    "title": "2  Data",
    "section": "2.9 Causal Relationship",
    "text": "2.9 Causal Relationship\n\nMaking causal conclusions based on experiments is often more reasonable than making the same causal conclusions based on observational data.\nObservational studies are generally only sufficient to show associations, not causality."
  },
  {
    "objectID": "intro-data.html#simple-random-sample",
    "href": "intro-data.html#simple-random-sample",
    "title": "2  Data",
    "section": "2.10 Simple Random Sample",
    "text": "2.10 Simple Random Sample\n\nRandom Sample: Each member of a population is equally likely to be selected.\nSimple Random Sample (SRS): Every possible sample of sample size \\(n\\) has the same chance to be chosen.\nExample: If I sample 100 students from all, say 10,000 Marquette students, I would randomly assign each student a number (from 1 to 10,000) and then randomly select 100 numbers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://research-methodology.net/sampling-in-primary-data-collection/random-sampling/"
  },
  {
    "objectID": "intro-data.html#stratified-random-sample",
    "href": "intro-data.html#stratified-random-sample",
    "title": "2  Data",
    "section": "2.11 Stratified Random Sample",
    "text": "2.11 Stratified Random Sample\n\nStratified Sampling: Subdivide the population into different subgroups (strata) that share the same characteristics, then draw a simple random sample from each subgroup.\nHomogeneous within strata; Non-homogeneous between strata"
  },
  {
    "objectID": "intro-data.html#cluster-sampling",
    "href": "intro-data.html#cluster-sampling",
    "title": "2  Data",
    "section": "2.13 Cluster Sampling",
    "text": "2.13 Cluster Sampling\n\nCluster Sampling: Divide the population into clusters, then randomly select some of those clusters, and then choose all the members from those selected clusters.\nHomogeneous between clusters; Non-homogeneous within clusters"
  },
  {
    "objectID": "intro-data.html#categorical-vs.-numerical-variables",
    "href": "intro-data.html#categorical-vs.-numerical-variables",
    "title": "2  Data",
    "section": "2.16 Categorical vs. Numerical Variables",
    "text": "2.16 Categorical vs. Numerical Variables\n\nA categorical variable provides non-numerical information which can be placed in one (and only one) category from two or more categories.\n\nGender (Male 👨, Female 👩, Trans 🏳️‍🌈) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA 🇺🇸, Canada 🇨🇦, UK 🇬🇧, Germany 🇩🇪, Japan 🇯🇵, Korea 🇰🇷) \n\nA numerical variable is recorded in a numerical value representing counts or measurements.\n\n GPA \n The number of relationships you’ve had \n Height"
  },
  {
    "objectID": "intro-data.html#numerical-variables-can-be-discrete-or-continuous",
    "href": "intro-data.html#numerical-variables-can-be-discrete-or-continuous",
    "title": "2  Data",
    "section": "2.17 Numerical Variables can be Discrete or Continuous",
    "text": "2.17 Numerical Variables can be Discrete or Continuous\n\nA discrete variable takes on values of a finite or countable number.\nA continuous variable takes on values anywhere over a particular range without gaps or jumps.\n\n GPA is continuous because it can be any value between 0 and 4. \n The number of relationships you’ve had is discrete because you can count the number and it is finite.\n Height is continuous because it can be any number within a range."
  },
  {
    "objectID": "intro-data.html#categorical-variables-are-usually-recorded-as-numbers",
    "href": "intro-data.html#categorical-variables-are-usually-recorded-as-numbers",
    "title": "2  Data",
    "section": "2.18 Categorical Variables are Usually Recorded as Numbers",
    "text": "2.18 Categorical Variables are Usually Recorded as Numbers\n\nGender (Male = 0, Female = 1, Trans = 2) \nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5) \nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301) \nUnited Airlines boarding groups\nThe numbers represent categories only; differences between them are meaningless.\n\nCanada - USA = 101 - 100 = 1?\nGraduate - Sophomore = 5 - 2 = 3 = Junior?\n\nWe need to learn the level of measurements to know whether or which arithmetic operations are meaningful."
  },
  {
    "objectID": "intro-data.html#levels-of-measurements-nominal-and-ordinal-for-categorical-variables",
    "href": "intro-data.html#levels-of-measurements-nominal-and-ordinal-for-categorical-variables",
    "title": "2  Data",
    "section": "2.19 Levels of Measurements: Nominal and Ordinal for Categorical Variables",
    "text": "2.19 Levels of Measurements: Nominal and Ordinal for Categorical Variables\n\nNominal: The data can NOT be ordered in a meaningful or natural way.\n\nGender (Male = 0, Female = 1, Trans = 2)  is nominal because Male, Female and Trans cannot be ordered.\nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301)  is nominal.\n\nOrdinal: The data can be arranged in some meaningful order, but differences between data values can NOT be determined or are meaningless.\n\nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5)  is ordinal because Sophomore is one class higher than Freshman."
  },
  {
    "objectID": "intro-data.html#converting-numerical-to-categorical",
    "href": "intro-data.html#converting-numerical-to-categorical",
    "title": "2  Data",
    "section": "2.21 Converting Numerical to Categorical",
    "text": "2.21 Converting Numerical to Categorical\n\nYou’ve already seen an example.\n\n\n\n\n\n \n  \n    Grade \n    Percentage \n  \n \n\n  \n    A \n    [94, 100] \n  \n  \n    A- \n    [90, 94) \n  \n  \n    B+ \n    [87, 90) \n  \n  \n    B \n    [83, 87) \n  \n  \n    B- \n    [80, 83) \n  \n  \n    C+ \n    [77, 80) \n  \n  \n    C \n    [73, 77) \n  \n  \n    C- \n    [70, 73) \n  \n  \n    D+ \n    [65, 70) \n  \n  \n    D \n    [60, 65) \n  \n  \n    F \n    [0, 60) \n  \n\n\n\n\n\n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify data type of each variable in the Marquette men’s basketball player data"
  },
  {
    "objectID": "intro-stats.html#statistics-as-numeric-records",
    "href": "intro-stats.html#statistics-as-numeric-records",
    "title": "1  Science of Data and Data Science",
    "section": "1.1 Statistics as Numeric Records",
    "text": "1.1 Statistics as Numeric Records\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records as shown in Figure 1.1.\n\n\n\n\n\nFigure 1.1: Example of statistics or numeric records. (Source: https://www.nba.com/stats/player/893/career)\n\n\n\n\n\n\nInterestingly someone defines statistics as the only field where two experts, using identical data, may come to completely opposite conclusions Figure 1.2, which is true in some sense. We’ll see why later in this course. With the same data, different statistical methods may produce different results and lead to difference conclusions.\n\n\n\n\n\n\nFigure 1.2: Statistics Shirt (Source: shorturl.at/vEMNS)"
  },
  {
    "objectID": "intro-data.html#data",
    "href": "intro-data.html#data",
    "title": "2  Data",
    "section": "2.2 Data",
    "text": "2.2 Data\n\nWe usually store a data set in a matrix form that has rows and columns.\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\nThis structure allows new cases to be added as rows or new variables as new columns."
  },
  {
    "objectID": "intro-data.html#target-population",
    "href": "intro-data.html#target-population",
    "title": "2  Data",
    "section": "2.3 Target Population",
    "text": "2.3 Target Population\n\nThe first step in conducting a study is to identify questions to be investigated.\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\nTarget Population: The complete collection of data we’d like to make inference about.\nWhat is the average GPA of currently enrolled Marquette students?\n\n\n\n\n\n\n\n\n\n\n\nThe population is a set of all objects which we are interested in studying from.\nBecause all Marquette undergrads that are currently enrolled is the complete collection of data we’d like to make inference about, each currently enrolled Marquette undergrad is an object.\n\nNote that students who are not currently enrolled or students that are already graduated are not our interest, and they shouldn’t be a part of target population.\n\nAverage GPA is the variable or population property we would like to make inference about."
  },
  {
    "objectID": "intro-data.html#two-types-of-studies-to-collect-sample-data",
    "href": "intro-data.html#two-types-of-studies-to-collect-sample-data",
    "title": "2  Data",
    "section": "2.7 Two Types of Studies to Collect Sample Data",
    "text": "2.7 Two Types of Studies to Collect Sample Data\n\nObservational Study: Observe and measure characteristics/variables, and do NOT attempt to modify or intervene with the subjects being studied.\n\n Sample from 1️⃣ the heart disease and 2️⃣ heart disease-free populations. Then record the fat content of the diets for the two groups. \n\nExperimental Study: Apply some treatment(s) and then proceed to observe its responses or effects on the individuals (experimental units).\n\nAssign volunteers to one of several diets with different levels of dietary fat (treatments). Then compare the treatments with respect to the incidence of heart disease after a period of time. \n\n\n\n\n\n\n\n\nObservational or Experimental?\n\n\n\n\nRandomly select 40 males and 40 females to see the difference in blood pressure levels between male and female. \n Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo)."
  },
  {
    "objectID": "intro-data.html#limitation-of-observational-studies-confounding",
    "href": "intro-data.html#limitation-of-observational-studies-confounding",
    "title": "2  Data",
    "section": "2.8 Limitation of Observational Studies: Confounding",
    "text": "2.8 Limitation of Observational Studies: Confounding\n\nConfounder: A variable NOT included in a study but that affects the variables in the study.\nObserve past data that shows that increases in ice cream sales are associated with increases in drownings, and we conclude that eating ice cream causes drownings. 😱 😕 ⁉️\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the confounder that is not in the data but affects ice cream sales and the number of drownings?\n\n\n\n\n\n\n\nTemperature: As temperature increases, ice cream sales increase and the number of drownings goes up because more people swim."
  },
  {
    "objectID": "intro-data.html#stratified-random-sample-example",
    "href": "intro-data.html#stratified-random-sample-example",
    "title": "2  Data",
    "section": "2.12 Stratified Random Sample Example",
    "text": "2.12 Stratified Random Sample Example\n\nExample: Divide Marquette students into groups by colleges, then SRS for each group."
  },
  {
    "objectID": "intro-data.html#cluster-sampling-example",
    "href": "intro-data.html#cluster-sampling-example",
    "title": "2  Data",
    "section": "2.14 Cluster Sampling Example",
    "text": "2.14 Cluster Sampling Example\n\nExample: Studying 4720 students’ drinking habits by dividing the students into 9 groups, then randomly selecting 3 and interviewing all of the students in each of those clusters."
  },
  {
    "objectID": "intro-data.html#levels-of-measurements-interval-and-ratio-for-numerical-variables",
    "href": "intro-data.html#levels-of-measurements-interval-and-ratio-for-numerical-variables",
    "title": "2  Data",
    "section": "2.20 Levels of Measurements: Interval and Ratio for Numerical Variables",
    "text": "2.20 Levels of Measurements: Interval and Ratio for Numerical Variables\n\nInterval: The data have meaningful differences between any two values but the data do NOT have a natural zero or starting point. The data can do \\(\\color{red} +\\) and \\(\\color{red} -\\), but can’t reasonably do \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\nTemperature is interval because \\(80^{\\circ}\\)F is 40 degrees higher than \\(40^{\\circ}\\)F \\((80-40=40)\\), but \\(0^{\\circ}\\) does not mean NO heat and \\(80^{\\circ}\\)F is NOT twice as hot as \\(40^{\\circ}\\)F.\n\nRatio: The data have both meaningful differences and ratios, and there is a natural zero starting point that indicates none of the quantity. The data can do \\(\\color{red} +\\), \\(\\color{red} -\\), \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\nDistance is ratio because \\(80\\) miles is twice as far as \\(40\\) miles \\((80/40 = 2)\\), and \\(0\\) mile means no distance."
  },
  {
    "objectID": "intro-data.html#data-type",
    "href": "intro-data.html#data-type",
    "title": "2  Data",
    "section": "2.15 Data Type",
    "text": "2.15 Data Type"
  },
  {
    "objectID": "intro-r.html#r-and-rstudio",
    "href": "intro-r.html#r-and-rstudio",
    "title": "3  Tool foR Data",
    "section": "3.1 R and RStudio",
    "text": "3.1 R and RStudio\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR: free open-source programming language 📈\nR is mainly for doing data science with strength in statistical modeling, computing and data visualization\n\n\n\nRStudio 1: interface for R, Python, etc called an IDE (integrated development environment), e.g. “I write R code in the RStudio IDE”.\nRStudio is not a requirement for programming with R, but it’s commonly used by R developers, statisticians and data scientists.\n\n[1] RStudio company is becoming Posit starting October 2022."
  },
  {
    "objectID": "intro-r.html#rstudio-cloud---statistics-wo-hardware-hassles",
    "href": "intro-r.html#rstudio-cloud---statistics-wo-hardware-hassles",
    "title": "3  Tool foR Data",
    "section": "3.3 ☁️ RStudio Cloud - Statistics w/o hardware hassles",
    "text": "3.3 ☁️ RStudio Cloud - Statistics w/o hardware hassles\n\n😎 We can implement R programs without installing R and RStudio in your laptop!\n😎 RStudio Cloud lets you do, share and learn data science online for free!\n\n\n\n\n3.3.1 😞 R/RStudio: Lots of friction\n\nDownload and install R\nDownload and install RStudio\nInstall wanted R packages:\n\nrmarkdown\ntidyverse\n…\n\nLoad these packages\nDownload and install tools like Git\n\n\n\n3.3.2 🤓 RStudio Cloud: Much less friction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGo to https://rstudio.cloud/\nLog in\n\n\n\n\n\n\n\n\n>hello R!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n😃 We do statistical analysis using RStudio IDE, directly from a web browser!"
  },
  {
    "objectID": "intro-r.html#install-rstudio-cloud",
    "href": "intro-r.html#install-rstudio-cloud",
    "title": "3  Tool foR Data",
    "section": "3.4 Install RStudio Cloud",
    "text": "3.4 Install RStudio Cloud\n\n\n\n\n\n\nLab time!\n\n\n\n\nStep 1: In the RStudio website https://rstudio.com/, please choose Products > RStudio Cloud as shown below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab time!\n\n\n\n\nStep 2: Click GET STARTED FOR FREE.\nStep 3: Free > Sign Up. Sign up using your Marquette email address or the one you prefer."
  },
  {
    "objectID": "intro-r.html#new-projects",
    "href": "intro-r.html#new-projects",
    "title": "3  Tool foR Data",
    "section": "3.5 New Projects",
    "text": "3.5 New Projects\nIn RStudio Cloud, click New Project > New RStudio Project, then you are all set!"
  },
  {
    "objectID": "intro-r.html#first-r-code-in-rstudio-cloud",
    "href": "intro-r.html#first-r-code-in-rstudio-cloud",
    "title": "3  Tool foR Data",
    "section": "3.6 First R Code in RStudio Cloud!",
    "text": "3.6 First R Code in RStudio Cloud!\n\nGive your project a nice name (click Untitled Project), math-4720 for example.\nFirst R code: \"Hello WoRld!\" or 2 + 4 after > in the Console pane.\nChange the editor theme: Tools > Global Options > Appearance"
  },
  {
    "objectID": "intro-r.html#working-in-rstudiorstudio-cloud",
    "href": "intro-r.html#working-in-rstudiorstudio-cloud",
    "title": "3  Tool foR Data",
    "section": "3.7 Working in RStudio/RStudio Cloud",
    "text": "3.7 Working in RStudio/RStudio Cloud\n\n3.7.1 RStudio Panes\n\n\n\n\n\nRStudio 4 Panes\n\n\n\n\n\n\n3.7.2 R Script\n\nA R script is a .R file that contains R code.  \nTo create a R script, go to File > New > R Script, or click the green-plus icon on the topleft corner, and select R Script.\n\n\n\n\n\n\n\n\n\n\n\n\n3.7.3 Run Code\n\n Run : run the current line or selection of code.\n Icon right to the Run : re-run the previous selected code.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.7.4 Environment Tab\n\nThe (global) environment is our workspace (NOT the RStudio Cloud workspace).\nAnything created or imported into the current R session is stored in our environment and shown in the Environment tab.\nAfter we run the R script, objects stored in the environment are\n\nData set mtcars\nObject x storing integer values 1 to 10.\nObject y storing three numeric values 3, 5, 9.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.7.5 Help\n\nDon’t know how a function works or what a data set is about ❓\n👉 Simply type ? followed by the data name or function name like\n\n\n?mean\n?mtcars\n\n\nA document will show up in the Help tab, teaching you how to use the function or explaining the data set.\n\n\n\n\n\n\n\nWhat does the function mean() do?\n\n\n\n\n\n\n\n\n3.7.6 Resources\nIn RStudio Cloud sidebar,\n\nR and RStudio Cheat Sheats\nLearn more RStudio and statistical data science: Primers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nWhat is the size of mtcars data?\nType mtcars and hit Enter in the Console to see the data set.\nDiscuss data type of each variable.\nType mtcars[, 1] and hit Enter in the Console. What do you see?"
  },
  {
    "objectID": "intro-r.html#install-r",
    "href": "intro-r.html#install-r",
    "title": "3  Tool foR Data",
    "section": "3.8 Install R",
    "text": "3.8 Install R\n\n3.8.1 Step 1\n\nGo to https://cloud.r-project.org\nClick Download R for [your operating system]\n\n\n\n\n3.8.2 Step 2\n\nIf you are a Mac user, you should see the page as below. You are recommended to download and install the latest version of R (now R-4.2.1 (Funny-Looking Kid)), if your OS version allows to do so. Otherwise, choose a previous version, R-3.6.3.\nIf you are a Windows user, after clicking Download R for Windows, please choose base version, then click Download R-4.2.1 for Windows. \n\n\n\n\n3.8.3 Step 3\n\nOnce you install R successfully, when you open R, you should be able to see the following R terminal or console:\n\n\n\n\nWindows \n\n\nMac"
  },
  {
    "objectID": "intro-r.html#install-rstudio",
    "href": "intro-r.html#install-rstudio",
    "title": "3  Tool foR Data",
    "section": "3.10 Install RStudio",
    "text": "3.10 Install RStudio\n\n3.10.1 Step 1\n\nIn the RStudio website, please choose Products > RStudio as shown below.\n\n\n\n\n3.10.2 Step 2\n\nChoose RStudio Desktop and click DOWNLOAD RSTUDIO DESKTOP for the free version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.10.3 Step 3\n\nClick DOWNLOAD RSTUDIO FOR [YOUR SYSTEM]. Then follow standard installation steps, you should get the software soon.\nBe careful that R should be installed successfully in your computer before you download and install RStudio.\n[Note]: The latest version of RStudio is 2022.07.1+554."
  },
  {
    "objectID": "intro-r.html#rstudio-screen",
    "href": "intro-r.html#rstudio-screen",
    "title": "3  Tool foR Data",
    "section": "3.11 RStudio Screen",
    "text": "3.11 RStudio Screen\n\nWhen you open RStudio, you should see something similar to the figure below.\nIf you do, congratulations! You are able to do every statistical computation in R using RStudio locally in your computer."
  },
  {
    "objectID": "intro-r.html#r-is-a-calculator---arithmetic-operators",
    "href": "intro-r.html#r-is-a-calculator---arithmetic-operators",
    "title": "3  Tool foR Data",
    "section": "3.12 R is a Calculator - Arithmetic Operators",
    "text": "3.12 R is a Calculator - Arithmetic Operators\n\n\n\n\n\n\n\n\n\n\n3.12.1 Examples\n\n2 + 3 * 5 + 4\n\n[1] 21\n\n2 + 3 * (5 + 4)\n\n[1] 29\n\n\n\nWe have to do the operation in the parenthesis first"
  },
  {
    "objectID": "intro-r.html#r-does-comparisons---logical-operators",
    "href": "intro-r.html#r-does-comparisons---logical-operators",
    "title": "3  Tool foR Data",
    "section": "3.13 R Does Comparisons - Logical Operators",
    "text": "3.13 R Does Comparisons - Logical Operators\n\n\n\n\n\n\n\n\n\n\n3.13.1 Examples\n\n\n\n5 <= 5\n\n[1] TRUE\n\n5 <= 4\n\n[1] FALSE\n\n# Is 5 is NOT equal to 5? FALSE\n5 != 5\n\n[1] FALSE\n\n\n\n## Is TRUE not equal to FALSE?\nTRUE != FALSE\n\n[1] TRUE\n\n## Is not TRUE equal to FALSE?\n!TRUE == FALSE\n\n[1] TRUE\n\n## TRUE if either one is TRUE or both are TRUE\nTRUE | FALSE\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\nWhat does TRUE & FALSE return?"
  },
  {
    "objectID": "intro-r.html#built-in-functions",
    "href": "intro-r.html#built-in-functions",
    "title": "3  Tool foR Data",
    "section": "3.14 Built-in Functions",
    "text": "3.14 Built-in Functions\n\nR has lots of built-in functions, especially for mathematics, probability and statistics.\n\n\n\n\n\n\n\n\n\n\n\n3.14.1 Examples\n\n\n\nsqrt(144)\n\n[1] 12\n\nexp(1)  ## Euler's number\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nabs(-7)\n\n[1] 7\n\n\n\nfactorial(5)\n\n[1] 120\n\n## without specifying base value\n## it is a natural log with base e\nlog(100)\n\n[1] 4.60517\n\n## log function and we specify base = 2\nlog(100, base = 10)\n\n[1] 2"
  },
  {
    "objectID": "intro-r.html#commenting",
    "href": "intro-r.html#commenting",
    "title": "3  Tool foR Data",
    "section": "3.15 Commenting",
    "text": "3.15 Commenting\n\n\n\n\n\n\nYou’ve seen comments a lot! How do we write a comment in R?\n\n\n\n\n\n\n\nUse # to add a comment so that the text after # is not read as an R command.\nWriting (good) comments is highly recommended: help readers and more importantly yourself understand what the code is doing.\nComments should explain the why, not the what.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://www.reddit.com/r/ProgrammerHumor/comments/8w54mx/code_comments_be_like/"
  },
  {
    "objectID": "intro-r.html#objects-and-funtions-in-r",
    "href": "intro-r.html#objects-and-funtions-in-r",
    "title": "3  Tool foR Data",
    "section": "3.16 Objects and Funtions in R",
    "text": "3.16 Objects and Funtions in R\n\n Everything that exists is an object. \n Everything that happens is a function call. \n– John Chambers, the creator of the S programming language.\n\n\nWe have made lots of things happened!\nEven arithmetic and logical operators are functions!\n\n\n`+`(x = 2, y = 3)\n\n[1] 5\n\n`&`(TRUE, FALSE)\n\n[1] FALSE"
  },
  {
    "objectID": "intro-r.html#creating-variables",
    "href": "intro-r.html#creating-variables",
    "title": "3  Tool foR Data",
    "section": "3.17 Creating Variables",
    "text": "3.17 Creating Variables\n\nA variable stores a value that can be changed according to our need.\nUse <- operator to assign a value to the variable. (Highly recommended👍)\n\n\nx <- 5  ## we create an object, value 5, and call it x, which is a variable.\nx  ## type the variable name to see the value stored in the object x\n\n[1] 5\n\n\n\n\n(x <- x + 6)  # We can reassign any value to the variable we created\n\n[1] 11\n\nx == 5  # We can perform any operations on variables\n\n[1] FALSE\n\nlog(x) # Variables can also be used in any built-in functions\n\n[1] 2.397895"
  },
  {
    "objectID": "intro-r.html#bad-naming",
    "href": "intro-r.html#bad-naming",
    "title": "3  Tool foR Data",
    "section": "3.18 Bad Naming",
    "text": "3.18 Bad Naming\n\n❌ Unless you have a very good reason, don’t create a variable whose name is the same as any R built-in constant or function!\n😟 It causes lots of confusion when your code is long and when others read your code.\n\n\n## THIS IS BAD CODING! DON'T DO THIS!\npi  ## pi is a built-in constant\n\n[1] 3.141593\n\n(pi <- 20)\n\n[1] 20\n\nabs ## abs is a built-in function\n\nfunction (x)  .Primitive(\"abs\")\n\n(abs <- abs(pi))\n\n[1] 20"
  },
  {
    "objectID": "intro-r.html#object-types",
    "href": "intro-r.html#object-types",
    "title": "3  Tool foR Data",
    "section": "3.19 Object Types",
    "text": "3.19 Object Types\n\n3.19.1 Types of Variables\n\nUse typeof() to check which type a variable belongs to.\nCommon types include character, double, integer and logical.\nCheck if it???s of a specific type: is.character(), is.double(), is.integer(), is.logical(). \n\n\n\n\ntypeof(5)\n\n[1] \"double\"\n\ntypeof(5L)\n\n[1] \"integer\"\n\ntypeof(\"I_love_stat!\")\n\n[1] \"character\"\n\n\n\ntypeof(1 > 3)\n\n[1] \"logical\"\n\nis.double(5L)\n\n[1] FALSE\n\n\n\n\n\n\n3.19.2 Variable Types in R and in Statistics\n\nType character and logical correspond to categorical variables.\nType logical is a special type of categorical variables that has only two categories (binary). \nType double and integer correspond to numerical variables. (an exception later)\nType double is for continuous variables\nType integer is for discrete variables."
  },
  {
    "objectID": "intro-r.html#atomic-vector",
    "href": "intro-r.html#atomic-vector",
    "title": "3  Tool foR Data",
    "section": "3.20 (Atomic) Vector",
    "text": "3.20 (Atomic) Vector\n\nTo create a vector, use c(), short for concatenate or combine.\nAll elements of a vector must be of the same type.\n\n\n\n\n(dbl_vec <- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(int_vec <- c(1L, 6L, 10L))\n\n[1]  1  6 10\n\n## TRUE and FALSE can be written as T and F\n(log_vec <- c(TRUE, FALSE, F))  \n\n[1]  TRUE FALSE FALSE\n\n(chr_vec <- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5\n\n\n\n\n\n3.20.1 Operations on Vectors\n\nWe can do any operations on vectors as we do on a scalar variable (vector of length 1).\n\n\n\n\n# Create two vectors\nv1 <- c(3, 8)\nv2 <- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10\n\n\n\n\n\n\n3.20.2 Recycling of Vectors\n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\nv1 <- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 <- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16\n\n\n\n\n3.20.3 Subsetting Vectors\n\nTo extract element(s) in a vector, use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The first element\nv1[1] \n\n[1] 3\n\n## The second element\nv2[2]  \n\n[1] 100\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5"
  },
  {
    "objectID": "intro-r.html#factor",
    "href": "intro-r.html#factor",
    "title": "3  Tool foR Data",
    "section": "3.21 Factor",
    "text": "3.21 Factor\n\nA vector of type factor can be ordered in a meaningful way.\nCreate a factor by factor(). It is a type of integer, not character. 😲 🙄\n\n\nfac <- factor(c(\"med\", \"high\", \"low\"))\ntypeof(fac)\n\n[1] \"integer\"\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically.\n\n[1] \"high\" \"low\"  \"med\" \n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\norder_fac <- factor(c(\"med\", \"high\", \"low\"), levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1"
  },
  {
    "objectID": "intro-r.html#list-generic-vectors",
    "href": "intro-r.html#list-generic-vectors",
    "title": "3  Tool foR Data",
    "section": "3.22 List (Generic Vectors)",
    "text": "3.22 List (Generic Vectors)\n\nLists are different from vectors: Elements can be of any type, including lists.\nConstruct a list by using list() instead of c().\n\n\n\n\n## a list of 3 elements of different types\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\nx_lst\n\n$idx\n[1] 1 2 3\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1]  TRUE FALSE\n\n\n\nstr(x_lst)\n\nList of 3\n $ idx: int [1:3] 1 2 3\n $    : chr \"a\"\n $    : logi [1:2] TRUE FALSE\n\nnames(x_lst)\n\n[1] \"idx\" \"\"    \"\"   \n\nlength(x_lst)\n\n[1] 3\n\n\n\n\n\n3.22.1 Subsetting a List\n\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\n\n\n\n\nReturn an  element  of a list\n\n\nReturn a  sub-list  of a list\n\n\n\n\n## subset by name (a vector)\nx_lst$idx  \n\n[1] 1 2 3\n\n## subset by indexing (a vector)\nx_lst[[1]]  \n\n[1] 1 2 3\n\ntypeof(x_lst$idx)\n\n[1] \"integer\"\n\n\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n\n$idx\n[1] 1 2 3\n\n## subset by indexing (still a list)\nx_lst[1]  \n\n$idx\n[1] 1 2 3\n\ntypeof(x_lst[\"idx\"])\n\n[1] \"list\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\n— @RLangTip, https://twitter.com/RLangTip/status/268375867468681216"
  },
  {
    "objectID": "intro-r.html#matrix",
    "href": "intro-r.html#matrix",
    "title": "3  Tool foR Data",
    "section": "3.23 Matrix",
    "text": "3.23 Matrix\n\nA matrix is a two-dimensional analog of a vector.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat <- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n\n3.23.1 Subsetting a Matrix\n\nTo extract a sub-matrix, use the same indexing approach as vectors on rows and columns.\nUse comma , to separate row and column index.\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\n[1] 4 5 6\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\n[1] 2 5\n\n## The 1st and 3rd rows\nmat[c(1, 3), ] \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    3    6\n\n\n\n\n\n\n3.23.2 Binding Matrices\n\nWe can generalize c() used in vectors to cbind() (binding matrices by adding columns) and rbind() (binding matrices by adding rows) for matrices.\nWhen matrices are combined by columns (rows), they should have the same number of rows (columns).\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nmat_c <- matrix(data = c(7, 0, 0, 8, 2, 6), \n                nrow = 3, ncol = 2)\n## should have the same number of rows\ncbind(mat, mat_c)  \n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7    8\n[2,]    2    5    0    2\n[3,]    3    6    0    6\n\n\n\nmat_r <- matrix(data = 1:4, \n                nrow = 2, \n                ncol = 2)\n## should have the same number of columns\nrbind(mat, mat_r)  \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n[4,]    1    3\n[5,]    2    4"
  },
  {
    "objectID": "intro-r.html#data-frame-the-most-common-way-of-storing-data",
    "href": "intro-r.html#data-frame-the-most-common-way-of-storing-data",
    "title": "3  Tool foR Data",
    "section": "3.24 Data frame: The Most Common Way of Storing Data",
    "text": "3.24 Data frame: The Most Common Way of Storing Data\n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nMore general than matrix: Different columns can have different types.\nTo create a data frame, use data.frame() that takes named vectors as input.\n\n\n\n\n## data frame w/ an dbl column named  \n## and char column named grade.\n(df <- data.frame(age = c(19,21,40), \n                  gender = c(\"m\",\"f\",\"m\")))\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n## a data frame has a list structure\nstr(df)  \n\n'data.frame':   3 obs. of  2 variables:\n $ age   : num  19 21 40\n $ gender: chr  \"m\" \"f\" \"m\"\n\n\n\n## must set column names\n## or they are ugly and non-recognizable\ndata.frame(c(19, 21, 40), c(\"m\",\"f\", \"m\")) \n\n  c.19..21..40. c..m....f....m..\n1            19                m\n2            21                f\n3            40                m\n\n\n\n\n\n3.24.1 Properties of Data Frames\n\nData frame has properties of matrix and list.\n\n\n\n\nnames(df)  ## df as a list\n\n[1] \"age\"    \"gender\"\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\"    \"gender\"\n\nlength(df) ## df as a list\n\n[1] 2\n\nncol(df) ## df as a matrix\n\n[1] 2\n\ndim(df) ## df as a matrix\n\n[1] 3 2\n\n\n\n## rbind() and cbind() can be used on df\n\ndf_r <- data.frame(age = 10, \n                   gender = \"f\")\nrbind(df, df_r)\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n4  10      f\n\ndf_c <- \n    data.frame(col = c(\"red\",\"blue\",\"gray\"))\n(df_new <- cbind(df, df_c))\n\n  age gender  col\n1  19      m  red\n2  21      f blue\n3  40      m gray\n\n\n\n\n\n\n3.24.2 Subsetting a Data Frame\n\nWhen we subset data frames, we can use either list or matrix subsetting methods.\n\n\n\n\ndf_new\n\n  age gender  col\n1  19      m  red\n2  21      f blue\n3  40      m gray\n\n## Subset rows\ndf_new[c(1, 3), ]\n\n  age gender  col\n1  19      m  red\n3  40      m gray\n\n## select the row where age == 21\ndf_new[df_new$age == 21, ]\n\n  age gender  col\n2  21      f blue\n\n\n\n## Subset columns\n## like a list\ndf_new$age\n\n[1] 19 21 40\n\ndf_new[c(\"age\", \"gender\")]\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n## like a matrix\ndf_new[, c(\"age\", \"gender\")]\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nCreate a vector object called x that has 5 elements 3, 6, 2, 9, 14.\nCompute the average of elements of x.\nSubset the mtcars data set by selecting variables mpg and disp.\nSelect the cars (rows) in mtcars that have 4 cylinders."
  },
  {
    "objectID": "data-graphics.html#descriptive-statistics-data-summary",
    "href": "data-graphics.html#descriptive-statistics-data-summary",
    "title": "4  Data Visualization",
    "section": "4.1 Descriptive Statistics (Data Summary)",
    "text": "4.1 Descriptive Statistics (Data Summary)\n\nBefore doing inferential statistics, let’s first learn to understand our data by describing or summarizing it using a table, graph, or some important values, so that appropriate methods can be performed for better inference results."
  },
  {
    "objectID": "data-graphics.html#frequency-table-for-categorical-variable",
    "href": "data-graphics.html#frequency-table-for-categorical-variable",
    "title": "4  Data Visualization",
    "section": "4.2 Frequency Table for Categorical Variable",
    "text": "4.2 Frequency Table for Categorical Variable\n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nFrequency table for categorical data with \\(n\\) the number of data values:\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\n\\(C_1\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(C_2\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n…\n…\n…\n\n\n\\(C_k\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\nExample: A categorical variable color that has three categories\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\nRed 🔴\n8\n8/50 = 0.16\n\n\nBlue 🔵\n26\n26/50 = 0.52\n\n\nBlack ⚫\n16\n16/50 = 0.32\n\n\n\n\n4.2.1 Categorical Frequency Table in R: loan50 [OI]\n\n# install.packages(\"openintro\")\nlibrary(openintro)\nstr(loan50)\n\ntibble [50 × 18] (S3: tbl_df/tbl/data.frame)\n $ state                  : Factor w/ 51 levels \"\",\"AK\",\"AL\",\"AR\",..: 32 6 41 6 36 16 35 25 11 11 ...\n $ emp_length             : num [1:50] 3 10 NA 0 4 6 2 10 6 3 ...\n $ term                   : num [1:50] 60 36 36 36 60 36 36 36 60 60 ...\n $ homeownership          : Factor w/ 3 levels \"rent\",\"mortgage\",..: 1 1 2 1 2 2 1 2 1 2 ...\n $ annual_income          : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 80000 ...\n $ verified_income        : Factor w/ 4 levels \"\",\"Not Verified\",..: 2 2 4 2 2 3 3 2 2 3 ...\n $ debt_to_income         : num [1:50] 0.558 1.306 1.056 0.574 0.238 ...\n $ total_credit_limit     : int [1:50] 95131 51929 301373 59890 422619 349825 15980 258439 87705 330394 ...\n $ total_credit_utilized  : int [1:50] 32894 78341 79221 43076 60490 72162 2872 28073 23715 32036 ...\n $ num_cc_carrying_balance: int [1:50] 8 2 14 10 2 4 1 3 10 4 ...\n $ loan_purpose           : Factor w/ 14 levels \"\",\"car\",\"credit_card\",..: 4 3 4 3 5 5 4 3 3 4 ...\n $ loan_amount            : int [1:50] 22000 6000 25000 6000 25000 6400 3000 14500 10000 18500 ...\n $ grade                  : Factor w/ 8 levels \"\",\"A\",\"B\",\"C\",..: 3 3 6 3 3 3 5 2 2 4 ...\n $ interest_rate          : num [1:50] 10.9 9.92 26.3 9.92 9.43 ...\n $ public_record_bankrupt : int [1:50] 0 1 0 0 0 0 0 0 0 1 ...\n $ loan_status            : Factor w/ 7 levels \"\",\"Charged Off\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ has_second_income      : logi [1:50] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ total_income           : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 192000 ...\n\n\n\n\n4.2.2 Categorical Frequency Table in R: homeownership\n\n# 50 values (rent, mortgage, own) of categorical homeownership in loan50 data\n(x <- loan50$homeownership)\n\n [1] rent     rent     mortgage rent     mortgage mortgage rent     mortgage\n [9] rent     mortgage rent     mortgage rent     mortgage rent     mortgage\n[17] rent     rent     rent     mortgage mortgage mortgage mortgage rent    \n[25] mortgage rent     mortgage own      mortgage mortgage rent     mortgage\n[33] mortgage rent     rent     own      mortgage rent     mortgage rent    \n[41] mortgage rent     rent     mortgage mortgage mortgage mortgage rent    \n[49] own      mortgage\nLevels: rent mortgage own\n\n## frequency table\ntable(x)\n\nx\n    rent mortgage      own \n      21       26        3 \n\n\n\n\n\n\n\n\nIf we want to create a frequency table shown in definition, which R data structure we can use?\n\n\n\n\n\n\n\nfreq <- table(x)\nrel_freq <- freq / sum(freq)\ncbind(freq, rel_freq)\n\n         freq rel_freq\nrent       21     0.42\nmortgage   26     0.52\nown         3     0.06\n\n\n\n\n4.2.3 Visualizing a Frequency Table: Bar Chart\n\nbarplot(height = table(x), main = \"Bar Chart\", xlab = \"Homeownership\")\n\n\n\n\n\n\n4.2.4 Pie Chart\n\npie(x = table(x), main = \"Pie Chart\")"
  },
  {
    "objectID": "data-graphics.html#frequency-distribution-for-numerical-variables",
    "href": "data-graphics.html#frequency-distribution-for-numerical-variables",
    "title": "4  Data Visualization",
    "section": "4.3 Frequency Distribution for Numerical Variables",
    "text": "4.3 Frequency Distribution for Numerical Variables\n\nDivide the data into \\(k\\) non-overlapping groups of intervals (classes).\nConvert the data into \\(k\\) categories with an associated class interval.\nCount the number of measurements falling in a given class interval (class frequency).\n\n\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\n\\(1\\)\n\\([a_1, a_2]\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(2\\)\n\\((a_2, a_3]\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n…\n…\n…\n…\n\n\n\\(k\\)\n\\((a_k, a_{k+1}]\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\n\\((a_2 - a_1) = (a_3 - a_2) = \\cdots = (a_{k+1} - a_k)\\). All class widths are the same!\n\n\n\n\n\n\n\nCan our grade conversion be used for creating a frequency distribution?\n\n\n\n\n\n\n\n\n\n\n \n  \n    Grade \n    Percentage \n  \n \n\n  \n    A \n    [94, 100] \n  \n  \n    A- \n    [90, 94) \n  \n  \n    B+ \n    [87, 90) \n  \n  \n    B \n    [83, 87) \n  \n  \n    B- \n    [80, 83) \n  \n  \n    C+ \n    [77, 80) \n  \n  \n    C \n    [73, 77) \n  \n  \n    C- \n    [70, 73) \n  \n  \n    D+ \n    [65, 70) \n  \n  \n    D \n    [60, 65) \n  \n  \n    F \n    [0, 60) \n  \n\n\n\n\n\n\n4.3.1 Interest Rate in Data loan50 [OI]\n\n(int_rate <- round(loan50$interest_rate, 1))\n\n [1] 10.9  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n[16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n[31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n[46] 10.4 21.4 10.9  9.4  6.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.3.2 Frequency Distribution of Interest Rate\n\n\n\n\n Class Class_Intvl Freq Rel_Freq\n     1     5%-7.5%   11     0.22\n     2    7.5%-10%   15     0.30\n     3   10%-12.5%    8     0.16\n     4   12.5%-15%    5     0.10\n     5   15%-17.5%    4     0.08\n     6   17.5%-20%    4     0.08\n     7   20%-22.5%    1     0.02\n     8   22.5%-25%    1     0.02\n     9   25%-27.5%    1     0.02\n\n\n\n\nAll class widths are the same!\nNumber of classes should not be too big or too small.\nThe lower limit of the 1st class should not be greater than the minimum value of the data.\nThe upper limit of the last class should not be smaller than the maximum value of the data.\n\n\n\n\n\nrange(int_rate)\n\n[1]  5.3 26.3\n\n\n\n\n\n\n\n\n\n\nWonder how we choose the number of classes or the class width?\n\n\n\n\n\n\n\n\n\n\n\n\nR decides the number for us when we visualize the frequency distribution by a histogram.\n\n\n\n\n\n\n\n\n4.3.3 Visualizing Frequency Distribution by a Histogram\n\n\n\nUse default breaks (no need to specify)\n\n\nUse customized breaks\n\n\n\n\nhist(x = int_rate, \n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Defualt)\")\n\n\n\n\n\nclass_boundary\n\n [1]  5.0  7.5 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5\n\nhist(x = int_rate, \n     breaks = class_boundary, #<<\n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Ours)\")"
  },
  {
    "objectID": "data-graphics.html#skewness",
    "href": "data-graphics.html#skewness",
    "title": "4  Data Visualization",
    "section": "4.4 Skewness",
    "text": "4.4 Skewness\n\nKey characteristics of distributions includes shape, center and spread.\nSkewness provides a way to summarize the shape of a distribution."
  },
  {
    "objectID": "data-graphics.html#remembering-skewness",
    "href": "data-graphics.html#remembering-skewness",
    "title": "4  Data Visualization",
    "section": "4.5 Remembering Skewness",
    "text": "4.5 Remembering Skewness\n\n\n\n\n\n\nIs the interest rate histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.53"
  },
  {
    "objectID": "data-graphics.html#scatterplot-for-two-numerical-variables",
    "href": "data-graphics.html#scatterplot-for-two-numerical-variables",
    "title": "4  Data Visualization",
    "section": "4.6 Scatterplot for Two Numerical Variables",
    "text": "4.6 Scatterplot for Two Numerical Variables\n\n\nA scatterplot provides a case-by-case view of data for two numerical variables.\n\n\nplot(x = loan50$total_income, y = loan50$loan_amount,\n     xlab = \"Total income\", ylab = \"Loan amount\",\n     pch = 16, col = 4)"
  },
  {
    "objectID": "data-numerics.html#numerical-summaries-of-data",
    "href": "data-numerics.html#numerical-summaries-of-data",
    "title": "5  Data Sample Statistics",
    "section": "5.1 Numerical Summaries of Data",
    "text": "5.1 Numerical Summaries of Data\n\n\n\n\n\n\n\n\n\n\n\n\n\nMeasure of Center: We typically use the middle point. (What does “middle” mean?)\nMeasure of Variation: What values tell us how much variation a variable has?\n\n\n\n\n\n\n\n\n\n\nIf you need to choose one value that represents the entire data, what value would you choose?"
  },
  {
    "objectID": "data-numerics.html#measures-of-center-mean",
    "href": "data-numerics.html#measures-of-center-mean",
    "title": "5  Data Sample Statistics",
    "section": "5.2 Measures of Center: Mean",
    "text": "5.2 Measures of Center: Mean\n\nThe (arithmetic) mean or average is adding up all of the values, then dividing by the total number of them.\nLet \\(x_1, x_2, \\dots, x_n\\) denote the measurements observed in a sample of size \\(n\\). Then the sample mean is defined as \\[\\overline{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + x_2 + \\dots + x_n}{n}\\]\nIn the interest rate example, \\[\\overline{x} = \\frac{10.9\\% + 9.9\\% + \\cdots + 6.1\\%}{50} = 11.56\\%\\] \n\n\nmean(int_rate)\n\n[1] 11.558\n\n\n\n5.2.1 Balancing Point\n\nThink of the mean as the balancing point of the distribution."
  },
  {
    "objectID": "data-numerics.html#measures-of-center-median",
    "href": "data-numerics.html#measures-of-center-median",
    "title": "5  Data Sample Statistics",
    "section": "5.3 Measures of Center: Median",
    "text": "5.3 Measures of Center: Median\n\nMedian: the middle value when data values are sorted.\nHalf of the values are less than or equal to the median, and the other half are greater than it.\nTo find the median, we first sort the values.\n\\(n\\) is odd: the median is located in the exact middle of the ordered values.\n\n Data: (0, 2, 10, 14, 8) \n Sorted Data: (0, 2, 8, 10, 14) \n The median is \\(8\\) \n\n\\(n\\) is even: the median is the average of the two middle numbers.\n\n Data: (0, 2, 10, 14, 8, 12) \n Sorted Data: (0, 2, 8, 10, 12, 14) \n The median is \\(\\frac{8 + 10}{2} = 9\\) \n\n\n\n5.3.1 Calculate Median in R\n\nmedian(int_rate)  ## Compute the median using command median()\n\n[1] 9.9\n\n\n\n## Compute the median using definition\n(sort_rate <- sort(int_rate))  ## sort data\n\n [1]  5.3  5.3  5.3  6.1  6.1  6.1  6.7  6.7  7.3  7.3  7.3  8.0  8.0  8.0  8.0\n[16]  9.4  9.4  9.4  9.4  9.4  9.9  9.9  9.9  9.9  9.9  9.9 10.4 10.4 10.9 10.9\n[31] 10.9 10.9 10.9 12.0 12.6 12.6 12.6 14.1 15.0 16.0 17.1 17.1 17.1 18.1 18.4\n[46] 19.4 20.0 21.4 24.9 26.3\n\nlength(int_rate)  ## Check sample size is odd or even\n\n[1] 50\n\n(sort_rate[25] + sort_rate[26]) / 2  ## Verify the answer\n\n[1] 9.9\n\n\n\n(int_rate[25] + int_rate[26]) / 2  ## Using un-sorted data leads to a wrong answer!!\n\n[1] 8.1"
  },
  {
    "objectID": "data-numerics.html#measures-of-center-mode",
    "href": "data-numerics.html#measures-of-center-mode",
    "title": "5  Data Sample Statistics",
    "section": "5.4 Measures of Center: Mode",
    "text": "5.4 Measures of Center: Mode\n\nMode: the value that occurs most frequently.\nFor continuous numerical data, it is common to have no observations with the same value.\nPractical definition: A mode is represented by a prominent peak in the distribution.\n\n\n## Create a frequency table \n(table_data <- table(int_rate))\n\nint_rate\n 5.3  6.1  6.7  7.3    8  9.4  9.9 10.4 10.9   12 12.6 14.1   15   16 17.1 18.1 \n   3    3    2    3    4    5    6    2    5    1    3    1    1    1    3    1 \n18.4 19.4   20 21.4 24.9 26.3 \n   1    1    1    1    1    1 \n\n\n\n## Sort the table to find the mode that occurs most frequently\n## the number that happens most frequently will be the first one\nsort(table_data, decreasing = TRUE)\n\nint_rate\n 9.9  9.4 10.9    8  5.3  6.1  7.3 12.6 17.1  6.7 10.4   12 14.1   15   16 18.1 \n   6    5    5    4    3    3    3    3    3    2    2    1    1    1    1    1 \n18.4 19.4   20 21.4 24.9 26.3 \n   1    1    1    1    1    1"
  },
  {
    "objectID": "data-numerics.html#comparison-of-mean-median-and-mode",
    "href": "data-numerics.html#comparison-of-mean-median-and-mode",
    "title": "5  Data Sample Statistics",
    "section": "5.5 Comparison of Mean, Median and Mode",
    "text": "5.5 Comparison of Mean, Median and Mode\n\nMean is sensitive to extreme values (outliers).\nMedian/mode is more robust than mean.\n\n\ndata_extreme\n\n [1] 90.0  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n[16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n[31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n[46] 10.4 21.4 10.9  9.4  6.1\n\n\n\nmean(data_extreme)  ## Large mean! Original mean is 11.56\n\n[1] 13.14\n\nmedian(data_extreme)  ## Median does not change!\n\n[1] 9.9\n\nnames(sort(table(data_extreme), decreasing = TRUE))[1] ## Mode does not change too!\n\n[1] \"9.9\"\n\n\n\nMode is applicable for both categorical and numerical data, while median and mean work for numerical data only.\nThere may be more than one mode, but there is only one median and one mean."
  },
  {
    "objectID": "data-numerics.html#measures-of-variation",
    "href": "data-numerics.html#measures-of-variation",
    "title": "5  Data Sample Statistics",
    "section": "5.6 Measures of Variation",
    "text": "5.6 Measures of Variation\n\n\n\n\n\n\n\n\n\n\n5.6.1 p-th percentile\n\n\np-th percentile (quantile): a data value such that\n\nat most \\(p\\%\\) of the values are below it\nat most \\((1-p)\\%\\) of the values are above it \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nTwo data sets have the same mean 20.\n\nOne data set has 99-th percentile = 30, and 1-st percentile = 10.\nThe other has 99-th percentile = 40, and 1-st percentile = 0.\nWhich data set has larger variation?\n\n\n\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/ACT_(test)\n\n\n\n\n\n\n\n\n5.6.2 Interquartile Range (IQR)\n\nFirst Quartile (Q1): the 25-th percentile\nSecond Quartile (Q2): the 50-th percentile (Median)\nThird Quartile (Q3): the 75-th percentile\nInterquartile Range (IQR): Q3 - Q1\n\n\n\n\n## Use quantile() to find any percentile \n## through specifying the probability\nquantile(x = int_rate, \n         probs = c(0.25, 0.5, 0.75))\n\n   25%    50%    75% \n 8.000  9.900 13.725 \n\n## IQR by definition\nquantile(x = int_rate, probs = 0.75) - \n  quantile(x = int_rate, probs = 0.25) \n\n  75% \n5.725 \n\n\n\n## IQR()\nIQR(int_rate)  \n\n[1] 5.725\n\n## summary() to get the numeric summary\nsummary(int_rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.30    8.00    9.90   11.56   13.72   26.30 \n\n\n\n\n\n\n\n\n\n\nLarger IQR means more or less variation?\n\n\n\n\n\n\n\n\n5.6.3 Variance and Standard Deviation\n\nThe distance of an observation from its mean, \\(x_i - \\overline{x}\\), its deviation.\nSample Variance is defined as \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1} \\]\nSample Standard Deviation (SD) is defined as the square root of the variance \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}} \\] \nVariance is the average of squared deviation from the sample mean \\(\\overline{x}\\) or the mean squared deviation from the mean.\nSD is the root mean squared deviation from the mean. It measures, on average, how far the data spread out around the average.\n\n\n\n5.6.4 Compute Variance and SD\n\nvar(int_rate)\n\n[1] 25.54942\n\nsqrt(var(int_rate))\n\n[1] 5.054644\n\nsd(int_rate)\n\n[1] 5.054644"
  },
  {
    "objectID": "data-numerics.html#visualizing-data-variation-boxplot",
    "href": "data-numerics.html#visualizing-data-variation-boxplot",
    "title": "5  Data Sample Statistics",
    "section": "5.7 Visualizing Data Variation: Boxplot",
    "text": "5.7 Visualizing Data Variation: Boxplot\nWhen plotting the whiskers,\n\nminimum means the minimal value that is not an potential outlier.\nmaximum means the maximal value that is not an potential outlier.\n\n\n\n\n\n\nhttps://www.leansigmacorporation.com/box-plot-with-minitab/\n\n\n\n\n\n5.7.1 Interest Rate Boxplot\n\n\n\n\n\n\n\n\n\n\n\n5.7.2 Boxplot in R\n\n\n\nboxplot(int_rate,ylab =\"Interest Rate (%)\")\n\n\n\n\n\nsort(int_rate, decreasing = TRUE)[1:5]\n\n[1] 26.3 24.9 21.4 20.0 19.4\n\nsort(int_rate)[1:5]\n\n[1] 5.3 5.3 5.3 6.1 6.1\n\nQ3 <- quantile(int_rate, probs = 0.75, \n               names = FALSE)\nQ1 <- quantile(int_rate, probs = 0.25, \n               names = FALSE)\nIQR <- Q3 - Q1\nQ1 - 1.5 * IQR\n\n[1] -0.5875\n\nQ3 + 1.5 * IQR\n\n[1] 22.3125"
  },
  {
    "objectID": "prob-define.html#why-study-probability",
    "href": "prob-define.html#why-study-probability",
    "title": "6  Definition of Probability",
    "section": "6.1 Why Study Probability",
    "text": "6.1 Why Study Probability\n\nWe live in a world full of chances and uncertainty! (Sept 19, 2022)"
  },
  {
    "objectID": "prob-define.html#why-probability-before-statistics",
    "href": "prob-define.html#why-probability-before-statistics",
    "title": "6  Definition of Probability",
    "section": "6.2 Why Probability Before Statistics?",
    "text": "6.2 Why Probability Before Statistics?\n\n\n Probability : We know the process generating the data and are interested in properties of observations.\n Statistics : We observed the data (sample) and are interested in determining what is the process generating the data (population)."
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability-relative-frequency",
    "href": "prob-define.html#interpretation-of-probability-relative-frequency",
    "title": "6  Definition of Probability",
    "section": "6.3 Interpretation of Probability: Relative Frequency",
    "text": "6.3 Interpretation of Probability: Relative Frequency\n\n Relative Frequency : The probability that some outcome of a process will be obtained is interpreted as the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\n\n\n\n      Frequency Relative Frequency\nHeads         3                0.3\nTails         7                0.7\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       512              0.512\nTails       488              0.488\nTotal      1000              1.000\n---------------------\n\n\n\nIf we repeat tossing the coin 10 times, the probability of obtaining a head is 30%.\nIf 1000 times, the probability is 51.2%.\n\n\n\n\n\n\n\nAny issue of relative frequency probability?"
  },
  {
    "objectID": "prob-define.html#issues-of-relative-frequency",
    "href": "prob-define.html#issues-of-relative-frequency",
    "title": "6  Definition of Probability",
    "section": "6.4 Issues of Relative Frequency",
    "text": "6.4 Issues of Relative Frequency\n\n😕 How large of a number is large enough?\n😕 Meaning of “under similar conditions”\n😕 The relative frequency is reliable under identical conditions?\n👉 We only obtain an approximation instead of exact value.\n😂 How do you compute the probability that Chicago Cubs wins the World Series next year?"
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability-classical-approach",
    "href": "prob-define.html#interpretation-of-probability-classical-approach",
    "title": "6  Definition of Probability",
    "section": "6.5 Interpretation of Probability: Classical Approach",
    "text": "6.5 Interpretation of Probability: Classical Approach\n\n Classical probability : The probability is based on the concept of equally likely outcomes.\nIf the outcome of some process must be one of \\(n\\) different outcomes, the probability of each outcome is \\(1/n\\).\nExample:\n\ntoss a fair coin (2 outcomes) p\u001f*\u0019\nroll a well-balanced die (6 outcomes) 🎲\ndraw one from a deck of cards (52 outcomes) 🃏\n\n\n\n\n\n\n\n\nAny issue of classical probability?\n\n\n\n\n\n\n\nThe probability that [you name it] wins the World Series next year is 1/30?!"
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability-subjective-approach",
    "href": "prob-define.html#interpretation-of-probability-subjective-approach",
    "title": "6  Definition of Probability",
    "section": "6.6 Interpretation of Probability: Subjective Approach",
    "text": "6.6 Interpretation of Probability: Subjective Approach\n\n Subjective probability : The probability is assigned or estimated using people’s knowledge, beliefs and information about the data generating process.\nA person’s subjective probability of an outcome, rather than the true probability of that outcome.\nI think “the probability that Milwaukee Brewers wins the World Series this year is 30%”.\nMy probability that Milwaukee Brewers wins the World Series next year is different from an ESPN MLB analyst’s probability.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAny probability operations and rules do NOT depend on interpretation of probability!"
  },
  {
    "objectID": "prob-rule.html#experiments-events-and-sample-space",
    "href": "prob-rule.html#experiments-events-and-sample-space",
    "title": "7  Probability Rules",
    "section": "7.1 Experiments, Events and Sample Space",
    "text": "7.1 Experiments, Events and Sample Space\n\nExperiment: any process in which the possible outcomes can be identified ahead of time.\nEvent: a set of possible outcomes of the experiment.\nSample space \\((\\mathcal{S})\\) of an experiment: the collection of ALL possible outcomes of the experiment.\n\n\n\n\n\n\n\n\n\n\nExperiment\nPossible Outcomes\nSome Events\nSample Space\n\n\n\n\nFlip a coin p\u001f*\u0019\nHeads, Tails\n{Heads}, {Heads, Tails}, …\n{Heads, Tails}\n\n\nRoll a die 🎲\n1, 2, 3, 4, 5, 6\n{1, 3, 5}, {2, 4, 6}, {2}, {3, 4, 5, 6}, …\n{1, 2, 3, 4, 5, 6}\n\n\n\n\n\n\n\n\n\nIs the sample space also an event?\n\n\n\n\n\n\n\nYes, the sample space itself is an event because it is also a set of possible outcomes of the experiment."
  },
  {
    "objectID": "prob-rule.html#set-concept-example-of-rolling-a-six-side-balanced-die",
    "href": "prob-rule.html#set-concept-example-of-rolling-a-six-side-balanced-die",
    "title": "7  Probability Rules",
    "section": "7.2 Set Concept: Example of Rolling a six-side balanced die",
    "text": "7.2 Set Concept: Example of Rolling a six-side balanced die\n\n\n\n\n\n\nTip\n\n\n\nDraw a Venn Diagram every time you get stuck!\n\n\n\n Complement  of an event (set) \\(A\\),  \\(A^c\\) : a set of all outcomes (elements) of \\(\\mathcal{S}\\) in which \\(A\\) does not occur.\n\nLet \\(A\\) be an event that a number greater than 2. Then \\(A = \\{3, 4, 5, 6\\}\\) and \\(A^c = \\{1, 2\\}\\).\n\n Union \\((A \\cup B)\\) : a set of all outcomes of \\(\\mathcal{S}\\) in \\(A\\) or \\(B\\).\n\nLet \\(B\\) be an event that an even number is obtained. (What is \\(B\\) in terms of a set?)\n \\(B = \\{2, 4, 6\\}\\), \\(A \\cup B = \\{2, 3, 4, 5, 6\\}\\).\n\n Intersection \\((A \\cap B)\\) : a set of all outcomes of \\(\\mathcal{S}\\) in both \\(A\\) and \\(B\\).\n\n \\(A \\cap B = \\{4, 6\\}\\).\n\n\\(A\\) and \\(B\\) are disjoint (or mutually exclusive) if they have no outcomes in common \\((A \\cap B = \\emptyset)\\).\n\n\\(\\emptyset\\) means an empty set, \\(\\{\\}\\), i.e., no elements in the set.\n Let \\(C\\) be an event that an odd number is obtained. Then \\(C = \\{1, 3, 5\\}\\) and \\(B \\cap C = \\emptyset\\). \n\nContainment \\((A \\subset B)\\): every elements of \\(A\\) also belongs to \\(B\\). If \\(A\\) occurs then so does \\(B\\).\n\n\n\n\n\n\n\n\n\n\n\n \\(B\\) is an event that an even number is obtained. \n \\(D\\) is an event that a number greater than 1 is obtained. \n \\(B = \\{2, 4, 6\\}\\) and \\(D = \\{2, 3, 4, 5, 6\\}\\). \n\n\n\n\n\n\n\nIs \\(B \\subset D\\) or \\(D \\subset B\\)?"
  },
  {
    "objectID": "prob-rule.html#probability-rules",
    "href": "prob-rule.html#probability-rules",
    "title": "7  Probability Rules",
    "section": "7.3 Probability Rules",
    "text": "7.3 Probability Rules\nDenote the probability of an event \\(A\\) on a sample space \\(\\mathcal{S}\\) as \\(P(A)\\).\n\n\n\n\n\n\nTip\n\n\n\nTreat the probability of an event as the area of the event in the Venn diagram.\n\n\n\nAxioms\n\n\\(P(\\mathcal{S}) = 1\\)\nFor any event \\(A\\), \\(P(A) \\ge 0\\)\nIf \\(A\\) and \\(B\\) are disjoint, \\(P(A \\cup B) = P(A) + P(B)\\)\n\nProperties\n\n\\(P(\\emptyset) = 0\\).\n\\(0 \\le P(A) \\le 1\\)\n\\(P(A^c) = 1 - P(A)\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) (Addition Rule)\nIf \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)"
  },
  {
    "objectID": "prob-rule.html#venn-diagram-illustration",
    "href": "prob-rule.html#venn-diagram-illustration",
    "title": "7  Probability Rules",
    "section": "7.4 Venn Diagram Illustration",
    "text": "7.4 Venn Diagram Illustration\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\n\n\n\n\n\n\n\n\n\nDisjoint case: \\(P(A \\cup B) = P(A) + P(B)\\) because \\(P(A \\cap B) = 0\\)!"
  },
  {
    "objectID": "prob-rule.html#example-mm-colors",
    "href": "prob-rule.html#example-mm-colors",
    "title": "7  Probability Rules",
    "section": "7.5 Example: M&M Colors",
    "text": "7.5 Example: M&M Colors\nThe makers of the candy M&Ms report that their plain M&Ms are composed of\n\n15% Yellow; 10% Red; 20% Orange; 25% Blue; 15% Green; 15% Brown\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\n\n\n\nIt is brown.\n\n\nIt is red or green.\n\n\nIt is not blue.\n\n\nIt is red and brown.\n\n\n\n\n\n\nSolution:\n\n\\(P(\\mathrm{Brown}) = 0.15\\)\n\\(\\small \\begin{align} P(\\mathrm{Red} \\cup \\mathrm{Green}) &= P(\\mathrm{Red}) + P(\\mathrm{Green}) - P(\\mathrm{Red} \\cap \\mathrm{Green}) \\\\ &= 0.10 + 0.15 - 0 = 0.25 \\end{align}\\)\n\\(P(\\text{Not Blue}) = 1 - P(\\text{Blue}) = 1 - 0.25 = 0.75\\)\n\\(P(\\text{Red and Brown}) = P(\\emptyset) = 0\\)\n\n\n\n\n\n\n\nBy the way, which interpretation of probability is used in this question?"
  },
  {
    "objectID": "prob-rule.html#conditional-probability",
    "href": "prob-rule.html#conditional-probability",
    "title": "7  Probability Rules",
    "section": "7.6 Conditional Probability",
    "text": "7.6 Conditional Probability\n\nThe conditional probability of \\(A\\) given \\(B\\) is \\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)} \\] if \\(P(B) > 0\\), and it is undefined if \\(P(B) = 0\\). \n“Given \\(B\\)” means that event \\(B\\) has already occurred.\n\n\n\n\n\n\n\n\n\n\n\n\n\nMultiplication Rule: \\(P(A \\cap B) = P(A \\mid B)P(B) = P(B \\mid A)P(A)\\)\n\\(P(A)\\) and \\(P(B)\\) are unconditional or marginal probabilities."
  },
  {
    "objectID": "prob-rule.html#difference-between-pa-and-pa-mid-b",
    "href": "prob-rule.html#difference-between-pa-and-pa-mid-b",
    "title": "7  Probability Rules",
    "section": "7.7 Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)",
    "text": "7.7 Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)"
  },
  {
    "objectID": "prob-rule.html#example-peanut-butter-and-jelly",
    "href": "prob-rule.html#example-peanut-butter-and-jelly",
    "title": "7  Probability Rules",
    "section": "7.8 Example: Peanut Butter and Jelly",
    "text": "7.8 Example: Peanut Butter and Jelly\n\nSuppose 80% of people like peanut butter, 89% like jelly, and 78% like both.\nGiven that a randomly sampled person likes peanut butter, what is the probability that she also likes jelly?\n\n\n\n\n\n\n\n\n\n\nWe want \\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)}\\).\nFrom the problem we have \\(P(PB) = 0.8\\), \\(P(J) = 0.89\\), \\(P(PB \\cap J) = 0.78\\)\n\\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)} = \\frac{0.78}{0.8} = 0.975\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf we don’t know if the person loves peanut butter, the probability that she loves jelly is 89%.\nIf we do know she loves peanut butter, the probability that she loves jelly is going up to 97.5%."
  },
  {
    "objectID": "prob-rule.html#independence",
    "href": "prob-rule.html#independence",
    "title": "7  Probability Rules",
    "section": "7.9 Independence",
    "text": "7.9 Independence\n\n\\(A\\) and \\(B\\) are independent if \\(\\begin{align} P(A \\mid B) &= P(A) \\text{ or }\\\\ P(B \\mid A) &= P(B) \\text{ or } \\\\P(A\\cap B) &= P(A)P(B)\\end{align}\\) \\(\\text{ if } P(A) > 0 \\text{ and } P(B) > 0\\)\nIntuition: Knowing \\(B\\) occurs does not change the probability that \\(A\\) occurs, and vice versa.\n\n\n\n\n\n\n\nCan we compute \\(P(A \\cap B)\\) if we only know \\(P(A)\\) and \\(P(B)\\)?\n\n\n\n\n\n\n\nNo, we cannot compute \\(P(A \\cap B)\\) since we do not know if \\(A\\) and \\(B\\) are independent.\nWe only could if \\(A\\) and \\(B\\) were independent.\nIn general, we need the multiplication rule \\(P(A \\cap B) = P(A \\mid B)P(B)\\)."
  },
  {
    "objectID": "prob-rule.html#venn-diagram-explanation-of-independence",
    "href": "prob-rule.html#venn-diagram-explanation-of-independence",
    "title": "7  Probability Rules",
    "section": "7.10 Venn Diagram Explanation of Independence",
    "text": "7.10 Venn Diagram Explanation of Independence"
  },
  {
    "objectID": "prob-rule.html#independence-example",
    "href": "prob-rule.html#independence-example",
    "title": "7  Probability Rules",
    "section": "7.11 Independence Example",
    "text": "7.11 Independence Example\n\n\n\n\n\n\nAssuming that events \\(A\\) and \\(B\\) are independent. \\(P(A) = 0.3\\) and \\(P(B) = 0.7\\).\n\n\n\n\n\\(P(A \\cap B)\\)?\n\\(P(A \\cup B)\\)?\n\\(P(A \\mid B)\\)?\n\n\n\n\n\n\n\\(P(A \\cap B) = P(A)P(B)=0.21\\)\n\\(P(A \\cup B) = P(A)+P(B)-P(A\\cap B) = 0.3+0.7-0.21=0.79\\)\n\\(P(A \\mid B) = P(A) = 0.3\\)"
  },
  {
    "objectID": "prob-rule.html#why-bayes-formula",
    "href": "prob-rule.html#why-bayes-formula",
    "title": "7  Probability Rules",
    "section": "7.12 Why Bayes’ Formula?",
    "text": "7.12 Why Bayes’ Formula?\n\nOften, we know \\(P(B \\mid A)\\) but are much more interested in \\(P(A \\mid B)\\).\nExample: diagnostic tests provide \\(P(\\text{positive test result} \\mid \\text{COVID})\\), but we are interested in \\(P(\\text{COVID} \\mid \\text{positive test result})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayes’ formula provides a way for finding \\(P(A \\mid B)\\) from \\(P(B \\mid A)\\)"
  },
  {
    "objectID": "prob-rule.html#bayes-formula",
    "href": "prob-rule.html#bayes-formula",
    "title": "7  Probability Rules",
    "section": "7.13 Bayes’ Formula",
    "text": "7.13 Bayes’ Formula\n\nIf \\(A\\) and \\(B\\) are any events whose probabilities are not 0 or 1, then \\[\\begin{align*} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\quad ( \\text{def. of cond. prob.}) \\\\ &= \\frac{P(A \\cap B)}{P((B \\cap A) \\cup (B \\cap A^c))} \\quad ( \\text{partition } B) \\\\ &= \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}  \\quad ( \\text{multiplication rule}) \\end{align*}\\]"
  },
  {
    "objectID": "prob-rule.html#example-passing-rate",
    "href": "prob-rule.html#example-passing-rate",
    "title": "7  Probability Rules",
    "section": "7.14 Example: Passing Rate",
    "text": "7.14 Example: Passing Rate\n\n\n\n\nAfter taking MATH 4720, \\(80\\%\\) of students understand the Bayes’ formula.\n\nOf those who understand the Bayes’ formula,\n\n\\(95\\%\\) passed\n\nOf those who do not understand the Bayes’ formula,\n\n\\(60\\%\\) passed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the probability that a student understand the Bayes’ formula given the fact that she passed."
  },
  {
    "objectID": "prob-rule.html#bayes-formula-step-by-step",
    "href": "prob-rule.html#bayes-formula-step-by-step",
    "title": "7  Probability Rules",
    "section": "7.15 Bayes Formula: Step-by-Step",
    "text": "7.15 Bayes Formula: Step-by-Step\n\n Step 1: Formulate what we would like to compute \n\n\\(P(\\text{understand} \\mid \\text{passed})\\)\n\n Step 2: Define relevant events in the formula: \\(A\\), \\(A^c\\) and \\(B\\) \n\nLet \\(A =\\) understand. \\(B =\\) passed. Then \\(A^c =\\) don’t understand and \\(P(\\text{understand} \\mid \\text{passed}) = P(A \\mid B)\\).\n\n Step 3: Find probabilities in the Bayes’ formula using provided information. \n\n\\(P(B \\mid A) = P(\\text{passed} \\mid \\text{understand}) = 0.95\\), \\(P(B \\mid A^c) = P(\\text{passed} \\mid \\text{don't understand}) = 0.6\\)\n\\(P(A) = P(\\text{understand}) = 0.8\\), \\(P(A^c) = 1 - P(A) = 0.2\\).\n\n Step 4: Apply Bayes’ formula. \n\n\\(\\small P(\\text{understand} \\mid \\text{passed}) = P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)} = \\frac{(0.95)(0.8)}{(0.95)(0.8) + (0.6)(0.2)} = 0.86\\)"
  },
  {
    "objectID": "prob-rule.html#bayes-formula-tree-diagram-illustration",
    "href": "prob-rule.html#bayes-formula-tree-diagram-illustration",
    "title": "7  Probability Rules",
    "section": "7.16 Bayes Formula: Tree Diagram Illustration",
    "text": "7.16 Bayes Formula: Tree Diagram Illustration\n\n\\(80\\%\\) of students understand the Bayes’ formula.\nOf those who understand the Bayes’ formula, \\(95\\%\\) passed ( \\(5\\%\\) failed).\nOf those who do not understand the formula, \\(60\\%\\) passed ( \\(40\\%\\) failed).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{P(\\text{pass | yes})P(\\text{yes})}{P(\\text{pass | yes})P(\\text{yes}) + P(\\text{pass | no})P(\\text{no})} \\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]"
  },
  {
    "objectID": "prob-rv.html#random-variables",
    "href": "prob-rv.html#random-variables",
    "title": "8  Random Variables",
    "section": "8.1 Random Variables",
    "text": "8.1 Random Variables\n\nRecap: A variable in a data set is a characteristic that varies from one to another.\n\nA variable can be either categorical or numerical.\nNumerical variables can be either discrete or continuous.\n\nA random variable, usually written as \\(X\\) 1, is a variable whose possible values are numerical outcomes determined by chance or randomness of a procedure or experiment.\n\n Toss a coin 2 times. \\(X\\) = # of heads. \n \\(X\\) = # of accidents in W. Wisconsin Ave. per day.\n\n\nA random variable has a probability distribution associated with it, accounting for its randomness.\n\n[1] Usually in statistics, a capital \\(X\\) represents a random variable and a small \\(x\\) represents a realized value of \\(X\\)."
  },
  {
    "objectID": "prob-rv.html#discrete-and-continuous-random-variables",
    "href": "prob-rv.html#discrete-and-continuous-random-variables",
    "title": "8  Random Variables",
    "section": "8.2 Discrete and Continuous Random Variables",
    "text": "8.2 Discrete and Continuous Random Variables\n\nA discrete random variable takes on a finite or countable number of values.\nA continuous random variable has infinitely many values, and the collection of values is uncountable.\n The number of relationships you’ve ever had is discrete variable because we can count the number and it is finite.\n\nIf we can further determine the probability that the number is 0, 1, 2, or any possible number, it is a discrete random variable.\n\n Height is continuous because it can be any number within a range. \n\nIf we have a way to quantify the probability that the height is from any value \\(a\\) to any value \\(b\\), it is a continuous random variable."
  },
  {
    "objectID": "prob-disc.html#a-statistician-should-know",
    "href": "prob-disc.html#a-statistician-should-know",
    "title": "9  Discrete Probability Distributions",
    "section": "\n9.1 A Statistician Should Know",
    "text": "9.1 A Statistician Should Know\n\n\n\n\nhttps://github.com/rasmusab/distribution_diagrams"
  },
  {
    "objectID": "prob-disc.html#discrete-probability-distribution",
    "href": "prob-disc.html#discrete-probability-distribution",
    "title": "9  Discrete Probability Distributions",
    "section": "9.1 Discrete Probability Distribution",
    "text": "9.1 Discrete Probability Distribution\n\nThe probability (mass) function of a discrete random variable (rv) \\(X\\) is a function \\(P(X = x)\\) (or \\(p(x)\\)) that assigns a probability for every possible number \\(x\\).\nThe probability distribution for a discrete r.v. \\(X\\) displays its probability function.\nThe display can be a table, graph, or mathematical formula of \\(P(X = x)\\).\n\n Example:🪙🪙 Toss a fair coin twice independently and \\(X\\) is the number of heads. \n\nThe probability distribution of \\(X\\) as a table is\n\n\n\n\n\n\n  \n    x \n    0 \n    1 \n    2 \n  \n  \n    P(X = x) \n    0.25 \n    0.5 \n    0.25 \n  \n\n\n\n\n\n\n\n\n\n\n\n👉 \\(\\{X = x\\}\\) is an event corresponding to an event of some experiment.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the event that \\(\\{X = 0\\}\\) corresponds to?\n\n\n\n\n\n\n\n\n\n\n\n\nHow do we get \\(P(X = 0)\\), \\(P(X=1)\\) and \\(P(X=2)\\) ?"
  },
  {
    "objectID": "prob-disc.html#discrete-probability-distribution-as-a-graph",
    "href": "prob-disc.html#discrete-probability-distribution-as-a-graph",
    "title": "9  Discrete Probability Distributions",
    "section": "9.2 Discrete Probability Distribution as a Graph",
    "text": "9.2 Discrete Probability Distribution as a Graph\n\n\n\n\n\n\n\n\n\n\n\n\\(0 \\le P(X = x) \\le 1\\) for every value \\(x\\) of \\(X\\).\n\n \\(x = 0, 1, 2\\) \n\n\\(\\sum_{x}P(X=x) = 1\\), where \\(x\\) assumes all possible values.\n\n \\(P(X=0) + P(X = 1) + P(X = 2) = 1\\) \n\nThe probabilities for a discrete r.v. are additive because \\(\\{X = a\\}\\) and \\(\\{X = b\\}\\) are disjoint for any possible values \\(a \\ne b\\).\n\n \\(P(X = 1 \\text{ or } 2) = P(\\{X = 1\\} \\cup \\{X = 2\\}) = P(X = 1) + P(X = 2)\\)."
  },
  {
    "objectID": "prob-disc.html#mean-of-a-discrete-random-variable",
    "href": "prob-disc.html#mean-of-a-discrete-random-variable",
    "title": "9  Discrete Probability Distributions",
    "section": "9.3 Mean of a Discrete Random Variable",
    "text": "9.3 Mean of a Discrete Random Variable\n\nSuppose \\(X\\) takes values \\(x_1, \\dots, x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\).\nThe mean or expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[E(X) := x_1 \\times P(X = x_1) + \\dots + x_k \\times P(X = x_k) = \\sum_{i=1}^kx_iP(X=x_i)\\]\nThe Greek letter \\(\\mu\\) may be used in place of the notation \\(E(X)\\).\n\n\n\n\n\n\n\n👉 The mean of a discrete random variable \\(X\\) is the weighted average of possible values \\(x\\) weighted by their corresponding probability.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the mean of \\(X\\) (the number of heads) in the previous example?"
  },
  {
    "objectID": "prob-disc.html#variance-of-a-discrete-random-variable",
    "href": "prob-disc.html#variance-of-a-discrete-random-variable",
    "title": "9  Discrete Probability Distributions",
    "section": "9.4 Variance of a Discrete Random Variable",
    "text": "9.4 Variance of a Discrete Random Variable\n\nSuppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\).\nThe variance of \\(X\\), denoted by \\(Var(X)\\) or \\(\\sigma^2\\), is \\[\\small Var(X) := (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) = \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i)\\]\nThe standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance.\n\n\n\n\n\n\n\n👉 The variance of a discrete random variable \\(X\\) is the weighted sum of squared deviation from the mean weighted by probability values.\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the variance of \\(X\\) (the number of heads) in the previous example?"
  },
  {
    "objectID": "prob-disc.html#binomial-experiment-and-random-variable",
    "href": "prob-disc.html#binomial-experiment-and-random-variable",
    "title": "9  Discrete Probability Distributions",
    "section": "9.5 Binomial Experiment and Random Variable",
    "text": "9.5 Binomial Experiment and Random Variable\n\nA binomial experiment is the one having the following properties:\n\n👉 The experiment consists of a fixed number of identical trials \\(n\\).\n👉 Each trial results in one of exactly two outcomes (success (S) and failure (F)).\n👉 Trials are independent, meaning that the outcome of any trial does not affect the outcome of any other trial.\n👉 The probability of success is constant for all trials.\n\nIf \\(X\\) is defined as  the number of successes observed in \\(n\\) trials , \\(X\\) is a binomial random variable.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe word success just means one of the two outcomes, and does not necessarily mean something good. \n😲 We can define Drug abuse as success and No drug abuse as failure."
  },
  {
    "objectID": "prob-disc.html#binomial-distribution",
    "href": "prob-disc.html#binomial-distribution",
    "title": "9  Discrete Probability Distributions",
    "section": "9.6 Binomial Distribution",
    "text": "9.6 Binomial Distribution\n\nThe probability function \\(P(X = x)\\) of a binomial r.v. \\(X\\) can be fully determined by\n\nthe number of trials \\(n\\)\nprobability of success \\(\\pi\\)\n\nDifferent \\((n, \\pi)\\) pairs generate different binomial probability distributions.\n\\(X\\) is said to follow a binomial distribution with parameters \\(n\\) and \\(\\pi\\), written as \\(\\color{blue}{X \\sim binomial(n, \\pi)}\\).\nThe binomial probability function is \\[ \\color{blue}{P(X = x \\mid n, \\pi) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x}, \\quad x = 0, 1, 2, \\dots, n}\\] with mean \\(\\mu = E(X) = n\\pi\\) and variance \\(\\sigma^2 = Var(X) = n\\pi(1-\\pi)\\).\n\n\n\n\n\n\n\nIf we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial r.v.?"
  },
  {
    "objectID": "prob-disc.html#binomial-distribution-example",
    "href": "prob-disc.html#binomial-distribution-example",
    "title": "9  Discrete Probability Distributions",
    "section": "9.7 Binomial Distribution Example",
    "text": "9.7 Binomial Distribution Example\n\n\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose it is a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit.\n\\(X \\sim binomial(15, 0.2)\\).\n\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, n}\\]\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\n\n\n\n\n\nTip\n\n\n\nNever do this by hand. We can compute them using R!"
  },
  {
    "objectID": "prob-disc.html#binomial-distribution-example-x-sim-binomial15-0.2",
    "href": "prob-disc.html#binomial-distribution-example-x-sim-binomial15-0.2",
    "title": "9  Discrete Probability Distributions",
    "section": "9.8 Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)",
    "text": "9.8 Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)"
  },
  {
    "objectID": "prob-disc.html#binomial-distribution-example-1",
    "href": "prob-disc.html#binomial-distribution-example-1",
    "title": "9  Discrete Probability Distributions",
    "section": "9.9 Binomial Distribution Example",
    "text": "9.9 Binomial Distribution Example\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\n\n\n\n\n\nTip\n\n\n\nNever do this by hand. We compute them using R!"
  },
  {
    "objectID": "prob-disc.html#binomial-example-computation-in-r",
    "href": "prob-disc.html#binomial-example-computation-in-r",
    "title": "9  Discrete Probability Distributions",
    "section": "9.9 Binomial Example Computation in R",
    "text": "9.9 Binomial Example Computation in R\n\n\nWith size the number of trials and prob the probability of success,\n\ndbinom(x, size, prob) to compute \\(P(X = x)\\)\npbinom(q, size, prob) to compute \\(P(X \\le q)\\)\npbinom(q, size, prob, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n## 1. P(X = 6)\ndbinom(x = 6, size = 15, prob = 0.2) \n\n[1] 0.04299262\n\n## 2. P(X >= 6) = 1 - P(X <= 5)\n1 - pbinom(q = 5, size = 15, prob = 0.2) \n\n[1] 0.06105143\n\n\n\n## 2. P(X >= 6) = P(X > 5)\npbinom(q = 5, size = 15, prob = 0.2, \n       lower.tail = FALSE)  \n\n[1] 0.06105143"
  },
  {
    "objectID": "prob-disc.html#binomial15-0.2",
    "href": "prob-disc.html#binomial15-0.2",
    "title": "9  Discrete Probability Distributions",
    "section": "9.10 Binomial(15, 0.2)",
    "text": "9.10 Binomial(15, 0.2)\n\nplot(x = 0:15, y = dbinom(0:15, size = 15, prob = 0.2), \n     type = 'h', xlab = \"x\", ylab = \"P(X = x)\", \n     lwd = 5, main = \"Binomial(15, 0.2)\")"
  },
  {
    "objectID": "prob-disc.html#poisson-random-variables",
    "href": "prob-disc.html#poisson-random-variables",
    "title": "9  Discrete Probability Distributions",
    "section": "9.11 Poisson Random Variables",
    "text": "9.11 Poisson Random Variables\n\nIf we like to count the number of occurrences of some event over a unit of time or space (region) and its associated probability, we could consider the Poisson distribution.\n\nNumber of COVID patients arriving at ICU in one hour\nNumber of Marquette students logging onto D2L in one day\nNumber of dandelions per square meter in Marquette campus\n\nLet \\(X\\) be a Poisson r.v. Then \\(\\color{blue}{X \\sim Poisson(\\lambda)}\\), where \\(\\lambda\\) is the parameter representing the mean number of occurrences of the event in the interval. \\[\\color{blue}{P(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0, 1, 2, \\dots}\\] with both mean and variance being equal to \\(\\lambda\\)."
  },
  {
    "objectID": "prob-disc.html#assumptions-and-properties-of-poisson-variables",
    "href": "prob-disc.html#assumptions-and-properties-of-poisson-variables",
    "title": "9  Discrete Probability Distributions",
    "section": "9.12 Assumptions and Properties of Poisson Variables",
    "text": "9.12 Assumptions and Properties of Poisson Variables\n\n👉 Events occur one at a time; two or more events do not occur at the same time or in the same space or spot.\n👉 The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a nonoverlapping time period or region of space.\n👉 \\(\\lambda\\) is constant of any period or region.\n\n\n\n\n\n\n\nCan you find the difference between Binomial and Poisson distributions?\n\n\n\n\n\n\n\nThe Poisson distribution\n\nis determined by one single parameter \\(\\lambda\\)\nhas possible values \\(x = 0, 1, 2, \\dots\\) with no upper limit (countable), while a binomial variable has possible values \\(0, 1, 2, \\dots, n\\) (finite)"
  },
  {
    "objectID": "prob-disc.html#poisson-distribution-example",
    "href": "prob-disc.html#poisson-distribution-example",
    "title": "9  Discrete Probability Distributions",
    "section": "9.13 Poisson Distribution Example",
    "text": "9.13 Poisson Distribution Example\n\n\n\n\nLast year there were 4200 births at the University of Wisconsin Hospital. Assume \\(X\\) be the number of births in a given day at the center, and \\(X \\sim Poisson(\\lambda)\\). Find\n\n\\(\\lambda\\), the mean number of births per day.\nthe probability that on a randomly selected day, there are exactly 10 births.\n\\(P(X > 10)\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\small \\lambda = \\frac{\\text{Number of birth in a year}}{\\text{Number of days}} = \\frac{4200}{365} = 11.5\\)\n\\(\\small P(X = 10 \\mid \\lambda = 11.5) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = \\frac{11.5^{10} e^{-11.5}}{10!} = 0.113\\)\n\\(\\small P(X > 10) = p(11) + p(12) + \\dots + p(20) + \\dots\\) (No end!) \\(\\small P(X > 10) = 1 - P(X \\le 10) = 1 - (p(1) + p(2) + \\dots + p(10))\\)."
  },
  {
    "objectID": "prob-disc.html#poisson-example-compuatation-in-r",
    "href": "prob-disc.html#poisson-example-compuatation-in-r",
    "title": "9  Discrete Probability Distributions",
    "section": "9.14 Poisson Example Compuatation in R",
    "text": "9.14 Poisson Example Compuatation in R\n\n\n\nWith lambda the mean of Poisson distribution,\n\ndpois(x, lambda) to compute \\(P(X = x)\\)\nppois(q, lambda) to compute \\(P(X \\le q)\\)\nppois(q, lambda, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n(lam <- 4200 / 365)\n\n[1] 11.50685\n\n## P(X = 10)\ndpois(x = 10, lambda = lam)  \n\n[1] 0.112834\n\n\n\n## P(X > 10) = 1 - P(X <= 10)\n1 - ppois(q = 10, lambda = lam)  \n\n[1] 0.5990436\n\n## P(X > 10)\nppois(q = 10, lambda = lam, \n      lower.tail = FALSE) \n\n[1] 0.5990436"
  },
  {
    "objectID": "prob-disc.html#poisson11.5",
    "href": "prob-disc.html#poisson11.5",
    "title": "9  Discrete Probability Distributions",
    "section": "9.15 Poisson(11.5)",
    "text": "9.15 Poisson(11.5)\n\n\\(X\\) has no upper limit. The graph is truncated at \\(x = 24\\).\n\n\nplot(0:24, dpois(0:24, lambda = lam), type = 'h', lwd = 5, \n     ylab = \"P(X = x)\", xlab = \"x\", main = \"Poisson(11.5)\")"
  },
  {
    "objectID": "prob-cont.html#continuous-probability-distributions",
    "href": "prob-cont.html#continuous-probability-distributions",
    "title": "10  Continuous Probability Distributions",
    "section": "10.1 Continuous Probability Distributions",
    "text": "10.1 Continuous Probability Distributions\n\nA continuous r.v. can take on any values from an interval of the real line.\nInstead of probability functions, a continuous r.v. \\(X\\) has the probability density function (pdf) \\(f(x)\\) such that for any real value \\(a < b\\), \\[P(a < X < b) = \\int_{a}^b f(x) dx\\]\nThe cumulative distribution function (cdf) of \\(X\\) is defined as \\[F(x) := P(X \\le x) = \\int_{-\\infty}^x f(t)dt\\]\nEvery pdf must satisfy  (1) \\(f(x) \\ge 0\\) for all \\(x\\); (2) \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\) \n\n😎 Luckily we don’t deal with integrals in this course."
  },
  {
    "objectID": "prob-cont.html#density-curve",
    "href": "prob-cont.html#density-curve",
    "title": "10  Continuous Probability Distributions",
    "section": "10.2 Density Curve",
    "text": "10.2 Density Curve\n\nA pdf generates a graph called the density curve that shows the likelihood of a random variable at all possible values.\n\\(P(a < X < b) = \\int_{a}^b f(x) dx\\): The area under the density curve between \\(a\\) and \\(b\\).\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\): The total area under any density curve is equal to 1."
  },
  {
    "objectID": "prob-cont.html#commonly-used-continuous-distributions",
    "href": "prob-cont.html#commonly-used-continuous-distributions",
    "title": "10  Continuous Probability Distributions",
    "section": "10.3 Commonly Used Continuous Distributions",
    "text": "10.3 Commonly Used Continuous Distributions\n\nR Shiny app is at Continuous Distribution\nIn this course, we will touch normal (Gaussian), student’s t, chi-square, F\nSome other common distributions include uniform, exponential, gamma, beta, inverse gamma, Cauchy, etc. (MATH 4700)"
  },
  {
    "objectID": "prob-cont.html#normal-gaussian-distribution",
    "href": "prob-cont.html#normal-gaussian-distribution",
    "title": "10  Continuous Probability Distributions",
    "section": "10.4 Normal (Gaussian) Distribution",
    "text": "10.4 Normal (Gaussian) Distribution\n\nThe normal distribution, \\(N(\\mu, \\sigma^2\\)), has the pdf given by \\[\\small f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty < x < \\infty\\]\n\nTwo parameters mean \\(\\mu\\) and variance \\(\\sigma^2\\) (standard deviation \\(\\sigma\\))\nAlways bell shaped, and symmetric about the mean \\(\\mu\\)\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), \\(N(0, 1)\\) is called standard normal."
  },
  {
    "objectID": "prob-cont.html#normal-density-curves",
    "href": "prob-cont.html#normal-density-curves",
    "title": "10  Continuous Probability Distributions",
    "section": "10.5 Normal Density Curves",
    "text": "10.5 Normal Density Curves"
  },
  {
    "objectID": "prob-cont.html#standardization-and-z-scores",
    "href": "prob-cont.html#standardization-and-z-scores",
    "title": "10  Continuous Probability Distributions",
    "section": "10.6 Standardization and Z-Scores",
    "text": "10.6 Standardization and Z-Scores\n\nStandardization: Convert \\(N(\\mu, \\sigma^2)\\) to \\(N(0, 1)\\).\nWhy standardization: Put data onto a standardized scale, making comparisons easier!\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nMeasure\nSAT\nACT\n\n\n\n\nMean\n1100\n21\n\n\nSD\n200\n6\n\n\n\n\nThe distribution of SAT and ACT scores are both nearly normal.\nSuppose Anna scored 1300 on her SAT and Tommy scored 24 on his ACT. Who performed better?\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf \\(x\\) is an observation from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the standardized value of \\(x\\) is so-called \\(z\\)-score: \\[z = \\frac{x - \\mu}{\\sigma}\\]\nA \\(z\\)-score tells us how many standard deviations \\(x\\) falls away from the mean, and in which direction.\n\nObservations larger (smaller) than the mean have positive (negative) \\(z\\)-scores.\nA \\(z\\)-score -1.2 means that \\(x\\) is 1.2 standard deviations to the left of (below) the mean.\nA \\(z\\)-score 1.8 means that \\(x\\) is 1.8 standard deviations to the right of (above) the mean.\n\n If \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma}\\) follows the standard normal distribution, i.e., \\(Z \\sim N(0, 1)\\)."
  },
  {
    "objectID": "prob-cont.html#standardization-illustration",
    "href": "prob-cont.html#standardization-illustration",
    "title": "10  Continuous Probability Distributions",
    "section": "10.7 Standardization Illustration",
    "text": "10.7 Standardization Illustration\n\n\\(X - \\mu\\) shifts the mean from \\(\\mu\\) to 0\n\n\n\n\n\n\n\n\n\n\n\n\\(\\frac{X - \\mu}{\\sigma}\\) scales the variation from 4 to 1\n\n\n\n\n\n\n\n\n\n\n\nA value of \\(x\\) that is 2 standard deviation below \\(\\mu\\) corresponds to \\(z = -2\\).\n\\(z = \\frac{x -\\mu}{\\sigma} \\iff x = \\mu + z\\sigma\\). If \\(z = -2\\), \\(x = \\mu - 2\\sigma\\)."
  },
  {
    "objectID": "prob-cont.html#sat-and-act-example",
    "href": "prob-cont.html#sat-and-act-example",
    "title": "10  Continuous Probability Distributions",
    "section": "10.8 SAT and ACT Example",
    "text": "10.8 SAT and ACT Example\n\n\\(z_{A} = \\frac{x_{A} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1300-1100}{200} = 1\\); \\(z_{T} = \\frac{x_{T} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24-21}{6} = 0.5\\)."
  },
  {
    "objectID": "prob-cont.html#finding-tail-areas-px-x",
    "href": "prob-cont.html#finding-tail-areas-px-x",
    "title": "10  Continuous Probability Distributions",
    "section": "10.9 Finding Tail Areas \\(P(X < x)\\)",
    "text": "10.9 Finding Tail Areas \\(P(X < x)\\)\n\n\n\n\n\n\nWhat fraction of students have an SAT score below Anna’s score of 1300?\n\n\n\n\n\n\n\nThis is the same as the percentile Anna is at, which is the percentage of cases that have lower scores than Anna.\nNeed \\(P(X < 1300 \\mid \\mu = 1100, \\sigma = 200)\\) or \\(P(Z < 1 \\mid \\mu = 0, \\sigma = 1)\\)."
  },
  {
    "objectID": "prob-cont.html#finding-tail-areas-px-x-in-r",
    "href": "prob-cont.html#finding-tail-areas-px-x-in-r",
    "title": "10  Continuous Probability Distributions",
    "section": "10.10 Finding Tail Areas \\(P(X < x)\\) in R",
    "text": "10.10 Finding Tail Areas \\(P(X < x)\\) in R\n\nWith mean and sd representing the mean and standard deviation of a normal distribution\n\npnorm(q, mean, sd) to compute \\(P(X \\le q)\\)\npnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n\n\n\n\n\npnorm(1, mean = 0, sd = 1)\n\n[1] 0.8413447\n\npnorm(1300, mean = 1100, sd = 200)\n\n[1] 0.8413447\n\n\n\n\n\nThe shaded area represents the proportion 84.1% of SAT test takers who had z-score below 1."
  },
  {
    "objectID": "prob-cont.html#sat-example-contd",
    "href": "prob-cont.html#sat-example-contd",
    "title": "10  Continuous Probability Distributions",
    "section": "10.11 SAT Example Cont’d",
    "text": "10.11 SAT Example Cont’d\n\nSAT score follows \\(N(1100, 200^2)\\). Shannon is a SAT taker, and nothing is known about Shannon’s SAT aptitude. What is the probability Shannon SAT scores at least 1190?\n Step 1: State the problem \n\n We like to compute \\(P(X \\ge 1190)\\). \n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Step 3: Find \\(z\\)-score :\n\n \\(z = \\frac{1190 - 1100}{200} = 0.45\\) and we like to compute \\(P(X > 1190) = P\\left( \\frac{X - \\mu}{\\sigma} > \\frac{1190 - 1000}{200} \\right) = P(Z > 0.45) = 1 - P(Z \\le 0.45)\\) \n\n Step 4: Find the area using pnorm() \n\n\n1 - pnorm(0.45)\n\n[1] 0.3263552"
  },
  {
    "objectID": "prob-cont.html#normal-percentiles-in-r",
    "href": "prob-cont.html#normal-percentiles-in-r",
    "title": "10  Continuous Probability Distributions",
    "section": "10.12 Normal Percentiles in R",
    "text": "10.12 Normal Percentiles in R\n\nTo get the \\(100p\\)-th percentile (or the \\(p\\) quantile \\(q\\) ), given probability \\(p\\), we use\n\nqnorm(p, mean, sd) to get a value of \\(X\\), \\(q\\), such that \\(P(X \\le q) = p\\)\nqnorm(p, mean, sd, lower.tail = FALSE) to get \\(q\\) such that \\(P(X \\ge q) = p\\)"
  },
  {
    "objectID": "prob-cont.html#sat-example",
    "href": "prob-cont.html#sat-example",
    "title": "10  Continuous Probability Distributions",
    "section": "10.13 SAT Example",
    "text": "10.13 SAT Example\n\nWhat is the 95th percentile for SAT scores?\n Step 1: State the problem \n\n We want to find \\(x\\) s.t \\(P(X < x) = 0.95\\). \n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFind an \\(x\\) value of the normal distribution, not an area (probability), which is 0.95."
  },
  {
    "objectID": "prob-cont.html#sat-example-contd-1",
    "href": "prob-cont.html#sat-example-contd-1",
    "title": "10  Continuous Probability Distributions",
    "section": "10.14 SAT Example Cont’d",
    "text": "10.14 SAT Example Cont’d\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z < z) = 0.95\\) using qnorm():\n\n\n(z_95 <- qnorm(0.95))\n\n[1] 1.644854\n\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma\\). \n\n\n\n(x_95 <- 1100 + z_95 * 200)\n\n[1] 1428.971\n\n\n\nThe 95th percentile for SAT scores is 1429.\n\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1428.971"
  },
  {
    "objectID": "prob-cont.html#finding-probabilties",
    "href": "prob-cont.html#finding-probabilties",
    "title": "10  Continuous Probability Distributions",
    "section": "10.15 Finding Probabilties",
    "text": "10.15 Finding Probabilties\n👉 ALWAYS draw and label the normal curve and shade the area of interest.\n\n👉 Less than\n\n\\(\\small P(X < x) = P(Z < z)\\)\npnorm(z, mean = 0, sd = 1)\npnorm(x, mean = mu, sd = sigma)\n\n👉 Greater than\n\n\\(\\small P(X > x) = P(Z > z) = 1 - P(Z \\le z)\\)\n1 - pnorm(z)\n\n\n\n\n\n\n\n\nNote\n\n\n\n👉 Standardization is not a must.\n👉 We have to specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma).\n\n\n\n👉 Between two numbers\n\n\\(\\small P(a < X < b) = P(z_a < Z < z_b) = P(Z < z_b) - P(Z < z_a)\\)\npnorm(z_b) - pnorm(z_a)\n\n👉 Outside of two numbers \\((a < b)\\) \\[\\small \\begin{align} P(X < a \\text{ or } X > b) &= P(Z < z_a \\text{ or } Z > z_b) \\\\ &= P(Z < z_a) + P(Z > z_b) \\\\ &= P(Z < z_a) + 1 - P(Z < z_b) \\end{align}\\]\n\npnorm(z_a) + 1 - pnorm(z_b)\npnorm(z_a) + pnorm(z_b, lower.tail = FALSE)\n\n\n\n\n\n\n\n\nNote\n\n\n\n👉 Any probability can be computed using the “less than” form (lower or left tail).\n👉 If the calculation involves the “greater than” form, add lower.tail = FALSE in pnorm()."
  },
  {
    "objectID": "prob-cont.html#checking-normality-normal-quantile-plot",
    "href": "prob-cont.html#checking-normality-normal-quantile-plot",
    "title": "10  Continuous Probability Distributions",
    "section": "10.16 Checking Normality: Normal Quantile Plot",
    "text": "10.16 Checking Normality: Normal Quantile Plot\n\nMany statistical methods assume variables are normally distributed.\nTesting the appropriateness of the normal assumption is a key step.\nA normal quantile plot (normal probability plot) or a Quantile-Quantile plot (QQ plot) helps us check normality assumption.\n\n\\(X\\)-axis: Quantiles of the ordered data if the data were normally distributed.\n\\(Y\\)-axis: Ordered data values\n\nIf the data are like normally distributed, the points on the QQ plot will lie close to a straight line."
  },
  {
    "objectID": "prob-cont.html#qq-plot-in-r",
    "href": "prob-cont.html#qq-plot-in-r",
    "title": "10  Continuous Probability Distributions",
    "section": "10.17 QQ plot in R",
    "text": "10.17 QQ plot in R\n\nqqnorm(normal_sample, main = \"Normal data\", col = 4)\nqqline(normal_sample)\nqqnorm(right_skewed_sample, main = \"Right-skewed data\", col = 6)\nqqline(right_skewed_sample)"
  },
  {
    "objectID": "prob-samdist.html#parameter",
    "href": "prob-samdist.html#parameter",
    "title": "11  Sampling Distribution",
    "section": "11.1 Parameter",
    "text": "11.1 Parameter\n\nA parameter is a number that describes a probability distribution.\n\n Binomial: two parameters \\(n\\) and \\(\\pi\\) \n Poisson: one parameter \\(\\mu\\) \n Normal: two parameters \\(\\mu\\) and \\(\\sigma\\) \n\nIn statistics, we usually assume our target population follows some distribution, but its parameters are unknown to us.\n\n\n\n\n Human weight follows \\(N(\\mu, \\sigma^2)\\) \n\n\n # of snowstorms in one year follows \\(Poisson(\\lambda)\\)"
  },
  {
    "objectID": "prob-samdist.html#treat-each-data-point-as-a-random-variable",
    "href": "prob-samdist.html#treat-each-data-point-as-a-random-variable",
    "title": "11  Sampling Distribution",
    "section": "11.2 Treat Each Data Point as a Random Variable",
    "text": "11.2 Treat Each Data Point as a Random Variable\n\n\\(n\\) random variables: \\(X_1, X_2, \\dots, X_n\\).\nAssume \\(X_1, X_2, \\dots, X_n\\) follow the same distribution.\n\n\n\n\n\n\n\nView \\(X_i\\) as a data point to be drawn from a population with some distribution, say \\(N(\\mu, \\sigma^2)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssume that \\(X_1, X_2, \\dots, X_n\\) are independent, i.e., the distribution/value of \\(X_i\\) is not affected by any other \\(X_j\\).\nWith the same distribution, \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed (i.i.d.), for example,  \\(X_1, X_2, \\dots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) \n\\((X_1, X_2, \\dots, X_n)\\) is a random sample of size \\(n\\) from the population.\n\n \\(X_1, X_2, \\dots, X_{50}\\) are randomly selected SAT scores from the SAT score population that follows \\(N(1100, 200^2)\\) \n\n\n\n\n\n\n\n\nNote\n\n\n\n\nBefore we actually collect the data, the data \\(X_1, X_2, \\dots, X_n\\) are random variables from the population distribution \\(N(\\mu, \\sigma^2)\\).\nOnce we collect the data, we know the realized value of these random variables: \\(x_1, x_2, \\dots, x_n\\)."
  },
  {
    "objectID": "prob-samdist.html#sampling-distribution",
    "href": "prob-samdist.html#sampling-distribution",
    "title": "11  Sampling Distribution",
    "section": "11.3 Sampling Distribution",
    "text": "11.3 Sampling Distribution\n\nAny value computed from a sample \\((X_1, X_2, \\dots, X_n)\\) is called a (sample) statistic.\n\n Sample mean \\(\\frac{1}{n}\\sum_{i=1}^n X_i\\) is a statistic. \n\n\n\n\n\n\n\n\nCan you provide another statistic?\n\n\n\n\n\n\n\n Sample variance \\(\\frac{\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2}{n-1}\\) is a statistic. \nSince \\(X_1, X_2, \\dots, X_n\\) are random variables, any transformation or function of \\((X_1, X_2, \\dots, X_n)\\), or statistic, is also a random variable.\nThe probability distribution of a statistic is called the sampling distribution of that statistic.\n\n\n\n\n\n\n\nDoes the sample mean \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) have a sampling distribution?\n\n\n\n\n\n\n\nIt is the probability distribution of that statistic if we were to repeatedly draw samples of the same size from the population.\n\n\n\n\n\n\nBiostatistics for the Biological and Health Sciences p.241\n\n\n\n\n\nSampling Distribution Applet\n\n\n\n\n\n\n\nWhat are the differences between the sampling distribution of \\(\\overline{X}\\) and the population distribution each individual r.v. \\(X_i\\) is drawn from?\n\n\n\n\n\n\n\nSample means \\((\\overline{X})\\) are  less variable  than individual observations \\(X_i\\).\nSample means \\((\\overline{X})\\) are  more normal  than individual observations \\(X_i\\)."
  },
  {
    "objectID": "prob-samdist.html#example-sampling-distribution-of-the-sample-mean",
    "href": "prob-samdist.html#example-sampling-distribution-of-the-sample-mean",
    "title": "11  Sampling Distribution",
    "section": "11.4 Example: Sampling Distribution of the Sample Mean",
    "text": "11.4 Example: Sampling Distribution of the Sample Mean\n\nRoll a fair die 3 times 🎲🎲 🎲 independently to obtain 3 values from the population \\(\\{1, 2, 3, 4, 5, 6\\}\\).\nRepeat the process 10,000 times and plot the histogram of the sampling mean."
  },
  {
    "objectID": "prob-samdist.html#sampling-distribution-of-sample-mean",
    "href": "prob-samdist.html#sampling-distribution-of-sample-mean",
    "title": "11  Sampling Distribution",
    "section": "11.5 Sampling Distribution of Sample Mean",
    "text": "11.5 Sampling Distribution of Sample Mean\n\n\nSuppose \\((X_1, \\dots, X_n)\\) is the random sample from a population distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\nThe mean of the sampling distribution of the sample mean, \\(\\overline{X} = \\frac{\\sum_{i=1}^nX_i}{n}\\), is  \\(\\mu_{\\overline{X}} = \\mu\\) .\nThe standard deviation of the sampling distribution of the sample mean \\(\\overline{X}\\) is  \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) . \nIf the population distribution is  \\(N(\\mu, \\sigma^2)\\) , the sampling distribution of \\(\\overline{X}\\) is exactly  \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\) ."
  },
  {
    "objectID": "prob-samdist.html#standardization-of-sample-mean",
    "href": "prob-samdist.html#standardization-of-sample-mean",
    "title": "11  Sampling Distribution",
    "section": "11.6 Standardization of Sample Mean",
    "text": "11.6 Standardization of Sample Mean\n\nFor a single random variable \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\).\nFor the sample mean of \\(n\\) variables, \\(\\overline{X} \\sim N(\\mu_{\\overline{X}}, \\sigma^2_{\\overline{X}}) = N(\\mu, \\frac{\\sigma^2}{n})\\), and hence  \\[Z = \\frac{\\overline{X} - \\mu_{\\overline{X}}}{\\sigma_{\\overline{X}}} = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "prob-samdist.html#example---psychomotor-retardation",
    "href": "prob-samdist.html#example---psychomotor-retardation",
    "title": "11  Sampling Distribution",
    "section": "11.7 Example - Psychomotor retardation",
    "text": "11.7 Example - Psychomotor retardation\n\n\n\n\nPsychomotor retardation scores for a group of patients have a normal distribution with a mean of 930 and a standard deviation of 130.\nWhat is the probability that the mean retardation score of a random sample of 20 patients was between 900 and 960?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(X_1, \\dots, X_{20} \\stackrel{iid}{\\sim} N(930, 130^2)\\), then \\(\\overline{X} = \\frac{\\sum_{i=1}^{20}X_i}{20} \\sim N\\left(930, \\frac{130^2}{20} \\right)\\).\n\n\\[\\small \\begin{align}\nP(900 < \\overline{X} < 960) &= P\\left( \\frac{900-930}{130/\\sqrt{20}} < \\frac{\\overline{X}-930}{130/\\sqrt{20}} < \\frac{960-930}{130/\\sqrt{20}}\\right)=P(-1.03 < Z < 1.03)\\\\\n&=P(Z < 1.03) - P(Z < -1.03)\n  \\end{align}\\]\n\npnorm(1.03) - pnorm(-1.03)\n\n[1] 0.69699\n\n\n\npnorm(960, mean = 930, sd = 130/sqrt(20)) - pnorm(900, mean = 930, sd = 130/sqrt(20))\n\n[1] 0.6979426"
  },
  {
    "objectID": "prob-llnclt.html#why-use-normal-central-limit-theorem",
    "href": "prob-llnclt.html#why-use-normal-central-limit-theorem",
    "title": "12  Law of Large Numbers and Central Limit Theorem",
    "section": "12.2 Why Use Normal? Central Limit Theorem",
    "text": "12.2 Why Use Normal? Central Limit Theorem\n\nCentral Limit Theorem (CLT): Suppose \\(\\overline{X}\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma < \\infty\\). As \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\), regardless of the distribution from which we are sampling!\n\n\n\n\n\n\nhttps://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png"
  },
  {
    "objectID": "prob-llnclt.html#clt-illustration-a-right-skewed-distribution",
    "href": "prob-llnclt.html#clt-illustration-a-right-skewed-distribution",
    "title": "12  Law of Large Numbers and Central Limit Theorem",
    "section": "12.3 CLT Illustration: A Right-Skewed Distribution",
    "text": "12.3 CLT Illustration: A Right-Skewed Distribution"
  },
  {
    "objectID": "prob-llnclt.html#clt-illustration-a-u-shaped-distribution",
    "href": "prob-llnclt.html#clt-illustration-a-u-shaped-distribution",
    "title": "12  Law of Large Numbers and Central Limit Theorem",
    "section": "12.4 CLT Illustration: A U-shaped Distribution",
    "text": "12.4 CLT Illustration: A U-shaped Distribution"
  },
  {
    "objectID": "prob-llnclt.html#why-clt-is-important",
    "href": "prob-llnclt.html#why-clt-is-important",
    "title": "12  Law of Large Numbers and Central Limit Theorem",
    "section": "12.5 Why CLT is Important?",
    "text": "12.5 Why CLT is Important?\n\n\nMany well-developed statistical methods are based on normal distribution assumption.\nWith CLT, we can use those methods even if we are sampling from a non-normal distribution, or we have no idea of the population distribution, provided that the sample size is large."
  },
  {
    "objectID": "prob-llnclt.html#clt-example",
    "href": "prob-llnclt.html#clt-example",
    "title": "12  Law of Large Numbers and Central Limit Theorem",
    "section": "12.6 CLT Example",
    "text": "12.6 CLT Example\n\n\n\n\nSuppose that selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000.\nIn 100 randomly selected sales, what is the probability the average selling price is more than $400,000?\nSince the sample size is fairly large \\((n = 100)\\), by CLT, the sampling distribution of the average selling price is approximately normal with mean 382,000 and SD \\(150,000 / \\sqrt{100}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(\\overline{X} > 400000) = P\\left(\\frac{\\overline{X} - 382000}{150000/\\sqrt{100}} > \\frac{400000 - 382000}{150000/\\sqrt{100}}\\right) \\approx P(Z > 1.2)\\) where \\(Z \\sim N(0, 1)\\).\n\n\n\npnorm(1.2, lower.tail = FALSE)\n\n[1] 0.1150697\n\npnorm(400000, mean = 382000, sd = 150000/sqrt(100), lower.tail = FALSE)\n\n[1] 0.1150697"
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Introduction to Statistics",
    "section": "Welcome",
    "text": "Welcome\nThis is the website for my introductory statistics book. This book serves as a main reference book for my MATH 4720 Statistical Methods and MATH 4740 Biostatistical Methods at Marquette University.1 Some topics can also be discussed in an introductory data science course. You’ll learn basic probability and statistical concepts as well as data analysis techniques such as linear regression using R computing software. The book balances the following aspects of statistics:\n\nmathematical derivation and statistical computation\ndistribution-based and simulation-based inferences\nmethodology and applications\nclassical/frequentist and Bayesian approaches\n\nCourse materials are borrowed from the following books:\n\nOpenIntro Statsitics (data oriented)\nIntroduction to Modern Statistics (computation oriented)\nAn Introduction to Statistical Methods & Data Analysis, 7th edition (mathematics oriented)"
  },
  {
    "objectID": "prob-rv.html#a-statistician-should-know",
    "href": "prob-rv.html#a-statistician-should-know",
    "title": "8  Random Variables",
    "section": "8.3 A Statistician Should Know",
    "text": "8.3 A Statistician Should Know\n\n\n\n\n\nhttps://github.com/rasmusab/distribution_diagrams"
  },
  {
    "objectID": "infer-ci.html#inference-framework",
    "href": "infer-ci.html#inference-framework",
    "title": "13  Confidence Interval",
    "section": "13.1 Inference Framework",
    "text": "13.1 Inference Framework\n\nInferential statistics uses sample data to learn about an unknown population.\nIdea: Assume the target population follows some distribution but with unknown parameters.\n\n Assume the population is normally distributed, but don’t know its mean and/or variance. Marquette students’ mean GPA for example. \n\nGoal: Learning the unknown parameters of the assumed population distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTwo approaches in parameter learning: Estimation and Hypothesis testing."
  },
  {
    "objectID": "infer-ci.html#point-estimator",
    "href": "infer-ci.html#point-estimator",
    "title": "13  Confidence Interval",
    "section": "13.2 Point Estimator",
    "text": "13.2 Point Estimator\n\n\n\n\n\n\n\n\nIf you could only use one single number to guess the unknown population mean \\(\\mu\\), what would you like to use?\n\n\n\n\n\n\n\nThe one single point used to estimate the unknown parameter is called a point estimator.\nA point estimator is any function of data \\((X_1, X_2, \\dots, X_n)\\) (before actually being collected).\n\nAny statistic is a point estimator.\n\nA point estimate is a value of a point estimator used to estimate a population parameter. (A value calculated by the collected data)\nSample mean \\((\\overline{X})\\) is a statistic and a point estimator for the population mean \\(\\mu\\)."
  },
  {
    "objectID": "infer-ci.html#sample-mean-as-an-point-estimator",
    "href": "infer-ci.html#sample-mean-as-an-point-estimator",
    "title": "13  Confidence Interval",
    "section": "13.3 Sample Mean as an Point Estimator",
    "text": "13.3 Sample Mean as an Point Estimator\n\nDraw 5 values from the population that follows \\(N(2, 1)\\) as sample data \\((x_1, x_2, x_3, x_4, x_5)\\).\n\n\n## Generate sample data x1, x2, x3, x4, x5, each from population distribution N(2, 1)\nx_data_1 <- rnorm(n = 5, mean = 2, sd = 1)\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n\n\n1.54\n1.64\n1.84\n1.23\n0.83\n1.42\n\n\n\n\n\n\n\\(\\mu = 2\\), and we use the point estimate \\(\\overline{x}=\\) 1.42 to estimate it.\n\n\n\n\n\n\n\nWhy \\(\\overline{x}\\) is not equal to \\(\\mu\\)?\n\n\n\n\n\n\n\nDue to its randomness nature:"
  },
  {
    "objectID": "infer-ci.html#variability-in-estimates",
    "href": "infer-ci.html#variability-in-estimates",
    "title": "13  Confidence Interval",
    "section": "13.4 Variability in Estimates",
    "text": "13.4 Variability in Estimates\n\nIf another sample of size \\(5\\) is drawn from the same population:\n\n\nx_data_2 <- rnorm(5, mean = 2, sd = 1)\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n\n\n1.68\n1.65\n1.41\n0.41\n3.69\n1.77\n\n\n\n\n\n\nThe second sample mean \\(\\overline{x} =\\) 1.77 is different from the first one.\n\n\n\n\n\n\n\nWhy do the first sample and the second sample give us different sample means?\n\n\n\n\n\n\n\nA point estimator has its own sampling distribution."
  },
  {
    "objectID": "infer-ci.html#why-point-estimates-are-not-enough",
    "href": "infer-ci.html#why-point-estimates-are-not-enough",
    "title": "13  Confidence Interval",
    "section": "13.5 Why Point Estimates Are Not Enough",
    "text": "13.5 Why Point Estimates Are Not Enough\n\n\n\n\n\n\nIf you want to estimate \\(\\mu\\), do you prefer to report a range of values the parameter might be in, or a single estimate like \\(\\overline{x}\\)?\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to catch a fish, do you prefer a spear or a net?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue to variation of \\(\\overline{X}\\), if we report a point estimate \\(\\overline{x}\\), we probably won’t hit the exact \\(\\mu\\).\nIf we report a range of plausible values, we have a better shot at capturing the parameter!"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals",
    "href": "infer-ci.html#confidence-intervals",
    "title": "13  Confidence Interval",
    "section": "13.6 Confidence Intervals",
    "text": "13.6 Confidence Intervals\nA plausible range of values for \\(\\mu\\) is called a confidence interval (CI).\n\nTo construct a CI we need to quantify the variability of our sample mean.  \nQuantifying this uncertainty requires a measurement of how much we would expect the sample statistic to vary from sample to sample.\n\nThat is the variance of the sampling distribution of the sample mean!\n\n\n\n\n\n\n\n\nNote\n\n\n\n👉 The larger variation of \\(\\overline{X}\\) is, the wider the CI for \\(\\mu\\) will be given the same “level of confidence”.\n\n\n\n\n\n\n\n\nDo we know the variance of \\(\\overline{X}\\)?\n\n\n\n\n\n\n\nBy CLT, \\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\) regardless of what the population distribution is."
  },
  {
    "objectID": "infer-ci.html#precision-vs.-reliability",
    "href": "infer-ci.html#precision-vs.-reliability",
    "title": "13  Confidence Interval",
    "section": "13.7 Precision vs. Reliability",
    "text": "13.7 Precision vs. Reliability\n\n\n\n\n\n\nIf we want to be very certain that we capture \\(\\mu\\), should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the sample size fixed, precision and reliability have a trade-off relationship."
  },
  {
    "objectID": "infer-ci.html#confidence-interval-is-for-a-parameter",
    "href": "infer-ci.html#confidence-interval-is-for-a-parameter",
    "title": "13  Confidence Interval",
    "section": "13.8 Confidence Interval Is for a Parameter",
    "text": "13.8 Confidence Interval Is for a Parameter\n\nA confidence interval is for a parameter, NOT a statistic.\n\nUse the sample mean to form a confidence interval for the population mean.\n\nWe never say “The confidence interval of the sample mean \\(\\overline{X}\\) is …”\nWe say “The confidence interval for the true population mean \\(\\mu\\) is …”\nIn general, a confidence interval for \\(\\mu\\) has the form\n\n\n\\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\n\nThe \\(m\\) is called margin of error.\n\\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval.\nThe point estimate \\(\\overline{x}\\) and margin of error \\(m\\) can be obtained from known quantities and our data once sampled."
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-z-score",
    "href": "infer-ci.html#confidence-intervals-for-mu-z-score",
    "title": "13  Confidence Interval",
    "section": "13.10 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Z-score",
    "text": "13.10 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Z-score\n\n\n\n\n\\(\\alpha = 0.05\\)\nStart with the sampling distribution of \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\\(\\overline{x}\\) will be within 1.96 SDs of the population mean \\(\\mu\\) \\(95\\%\\) of the time.\nThe \\(z\\)-score of 1.96 is associated with 2.5% area to the right, and called a critical value denoted as \\(z_{0.025}\\) ."
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-probability",
    "href": "infer-ci.html#confidence-intervals-for-mu-probability",
    "title": "13  Confidence Interval",
    "section": "13.11 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Probability",
    "text": "13.11 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Probability\n\\[P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} < \\overline{X} < \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\]\n\n\n\n\n\n\nIs the interval \\(\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right)\\) our confidence interval?\n\n\n\n\n\n\n\n❌ No! We don’t know \\(\\mu\\), the quantity we like to estimate!\nBut we’re almost there!"
  },
  {
    "objectID": "infer-ci.html#alpha100-confidence-intervals",
    "href": "infer-ci.html#alpha100-confidence-intervals",
    "title": "13  Confidence Interval",
    "section": "13.9 \\((1 - \\alpha)100\\%\\) Confidence Intervals",
    "text": "13.9 \\((1 - \\alpha)100\\%\\) Confidence Intervals\n\nThe confidence level \\(1-\\alpha\\): the proportion of times that the CI contains the population parameter, assuming that the estimation process is repeated a large number of times.\nThe common choices for the confidence level are\n\n90% \\((\\alpha = 0.10)\\)\n95% \\((\\alpha = 0.05)\\)\n99% \\((\\alpha = 0.01)\\)\n\n95% is the most common level because of good balance between precision (width of the CI) and reliability (confidence level)\n\n High reliability and Low precision. I am 100% confident that the mean height of Marquette students is between 3’0” and 8’0”.  duh…🤷\n Low reliability and High precision. I am 20% confident that mean height of Marquette students is between 5’6” and 5’7”.  far from it…🙅"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-formula",
    "href": "infer-ci.html#confidence-intervals-for-mu-formula",
    "title": "13  Confidence Interval",
    "section": "13.12 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Formula",
    "text": "13.12 \\(95\\%\\) Confidence Intervals for \\(\\mu\\): Formula\n\\[\\begin{align}\n&P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} < \\overline{X} < \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\\\\n&P\\left( \\boxed{\\overline{X}-1.96\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}} \\right) = 0.95\n\\end{align}\\]\n\n With sample data of size \\(n\\), \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) is our \\(95\\%\\) CI for \\(\\mu\\) if \\(\\sigma\\) is known to us! \nThe margin of error \\(m = 1.96\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "title": "13  Confidence Interval",
    "section": "13.13 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "13.13 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\nRequirements for estimating \\(\\mu\\) when \\(\\sigma\\) is known:\n\n👉 The sample should be a random sample, i.e. All data \\(X_i\\) are drawn from the same population, and \\(X_i\\) and \\(X_j\\) are independent.\n\n Any methods in the course are based on a random sample \n\n👉 The population standard deviation \\(\\sigma\\) is known.\n👉 The population is either normally distributed or \\(n > 30\\) or both, i.e., \\(X_i \\sim N(\\mu, \\sigma^2)\\).\n\n \\(n > 30\\) allows CLT to be applied and hence normality is satisfied."
  },
  {
    "objectID": "infer-ci.html#alpha100-confidence-intervals-for-mu",
    "href": "infer-ci.html#alpha100-confidence-intervals-for-mu",
    "title": "13  Confidence Interval",
    "section": "13.14 \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\):",
    "text": "13.14 \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\)   \\(\\left(\\overline{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) \n\n\n \\(\\left(\\overline{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\)"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known-1",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known-1",
    "title": "13  Confidence Interval",
    "section": "13.15 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "13.15 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\nProcedures for constructing a confidence interval for \\(\\mu\\) when \\(\\sigma\\) known:\n\nCheck that the requirements are satisfied.\nDecide \\(\\alpha\\) or confidence level \\((1 - \\alpha)\\).\nFind the critical value \\(z_{\\alpha/2}\\).\nEvaluate margin of error \\(m = z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)\nConstruct the \\((1 - \\alpha)100\\%\\) CI for \\(\\mu\\) using sample mean \\(\\overline{x}\\) and margin of error \\(m\\):\n\n\n \\[\\boxed{\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\text{  or  } \\left( \\overline{x} -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\, \\overline{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)}\\]"
  },
  {
    "objectID": "infer-ci.html#example-ci-for-mu-when-sigma-is-known",
    "href": "infer-ci.html#example-ci-for-mu-when-sigma-is-known",
    "title": "13  Confidence Interval",
    "section": "13.16 Example: CI for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "13.16 Example: CI for \\(\\mu\\) When \\(\\sigma\\) is Known\n\n\n\n\nSuppose we want to know the mean systolic blood pressure (SBP) of a population.\n\nAssume that the population distribution is normal with the standard deviation of 5 mmHg.\nWe have a random sample of 16 subjects of this population with mean 121.5.\nEstimate the mean SBP with a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\nRequirements:  Normality is assumed, \\(\\sigma = 5\\) is known and a random sample is collected.\nDecide \\(\\alpha\\):  \\(\\alpha = 0.05\\) \nFind the critical value \\(z_{\\alpha/2}\\):  \\(z_{\\alpha/2} = z_{0.025} = 1.96\\) \nEvaluate margin of error \\(m = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\):  \\(m = (1.96) \\frac{5}{\\sqrt{16}} = 2.45\\) \nConstruct the \\((1 - \\alpha)100\\%\\) CI:  The 95% CI for the mean SBP is \\(\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = (121.5 -2.45, 121.5 + 2.45) = (119.05, 123.95)\\)"
  },
  {
    "objectID": "infer-ci.html#computation-in-r",
    "href": "infer-ci.html#computation-in-r",
    "title": "13  Confidence Interval",
    "section": "13.17 Computation in R",
    "text": "13.17 Computation in R\n\n## save all information we have\nalpha <- 0.05\nn <- 16\nx_bar <- 121.5\nsig <- 5\n\n## 95% CI\n## z-critical value\n(cri_z <- qnorm(p = alpha / 2, lower.tail = FALSE))  \n\n[1] 1.959964\n\n## margin of error\n(m_z <- cri_z * (sig / sqrt(n)))  \n\n[1] 2.449955\n\n## 95% CI for mu when sigma is known\nx_bar + c(-1, 1) * m_z  \n\n[1] 119.05 123.95\n\n\n\n\n\n\n\n\nConstruct a 99% CI for the mean SBP. Do you expect to have a wider or narrower interval? Why?"
  },
  {
    "objectID": "infer-ci.html#interpreting-a-confidence-interval",
    "href": "infer-ci.html#interpreting-a-confidence-interval",
    "title": "13  Confidence Interval",
    "section": "13.18 Interpreting a Confidence Interval",
    "text": "13.18 Interpreting a Confidence Interval\n\n “We are 95% confident that the mean SBP lies between 119.1 mm and 123.9 mm.” \nSuppose we were able to collect our dataset many times and build the corresponding CIs.\nWe would expect about 95% of those intervals would contain the true population parameter, here the mean systolic blood pressure.\n\n Remember: \\(\\overline{x}\\) varies from sample to sample, so does its corresponding CI .\n\nWe never know if in fact 95% of them do, or whether any interval contains the true parameter!"
  },
  {
    "objectID": "infer-ci.html#generate-100-confidence-intervals-assuming-mu-120.",
    "href": "infer-ci.html#generate-100-confidence-intervals-assuming-mu-120.",
    "title": "13  Confidence Interval",
    "section": "13.19 Generate 100 Confidence Intervals Assuming \\(\\mu = 120\\).",
    "text": "13.19 Generate 100 Confidence Intervals Assuming \\(\\mu = 120\\)."
  },
  {
    "objectID": "infer-ci.html#interpreting-a-confidence-interval-do-not-say",
    "href": "infer-ci.html#interpreting-a-confidence-interval-do-not-say",
    "title": "13  Confidence Interval",
    "section": "13.20 Interpreting a Confidence Interval DO NOT SAY",
    "text": "13.20 Interpreting a Confidence Interval DO NOT SAY\n\nWRONG ❌ “There is a 95% chance/probability that the true population mean will fall between 119.1 mm and 123.9 mm.”\nWRONG ❌ “The probability that the true population mean falls between 119.1 mm and 123.9 mm is 95%.”\n 👉 The sample mean is a random variable with a sampling distribution, so it makes sense to compute a probability of it being in some interval. \n 👉 The population mean is unknown and FIXED. We cannot assign or compute any probability of it. \nAnother inference method, Bayesian inference, treats \\(\\mu\\) as a random variable and therefore we can compute any probability associated with it. (MATH 4790 Bayesian Statistics)"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "title": "13  Confidence Interval",
    "section": "13.21 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "13.21 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\n\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}\\), \\(N\\) is the population size.\nIt’s rare that we do not know \\(\\mu\\), but know \\(\\sigma\\).\nWe use the Student t distribution to construct a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown.\nStill need\n\nRandom sample\nPopulation is normally distributed and/or \\(n > 30\\).\n\n\n\n\n\n\n\n\nWhat is a natural estimator for the unknown \\(\\sigma\\)?\n\n\n\n\n\n\n\nSince \\(\\sigma\\) is unknown, we use the sample standard deviation \\(S = \\sqrt{\\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{n-1}}\\) instead when constructing the CI."
  },
  {
    "objectID": "infer-ci.html#student-t-distribution",
    "href": "infer-ci.html#student-t-distribution",
    "title": "13  Confidence Interval",
    "section": "13.22 Student t Distribution",
    "text": "13.22 Student t Distribution\n\nIf the population is normally distributed or \\(n > 30\\),\n\n\\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\)\n\\(Z = \\frac{\\overline{X} - \\mu}{\\color{red}\\sigma/\\sqrt{n}} \\sim N(0, 1)\\)\n \\(T = \\frac{\\overline{X} - \\mu}{\\color{red}S/\\sqrt{n}} \\sim t_{n-1}\\) \n\\(t_{n-1}\\) denotes the Student t distribution with degrees of freedom (df) \\(n-1\\)."
  },
  {
    "objectID": "infer-ci.html#properties-of-student-t-distribution",
    "href": "infer-ci.html#properties-of-student-t-distribution",
    "title": "13  Confidence Interval",
    "section": "13.23 Properties of Student t Distribution",
    "text": "13.23 Properties of Student t Distribution\n\nSymmetric about the mean 0 and bell-shaped as \\(N(0, 1)\\).\nMore variability than \\(N(0, 1)\\) (heavier tails and lower peak).\nThe variability is different for different sample sizes (degrees of freedom).\nAs \\(n \\rightarrow \\infty\\) \\((df \\rightarrow \\infty)\\), the Student t distribution approaches to \\(N(0, 1)\\)."
  },
  {
    "objectID": "infer-ci.html#critical-values-of-t_alpha2-n-1",
    "href": "infer-ci.html#critical-values-of-t_alpha2-n-1",
    "title": "13  Confidence Interval",
    "section": "13.24 Critical Values of \\(t_{\\alpha/2, n-1}\\)",
    "text": "13.24 Critical Values of \\(t_{\\alpha/2, n-1}\\)\n\nWhen \\(\\sigma\\) is unknown, we use \\(t_{\\alpha/2, n-1}\\) as the critical value, instead of \\(z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWith the same \\(\\alpha\\), \\(t_{\\alpha, n-1}\\) or \\(z_{\\alpha}\\) is larger?\n\n\n\n\n\n\n\nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} > z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58"
  },
  {
    "objectID": "infer-ci.html#critical-values-of-t_alpha2-n-1-1",
    "href": "infer-ci.html#critical-values-of-t_alpha2-n-1-1",
    "title": "13  Confidence Interval",
    "section": "13.25 Critical Values of \\(t_{\\alpha/2, n-1}\\)",
    "text": "13.25 Critical Values of \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} > z_{\\alpha/2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58"
  },
  {
    "objectID": "infer-ci.html#ci-for-mu-when-sigma-is-unknown",
    "href": "infer-ci.html#ci-for-mu-when-sigma-is-unknown",
    "title": "13  Confidence Interval",
    "section": "13.25 CI for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "13.25 CI for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown is  \\[\\left(\\overline{x} - t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}, \\overline{x} + t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\right)\\] \nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} > z_{\\alpha/2}\\).\n\n\n\n\n\n\n\nWe are more “uncertain” when doing inference about \\(\\mu\\) because we also don’t have information about \\(\\sigma\\), and replacing it with \\(s\\) adds additional uncertainty."
  },
  {
    "objectID": "infer-ci.html#computation-in-r-t-interval",
    "href": "infer-ci.html#computation-in-r-t-interval",
    "title": "13  Confidence Interval",
    "section": "13.26 Computation in R (t interval)",
    "text": "13.26 Computation in R (t interval)\n\nBack to the systolic blood pressure (SBP) example. We have \\(n=16\\) and \\(\\overline{x} = 121.5\\).\nEstimate the mean SBP with a 95% confidence interval with unknown \\(\\sigma\\) and \\(s = 5\\).\n\n\nalpha <- 0.05\nn <- 16\nx_bar <- 121.5\ns <- 5  ## sigma is unknown and s = 5\n\n## t-critical value\n(cri_t <- qt(p = alpha / 2, df = n - 1, lower.tail = FALSE)) \n\n[1] 2.13145\n\n## margin of error\n(m_t <- cri_t * (s / sqrt(n)))  \n\n[1] 2.664312\n\n## 95% CI for mu when sigma is unknown\nx_bar + c(-1, 1) * m_t  \n\n[1] 118.8357 124.1643"
  },
  {
    "objectID": "infer-ci.html#summary",
    "href": "infer-ci.html#summary",
    "title": "13  Confidence Interval",
    "section": "13.27 Summary",
    "text": "13.27 Summary\n\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\) unknown\n\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\nPopulation Mean \\(\\mu\\)\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\n\nRemember to check if the population is normally distributed or \\(n>30\\).\nWhat if the population is not normal and \\(n \\le 30\\)?\nUse a so-called nonparametric method, for example bootstrapping."
  },
  {
    "objectID": "intro-data.html#randomly-select-40-males-and-40-females-to-see-the-difference-in-blood-pressure-levels-between-male-and-female.",
    "href": "intro-data.html#randomly-select-40-males-and-40-females-to-see-the-difference-in-blood-pressure-levels-between-male-and-female.",
    "title": "2  Data",
    "section": "2.8 - Randomly select 40 males and 40 females to see the difference in blood pressure levels between male and female. ",
    "text": "2.8 - Randomly select 40 males and 40 females to see the difference in blood pressure levels between male and female."
  },
  {
    "objectID": "intro-data.html#test-the-effects-of-a-new-drug-by-randomly-dividing-patients-into-3-groups-high-dosage-low-dosage-placebo.",
    "href": "intro-data.html#test-the-effects-of-a-new-drug-by-randomly-dividing-patients-into-3-groups-high-dosage-low-dosage-placebo.",
    "title": "2  Data",
    "section": "2.9 -  Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo). ",
    "text": "2.9 -  Test the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo)."
  },
  {
    "objectID": "intro-r.html#lab-time",
    "href": "intro-r.html#lab-time",
    "title": "3  Tool foR Data",
    "section": "3.5 Lab time!",
    "text": "3.5 Lab time!\n\nStep 1: In the RStudio website https://rstudio.com/, please choose Products > RStudio Cloud as shown below.\n\n:::\n\n\n\n\n\n\n\n\n\n\nStep 2: Click GET STARTED FOR FREE.\nStep 3: Free > Sign Up. Sign up using your Marquette email address or the one you prefer."
  },
  {
    "objectID": "intro-r.html#welcome-to-the-r-world",
    "href": "intro-r.html#welcome-to-the-r-world",
    "title": "3  Tool foR Data",
    "section": "3.9 Welcome to the R World!",
    "text": "3.9 Welcome to the R World!\n\nNow you are ready to use R to do statistical computation.\nYou can use R like a calculator. After typing your formula, simply hit Enter, you get the answer! For example,\n\n\n1 + 2\n\n[1] 3\n\n30 * 42 / 3\n\n[1] 420\n\nlog(5) - exp(3) * sqrt(7)\n\n[1] -51.5319"
  },
  {
    "objectID": "intro-r.html#youve-seen-comments-a-lot-how-do-we-write-a-comment-in-r",
    "href": "intro-r.html#youve-seen-comments-a-lot-how-do-we-write-a-comment-in-r",
    "title": "3  Tool foR Data",
    "section": "3.16 You’ve seen comments a lot! How do we write a comment in R?",
    "text": "3.16 You’ve seen comments a lot! How do we write a comment in R?\n:::\n\nUse # to add a comment so that the text after # is not read as an R command.\nWriting (good) comments is highly recommended: help readers and more importantly yourself understand what the code is doing.\nComments should explain the why, not the what.\n\n\n\n\n\n\nhttps://www.reddit.com/r/ProgrammerHumor/comments/8w54mx/code_comments_be_like/"
  },
  {
    "objectID": "data-graphics.html#is-the-interest-rate-histogram-left-skewed-or-right-skewed",
    "href": "data-graphics.html#is-the-interest-rate-histogram-left-skewed-or-right-skewed",
    "title": "4  Data Visualization",
    "section": "4.6 Is the interest rate histogram left skewed or right skewed?",
    "text": "4.6 Is the interest rate histogram left skewed or right skewed?"
  },
  {
    "objectID": "data-numerics.html#larger-iqr-means-more-or-less-variation",
    "href": "data-numerics.html#larger-iqr-means-more-or-less-variation",
    "title": "5  Data Sample Statistics",
    "section": "5.7 Larger IQR means more or less variation?",
    "text": "5.7 Larger IQR means more or less variation?\n::: \n\n5.7.1 Variance and Standard Deviation\n\nThe distance of an observation from its mean, \\(x_i - \\overline{x}\\), its deviation.\nSample Variance is defined as \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1} \\]\nSample Standard Deviation (SD) is defined as the square root of the variance \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}} \\] \nVariance is the average of squared deviation from the sample mean \\(\\overline{x}\\) or the mean squared deviation from the mean.\nSD is the root mean squared deviation from the mean. It measures, on average, how far the data spread out around the average.\n\n\n\n5.7.2 Compute Variance and SD\n\nvar(int_rate)\n\n[1] 25.54942\n\nsqrt(var(int_rate))\n\n[1] 5.054644\n\nsd(int_rate)\n\n[1] 5.054644"
  },
  {
    "objectID": "prob-disc.html#if-we-toss-a-fair-coin-two-times-independently-and-let-x-of-heads-is-x-a-binomial-r.v.",
    "href": "prob-disc.html#if-we-toss-a-fair-coin-two-times-independently-and-let-x-of-heads-is-x-a-binomial-r.v.",
    "title": "9  Discrete Probability Distributions",
    "section": "9.7 If we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial r.v.?",
    "text": "9.7 If we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial r.v.?\n:::"
  },
  {
    "objectID": "prob-cont.html#we-have-to-specify-the-mean-and-sd-of-the-original-distribution-of-x-like-pnormx-mean-mu-sd-sigma.",
    "href": "prob-cont.html#we-have-to-specify-the-mean-and-sd-of-the-original-distribution-of-x-like-pnormx-mean-mu-sd-sigma.",
    "title": "10  Continuous Probability Distributions",
    "section": "10.16 👉 We have to specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma).",
    "text": "10.16 👉 We have to specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma)."
  },
  {
    "objectID": "infer-ci.html#large-overlinex-pm-m-overlinex---m-overlinex-m",
    "href": "infer-ci.html#large-overlinex-pm-m-overlinex---m-overlinex-m",
    "title": "13  Confidence Interval",
    "section": "13.9 \\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)",
    "text": "13.9 \\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\n\nThe \\(m\\) is called margin of error.\n\\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval.\nThe point estimate \\(\\overline{x}\\) and margin of error \\(m\\) can be obtained from known quantities and our data once sampled."
  },
  {
    "objectID": "infer-ht.html#what-is-a-hypothesis-testing",
    "href": "infer-ht.html#what-is-a-hypothesis-testing",
    "title": "15  Hypothesis Testing",
    "section": "15.1 What is a Hypothesis Testing?",
    "text": "15.1 What is a Hypothesis Testing?\n\n\nA hypothesis is a claim or statement about a property of a population, often the value of a population parameter.\n\n The mean body temperature of humans is less than \\(98.6^{\\circ}\\) F, or \\(\\mu < 98.6\\). \n Marquette students’ IQ scores has standard deviation equal to 15, or \\(\\sigma = 15\\). \n\n\n\n\nNull hypothesis \\((H_0)\\): a statement that the value of a parameter is\n\nequal to some claim value\nthe negation of the alternative hypothesis\noften represents a skeptical perspective to be tested\n\nAlternative hypothesis \\((H_1\\) or \\(H_a)\\): a claim that the parameter is less than, greater than or not equal to some value.\n\nusually our research hypothesis of some new scientific theory or finding"
  },
  {
    "objectID": "infer-ht.html#null-and-alternative-hypothesis",
    "href": "infer-ht.html#null-and-alternative-hypothesis",
    "title": "15  Hypothesis Testing",
    "section": "15.2 Null and Alternative Hypothesis",
    "text": "15.2 Null and Alternative Hypothesis\n\n\n\n\n\n\nA \\(H_0\\) claim or \\(H_1\\) claim?\n\n\n\n\n\n\n\n The percentage of Marquette female students loving Japanese food is equal to 80%.\n On average, Marquette students consume less than 3 drinks per week. \nHypothesis testing 1 is a procedure to decide whether to reject \\(H_0\\) or not by how much evidence against \\(H_0\\).\n\n\n[1] Null Hypothesis Statistical Testing (NHST), statistical testing or test of significance."
  },
  {
    "objectID": "infer-ht.html#hypothesis-testing-example",
    "href": "infer-ht.html#hypothesis-testing-example",
    "title": "15  Hypothesis Testing",
    "section": "15.3 Hypothesis Testing Example",
    "text": "15.3 Hypothesis Testing Example\n\n\n\n\nA person is charged with a crime.\n\nA jury decide whether the person is guilty or not.\nThe accuse is assumed to be innocent until the jury declares otherwise.\nOnly if overwhelming evidence of the person’s guilt can be shown is the jury expected to declare the person guilty, otherwise the person is considered not guilty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat should \\(H_0\\) and \\(H_a\\) be?\n\n\n\n\n\n\n\n\\(H_0:\\) The person is  not guilty  🙂\n\\(H_1:\\) The person is  guilty  😟\nEvidence:  Photos, videos, witness, fingerprint, DNA \nDecision Rule:  Jury’s voting \nConclusion: Verdict  “guilty”  or  “NOT enough evidence to convict”"
  },
  {
    "objectID": "infer-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "href": "infer-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "title": "15  Hypothesis Testing",
    "section": "15.4 How to Formally Do a Statistical Hypothesis Testing",
    "text": "15.4 How to Formally Do a Statistical Hypothesis Testing\nStep 0: Check Method Assumptions \nStep 1: Set the \\(H_0\\) and \\(H_a\\) in Symbolic Form from a Claim \nStep 2: Set the Significance Level \\(\\alpha\\) \nStep 3: Calculate the Test Statistic (Evidence) \n\n\n\nDecision Rule I: Critical Value Method\n\n\nDecision Rule II: P-Value Method\n\n\n\n\n Step 4-c: Find the Critical Value \n\n\n Step 4-p: Find the P-Value \n\n\n\n\n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n\n Step 5-p: Draw a Conclusion Using P-Value Method \n\n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\n\n😎 No worries, we will learn this step by step!"
  },
  {
    "objectID": "infer-ht.html#the-new-treatment-is-effective",
    "href": "infer-ht.html#the-new-treatment-is-effective",
    "title": "15  Hypothesis Testing",
    "section": "15.5 The New Treatment is Effective?",
    "text": "15.5 The New Treatment is Effective?\n\n\n\n\nA population of hypertension group is normal and has mean blood pressure (BP) 150.\nAfter 6 months of treatment, BP was recorded on 25 patients of this population, and \\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\nGoal: Determine whether a new treatment is effective in reducing BP."
  },
  {
    "objectID": "infer-ht.html#step-0-check-method-assumptions",
    "href": "infer-ht.html#step-0-check-method-assumptions",
    "title": "15  Hypothesis Testing",
    "section": "15.6 Step 0: Check Method Assumptions",
    "text": "15.6 Step 0: Check Method Assumptions\nThe testing methods are based on normality or approximate normality by CLT.\n\nRandom sample\nNormally distributed and/or \\(n > 30\\)"
  },
  {
    "objectID": "infer-ht.html#step-1-set-the-h_0-and-h_1-from-a-claim",
    "href": "infer-ht.html#step-1-set-the-h_0-and-h_1-from-a-claim",
    "title": "15  Hypothesis Testing",
    "section": "15.7 Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim",
    "text": "15.7 Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim\n\n🧑‍🏫 The mean IQ score of statistics professors is higher than 120.\n\n \\(\\begin{align}&H_0: \\mu \\le 120 \\\\ &H_1: \\mu > 120 \\end{align}\\) \n\n💵 The mean starting salary for Marquette graduates who didn’t take MATH 4720 is less than $60,000.\n\n \\(\\begin{align} &H_0: \\mu \\ge 60000 \\\\ &H_1: \\mu < 60000 \\end{align}\\) \n\n📺 The mean time between uses of a TV remote control by males during commercials equals 5 sec. \n\n \\(\\begin{align} &H_0: \\mu = 5 \\\\ &H_1: \\mu \\ne 5 \\end{align}\\) \n\nThe claim that the new treatment is effective in reducing BP means the mean BP is less than 150. ( \\(H_1\\) claim )\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu < 150 \\end{align}\\)"
  },
  {
    "objectID": "infer-ht.html#step-2-set-the-significance-level-alpha",
    "href": "infer-ht.html#step-2-set-the-significance-level-alpha",
    "title": "15  Hypothesis Testing",
    "section": "15.8 Step 2: Set the Significance Level \\(\\alpha\\)",
    "text": "15.8 Step 2: Set the Significance Level \\(\\alpha\\)\n\nThe significant level \\(\\alpha\\) determines how rare or unlikely our evidence must be in order to represent sufficient evidence against \\(H_0\\).\nAn \\(\\alpha\\) level of 0.05 implies that evidence occurring with probability lower than 5% will be considered sufficient evidence against \\(H_0\\) (Reject \\(H_0\\)).\n\\(\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true})\\)\n\\(\\alpha = 0.05\\) means that we incorrectly reject \\(H_0\\) 5 out of every 100 times we collect a sample and run the test.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRare Event Rule: If, under a given assumption, the probability of a particular observed event is exceptional small, we conclude that the assumption is probably not correct.\n\n\n\n\n\n\n\n\n\nLet’s set \\(\\alpha= 0.05\\).\nThis means we are asking, “Is there a sufficient evidence at \\(\\alpha= 0.05\\) that the new treatment is effective?”"
  },
  {
    "objectID": "infer-ht.html#step-3-calculate-the-test-statistic",
    "href": "infer-ht.html#step-3-calculate-the-test-statistic",
    "title": "15  Hypothesis Testing",
    "section": "15.9 Step 3: Calculate the Test Statistic",
    "text": "15.9 Step 3: Calculate the Test Statistic\n\nA test statistic is a statistic value used in making a decision about the \\(H_0\\).\n\n\n\nSuppose  \\(H_0: \\mu = \\mu_0 \\quad H_1: \\mu < \\mu_0\\) \nWhen computing a test statistic, we assume \\(H_0\\) is true.\nWhen \\(\\sigma\\) is known, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\boxed{ z_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{\\sigma/\\sqrt{n}} }\\]\n\n\n\n\n\n\nGuess what test statistic we use when \\(\\sigma\\) is unknown!\n\n\n\n\n\n\n\nWhen \\(\\sigma\\) is unknown, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\boxed{ t_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}} }\\]\n\n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)"
  },
  {
    "objectID": "infer-ht.html#step-4-c-find-the-critical-value",
    "href": "infer-ht.html#step-4-c-find-the-critical-value",
    "title": "15  Hypothesis Testing",
    "section": "15.10 Step 4-c: Find the Critical Value",
    "text": "15.10 Step 4-c: Find the Critical Value\n\nThe critical value(s) separates the rejection region or critical region (where we reject \\(H_0\\)) from the values of the test statistic that do not lead to rejection of \\(H_0\\).\n\nThey depend on whether the test is a right-tailed, left-tailed or two-tailed test.\n\n\n\n\n\n\n\n\n\n\n\n\n👉 \\(z_{\\alpha}\\) is such that \\(P(Z > z_{\\alpha}) = \\alpha\\) and \\(Z \\sim N(0, 1)\\).\n👉 \\(t_{\\alpha, n-1}\\) is such that \\(P(T > t_{\\alpha, n-1}) = \\alpha\\) and \\(T \\sim t_{n-1}\\).\n\n\n\n\n\n\n\n\n\n\nCondition    \nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{\\alpha}\\)\n\\(-z_{\\alpha}\\)\n\\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\)\n\n\n\\(\\sigma\\) unknown\n\\(t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha/2, n-1}\\) and \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n\\(z_{0.025} =\\) 1.96, \\(z_{0.05} =\\) 1.64\n\n\n\n\\(z_{\\alpha}\\) and \\(t_{\\alpha, n-1}\\) are always positive.\n The critical value is \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\)"
  },
  {
    "objectID": "infer-ht.html#step-5-c-draw-a-conclusion-using-critical-value",
    "href": "infer-ht.html#step-5-c-draw-a-conclusion-using-critical-value",
    "title": "15  Hypothesis Testing",
    "section": "15.11 Step 5-c: Draw a Conclusion Using Critical Value",
    "text": "15.11 Step 5-c: Draw a Conclusion Using Critical Value\n\nIf the test statistic is\n\nin the rejection region, we reject \\(H_0\\).\nnot in the rejection region, we do not or fail to reject \\(H_0\\).\n\nReject \\(H_0\\) if\n\n\n\n\n\n\n\n\n\n\nCondition    \nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{test} > z_{\\alpha}\\)\n\\(z_{test} < -z_{\\alpha}\\)\n\\(\\mid z_{test}\\mid \\, > z_{\\alpha/2}\\)\n\n\n\\(\\sigma\\) unknown\n\\(t_{test} > t_{\\alpha, n-1}\\)\n\\(t_{test} < -t_{\\alpha, n-1}\\)\n\\(\\mid t_{test}\\mid \\, > t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\) \n \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) \n We reject \\(H_0\\) if \\(t_{test} < -t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = -2.55 < -1.711 = -t_{\\alpha, n-1}\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "infer-ht.html#step-4-p-find-the-p-value",
    "href": "infer-ht.html#step-4-p-find-the-p-value",
    "title": "15  Hypothesis Testing",
    "section": "15.12 Step 4-p: Find the P-Value",
    "text": "15.12 Step 4-p: Find the P-Value\n\nThe \\(p\\)-value measures the strength of the evidence against \\(H_0\\) provided by the data.\nThe smaller the \\(p\\)-value, the greater the evidence against \\(H_0\\).\nThe \\(p\\)-value is the probability of getting a test statistic value that is at least as extreme as the one obtained from the data, assuming that \\(H_0\\) is true. \\((\\mu = \\mu_0)\\)\n\nFor example, \\(p\\)-value \\(= P(Z \\ge z_{test} \\mid H_0)\\) for a right-tailed test."
  },
  {
    "objectID": "infer-ht.html#p-value-illustration",
    "href": "infer-ht.html#p-value-illustration",
    "title": "15  Hypothesis Testing",
    "section": "15.13 P-Value Illustration",
    "text": "15.13 P-Value Illustration\n\n\n\n\n\n\nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T < t_{test})=P(T < -2.55) =\\) 0.01"
  },
  {
    "objectID": "infer-ht.html#step-5-p-draw-a-conclusion-using-p-value-method",
    "href": "infer-ht.html#step-5-p-draw-a-conclusion-using-p-value-method",
    "title": "15  Hypothesis Testing",
    "section": "15.14 Step 5-p: Draw a Conclusion Using P-Value Method",
    "text": "15.14 Step 5-p: Draw a Conclusion Using P-Value Method\n\nIf the \\(p\\)-value \\(\\le \\alpha\\) , we reject \\(H_0\\).\nIf the \\(p\\)-value \\(> \\alpha\\), we do not reject or fail to reject \\(H_0\\).\n\n\n\n\n\n\n\n\n\n\nCondition    \nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\\(\\sigma\\) known\n\\(P(Z > z_{test} \\mid H_0)\\)\n\\(P(Z < z_{test} \\mid H_0)\\)\n\\(2P(Z > \\,\\mid z_{test} \\mid \\, \\mid H_0)\\)\n\n\n\\(\\sigma\\) unknown\n\\(P(T > t_{test} \\mid H_0)\\)\n\\(P(T < t_{test} \\mid H_0)\\)\n\\(2P(T > \\, \\mid t_{test} \\mid \\, \\mid H_0)\\)\n\n\n\n\n\n We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(= 0.01 < 0.05 = \\alpha\\), we reject \\(H_0\\)."
  },
  {
    "objectID": "infer-ht.html#both-methods-lead-to-the-same-conclusion",
    "href": "infer-ht.html#both-methods-lead-to-the-same-conclusion",
    "title": "15  Hypothesis Testing",
    "section": "15.15 Both Methods Lead to the Same Conclusion",
    "text": "15.15 Both Methods Lead to the Same Conclusion"
  },
  {
    "objectID": "infer-ht.html#step-6-restate-the-conclusion-in-nontechnical-terms-and-address-the-original-claim",
    "href": "infer-ht.html#step-6-restate-the-conclusion-in-nontechnical-terms-and-address-the-original-claim",
    "title": "15  Hypothesis Testing",
    "section": "15.16 Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim",
    "text": "15.16 Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\n\n\n\n\n\nhttps://www.drdawnwright.com/category/statistics/\n\n\n\n\n\n\n\n\n\nhttps://www.pinterest.com/pin/287878601159173631/\n\n\n\n\n\n There is sufficient evidence to support the claim that the new treatment is effective."
  },
  {
    "objectID": "infer-ht.html#example-calculation-in-r",
    "href": "infer-ht.html#example-calculation-in-r",
    "title": "15  Hypothesis Testing",
    "section": "15.17 Example Calculation in R",
    "text": "15.17 Example Calculation in R\n\n## create objects for any information we have\nalpha <- 0.05; mu_0 <- 150; \nx_bar <- 147.2; s <- 5.5; n <- 25\n\n## Test statistic\n(t_test <- (x_bar - mu_0) / (s / sqrt(n))) \n\n[1] -2.545455\n\n## Critical value\n(t_cri <- qt(alpha, df = n - 1, lower.tail = TRUE)) \n\n[1] -1.710882\n\n## p-value\n(p_val <- pt(t_test, df = n - 1, lower.tail = TRUE)) \n\n[1] 0.008878158"
  },
  {
    "objectID": "infer-ht.html#example-2-two-tailed-z-test",
    "href": "infer-ht.html#example-2-two-tailed-z-test",
    "title": "15  Hypothesis Testing",
    "section": "15.18 Example 2: Two-tailed z-test",
    "text": "15.18 Example 2: Two-tailed z-test\n\n\n\n\nThe milk price of a gallon of 2% milk is normally distributed with standard deviation of $0.10.\n\nLast week the mean milk price was 2.78. This week, based on a sample of size 25, the sample mean milk price \\(\\overline{x} = 2.80\\).\nUnder \\(\\alpha = 0.05\\), determine if this week the mean price is different.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1: This is a \\(H_1\\) claim  \\(\\small \\begin{align}&H_0: \\mu = 2.78 \\\\ &H_1: \\mu \\ne 2.78 \\end{align}\\) \nStep 2:  \\(\\small \\alpha = 0.05\\) \nStep 3:  \\(\\small z_{test} = \\frac{\\overline{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{2.8 - 2.78}{0.1/\\sqrt{25}} = 1.00\\) \nStep 4-c:  \\(\\small z_{0.05/2} = 1.96\\). \nStep 5-c: This is a two-tailed test and we reject \\(H_0\\) if \\(|z_{test}| > z_{\\alpha/2}\\). Since \\(\\small |z_{test}| = 1 < 1.96 = z_{\\alpha/2}\\), we DO NOT reject \\(H_0\\).\nStep 4-p: This is a two-tailed test, and the test statistic is on the right \\((> 0)\\), so the \\(p\\)-value is \\(2P(Z > z_{test})=\\) 0.317 \nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(= 0.317 > 0.05 = \\alpha\\), we DO NOT reject \\(H_0\\).\nStep 6:  There is insufficient evidence to support the claim that this week the mean milk price is different from the price last week."
  },
  {
    "objectID": "infer-ht.html#example-2-calculation-in-r",
    "href": "infer-ht.html#example-2-calculation-in-r",
    "title": "15  Hypothesis Testing",
    "section": "15.19 Example 2 Calculation in R",
    "text": "15.19 Example 2 Calculation in R\n\n## create objects to be used\nalpha <- 0.05; mu_0 <- 2.78; \nx_bar <- 2.8; sigma <- 0.1; n <- 25\n\n## Test statistic\n(z_test <- (x_bar - mu_0) / (sigma / sqrt(n))) \n\n[1] 1\n\n## Critical value\n(z_crit <- qnorm(alpha/2, lower.tail = FALSE)) \n\n[1] 1.959964\n\n## p-value\n(p_val <- 2 * pnorm(z_test, lower.tail = FALSE)) \n\n[1] 0.3173105"
  },
  {
    "objectID": "infer-ht.html#testing-summary",
    "href": "infer-ht.html#testing-summary",
    "title": "15  Hypothesis Testing",
    "section": "15.20 Testing Summary",
    "text": "15.20 Testing Summary\n\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\)  unknown \n\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\nPopulation Mean \\(\\mu\\)\n\n\nTest Type\nOne sample \\(\\color{blue}{z}\\) test \\(H_0: \\mu = \\mu_0\\)\nOne sample \\(\\color{blue}{t}\\) test \\(H_0: \\mu = \\mu_0\\)\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{\\color{blue}{s}}{\\sqrt{n}}\\)\n\n\nTest Stat under \\(H_0\\) \n\\(z_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\\(t_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\color{blue}{s}}{\\sqrt{n}}}\\)\n\n\n** \\(p\\)-value under \\(H_0\\) **\n\\(H_1: \\mu < \\mu_0\\)  \\(p\\)-value \\(=P(Z \\le z_{test})\\)\n\\(H_1: \\mu < \\mu_0\\)  \\(p\\)-value \\(=P(T_{n-1} \\le t_{test})\\)\n\n\n\n\\(H_1: \\mu > \\mu_0\\)  \\(p\\)-value \\(=P(Z \\ge z_{test})\\)\n\\(H_1: \\mu < \\mu_0\\)  \\(p\\)-value \\(=P(T_{n-1} \\ge t_{test})\\)\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\)  \\(p\\)-value \\(=2P(Z \\ge \\, \\mid z_{test}\\mid)\\)\n\\(H_1: \\mu \\ne \\mu_0\\)  \\(p\\)-value \\(=2P(T_{n-1} \\ge \\, \\mid t_{test} \\mid)\\)"
  },
  {
    "objectID": "infer-ht.html#type-i-and-type-ii-errors",
    "href": "infer-ht.html#type-i-and-type-ii-errors",
    "title": "15  Hypothesis Testing",
    "section": "15.21 Type I and Type II Errors",
    "text": "15.21 Type I and Type II Errors\n\n\n\nDecision\n\\(H_0\\) is true\n\\(H_0\\) is false\n\n\n\n\nReject \\(H_0\\)\nType I error\nCorrect decision\n\n\nDo not reject \\(H_0\\)\nCorrect decision\nType II error\n\n\n\n\nBack to the crime example: \\(H_0:\\) The person is  not guilty  v.s. \\(H_1:\\) The person is  guilty \n\n\n\n\n\n\n\n\n\nDecision\nTruth is the person innocent\nTruth is the person guilty\n\n\n\n\nJury decides the person guilty\nType I error\nCorrect decision\n\n\nJury decides the person innocent\nCorrect decision\nType II error\n\n\n\n\n\\(\\alpha = P(\\text{type I error}) = P(\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true})\\)\n\\(\\beta = P(\\text{type II error}) = P(\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false})\\)\n\n\n\n\n\n\nhttps://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg"
  },
  {
    "objectID": "infer-twomean.html#why-compare-two-populations",
    "href": "infer-twomean.html#why-compare-two-populations",
    "title": "16  Comparing Two Population Means",
    "section": "16.1 Why Compare Two Populations?",
    "text": "16.1 Why Compare Two Populations?\n\nOften we are faced with a comparison of parameters from different populations.\n\n Comparing the mean annual income for Male and Female groups. \n Testing if a diet used for losing weight is effective from Placebo samples and New Diet samples. \n\nIf these two samples are drawn from populations with means \\(\\mu_1\\) and \\(\\mu_2\\) respectively, the testing problem can be formulated as  \\[\\begin{align}\n&H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 > \\mu_2\n\\end{align}\\] \n\n\\(\\mu_1\\): male mean annual income; \\(\\mu_2\\): female mean annual income\n\\(\\mu_1\\): mean weight loss from the New Diet group; \\(\\mu_2\\): mean weight loss from the Placebo group"
  },
  {
    "objectID": "infer-twomean.html#dependent-and-independent-samples",
    "href": "infer-twomean.html#dependent-and-independent-samples",
    "title": "16  Comparing Two Population Means",
    "section": "16.2 Dependent and Independent Samples",
    "text": "16.2 Dependent and Independent Samples\n\nThe two samples collected can be independent or dependent.\n\n\n\n\n\nTwo samples are dependent or matched pairs if the sample values are matched, where the matching is based on some inherent relationship.\n\n Height data of fathers and daughters. The height of each dad is matched with the height of his daughter. \n Weights of subjects measure before and after some diet treatment. The subjects are the same before and after measurements."
  },
  {
    "objectID": "infer-twomean.html#dependent-samples-matched-pairs",
    "href": "infer-twomean.html#dependent-samples-matched-pairs",
    "title": "16  Comparing Two Population Means",
    "section": "16.3 Dependent Samples (Matched Pairs)",
    "text": "16.3 Dependent Samples (Matched Pairs)\n\nSubject 1 may refer to\n\nthe same person with two measurements (before and after)\nthe first matched pair (dad-daughter).\n\n\n\n\n\n\n\n\nSubject\n(Dad) Before\n(Daughter) After\n\n\n\n\n1\n\\(x_{b1}\\)\n\\(x_{a1}\\)\n\n\n2\n\\(x_{b2}\\)\n\\(x_{a2}\\)\n\n\n3\n\\(x_{b3}\\)\n\\(x_{a3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{bn}\\)\n\\(x_{an}\\)"
  },
  {
    "objectID": "infer-twomean.html#independent-samples",
    "href": "infer-twomean.html#independent-samples",
    "title": "16  Comparing Two Population Means",
    "section": "16.4 Independent Samples",
    "text": "16.4 Independent Samples\n\n\n\n\nTwo samples are independent if the sample values from one population are not related to the sample values from the other.\n\n Salary samples of men and women. Two samples are drawn independently from the male and female groups. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject 1 of the Group 1 has nothing to do with the Subject 1 of the Group 2.\n\n\n\n\n\n\n\n\n\n\nSubject of Group 1 (Male)\nMeasurement of Group 1\nSubject of Group 2 (Female)\nMeasurement of Group 2\n\n\n\n\n1\n\\(x_{11}\\)\n1\n\\(x_{21}\\)\n\n\n2\n\\(x_{12}\\)\n2\n\\(x_{22}\\)\n\n\n3\n\\(x_{13}\\)\n3\n\\(x_{23}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n_1\\)\n\\(x_{1n_1}\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\\(n_2\\)\n\\(x_{2n_2}\\)"
  },
  {
    "objectID": "infer-twomean.html#inference-from-two-samples",
    "href": "infer-twomean.html#inference-from-two-samples",
    "title": "16  Comparing Two Population Means",
    "section": "16.5 Inference from Two Samples",
    "text": "16.5 Inference from Two Samples\n\n\n\n\nThe statistical methods are different for these two types of samples.\nGood news: The concepts of CI and HT for one population can be applied to two-population cases.\n\\(\\text{CI = point estimate} \\pm \\text{margin of error (E)}\\), e.g., \\(\\overline{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\nMargin of error = critical value \\(\\times\\) standard error of the point estimator\nThe 6 testing steps are the same, and both critical value and \\(p\\)-value method can be applied too, e.g., \\(t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}}\\)"
  },
  {
    "objectID": "infer-twomean.html#hypothesis-testing-for-dependent-samples",
    "href": "infer-twomean.html#hypothesis-testing-for-dependent-samples",
    "title": "16  Comparing Two Population Means",
    "section": "16.6 Hypothesis Testing for Dependent Samples",
    "text": "16.6 Hypothesis Testing for Dependent Samples\n\n\n\n\n\n\nTo analyze a paired data set, simply analyze the differences!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(x_1\\)\n\\(x_2\\)\nDifference \\(d = x_1 - x_2\\)\n\n\n\n\n1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(\\color{red}{d_1}\\)\n\n\n2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(\\color{red}{d_2}\\)\n\n\n3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(\\color{red}{d_3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\color{red}{\\vdots}\\)\n\n\n\\(n\\)\n\\(x_{1n}\\)\n\\(x_{2n}\\)\n\\(\\color{red}{d_n}\\)\n\n\n\n\n\n\n\\(\\mu_d = \\mu_1 - \\mu_2\\)\n \\(\\begin{align} & H_0: \\mu_1 - \\mu_2 = 0 \\iff \\mu_d = 0 \\\\ & H_1: \\mu_1 - \\mu_2 > 0 \\iff \\mu_d > 0 \\\\ & H_1: \\mu_1 - \\mu_2 < 0 \\iff \\mu_d < 0 \\\\ & H_1: \\mu_1 - \\mu_2 \\ne 0 \\iff \\mu_d \\ne 0 \\end{align}\\) \n\n\n\n\n\n\n\n\n\n\nThe point estimate of \\(\\mu_1 - \\mu_2\\) is \\(\\overline{x}_1 - \\overline{x}_2 = \\overline{d}\\)."
  },
  {
    "objectID": "infer-twomean.html#inference-for-paired-data",
    "href": "infer-twomean.html#inference-for-paired-data",
    "title": "16  Comparing Two Population Means",
    "section": "16.7 Inference for Paired Data",
    "text": "16.7 Inference for Paired Data\n\nRequirements: the sample differences \\(\\color{blue}{d_i}\\)s are\n\nrandom sample\nfrom a normal distribution and/or \\(n > 30\\) (tested by QQ-plot of \\(d_i\\)s)\n\nFollow the same procedure as the one-sample \\(t\\)-test!\nThe test statistic is \\(\\color{blue}{t_{test} = \\frac{\\overline{d}-0}{s_d/\\sqrt{n}}} \\sim T_{n-1}\\) under \\(H_0\\) where \\(\\overline{d}\\) and \\(s_d\\) are the mean and SD of the difference samples \\((d_1, d_2, \\dots, d_n)\\).\nThe critical value \\(t_{\\alpha, n-1}\\) and \\(t_{\\alpha/2, n-1}\\).\n\n\n\n\n\n\n\n\n\nPaired \\(t\\)-test\nTest Statistic\nConfidence Interval for \\(\\mu_d = \\mu_1 - \\mu_2\\)\n\n\n\n\n\\(\\sigma_d\\) is unknown\n\\(\\large t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}}\\)\n\\(\\large \\overline{d} \\pm t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\)\n\n\n\n\nThe test from matched pairs is called a paired \\(t\\)-test."
  },
  {
    "objectID": "infer-twomean.html#example",
    "href": "infer-twomean.html#example",
    "title": "16  Comparing Two Population Means",
    "section": "16.8 Example",
    "text": "16.8 Example\n\nConsider a capsule used to reduce blood pressure (BP) for the hypertensive individuals. Sample of 10 hypertensive individuals take the medicine for 4 weeks.\nDoes the data provide sufficient evidence that the treatment is effective in reducing BP?\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\nBefore \\((x_b)\\)\nAfter \\((x_a)\\)\nDifference \\(d = x_b - x_a\\)\n\n\n\n\n1\n143\n124\n19\n\n\n2\n153\n129\n24\n\n\n3\n142\n131\n11\n\n\n4\n139\n145\n-6\n\n\n5\n172\n152\n20\n\n\n6\n176\n150\n26\n\n\n7\n155\n125\n30\n\n\n8\n149\n142\n7\n\n\n9\n140\n145\n-5\n\n\n10\n169\n160\n9\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\overline{d} = 13.5\\), \\(s_d= 12.48\\).\n\\(\\mu_1 =\\) Mean Before, \\(\\mu_2 =\\) Mean After, and \\(\\mu_d = \\mu_1 - \\mu_2\\).\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\iff \\mu_d = 0\\\\ &H_1: \\mu_1 > \\mu_2 \\iff \\mu_d > 0 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}} = \\frac{13.5}{12.48/\\sqrt{10}} = 3.42\\) \nStep 4-c:  \\(t_{\\alpha, n-1} = t_{0.05, 9} = 1.833\\).\nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} > t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = 3.42 > 1.833 = t_{\\alpha, n-1}\\), we reject \\(H_0\\). \n\n\n\nStep 6:  There is sufficient evidence to support the claim that the drug is effective in reducing blood pressure. \n\n\n\n\n\n\n\n\n\n\n\nThe 95% CI for \\(\\mu_d = \\mu_1 - \\mu_2\\) is \\[\\begin{align}\\overline{d} \\pm t_{\\alpha/2, df} \\frac{s_d}{\\sqrt{n}} &= 13.5 \\pm t_{0.025, 9}\\frac{12.48}{\\sqrt{10}}\\\\ &= 13.5 \\pm 8.927 \\\\ &= (4.573, 22.427).\\end{align}\\]\n95% confident that the mean difference in blood pressure is between 4.57 and 22.43.\nSince the interval does NOT include 0, it leads to the same conclusion as rejection of \\(H_0\\)."
  },
  {
    "objectID": "infer-twomean.html#two-sample-paired-test-in-r",
    "href": "infer-twomean.html#two-sample-paired-test-in-r",
    "title": "16  Comparing Two Population Means",
    "section": "16.9 Two-Sample Paired Test in R",
    "text": "16.9 Two-Sample Paired Test in R\n\n\n\npair_data\n\n   before after\n1     143   124\n2     153   129\n3     142   131\n4     139   145\n5     172   152\n6     176   150\n7     155   125\n8     149   142\n9     140   145\n10    169   160\n\n(d <- pair_data$before - pair_data$after)\n\n [1] 19 24 11 -6 20 26 30  7 -5  9\n\n(d_bar <- mean(d))\n\n[1] 13.5\n\n\n\n(s_d <- sd(d))\n\n[1] 12.48332\n\n## t_test\n(t_test <- d_bar/(s_d/sqrt(length(d))))\n\n[1] 3.419823\n\n## t_cv\nqt(p = 0.95, df = length(d) - 1)\n\n[1] 1.833113\n\n## p_value\npt(q = t_test, df = length(d) - 1, \n   lower.tail = FALSE)\n\n[1] 0.003815036"
  },
  {
    "objectID": "infer-twomean.html#two-sample-paired-test-in-r-1",
    "href": "infer-twomean.html#two-sample-paired-test-in-r-1",
    "title": "16  Comparing Two Population Means",
    "section": "16.10 Two-Sample Paired Test in R",
    "text": "16.10 Two-Sample Paired Test in R\n\nd_bar + c(-1, 1) * qt(p = 0.975, df = length(d) - 1) * (s_d / sqrt(length(d))) ## CI\n\n[1]  4.569969 22.430031\n\n\n\n## t.test() function\nt.test(x = pair_data$before, y = pair_data$after,\n       alternative = \"greater\", mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pair_data$before and pair_data$after\nt = 3.4198, df = 9, p-value = 0.003815\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 6.263653      Inf\nsample estimates:\nmean difference \n           13.5 \n\n\n\nBe careful about the one-sided CI! We should use the two-sided CI!"
  },
  {
    "objectID": "infer-twomean.html#compare-population-means-independent-samples",
    "href": "infer-twomean.html#compare-population-means-independent-samples",
    "title": "16  Comparing Two Population Means",
    "section": "16.11 Compare Population Means: Independent Samples",
    "text": "16.11 Compare Population Means: Independent Samples\n\nWhether stem cells can improve heart function.\nThe relationship between pregnant womens’ smoking habits and newborns’ weights.\nWhether one variation of an exam is harder than another variation."
  },
  {
    "objectID": "infer-twomean.html#inferences-from-two-independent-samples-overview",
    "href": "infer-twomean.html#inferences-from-two-independent-samples-overview",
    "title": "16  Comparing Two Population Means",
    "section": "16.12 Inferences from Two Independent Samples: Overview",
    "text": "16.12 Inferences from Two Independent Samples: Overview"
  },
  {
    "objectID": "infer-twomean.html#testing-for-independent-samples-sigma_1-ne-sigma_2",
    "href": "infer-twomean.html#testing-for-independent-samples-sigma_1-ne-sigma_2",
    "title": "16  Comparing Two Population Means",
    "section": "16.13 Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)",
    "text": "16.13 Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\nRequirements:\n\nThe two samples are independent.\nBoth samples are a random sample.\n\\(n_1 > 30\\), \\(n_2 > 30\\) and/or both samples are from a normally distributed population.\n\nInterested in whether the two population means ** \\(\\mu_1\\) and \\(\\mu_2\\) are equal or not **, or one is larger than the other.\n\\(H_0: \\mu_1 = \\mu_2\\)\nIt is equivalent to testing if their difference is zero.\n\\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\n\n\n\n\n\n\n\nWe start with finding a point estimate for \\(\\mu_1 - \\mu_2\\). What is the best point estimator for \\(\\mu_1 - \\mu_2\\)?\n\n\n\n\n\n\n\n\\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\)!"
  },
  {
    "objectID": "infer-twomean.html#sampling-distribution-of-overlinex_1---overlinex_2",
    "href": "infer-twomean.html#sampling-distribution-of-overlinex_1---overlinex_2",
    "title": "16  Comparing Two Population Means",
    "section": "16.14 Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\)",
    "text": "16.14 Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\)\n\nIf the two samples are from independent normally distributed populations or \\(n_1 > 30\\) and \\(n_2 > 30\\), \\[\\small \\overline{X}_1 \\sim N\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1} \\right), \\quad \\overline{X}_2 \\sim N\\left(\\mu_2,\n\\frac{\\sigma_2^2}{n_2} \\right)\\]\n\\(\\overline{X}_1 - \\overline{X}_2\\) has the sampling distribution \\[\\small \\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} \\color{red}{+} \\color{black}{\\frac{\\sigma_2^2}{n_2}} \\right) \\]\n\n\\[\\small Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2",
    "href": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2",
    "title": "16  Comparing Two Population Means",
    "section": "16.15 Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)",
    "text": "16.15 Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\nWith \\(D_0\\) a hypothesized value (often 0), our testing problem is\n\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\le D_0\\\\ &H_1: \\mu_1 - \\mu_2 > D_0 \\end{align}\\)  (right-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\ge D_0\\\\ &H_1: \\mu_1 - \\mu_2 < D_0 \\end{align}\\)  (left-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 = D_0\\\\ &H_1: \\mu_1 - \\mu_2 \\ne D_0 \\end{align}\\)  (two-tailed)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\small \\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\]\nThen find \\(z_{\\alpha}\\) or \\(z_{\\alpha/2}\\) and follow our testing steps!"
  },
  {
    "objectID": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2-1",
    "href": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-ne-sigma_2-1",
    "title": "16  Comparing Two Population Means",
    "section": "16.16 Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)",
    "text": "16.16 Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, the test statistic becomes \\(t_{test}\\):\n\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} \\] \n\nThe critical value \\(t_{\\alpha, df}\\) (one-tailed) and \\(t_{\\alpha/2, df}\\) (two-tailed), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\] where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\).\n\n\n\nIf the \\(df\\) is not an integer, we round it down to an integer."
  },
  {
    "objectID": "infer-twomean.html#inference-from-independent-samples-sigma_1-ne-sigma_2",
    "href": "infer-twomean.html#inference-from-independent-samples-sigma_1-ne-sigma_2",
    "title": "16  Comparing Two Population Means",
    "section": "16.17 Inference from Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)",
    "text": "16.17 Inference from Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\)\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 \\ne \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}\\)\n\n\n\n\n\nUse \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\) to get the \\(p\\)-value, critical value, and CI.\nThe unequal-variance t-test is called Welch’s t-test."
  },
  {
    "objectID": "infer-twomean.html#example-two-sample-t-test",
    "href": "infer-twomean.html#example-two-sample-t-test",
    "title": "16  Comparing Two Population Means",
    "section": "16.18 Example: Two-Sample t-Test",
    "text": "16.18 Example: Two-Sample t-Test\n\n\n\n\nDoes an oversized tennis racket exert less stress/force on the elbow? The data show\n\nOversized: \\(n_1 = 33\\), \\(\\overline{x}_1 = 25.2\\), \\(s_1 = 8.6\\)\nConventional: \\(n_2 = 12\\), \\(\\overline{x}_2 = 33.9\\), \\(s_2 = 17.4\\)\nThe two populations are nearly normal.\nThe large difference in the sample SD suggests \\(\\sigma_1 \\ne \\sigma_2\\).\nForm a hypothesis test with \\(\\alpha = 0.05\\) and construct a 95% CI for the mean difference of force on the elbow.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(25.2 - 33.9) - 0}{\\sqrt{\\frac{\\color{red}{8.6^2}}{33} + \\frac{\\color{red}{17.4^2}}{12}}} = -1.66\\)\n\\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\)\n\\(\\small A = \\dfrac{8.6^2}{33}\\), \\(\\small B = \\dfrac{17.4^2}{12}\\), \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01\\)\n\n\n\n\n\n\n\n\nIf the computed value of \\(df\\) is not an integer, always round down to the nearest integer.\n\n\n\n\n\n\n\nStep 4-c:  \\(-t_{0.05, 13} = -1.771\\). \nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} < -t_{\\alpha, df}\\). \\(\\small t_{test} = -1.66 > -1.771 = -t_{\\alpha, df}\\), we fail to reject \\(H_0\\). \nStep 6:  There is insufficient evidence to support the claim that the the oversized racket delivers less stress to the elbow. \nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is\n\n\\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}} &= (25.2 - 33.9) \\pm t_{0.025,13}\\sqrt{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}}\\\\&= -8.7 \\pm 11.32 = (-20.02, 2.62).\\end{align}\\]\n\nWe are 95% confident that the difference in the mean forces is between -20.02 and 2.62.\nSince the interval includes 0, it leads to the same conclusion as failing to reject \\(H_0\\)."
  },
  {
    "objectID": "infer-twomean.html#two-sample-t-test-in-r",
    "href": "infer-twomean.html#two-sample-t-test-in-r",
    "title": "16  Comparing Two Population Means",
    "section": "16.19 Two-Sample t-Test in R",
    "text": "16.19 Two-Sample t-Test in R\n\nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA <- s1^2 / n1; B <- s2^2 / n2\ndf <- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n(df <- floor(df))\n\n[1] 13\n\n## t_test\n(t_test <- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n\n[1] -1.659894\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.770933\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 0.06042575"
  },
  {
    "objectID": "infer-twomean.html#sampling-distribution-of-overlinex_1---overlinex_2-when-sigma_1-sigma_2-sigma",
    "href": "infer-twomean.html#sampling-distribution-of-overlinex_1---overlinex_2-when-sigma_1-sigma_2-sigma",
    "title": "16  Comparing Two Population Means",
    "section": "16.20 Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) when \\(\\sigma_1 = \\sigma_2 = \\sigma\\)",
    "text": "16.20 Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) when \\(\\sigma_1 = \\sigma_2 = \\sigma\\)\n\\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} \\right) \\] If \\(\\sigma_1 = \\sigma_2 = \\sigma\\), \\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\right) \\] \\[ Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1)\\]"
  },
  {
    "objectID": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma",
    "href": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma",
    "title": "16  Comparing Two Population Means",
    "section": "16.21 Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)",
    "text": "16.21 Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\]\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, similar to the one sample case, we use \\(t_{test}\\).\nAs \\(\\sigma_1 = \\sigma_2 = \\sigma\\), we don’t need two but one sample SD to replace the population SD \\(\\sigma\\).\nUse the pooled sample variance to estimate the common population variance \\(\\sigma^2\\): \\[ s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} \\] which is the weighted average of \\(s_1^2\\) and \\(s_2^2\\)."
  },
  {
    "objectID": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma-1",
    "href": "infer-twomean.html#test-statistic-for-independent-samples-sigma_1-sigma_2-sigma-1",
    "title": "16  Comparing Two Population Means",
    "section": "16.22 Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)",
    "text": "16.22 Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, \\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nHere, the critical value \\(t_{\\alpha, df}\\) (for one-tailed tests) and \\(t_{\\alpha/2, df}\\) (for two-tailed tests), and the \\(t\\) distribution used to compute the \\(p\\)-value have the degrees of freedom \\[df = n_1 + n_2 - 2\\]"
  },
  {
    "objectID": "infer-twomean.html#inference-from-independent-samples-sigma_1-sigma_2-sigma",
    "href": "infer-twomean.html#inference-from-independent-samples-sigma_1-sigma_2-sigma",
    "title": "16  Comparing Two Population Means",
    "section": "16.23 Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)",
    "text": "16.23 Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\)\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 = \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\color{red}{s_p}\\ \\color{black} sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\color{red}{s_p}\\ \\color{black} sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\n\n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\nUse \\(df = n_1+n_2-2\\) get the \\(p\\)-value, critical value and CI.\nThe test from two independent samples with \\(\\sigma_1 = \\sigma_2 = \\sigma\\) is usually called two-sample pooled \\(z\\)-test or two-sample pooled \\(t\\)-test."
  },
  {
    "objectID": "infer-twomean.html#example-weight-loss",
    "href": "infer-twomean.html#example-weight-loss",
    "title": "16  Comparing Two Population Means",
    "section": "16.24 Example: Weight Loss",
    "text": "16.24 Example: Weight Loss\n\n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\nIs there a sufficient evidence at \\(\\alpha = 0.05\\) to conclude that the program is effective?\nIf yes, construct a 95% CI for \\(\\mu_1 - \\mu_2\\) to show how much effective it is.\nStep 1:  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}\\) \nStep 2:  \\(\\alpha = 0.05\\) \nStep 3:  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\). \n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(10-1)0.5^2 + (10-1)0.7^2}{10+10-2}}=0.6083\\)\n \\(t_{test} = \\frac{(2.1 - 4.2) - 0}{0.6083\\sqrt{\\frac{1}{10} + \\frac{1}{10}}} = -7.72\\)\nStep 4-c:  \\(df = n_1 + n_2 - 2 = 10 + 10 - 2 = 18\\). So \\(-t_{0.05, df = 18} = -1.734\\). \nStep 5-c:  We reject \\(H_0\\) if \\(\\small t_{test} < -t_{\\alpha, df}\\). Since \\(\\small t_{test} = -7.72 < -1.734 = -t_{\\alpha, df}\\), we reject \\(H_0\\).\nStep 4-p:  The \\(p\\)-value is \\(P(T_{df=18} < t_{test}) \\approx 0\\) \nStep 5-p:  We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(\\approx 0 < 0.05 = \\alpha\\), we reject \\(H_0\\).\nStep 6:  There is sufficient evidence to support the claim that the weight loss program is effective."
  },
  {
    "objectID": "infer-twomean.html#example-contd",
    "href": "infer-twomean.html#example-contd",
    "title": "16  Comparing Two Population Means",
    "section": "16.25 Example Cont’d",
    "text": "16.25 Example Cont’d\n\nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is \\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\color{red}{s_p}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} &= (2.1 - 4.2) \\pm t_{0.025, 18} (0.6083)\\sqrt{\\frac{1}{10} + \\frac{1}{10}}\\\\ &= -2.1 \\pm 0.572 = (-2.672, -1.528) \\end{align}\\]\nWe are 95% confident that the difference in the mean weight is between -2.672 and -1.528.\nSince the interval does not include 0, it leads to the same conclusion as rejection of \\(H_0\\)."
  },
  {
    "objectID": "infer-twomean.html#two-sample-pooled-t-test-in-r",
    "href": "infer-twomean.html#two-sample-pooled-t-test-in-r",
    "title": "16  Comparing Two Population Means",
    "section": "16.26 Two-Sample Pooled t-Test in R",
    "text": "16.26 Two-Sample Pooled t-Test in R\n\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\nsp <- sqrt(((n1 - 1) * s1 ^ 2 + (n2 - 1) * s2 ^ 2) / (n1 + n2 - 2))\nsp\n\n[1] 0.6082763\n\ndf <- n1 + n2 - 2\n## t_test\n(t_test <- (x1_bar - x2_bar) / (sp * sqrt(1 / n1 + 1 / n2)))\n\n[1] -7.719754\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.734064\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 2.028505e-07"
  },
  {
    "objectID": "infer-ci.html#why-inference-for-population-variances",
    "href": "infer-ci.html#why-inference-for-population-variances",
    "title": "13  Confidence Interval",
    "section": "13.28 Why Inference for Population Variances?",
    "text": "13.28 Why Inference for Population Variances?\n\nWe want to know if \\(\\sigma_1 = \\sigma_2\\), so a correct or better method can be used.\n\n\n\n\n\n\n\nWhich test we learned needs \\(\\sigma_1 = \\sigma_2\\)?\n\n\n\n\n\n\n\nIn some situations, we care about variation!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n The variation in potency of drugs: affects patients’ health\n\n\n The variance of stock prices : the higher the variance, the riskier the investment"
  },
  {
    "objectID": "infer-ci.html#inference-for-population-variances",
    "href": "infer-ci.html#inference-for-population-variances",
    "title": "13  Confidence Interval",
    "section": "13.29 Inference for Population Variances",
    "text": "13.29 Inference for Population Variances\n\nThe sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance \\(\\sigma^2\\).\n\n\\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\), i.e., \\(E(S^2) = \\sigma^2\\)\n\nThe inference methods for \\(\\sigma^2\\) needs the population to be normal.\n\n\n\n\n\n\n\n❗ The methods can work poorly if the normality is violated, even the sample is large."
  },
  {
    "objectID": "infer-ci.html#chi-square-chi2-distribution",
    "href": "infer-ci.html#chi-square-chi2-distribution",
    "title": "13  Confidence Interval",
    "section": "13.30 Chi-Square \\(\\chi^2\\) Distribution",
    "text": "13.30 Chi-Square \\(\\chi^2\\) Distribution\nThe inference for \\(\\sigma^2\\) involves the so called \\(\\chi^2\\) distribution.\n\n\n\n\nParameter: degrees of freedom \\(df\\)\nRight skewed distribution\nDefined over positive numbers\nMore symmetric as \\(df\\) gets larger\nChi-Square Distribution"
  },
  {
    "objectID": "infer-ci.html#upper-tail-and-lower-tail-of-chi-square",
    "href": "infer-ci.html#upper-tail-and-lower-tail-of-chi-square",
    "title": "13  Confidence Interval",
    "section": "13.31 Upper Tail and Lower Tail of Chi-Square",
    "text": "13.31 Upper Tail and Lower Tail of Chi-Square\n\n\n\\(\\chi^2_{\\frac{\\alpha}{2},\\, df}\\) has area to the right of \\(\\alpha/2\\).\n\\(\\chi^2_{1-\\frac{\\alpha}{2},\\, df}\\) has area to the left of \\(\\alpha/2\\).\nIn \\(N(0, 1)\\), \\(z_{1-\\frac{\\alpha}{2}} = -z_{\\frac{\\alpha}{2}}\\). But \\(\\chi^2_{1-\\frac{\\alpha}{2},\\,df} \\ne -\\chi^2_{\\frac{\\alpha}{2},\\,df}\\)."
  },
  {
    "objectID": "infer-ci.html#sampling-distribution",
    "href": "infer-ci.html#sampling-distribution",
    "title": "13  Confidence Interval",
    "section": "13.32 Sampling Distribution",
    "text": "13.32 Sampling Distribution\n\nWhen a random sample of size \\(n\\) is from \\(\\color{red}{N(\\mu, \\sigma^2)}\\), \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\]\n\n\n\nThe inference method for \\(\\sigma^2\\) introduced here can work poorly if the normality assumption is violated, even for large samples."
  },
  {
    "objectID": "infer-ci.html#alpha100-confidence-interval-for-sigma2",
    "href": "infer-ci.html#alpha100-confidence-interval-for-sigma2",
    "title": "13  Confidence Interval",
    "section": "13.33 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)",
    "text": "13.33 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} \\right)}}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❗ The CI for \\(\\sigma^2\\) cannot be expressed as \\((S^2-m, S^2+m)\\) anymore!"
  },
  {
    "objectID": "infer-ci.html#example-supermodel-heights",
    "href": "infer-ci.html#example-supermodel-heights",
    "title": "13  Confidence Interval",
    "section": "13.34 Example: Supermodel Heights",
    "text": "13.34 Example: Supermodel Heights\nListed below are heights (cm) for the simple random sample of 16 female supermodels:\n\nheights <- c(178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176)\n\n\n\n\n\nThe supermodels’ height is normally distributed.\nConstruct a \\(95\\%\\) confidence interval for population standard deviation \\(\\sigma\\).\n\\(n = 16\\), \\(s^2 = 3.4\\), \\(\\alpha = 0.05\\).\n\\(\\chi^2_{\\alpha/2, n-1} = \\chi^2_{0.025, 15} = 27.49\\)\n\\(\\chi^2_{1-\\alpha/2, n-1} = \\chi^2_{0.975, 15} = 6.26\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe \\(95\\%\\) CI for \\(\\sigma\\) is \\(\\small \\left( \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}}} \\right) = \\left( \\sqrt{\\frac{(16-1)(3.4)}{27.49}}, \\sqrt{\\frac{(16-1)(3.4)}{6.26}}\\right) = (1.36, 2.85)\\)"
  },
  {
    "objectID": "infer-ci.html#example-computation-in-r",
    "href": "infer-ci.html#example-computation-in-r",
    "title": "13  Confidence Interval",
    "section": "13.35 Example: Computation in R",
    "text": "13.35 Example: Computation in R\n\nn <- 16\ns2 <- var(heights)\nalpha <- 0.05\n\n## two chi-square critical values\nchi2_right <- qchisq(alpha/2, n - 1, lower.tail = FALSE)\nchi2_left <- qchisq(alpha/2, n - 1, lower.tail = TRUE)\n\n## two bounds of CI for sigma2\nci_sig2_lower <- (n - 1) * s2 / chi2_right\nci_sig2_upper <- (n - 1) * s2 / chi2_left\n\n## two bounds of CI for sigma\n(ci_sig_lower <- sqrt(ci_sig2_lower))\n\n[1] 1.362104\n\n(ci_sig_upper <- sqrt(ci_sig2_upper))\n\n[1] 2.853802"
  },
  {
    "objectID": "infer-ci.html#example-contd-testing",
    "href": "infer-ci.html#example-contd-testing",
    "title": "13  Confidence Interval",
    "section": "13.36 Example Cont’d: Testing",
    "text": "13.36 Example Cont’d: Testing\nUse \\(\\alpha = 0.05\\) to test the claim that “supermodels have heights with a standard deviation that is less than \\(\\sigma = 7.5\\) cm for the population of women”.\n\nStep 1: \\(H_0: \\sigma = \\sigma_0\\) vs. \\(H_1: \\sigma < \\sigma_0\\). Here \\(\\sigma_0 = 7.5\\) cm\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Under \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), a statistic drawn from \\(\\chi^2_{n-1}\\).\n\n\n\n\n\nStep 4-c: This is a left-tailed test. The critical value is \\(\\chi_{1-\\alpha, df}^2 = \\chi_{0.95, 15}^2 = 7.26\\)\nStep-5-c: Reject \\(H_0\\) in favor of \\(H_1\\) if \\(\\chi_{test}^2 < \\chi_{1-\\alpha, df}^2\\). Since \\(0.91 < 7.26\\), we reject \\(H_0\\).\nStep 6: There is sufficient evidence to support the claim that supermodels have heights with a SD that is less than the SD for the population of women.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeights of supermodels vary less than heights of women in the general population."
  },
  {
    "objectID": "infer-ci.html#back-to-pooled-t-test",
    "href": "infer-ci.html#back-to-pooled-t-test",
    "title": "13  Confidence Interval",
    "section": "13.37 Back to Pooled t-Test",
    "text": "13.37 Back to Pooled t-Test\n\nIn a pooled t-test, we assume\n\n \\(n_1 \\ge 30\\) and \\(n_2 \\ge 30\\). If not, we assume that both samples are drawn from normal populations. \n \\(\\sigma_1 = \\sigma_2\\) \n\nUse QQ-plot (and normality tests, Anderson, Shapiro, etc) to check the assumption of normal distribution.\nWe learn to check the assumption \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "infer-ci.html#f-distribution",
    "href": "infer-ci.html#f-distribution",
    "title": "13  Confidence Interval",
    "section": "13.38 F Distribution",
    "text": "13.38 F Distribution\n\nWe use ** \\(F\\) distribution ** for the inference about two population variances.\n\n\n\n\nTwo parameters: \\(df_1\\), \\(df_2\\)\nRight skewed distribution\nDefined over positive numbers\nR Shiny app: F Distribution"
  },
  {
    "objectID": "infer-ci.html#upper-and-lower-tail-of-f-distribution",
    "href": "infer-ci.html#upper-and-lower-tail-of-f-distribution",
    "title": "13  Confidence Interval",
    "section": "13.39 Upper and Lower Tail of F Distribution",
    "text": "13.39 Upper and Lower Tail of F Distribution\n\nWe denote \\(F_{\\alpha, \\, df_1, \\, df_2}\\) as the \\(F\\) quantile so that \\(P(F_{df_1, df_2} > F_{\\alpha, \\, df_1, \\, df_2}) = \\alpha\\)."
  },
  {
    "objectID": "infer-ci.html#sampling-distribution-1",
    "href": "infer-ci.html#sampling-distribution-1",
    "title": "13  Confidence Interval",
    "section": "13.40 Sampling Distribution",
    "text": "13.40 Sampling Distribution\n\n\nWhen random samples of sizes \\(n_1\\) and \\(n_2\\) have been independently drawn from two normally distributed populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\), the ratio \\[\\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]"
  },
  {
    "objectID": "infer-ci.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "href": "infer-ci.html#alpha100-confidence-interval-for-sigma_12-sigma_22",
    "title": "13  Confidence Interval",
    "section": "13.41 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)",
    "text": "13.41 \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\color{blue}{\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, \\, n_1 - 1, \\, n_2 - 1}} \\right)}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n❗ The CI for \\(\\sigma_1^2 / \\sigma_2^2\\) cannot be expressed as \\(\\left(\\frac{s_1^2}{s_2^2}-m, \\frac{s_1^2}{s_2^2} + m\\right)\\) anymore!"
  },
  {
    "objectID": "infer-ci.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "href": "infer-ci.html#f-test-for-comparing-sigma_12-and-sigma_22",
    "title": "13  Confidence Interval",
    "section": "13.42 F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)",
    "text": "13.42 F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\)\n\nStep 1: right-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 \\le \\sigma_2 \\\\ &H_1: \\sigma_1 > \\sigma_2 \\end{align}\\)  and two-tailed  \\(\\small \\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\) \nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Under \\(H_0\\), \\(\\sigma_1 = \\sigma_2\\), and the test statistic is \\[\\small F_{test} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} = \\frac{s_1^2}{s_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\nStep 4-c:\n\nRight-tailed:  \\(F_{\\alpha, \\, n_1-1, \\, n_2-1}\\) .\nTwo-tailed:  \\(F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\) \n\nStep 5-c:\n\nRight-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha, \\, n_1-1, \\, n_2-1}\\).\nTwo-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{test} \\le F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\)"
  },
  {
    "objectID": "infer-ci.html#back-to-the-weight-loss-example",
    "href": "infer-ci.html#back-to-the-weight-loss-example",
    "title": "13  Confidence Interval",
    "section": "13.43 Back to the Weight Loss Example",
    "text": "13.43 Back to the Weight Loss Example\n\n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\nAssumptions:\n\n \\(\\sigma_1 = \\sigma_2\\) \nThe weight loss for both groups are normally distributed."
  },
  {
    "objectID": "infer-ci.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "href": "infer-ci.html#back-to-the-weight-loss-example-check-if-sigma_1-sigma_2",
    "title": "13  Confidence Interval",
    "section": "13.44 Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)",
    "text": "13.44 Back to the Weight Loss Example: Check if \\(\\sigma_1 = \\sigma_2\\)\n\n\n\n\n\\(n_1 = 10\\), \\(s_1 = 0.5 \\, lb\\)\n\\(n_2 = 10\\), \\(s_2 = 0.7 \\, lb\\)\nStep 1: \\(\\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\)\nStep 2: \\(\\alpha = 0.05\\)\nStep 3: The test statistic is \\(F_{test} = \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nStep 4-c: This is a two-tailed test, the critical value is \\(F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\) or \\(F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\).\nStep 5-c: Is \\(F_{test} > 4.03\\) or \\(F_{test} < 0.25\\)? No. \nStep 6: The evidence is not sufficient to reject the claim that \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "infer-ci.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "href": "infer-ci.html#back-to-the-weight-loss-example-95-ci-for-sigma_12-sigma_22",
    "title": "13  Confidence Interval",
    "section": "13.45 Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)",
    "text": "13.45 Back to the Weight Loss Example: 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\)\n\n\n\n\n\\(\\small F_{\\alpha/2, \\, df_1, \\, df_2} = F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\)\n\\(\\small F_{1-\\alpha/2, \\, df_1, \\, df_2} = F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\)\n\\(\\small \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\)\nThe 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\small \\begin{align} &\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, df_1, \\, df_2}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, df_1, \\, df_2}} \\right) \\\\ &= \\left( \\frac{0.51}{4.03}, \\frac{0.51}{0.25} \\right) = \\left(0.127, 2.04\\right)\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe are 95% confident that the ratio \\(\\sigma_1^2 / \\sigma_2^2\\) is between 0.127 and 2.04."
  },
  {
    "objectID": "infer-ci.html#implementing-f-test-in-r",
    "href": "infer-ci.html#implementing-f-test-in-r",
    "title": "13  Confidence Interval",
    "section": "13.46 Implementing F-test in R",
    "text": "13.46 Implementing F-test in R\n\n\n\n\nn1 <- 10; n2 <- 10\ns1 <- 0.5; s2 <- 0.7\nalpha <- 0.05\n\n## 95% CI for sigma_1^2 / sigma_2^2\nf_small <- qf(p = alpha / 2, \n              df1 = n1 - 1, df2 = n2 - 1, \n              lower.tail = TRUE)\nf_big <- qf(p = alpha / 2, \n            df1 = n1 - 1, df2 = n2 - 1, \n            lower.tail = FALSE)\n\n## lower bound\n(s1 ^ 2 / s2 ^ 2) / f_big\n\n[1] 0.1267275\n\n## upper bound\n(s1 ^ 2 / s2 ^ 2) / f_small\n\n[1] 2.054079\n\n\n\n## Testing sigma_1 = sigma_2\n(test_stats <- s1 ^ 2 / s2 ^ 2)\n\n[1] 0.5102041\n\n(cri_val_big <- qf(p = alpha/2, \n                   df1 = n1 - 1, \n                   df2 = n2 - 1, \n                   lower.tail = FALSE))\n\n[1] 4.025994\n\n(cri_val_small <- qf(p = alpha/2, \n                     df1 = n1 - 1, \n                     df2 = n2 - 1, \n                     lower.tail = TRUE))\n\n[1] 0.2483859\n\n# var.test(x, y, alternative = \"two.sided\")"
  },
  {
    "objectID": "model-anova.html#comparing-more-than-two-population-means",
    "href": "model-anova.html#comparing-more-than-two-population-means",
    "title": "23  Analysis of Variance",
    "section": "23.1 Comparing More Than Two Population Means",
    "text": "23.1 Comparing More Than Two Population Means\n\nIn many research settings, we’d like to compare 3 or more population means.\n\n\n\n\n 4 types of devices used to determine the pH of soil samples.   Determine whether there are differences in the mean readings of those 4 devices. \n\n\n Do different treatments (None, Fertilizer, Irrigation, Fertilizer and Irrigation) affect the mean weights of poplar trees?"
  },
  {
    "objectID": "model-anova.html#one-way-analysis-of-variance",
    "href": "model-anova.html#one-way-analysis-of-variance",
    "title": "23  Analysis of Variance",
    "section": "23.2 One-Way Analysis of Variance",
    "text": "23.2 One-Way Analysis of Variance\n\nA factor is a property or characteristic (categorical variable) that allows us to distinguish the different populations from one another.\n\nType of devices and treatment of trees are factors.\nOne-way ANOVA examines the effect of a categorical variable on the mean of a numerical variable (response).\nWe use analysis of  variance  to test the equality of 3 or more population  means. 🤔\nThe method is one-way because we use one single property (categorical variable) for categorizing the populations."
  },
  {
    "objectID": "model-anova.html#requirements-of-one-way-anova",
    "href": "model-anova.html#requirements-of-one-way-anova",
    "title": "23  Analysis of Variance",
    "section": "23.3 Requirements of One-Way ANOVA",
    "text": "23.3 Requirements of One-Way ANOVA\n\nThe populations of each category are normally distributed.\nThe populations have the same variance \\(\\sigma^2\\) (two sample pooled \\(t\\)-test).\nThe samples are random samples.\nThe samples are independent of each other. (not matched or paired in any way)"
  },
  {
    "objectID": "model-anova.html#rationale-for-anova",
    "href": "model-anova.html#rationale-for-anova",
    "title": "23  Analysis of Variance",
    "section": "23.4 Rationale for ANOVA",
    "text": "23.4 Rationale for ANOVA\n\nData 1 and Data 2 have the same group sample means \\(\\bar{y}_1\\), \\(\\bar{y}_2\\) and \\(\\bar{y}_3\\) denoted as red dots.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhich data you are more confident to say the population means \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not all the same?"
  },
  {
    "objectID": "model-anova.html#variation-between-samples-variation-within-samples",
    "href": "model-anova.html#variation-between-samples-variation-within-samples",
    "title": "23  Analysis of Variance",
    "section": "23.5 Variation Between Samples & Variation Within Samples",
    "text": "23.5 Variation Between Samples & Variation Within Samples\n\nData 1: Variability between samples is large in comparison to the variation within samples.\nData 2: Variation between samples is small relatively to the variation within samples.\n\n\n\n\n\n\n\nMore confident to conclude there is a difference in population means when variation between samples is relatively larger than variation within samples."
  },
  {
    "objectID": "model-anova.html#procedure-of-anova",
    "href": "model-anova.html#procedure-of-anova",
    "title": "23  Analysis of Variance",
    "section": "23.6 Procedure of ANOVA",
    "text": "23.6 Procedure of ANOVA\n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\\\ &H_1: \\text{Population means are not all equal} \\end{align}\\) \n\n\n\nStatistician Ronald Fisher found a way to define a variable that follows the \\(F\\) distribution: \\[\\frac{\\text{variance between samples}}{\\text{variance within samples}} \\sim F_{df_B,\\, df_W}\\]\nIf variance between samples is larger than variance within samples, i.e., \\(F_{test}\\) is much greater than 1, as Data 1, we reject \\(H_0\\).\n\n\n\n\n\n\n\nKey: Define variance between samples and variance within samples so that the ratio is \\(F\\) distributed."
  },
  {
    "objectID": "model-anova.html#variance-within-samples",
    "href": "model-anova.html#variance-within-samples",
    "title": "23  Analysis of Variance",
    "section": "23.7 Variance Within Samples",
    "text": "23.7 Variance Within Samples\n\nBack to two-sample pooled \\(t\\)-test with equal variance \\(\\sigma^2\\). We have \\[s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\\]\n\n\n\n\n\n\n\nHow about general \\(k\\) samples?\n\n\n\n\n\n\n\nANOVA assumes the populations have the same variance \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2 = \\sigma^2\\). \\[\\boxed{s_W^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2}{n_1 + n_2 + \\cdots + n_k - k}}\\] where \\(s_i^2\\), \\(i = 1, \\dots ,k\\), is the sample variance of group \\(i\\).\n\\(s_W^2\\) represents a combined estimate of the common variance \\(\\sigma^2\\). It measures variability of the observations within the \\(k\\) populations."
  },
  {
    "objectID": "model-anova.html#variance-between-samples",
    "href": "model-anova.html#variance-between-samples",
    "title": "23  Analysis of Variance",
    "section": "23.8 Variance Between Samples",
    "text": "23.8 Variance Between Samples\n\\[\\boxed{s^2_{B} = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^2}{k-1}}\\]\n\n\\(\\bar{y}_{i\\cdot}\\) is the \\(i\\)-th sample mean.\n\\(\\bar{y}_{\\cdot\\cdot}\\) is the grand sample mean with all data points in all groups combined.\n\\(s^2_{B}\\) is also an estimate of \\(\\sigma^2\\) and measures variability among sample means for the \\(k\\) groups.\nIf \\(H_0\\) is true \\((\\mu_1 = \\cdots = \\mu_k = \\mu)\\), any variation in the sample means is due to chance and randomness, and shouldn’t be too large.\n\n\\(\\bar{y}_{1\\cdot}, \\cdots, \\bar{y}_{k\\cdot}\\) should be close each other, and they are close to \\(\\bar{y}_{\\cdot \\cdot}\\)."
  },
  {
    "objectID": "model-anova.html#anova-table-sum-of-squares",
    "href": "model-anova.html#anova-table-sum-of-squares",
    "title": "23  Analysis of Variance",
    "section": "23.9 ANOVA Table: Sum of Squares",
    "text": "23.9 ANOVA Table: Sum of Squares\n\nTotal Sum of Squares (SST) measures total variation around \\(\\bar{y}_{\\cdot\\cdot}\\) in all of the sample data combined (ignoring the groups): \\[\\scriptsize{\\color{blue}{SST = \\sum_{j=1}^{n_i}\\sum_{i=1}^{k} \\left(y_{ij} - \\bar{y}_{\\cdot\\cdot}\\right)^2}}\\] where \\(y_{ij}\\) is the \\(j\\)-th data point in the \\(i\\)-th group.\nSum of Squares Between Samples (SSB) measures the variation between sample means: \\[\\scriptsize{ \\color{blue}{SSB = \\sum_{i=1}^{k}n_i \\left(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}\\right)^2}}\\]\nSum of Squares Within Samples (SSW) measures the variation of an value \\(y_{ij}\\) about its sample mean \\(\\bar{y}_{i\\cdot}\\): \\[\\scriptsize{ \\color{blue}{SSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\left(y_{ij} - \\bar{y}_{i\\cdot}\\right)^2 = \\sum_{i=1}^{k} (n_i - 1)s_i^2}}\\]"
  },
  {
    "objectID": "model-anova.html#sum-of-squares-identity",
    "href": "model-anova.html#sum-of-squares-identity",
    "title": "23  Analysis of Variance",
    "section": "23.10 Sum of Squares Identity",
    "text": "23.10 Sum of Squares Identity\n\n\n\n\\(SST = SSB + SSW\\)\n\\(df_{T} = df_{B} + df_{W} \\implies N - 1 = (k-1) + (N - k)\\) \n\\(\\text{Mean Square (MS)} = \\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\)\n\\(MSB = \\frac{SSB}{k-1} = s^2_{B}\\)\n\\(MSW = \\frac{SSW}{N-k} = s^2_{W}\\)\n\\(F_{test} = \\frac{MSB}{MSW}\\)\nUnder \\(H_0\\), \\(\\frac{S^2_{B}}{S_W^2} \\sim F_{k-1, \\, N-k}\\)\nReject \\(H_0\\) if\n\n\\(F_{test} > F_{\\alpha, \\, k - 1,\\, N-k}\\)\n\\(p\\)-value \\(P(F_{k - 1,\\, N-k} > F_{test}) < \\alpha\\)"
  },
  {
    "objectID": "model-anova.html#anova-table",
    "href": "model-anova.html#anova-table",
    "title": "23  Analysis of Variance",
    "section": "23.11 ANOVA Table",
    "text": "23.11 ANOVA Table"
  },
  {
    "objectID": "model-anova.html#example",
    "href": "model-anova.html#example",
    "title": "23  Analysis of Variance",
    "section": "23.12 Example",
    "text": "23.12 Example\n\nA hypothesis is that a nutrient “Isoflavones” varies among three types of food: (1) cereals and snacks, (2) energy bars, and (3) veggie burgers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sample of 5 each is taken and the amount of isoflavones is measured.\nIs there a sufficient evidence to conclude that the mean isoflavone levels vary among these food items? \\(\\alpha = 0.05\\)."
  },
  {
    "objectID": "model-anova.html#example---data",
    "href": "model-anova.html#example---data",
    "title": "23  Analysis of Variance",
    "section": "23.13 Example - Data",
    "text": "23.13 Example - Data\n\n\n\n\n\n\n\n\n\ndata\n\n   1  2  3\n1  3 19 25\n2 17 10 15\n3 12  9 12\n4 10  7  9\n5  4  5  8\n\n\n\nWe prefer data format like\n\n\n\n\n\n\n\ndata_anova\n\n    y    food\n1   3 cereals\n2  17 cereals\n3  12 cereals\n4  10 cereals\n5   4 cereals\n6  19  energy\n7  10  energy\n8   9  energy\n9   7  energy\n10  5  energy\n11 25  veggie\n12 15  veggie\n13 12  veggie\n14  9  veggie\n15  8  veggie\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo tell me what is the value of \\(y_{23}\\)!"
  }
]