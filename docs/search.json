[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Statistics",
    "section": "",
    "text": "Welcome\nThis is the website for my introductory statistics book. Currently this book serves as a main reference book for my MATH 4720 Statistical Methods and MATH 4740 Biostatistical Methods classes at Marquette University.1 Some topics can also be discussed in an introductory data science, regression, or other applied statistics courses. You‚Äôll learn basic probability and statistical concepts as well as data analysis techniques such as linear regression using R2 and Python.\nThe book balances the following aspects of statistics:\nThe materials in this book are adapted and modified from several existing statistics books, course websites, notes, and tutorials for best teaching and learning experience at Marquette. Main reference books are:",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Introduction to Statistics",
    "section": "",
    "text": "mathematical formulation and statistical computation\ndistribution-based and simulation-based inferences\nmethodology and applications\nclassical/frequentist and Bayesian approaches\n\n\n\nOpenIntro Statsitics (OS) (data focused)\nIntroduction to Modern Statistics (IMS) (simulation-based and computation focused)\nAn Introduction to Statistical Methods & Data Analysis, 7th edition (SMD) (distribution-based and mathematics focused)3",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Statistics",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License. If you‚Äôd like to give back, please consider reporting a typo or leaving a pull request at github.com/chenghanyustats/introstatsbook.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction to Statistics",
    "section": "",
    "text": "For some historical reason, the two courses are numbered as senior level courses. However, both are introductory courses mainly taken by undergraduate students in STEM fields.‚Ü©Ô∏é\nThis book uses base R (the default R syntax) rather than tidyverse R, the meta package ecosystem I teach in my MATH/COSC 3570 Introduction to Data Science course. While tidyverse offers a unified approach to data science and is becoming increasingly popular, I was somewhat persuaded by Prof.¬†Matloff‚Äôs TidyverseSkeptic post and believe there is value in learning and teaching base R in an introductory statistics course. To be clear, both base R and tidyverse are excellent tools (I use both in my research and teaching). Being proficient in both approaches can only enhance your data science skills.‚Ü©Ô∏é\nThis book has been served as the official textbook for MATH 4720 for years. However, I feel that this book is more suitable for senior undergraduate students or graduate students.‚Ü©Ô∏é",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html#footnotes",
    "href": "intro.html#footnotes",
    "title": "Statistics and Data",
    "section": "",
    "text": "https://en.wikipedia.org/wiki/History_of_statistics‚Ü©Ô∏é\nhttps://en.wikipedia.org/wiki/Statistical_inference‚Ü©Ô∏é",
    "crumbs": [
      "Statistics and Data"
    ]
  },
  {
    "objectID": "intro-stats.html",
    "href": "intro-stats.html",
    "title": "1¬† Science of Data and Data Science",
    "section": "",
    "text": "1.1 What is Statistics?\nThe first question we ask in this book is ‚ÄúWhat is Statistics?‚Äù\nStatistics can be defined in a variety of ways, and there doesn‚Äôt seem to be one definition that describes it best. For our purposes, statistics can be generally divided into two overarching categories: - Statistics as a set of numeric records - Statistics as a discipline\nStatistics as a Set of Numeric Records\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records. For example, Figure¬†1.1 below shows Michael Jordan‚Äôs career statistics from his time in the NBA. However, this is just one way of defining statistics.\nFigure¬†1.1: Example of statistics as a set of numeric records. Source: https://www.nba.com/stats/player/893/career\nStatistics as a Discipline\nAs previously stated, other definitions of statistics exist including the one shown in Figure¬†1.2.\nFigure¬†1.2: Statistics Shirt. Source: shorturl.at/vEMNS\nThis definition emphasizes the idea that with the same data, different statistical methods may produce different results and lead to different conclusions. This is true in some sense, and later we‚Äôll see why in the book.\nForget about that useless definition. Wiki lists a more formal definition of statistics in Figure¬†1.3 below.\nFigure¬†1.3: More formal definition of statistics. Source: https://en.wikipedia.org/wiki/Statistics\nStatistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. Without doubt, Statistics is a discipline dealing with data. Being viewed as a field or branch of mathematics, Statistics is a Science of Data. I am not saying statistics is THE science of data. There might be another science of data. Statistics is a science of data that uses statistical thinking, methods and models.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Science of Data and Data Science</span>"
    ]
  },
  {
    "objectID": "intro-stats.html#what-is-statistics",
    "href": "intro-stats.html#what-is-statistics",
    "title": "1¬† Science of Data and Data Science",
    "section": "",
    "text": "ü§î But wait, if statistics is a science of data, then what is DATA SCIENCE‚ùì",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Science of Data and Data Science</span>"
    ]
  },
  {
    "objectID": "intro-stats.html#difference-between-statistics-and-data-science",
    "href": "intro-stats.html#difference-between-statistics-and-data-science",
    "title": "1¬† Science of Data and Data Science",
    "section": "\n1.2 Difference between Statistics and Data Science",
    "text": "1.2 Difference between Statistics and Data Science\n\n Data Science \nBecause of their shared attributes, many find it hard to differentiate between statistics and data science. The tweets below poke fun at the lack of clarity surrounding the definition of data science/data scientists (Figure¬†1.6).\n\n\n\n\n\nFigure¬†1.4\n\n\n\n\n\n\n\nFigure¬†1.5\n\n\n\n\n\n\n\nFigure¬†1.6: Tweets about what Data Science is\n\n\nA more formal definition of data science can be found on Investopedia. This site defines Data Science as a field of applied mathematics and statistics that provides useful information based on large amounts of complex data or big data. Although this definition is helpful for understanding data science, Dan Ariely, a famous behavioral economist at Duke, joked about their use of the term big data in his tweet below (Figure¬†1.7).\n\n\n\n\n\nFigure¬†1.7: Professor Ariely on Big Data\n\n\nMore information can be gathered about the differences between these two fields from looking at the courses offered in the Statistics Department at UC Santa Cruz, my alma mater. From Figure¬†1.8 below, one can see that statistics primarily focuses on data analysis, methods and models.\n\n\n\n\n\n\n\n\nFigure¬†1.8: Courses offered by the Department of Statistics at UC Santa Cruz. Source: https://courses.engineering.ucsc.edu/courses/department/24\n\n\n\n\nThis statistics department, in particular, doesn‚Äôt talk a lot about data collection, organization, data presentation or data visualization. In typical statistics departments, there isn‚Äôt much instruction or research done on data collection, cleaning, storage, database management, and data visualization. Because statistics continues to focus on data analysis and modeling, Data Science now addresses these other processes that statistics passes over. The data science process includes the collection, organization, analysis, interpretation and presentation of data (Figure¬†1.9). Although statistics does not focus on these concepts, they are encompassed within the field of data science.\n\n\n\n\n\n\n\nFigure¬†1.9: The data science process created at Harvard by Joe Blitzstein and Hanspeter Pfister",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Science of Data and Data Science</span>"
    ]
  },
  {
    "objectID": "intro-stats.html#what-will-we-learn-in-this-course",
    "href": "intro-stats.html#what-will-we-learn-in-this-course",
    "title": "1¬† Science of Data and Data Science",
    "section": "\n1.3 What Will We Learn In this Course?",
    "text": "1.3 What Will We Learn In this Course?\nBelow the main topics of this book are listed in the order in which they will be covered.\n\n\n\n\n\n\n\n\nWe do touch data collection and data visualization and data summary, but we will spend most of our time talking about probability and statistical inference methods that are circled on the list above. This book focuses on the statistical methods for analyzing data.\nIn summary, we will learn useful information\n\nabout the population we are interested in\nfrom our sample data\nthrough statistical inferential methods, including estimation and testing\n\n\n\n\n\n\n\n\nFigure¬†1.10: Illustration of obtaining sample data from a population\n\n\n\n\nDon‚Äôt worry if you have no idea of these terms. These are what we will discuss throughout the book, and I‚Äôll explain each term in detail.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Science of Data and Data Science</span>"
    ]
  },
  {
    "objectID": "intro-data.html",
    "href": "intro-data.html",
    "title": "2¬† Data Collection and Data Type",
    "section": "",
    "text": "2.1 Data\nWhat is Data?\nBecause statistics is a science of data, we first understand what data is. Data can be described as a set of objects on which we observe or measure one or more characteristics. Objects are individuals, observations, subjects or cases in statistical studies. For example, if you are studying the Marquette students‚Äô heights and weights, your objects will be human beings, specifically the Marquette students. If you are interested in SUV car price, your subjects will be cars. A characteristic or attribute is also called a variable because it varies from one object to another. Your car data set may contain 10 car objects and 5 different characteristics or variables associated with each object, such as color, brand, weight, price, etc. Each car has its own value of those variables.\nWe usually store a data set in a matrix form that has rows and columns. Sometimes we call such data set data matrix or data frame. Each row corresponds to a unique case or observational unit. Each column represents a characteristic or variable. This data matrix structure allows new cases to be added as rows or new variables to be added as columns.\nExample\nFigure¬†2.1 below is a data set of Marquette basketball players stored in matrix form. The objects are individuals or players in the data and each have their own associated row. Each player has several characteristics or attributes shown in the columns associated with them. These include jersey number, class, position, height, weight, hometown and high school. These characteristics can also be referred to as variables because they vary from one player to another.\nFigure¬†2.1: Data set of 2019 Marquette men‚Äôs basketball players.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Collection and Data Type</span>"
    ]
  },
  {
    "objectID": "intro-data.html#population-and-sample",
    "href": "intro-data.html#population-and-sample",
    "title": "2¬† Data Collection and Data Type",
    "section": "\n2.2 Population and Sample",
    "text": "2.2 Population and Sample\n Target Population \nThe data set we collect for analysis is usually a sample of some target population in the study. The first step in conducting a study is to identify questions to be investigated. A clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\nOnce the research question is determined, it is important to identify the target population to be investigated. The target population is the complete collection of data we‚Äôd like to make inference about. Look at the following examples.\n GPA Example \n\n\nSuppose a Marquette professor has a research question: What is the average GPA of currently enrolled Marquette undergraduate students?\nThe target population in this study is  All Marquette undergraduate students that are currently enrolled. because all Marquette undergrads that are currently enrolled are the complete collection of data we‚Äôd like to make inference about or we are interested in some property or characteristic of these group of people. Each currently enrolled Marquette undergraduate student is an object in the population. Average GPA is the variable or population property we would like to make an inference about.\n\n\n\n\n\nSource: Upslash-Sarah Noltner\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nStudents who are not currently enrolled or students that have already graduated are not our interest, so they shouldn‚Äôt be a part of target population.\n\n\n Heart Disease Example \n\n\nDoes a new drug reduce mortality in patients with severe heart disease? If this is our research question, the target population is  All people with severe heart disease.  Mortality is the variable or population property we would like to make an inference about.\nDo you think it is possible the apply the new drug to all the patients with severe heart disease, and obtain the mortality we are interested?\n\n\n\n\n\nSource: Upslash-National Cancer Institute\n\n\n\n\n\n\n Sample Data \nIn some cases it‚Äôs possible to collect data of all the cases we are interested in. However, most of the time it is either too expensive or too time consuming to collect data for every case in a population. What if we tried to collect data on the average GPA of all students in Illinois? The U.S.? The world? üò± üò± üò±\n\n\nWhen we are not able to collect all the cases in the target population due to some budget or time constraint, but we still want to learn about the population property, our solution is sampling cases from the target population. A sample is a subset of cases selected from a population.\nWe are not able to collect the average GPA of every member of the population, but we can collect a sample from that population which has fewer objects (Figure¬†12.1). We can then compute the average GPA of the sample data. \n\n\n\n\n\n\n\n\n\n\nFigure¬†2.2: Sampling from the population reduces the number of objects from which to collect data.\n\n\n\n\n\n\nWe hope that the average GPA of the sample is close to the average GPA of the population, which is our main interest. For the sample‚Äôs average GPA to be close to population‚Äôs average GPA, we want the sample to look like the population such that they share similar attributes including GPA. In other words, we hope the sample is a small size of everything in the population, and the sample is representative of the population. Ideally, a sample is the small size of some dish and the population is the large size of that dish.\n\n Good Sample vs.¬†Bad Sample \n\n\n\n\n\n\nIs this 4720/5720 class a sample of the target population Marquette students?\n\n\n\n\n\n\nOf course, every member in the class is a Marquette student, so the class is a subset of the population Marquette students. Let me ask you another question.\n\n\n\n\n\n\nIs this 4720/5720 class a ‚Äúgood‚Äù sample of the target population?\n\n\n\n\n\n\nRemember a good sample should well represent the target population. Do you think the class is generally a small version of the entire Marquette student body? The sample is convenient to be collected, but as Figure¬†2.3 shows, it is NOT representative of the population. Because this class is primarily composed of STEM majors, it may not share the attributes necessary with the target population for the two to share a similar average GPA. We call this kind of sample a biased sample. The average GPA of the class may differ greatly from the average GPA of all Marquette undergrads.\n\n\n\n\n\n\n\nFigure¬†2.3: Majors of students in this 4720/5720 class\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.4: Sampling from a class of mostly STEM students is not representative of the entire population.\n\n\n\n\n\n\nIf the biased sample has no or tiny impact on the population attribute we‚Äôd like to discover, we are lucky, and the issue is not serious. However, if the sample is biased in a way that the attribute we get from the sample is quite different from that of the population, then we miss the mark. In the GPA example, if the average GPA depends on students major, the sample that is biased in students‚Äô major causes a trouble.\nAs shown in Figure¬†2.5, the average GPA differs based on students‚Äô majors. Because this class consists of mostly STEM majors, it is likely that the average GPA of its students is not the same as the average GPA of all Marquette undergraduates. Figure¬†2.4 depicts that sampling needs to be done appropriately to ensure the sample is representative of the population. \n\n\n\n\n\n\n\n\n\n\nFigure¬†2.5: UC Berkeley average GPAs by major\n\n\n\n\n\n\n\n\nHow do we collect a representative sample? We always seek to randomly select a sample from a population. Every member in the target population should be treated equally, and preferably has equal chance to be chosen in the sample. If you just collect data from STEM fields, we miss the information provided by students from arts, humanities, and other non-STEM majors. Random sampling usually give us a representative sample, as long as the sample size, the number of objects in the sample, is not too small. It is important to collect samples this way because many statistical methods are based on the random sampling assumption.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Collection and Data Type</span>"
    ]
  },
  {
    "objectID": "intro-data.html#data-collection",
    "href": "intro-data.html#data-collection",
    "title": "2¬† Data Collection and Data Type",
    "section": "\n2.3 Data Collection",
    "text": "2.3 Data Collection\nIn this section, we briefly discuss how data can be collected.\n Two Types of Studies to Collect Sample Data \nThere are two types of studies that are used to collect data: observational studies and experimental studies.\nAn observational study is a study in which those collecting the data observe and measure characteristics/variables, but do NOT attempt to modify or intervene with the subjects being studied.\n\n\n Example: Sample from 1Ô∏è‚É£ the heart disease and 2Ô∏è‚É£ heart disease-free populations and record the fat content of the diets for the two groups.  In this type of study, the researchers do not modify or intervene with the the subjects either with or without heart disease. They just record the fat content in their diets.\n\nThe other type of study is called the experimental study. In an experimental study, some treatment(s) is applied and then those collecting data proceed to observe its responses or effects on the individuals, the experimental units in such study.\n\n\n Example: Assign volunteers to one of several diets with different levels of dietary fat and compare the fat levels with respect to the incidence of heart disease after a period of time.  In this experimental study, the treatment is the fat level in diets. The researchers do not just observe the subjects behavior. Instead, they ask the subjects to take a specific diet they design for a period of time.\n\n\n\n\n\n\n\nObservational or Experimental?\n\n\n\n\nRandomly select 40 males and 40 females to see the difference in blood pressure levels between male and female.\nTest the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo).\n\n\n\n Limitation of Observational Studies: Confounding Variables \nOne limitation of observational studies is confounding. A confounder is a variable that is not included in a study, but affects the variables in the study. For example, a person observes past data showing that increases in ice cream sales are associated with increases in drownings and concludes that eating ice cream causes drownings. üò±üòï‚ÅâÔ∏è\n\n\n\n\n\n\nSource: Unsplash-Brooke Lark\n\n\n\n\n\n\n\n\n\n\nSource: Unsplash-Nate Neelson\n\n\n\n\n\n\n\n\n\n\n\nWhat is the confounder that is not in the data but affects ice cream sales and the number of drownings?\n\n\n\n\nTemperature\n\n\n\n\nAs temperature increases, ice cream sales increase and the number of drownings also rises because more people go swimming (Figure¬†2.6).\n\n\n\n\n\n\n\n\nFigure¬†2.6: Temperature acting as a confounder\n\n\n\n\n\nMaking causal conclusions based on experimental data is often more reasonable than making the same causal conclusions based on observational data. Observational studies are generally only sufficient to show associations, not causality.\n\n Types of Random Samples \nAs previously mentioned, many statistical methods are based on the randomness assumption. It‚Äôs important to understand what a random sample is and how to collect it. In a random sample, each member of a population is equally likely to be selected.\n Simple Random Sample \nFor a simple random sample (SRS), every possible sample of sample size \\(n\\) has the same chance to be chosen.\n\n\nExample: If I were to sample 100 students from all 10,000 Marquette students, I would randomly assign each student a number (from 1 to 10,000) and then randomly select 100 numbers.\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.7: Simple Random Sample\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†2.8: Simple random sample from a population of 15 (https://research-methodology.net/sampling-in-primary-data-collection/random-sampling/)\n\n\n\n\n\n\n Stratified Random Sample \nFor stratified sampling, we subdivide the population into different subgroups (strata) that share the same characteristics, then draw a simple random sample from each subgroup. Stratified sampling has a property: Homogeneous within strata; Non-homogeneous between strata. (Figure¬†2.9)\n\n\n\n\n\n\n\nFigure¬†2.9: Stratified Sampling. Source: https://www.datasciencemadesimple.com/stratified-random-sampling-in-r-dataframe-2/\n\n\n\n\n\n\nExample: Divide Marquette students into groups by colleges, then perform a SRS for each group (Figure¬†2.10). In this case, subjects within strata are homogeneous because people in the same stratum belong to the same college. Subjects are non-homogeneous between strata because students in one college is not a student in another college.\n\n\n\n\n\n\n\n\nFigure¬†2.10: Stratified sampling of Marquette Students\n\n\n\n\n Cluster Sampling \nFor cluster sampling, divide the population into clusters, then randomly select some of those clusters, and then keep all the members from those selected clusters. Cluster sampling has a property: Homogeneous between clusters; Non-homogeneous within clusters (Figure¬†2.11). Clusters look similar each other, but members in a cluster are not very alike. They have different characteristics.\n\n\n\n\n\n\n\nFigure¬†2.11: Cluster Sampling Source: https://research-methodology.net/sampling-in-primary-data-collection/cluster-sampling/\n\n\n\n\n\n\nExample: Study 4720 students‚Äô drinking habits by dividing the students into 9 groups, and then randomly selecting 3 and interviewing all of the students in each of those clusters (Figure¬†2.12). Subjects are homogeneous between clusters because clusters are like random partitions, and each one is a representative subset of the entire population. Subjects are non-homogeneous within clusters because everyone has their own characteristics, and subjects are not divided based on any characteristic such as major or college.\n\n\n\n\n\n\n\n\nFigure¬†2.12: Cluster sampling of Marquette students",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Collection and Data Type</span>"
    ]
  },
  {
    "objectID": "intro-data.html#data-type",
    "href": "intro-data.html#data-type",
    "title": "2¬† Data Collection and Data Type",
    "section": "\n2.4 Data Type",
    "text": "2.4 Data Type\nOK. We learn data collection and sampling methods. Now‚Äôs let‚Äôs learn some data types. Usually a statistical method is only for some type of data or variables. Knowing data types is important because it helps us choose the correct or appropriate statistical methods for analysis. It also helps us interprets the analysis result correctly. Figure¬†2.13 tells us everything about data type. We are going to learn each data type in the figure.\n\n\n\n\n\n\n\nFigure¬†2.13: Types of Data\n\n\n\n\n Categorical vs.¬†Numerical Variables \nA categorical variable provides non-numerical information which can be placed in one (and only one) category from two or more categories. Here are some examples.\n\nGender (Male üë®, Female üë©, Trans üè≥Ô∏è‚Äçüåà) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA üá∫üá∏, Canada üá®üá¶, UK üá¨üáß, Germany üá©üá™, Japan üáØüáµ, Korea üá∞üá∑) \n\nGender, Class, and Country are all categorical variables because they provide non-numerical information. Their possible ‚Äúvalues‚Äù are ‚Äúcategories‚Äù. Keep in mind that a data object can only belong to one category of that variable. You cannot be a freshman and sophomore.\nA numerical variable is recorded in a numerical value representing counts or measurements. Some examples are\n\n GPA \n The number of relationships you‚Äôve had \n Height \n\nThe possible values of the three variables are all numerical or numbers. You are a 6‚Äô2‚Äù tall student who had eight girlfriends and your GPA is 3.98.\n Numerical Variables \nNumerical variables can be discrete or continuous. A discrete variable takes on values of a finite or countable number, while a continuous variable takes on values anywhere over a particular range without gaps or jumps.\n\n GPA is continuous because theoretically it can be any value between 0 and 4. \n The number of relationships you‚Äôve had is discrete because you can count the number and it is finite. The possible values are 0, 1, 2, 3, and so on. Can you have a 0.5 relationship?\n Height is continuous because it can be any number within a range. \n\n Categorical Variables \nFor convenience, categorical variables are usually recorded as numbers in a data set. For example, we can have\n\nGender (Male = 0, Female = 1, Trans = 2) \nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5) \nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301) \n\nEven United Airlines boarding group is categorical. The group number does provide non-numerical information, which is the order of boarding. You cannot be in both boarding zone one and zone two for the same ticket. You can only be in one group.\nPlease note that the numbers represent categories only; taking differences of these numbers is meaningless. If we use the coding scheme in the examples,\n\nCanada - USA = 101 - 100 = 1???\nGraduate - Sophomore = 5 - 2 = 3 = Junior???\n\nThe arithmetic operations do not make sense. For any data or variables, we need to learn the level of measurements to know which arithmetic operations are meaningful for what type of data.\n\n Levels of Measurements \n Nominal and Ordinal for Categorical Variables \nA categorical variable can be of nominal or ordinal level of measurement.\nThe data is nominal if can not be ordered in a meaningful or natural way. For example,\n\nGender (Male = 0, Female = 1, Trans = 2)  is nominal because Male, Female and Trans cannot be ordered, even the numbering coding has an ordering.\nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301)  is nominal. There is no reason to put any country before any other country unless there is another variable giving those countries another attribute that can be ordered.\n\nOrdinal data can be arranged in some meaningful order, but differences between data values can NOT be determined or are meaningless.\n\n\nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5)  is ordinal because Sophomore is one class higher than Freshman, and so on. Here the difference is still meaningless. It seems that Junior is one year higher than Sophomore, and Junior - Sophomore = 1 kind of makes sense. However, ‚Äú1‚Äù does not mean one year higher; instead ‚Äú1‚Äù means Freshman. Moreover, we could even use the numbering (Freshman = 1, Sophomore = 10, Junior = 33, Senior = 44, Graduate = 50) for the Class variable.\n\n Interval and Ratio for Numerical Variables \nNumerical data can be interval or ratio level of measurement.\nInterval data have meaningful differences between any two values but the data do NOT have a natural zero or starting point. The data can do \\(\\color{red} +\\) and \\(\\color{red} -\\), but can‚Äôt reasonably do \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\n\nTemperature is interval because \\(80^{\\circ}\\)F is 40 degrees higher than \\(40^{\\circ}\\)F \\((80-40=40)\\), but \\(0^{\\circ}\\) does not mean NO heat or NO temperature, but a specific temperature. Also, \\(80^{\\circ}\\)F is NOT twice as hot as \\(40^{\\circ}\\)F.\n\nRatio data have both meaningful differences and ratios, and there is a natural zero starting point that indicates none of the quantity. The data can do \\(\\color{red} +\\), \\(\\color{red} -\\), \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\n\nDistance is ratio level of measurement because \\(80\\) miles is twice as far as \\(40\\) miles \\((80/40 = 2)\\), and \\(0\\) mile means NO distance.\n\n\n Converting Numerical to Categorical \nSometimes research purpose we may want to convert a numerical variable into a categorical variable. Figure¬†2.14 is an example of turning a 100% percentage grade into a letter grade which is categorical. Another is example is turning annual salary (numerical) into income level (categorical). We can say salary between $0 and $50,000 is ‚Äúlow‚Äù income level, salary between $50,000 and $120,000 is ‚Äúmiddle‚Äù income level, and above $120,000 is ‚Äúhigh‚Äù income level.\n\n\n\n\n\n\nGrade\n\n\nPercentage\n\n\n\n\n\nA\n\n\n[94, 100]\n\n\n\n\nA-\n\n\n[90, 94)\n\n\n\n\nB+\n\n\n[87, 90)\n\n\n\n\nB\n\n\n[83, 87)\n\n\n\n\nB-\n\n\n[80, 83)\n\n\n\n\nC+\n\n\n[77, 80)\n\n\n\n\nC\n\n\n[73, 77)\n\n\n\n\nC-\n\n\n[70, 73)\n\n\n\n\nD+\n\n\n[65, 70)\n\n\n\n\nD\n\n\n[60, 65)\n\n\n\n\nF\n\n\n[0, 60)\n\n\n\n\n\n\nFigure¬†2.14: Grading scale for this class\n\n\n\n\n Practice \n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify the data type of each variable in the Marquette men‚Äôs basketball player data\n\n\n\n\n\n\n\n\n\nFigure¬†2.15: 2019 Marquette men‚Äôs basketball player data set.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Collection and Data Type</span>"
    ]
  },
  {
    "objectID": "intro-data.html#exercises",
    "href": "intro-data.html#exercises",
    "title": "2¬† Data Collection and Data Type",
    "section": "\n2.5 Exercises",
    "text": "2.5 Exercises\n\n\nData Type: Identify each of the following as numerical or categorical data.\n\nThe names of the companies that manufacture paper towels\nThe colors of cars\nThe heights of football players\n\n\n\nLevel of Measurements: Identify the level of measurement used in each of the following.\n\nThe weights of people in a sample of people living in Milwaukee.\nA physician‚Äôs descriptions of ‚Äúabstains from smoking, light smoker, moderate smoker, heavy smoker.‚Äù\nFlower classifications of ‚Äúrose, tulip, daisy.‚Äù\nSuzy measures time in days, with 0 corresponding to her birth date. The day before her birth is -1, the day after her birth is +1, and so on. Suzy has converted the dates of major historical events to her numbering system. What is the level of measurement of these numbers?\n\n\n\nDiscrete vs Continuous: Determine whether the data are discrete or continuous.\n\nThe length of stay (in days) for each COVID patient in Wisconsin.\nSeveral subjects are randomly selected and their heights are recorded.\nFrom a data set, we see that a male had an arm circumference of 31.28 cm.\nA sample of married couples is randomly selected and the number of animals in each family is recorded.\n\n\n\nSampling Method: Identify which of these types of sampling is used: random, stratified, or cluster.\n\nDr.¬†Yu surveys his statistics class by identifying groups of males and females, then randomly selecting 7 students from each of those two groups.\nDr.¬†Yu conducts a survey by randomly selecting 5 different sports teams at Marquette and surveying all of the student-athletes on those teams.\n427 subjects were randomly assigned to (1) meditation or (2) no mediation group to study the effectiveness of this mindfulness activity on lowering blood pressure.\n\n\n\nStudy Type: Determine whether the study is an experiment or an observational study, then identify a major problem with this study.\n\nIn a survey conducted by USA Today, 998 Internet users chose to respond to the question:‚ÄúHow often do you seek medical advice online?‚Äù 42% of the respondents said ‚Äúfrequently.‚Äù\nThe Physicians‚Äô Health Study involved 21,045 female physicians. Based on random selections, 11,224 of them were treated with aspirin and other other 9,821 were given placebos. The study was stopped early because it became clear that aspirin did not reduce the risk of myocardial infarctions by a substantial amount.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Data Collection and Data Type</span>"
    ]
  },
  {
    "objectID": "intro-r.html",
    "href": "intro-r.html",
    "title": "3¬† ready foR data",
    "section": "",
    "text": "3.1 Computing Environment",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>ready fo*R* data</span>"
    ]
  },
  {
    "objectID": "intro-r.html#computing-environment",
    "href": "intro-r.html#computing-environment",
    "title": "3¬† ready foR data",
    "section": "",
    "text": "3.1.1 Posit cloud\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR and Python are popular programming languages for statistical analysis and data science.\nPosit Cloud offers a could computing environment that lets you access data science tools right in your browser, and no installation or complex configuration is required.\nIn Posit Cloud, you can generate a data science project using the integrated development environment (IDE) RStudio or JupyterLab/Jupyter Notebook. These IDEs are software for efficiently writing computer programs. For R users, RStudio is recommended because it provides many useful functionalities specifically for R programming language.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne of the advantages of using Posit Cloud is that you can implement R/Python programs without installing R/Python and the IDE locally on your laptop! Everything can be done in the cloud using web browsers. In other words, you can get assess to your code and any files in your projects on any computer. Also, Posit Cloud lets you do, share and learn data science online for free! Moreover, Posit Cloud prepares everything you need for doing statistics and data science. You don‚Äôt need to worry about any installation or configuration issues that you may encounter when you are trying to get your local machine ready for programming in R or Python. Go to https://posit.cloud/ and log in. Then we are all set.\n\n\nüòû Getting everything ready locally: Lots of friction\n\nDownload and install R/Python\nDownload and install IDE\nInstall wanted R/Python packages:\n\ntidymodels\ntidyverse\nNumPy\n‚Ä¶\n\n\nLoad these packages\nDownload and install tools like Git\n\n\nü§ì Posit Cloud: Much less friction\n\n\n\n\n\n\n\n\n\nGo to https://posit.cloud/\nLog in\n\n\n\n\nPlease follow the steps below to sign up Posit Cloud.\n Sign up Posit Cloud \n\n\n\n\n\n\nLab Time!\n\n\n\n\n\nStep 1: In the Posit website https://posit.co/, choose Products &gt; Posit Cloud as shown below.\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.1: Posit website\n\n\n\n\n\n\n\n\n\n\nLab Time!\n\n\n\n\n\nStep 2: Click GET STARTED.\n\nStep 3: Free &gt; Sign Up.\n\nI recommend signing up with your GitHub if you have one or use your Marquette email address. Currently the free version has 25 compute hours per month. You are encouraged to choose Plus version for more hours (75 hours) usage per month.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n New Projects \nOnce you have done the registration and log in successfully, you should be able to see a page like the one below. To get a IDE and create a new project, click New Project in the top right corner as shown above.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Workspaces \nWhen you create an account on Posit Cloud, you get a workspace of your own. You can add a new workspace (click + New Space in sidebar) and control its permissions.\n\n\n\n\n\n\n\n\n\n First R Code in Posit Cloud! \nWe will be using RStudio as our main IDE for coding in R.\n\n\n\n\n\n\nLab Time!\n\n\n\n\nIn the bar, click the desired workspace.\nClick New Project &gt; New RStudio Project to get into the IDE.\nClick Untitled Project and give your project a nice name, math-4720 for example.\nIn the Console pane, write your first R code: a string \"Hello WoRld!\" or math 2 + 4.\nChange the editor theme: Tools &gt; Global Options &gt; Appearance\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.2: How to change the editor theme\n\n\n\n\n\n More Tips \nFor more help, read the Posit Cloud guide.\n\n\n\n\n\n\n\nFigure¬†3.3: Posit Cloud Guide\n\n\n\n\n\n3.1.2 Working in RStudio\n Panes \nIn RStudio, there are 4 main panes, source pane, console pane, pane for environment/history and version control, and the pane for files, plots, packages and help page. Source pane is where you write your code. Your code will not be evaluated or interpreted until you ‚Äúrun‚Äù them or source them to the console. When your code is long, write your code in R scripts in the Source, so that the code can be saved and reused later. You type code into the Console if the code is short or you want to do some quick calculations or analysis. The code you type in the Console will not be saved in a script. In the environment/history, you can check any objects you create in the R environment and you can also view your command history in the history tab. You will see how the pane for file/plot/package/help can be used as we learn more about RStudio.\n\n\n\n\n\n\n\nFigure¬†3.4: RStudio Panes. Source: https://bookdown.org/ndphillips/YaRrr/the-four-rstudio-windows.html\n\n\n\n\n\n Source Pane \n R Script \nAn R script is a .R file that contains R code. To create an R script, go to File &gt; New &gt; R Script, or click the green-plus icon on the top left corner and select R Script.\nHere you see in this r script, I create two objects x and y, and I also load the data mtcars into R environment. Don‚Äôt worry if you don‚Äôt know these syntax. You will learn basic R syntax and programming later.\n\n\n\n\n\n\n\nFigure¬†3.5: Creating an R script\n\n\n\n\n Run Code \n\n\n Run : run the current line or selection of code.\n\n\nctrl + enter (Win) or cmd + enter (Mac)\n\n\n\n Icon to the right of Run : re-run the previous code.\n\n\nalt + ctrl + p (Win) or option + cmd + p (Mac)\n\n\n\n Source : run all the code in the R script.\n\n\nshift + ctrl + s (Win) or shift + cmd + s (Mac)\n\n\n\n Source with Echo : run all the code in the R script with the code printed in the console.\n\n\nshift + ctrl + enter (Win) or shift + cmd + enter (Mac)\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.6: Running R Code\n\n\n\n\n\n Environment Tab \nThe (global) environment is where we are currently working. Anything created or imported into the current R/Python session is stored in our environment and shown in the Environment tab. After we run the R script from Figure¬†3.5, the following objects are stored in the environment:\n\nData set mtcars\n\nObject x storing integer values 1 to 10.\nObject y storing three numeric values 3, 5, 9.\n\n\n\n\n\n\n\n\nFigure¬†3.7: Environment Pane\n\n\n\n\n\n History Tab \nThe History tab keeps a record of all previous commands.\n\n\nSave icon: save all history to a file\n\nTo Console: send the selected commands to the console.\n\nTo Source : inserted the selected commands into the current script.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn the console pane, use ‚¨ÜÔ∏è to show the previous commands.\n\n\n\n Help \nWhat if you don‚Äôt know how a function works or what a data set is about ‚ùì\nüëâ Simply type ? followed by the data name or function name to get more information.\n\n?mean\n?mtcars\n\n\n\n\n\n\n\nWhat does the function mean() do? What is the size of mpg?\n\n\n\n\n\n\nA document will show up in the Help tab in the bottom-right pane, teaching you how to use the function or explaining the data set. An example is shown for the mpg data set in Figure¬†3.8 below.\n\n\n\n\n\n\n\nFigure¬†3.8: Document explaining the mpg data set in the Help tab\n\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nWhat is the size of mtcars data?\nType mtcars and hit enter in the console to see the data set.\nDiscuss the data type of each variable.\nType mtcars[, 1] and hit enter in the console and discuss what you see.\n\n\n\n\n3.1.3 Install R and RStudio to your computer\nIf you would like to install R and RStudio to your local computer, the step-by-step procedure is provided in this section. Free to skip it and go to the next section if Posit Cloud works perfectly for you.\n Install R \n Step 1 \n\nGo to https://cloud.r-project.org.\nClick Download R for [your operating system].\n\n\n\n\n\n\nFigure¬†3.9: Downloading R\n\n\n Step 2 \n\nIf you are a Mac user, you should see the page shown below in Figure¬†3.10.\nYou are recommended to download and install the latest version of R (now R-4.3.1) if your OS version allows to do so.\nOtherwise, choose a previous version, such as R-3.6.3.\n\n\n\n\n\n\nFigure¬†3.10: Downloading R for Mac\n\n\n\nIf you are a Windows user, after clicking Download R for Windows, please choose base version and then click Download R-4.3.1 for Windows.\n\n Step 3 \n\nOnce you successfully install R, when you open R, you should be able to see the following R terminal or console:\n\n\n\n\nWindows\n\n\n\n\n\n\nFigure¬†3.11: Windows R Console\n\n\n\n\n\n\nMac\n\n\n\n\n\n\nFigure¬†3.12: Mac R Console\n\n\n\n\n Welcome to the R World! \nNow you are ready to use R for statistical computation. You can use R like a calculator. After typing your formula, simply hit enter and you get the answer!\n\n1 + 2\n# [1] 3\n30 * 42 / 3\n# [1] 420\nlog(5) - exp(3) * sqrt(7)\n# [1] -51.5\n\n\n Install RStudio \n Step 1 \n\nIn the Posit website, please choose Products &gt; RStudio IDE as shown in Figure¬†3.1.\n\n Step 2 \n\nIn the RStudio Desktop tab, Click DOWNLOAD RSTUDIO.\n\n\n\n\n\n\n\n\nFigure¬†3.13: Downloading RStudio Desktop\n\n\n\n\n Step 3 \nThe page will automatically detect your operating system and recommend a version of RStudio that works the best for you that is usually the latest version.\n\nClick DOWNLOAD RSTUDIO DESKTOP FOR [Your OS version] (Figure¬†3.14).\nFollow the standard installation steps and you should get the software.\nMake sure that R is installed successfully on your computer before you download and install RStudio.\n\n\n\n\n\n\n\n\nFigure¬†3.14: Latest version of RStudio\n\n\n\n\n\n RStudio Screen \nWhen you open RStudio, you should see something similar to Figure¬†3.15 below. If you do, congratulations! You can now do any statistical computation in R using RStudio locally on your computer.\n\n\n\n\n\n\n\nFigure¬†3.15: R Studio Screen",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>ready fo*R* data</span>"
    ]
  },
  {
    "objectID": "intro-r.html#basic-r",
    "href": "intro-r.html#basic-r",
    "title": "3¬† ready foR data",
    "section": "\n3.2 Basic R",
    "text": "3.2 Basic R\nWe are already equipped with the tools we need for doing statistics in this course. Now it‚Äôs time to program in R. We are going to go through basic R syntax and its commonly used data structures. This introduction is a must because we will be using R throughout the book. If you are already familiar with basic R syntax, feel free to move to the next chapter. \n\n3.2.1 R Packages üì¶\n\nR packages are a collection of reusable R functions, code and data. When we start an R session, only built-in packages like base, stats, graphics, etc. are available. Installing packages is an easy way to get access to other data and functions. Currently there are about 20 thousand packages on CRAN (The Comprehensive R Archive Network). In this book, we won‚Äôt use many packages because the basic R functions can sufficiently solve our statistics problems. Not really need more sophisticated functions in other packages.\n\n\n\n\n Installing R Packages \n\n\nTo install a package, such as the ggplot2 package, we use the command\n\ninstall.packages(\"ggplot2\")\n\nA different option in the right-bottom pane is Packages &gt; Install.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Loading R Packages üì¶ \n\n\nTo use any function or data in ggplot2, we write ggplot2:: followed by the name of the function or data.\n\nggplot2::ggplot(ggplot2::mpg, \n                ggplot2::aes(x = displ, y = hwy, \n                             colour = class)) + \n    ggplot2::geom_point()\n\n\n\n\n\n\n\nWhat happens when you run the code shown below?\n\n\n\n\nggplot(mpg, aes(x = displ, y = hwy, \n                colour = class)) + \n    geom_point()\n\n\n\nYou should get an error message like\nError in ggplot(mpg, aes(x = displ, y = hwy, colour = class)) :\ncould not find function \"ggplot\"\n\n\n\n\nYour computer has the source code and data of the ggplot2 package after you install it. However, without adding ggplot2::, R does not recognize the functions and data in the ggplot2 package unless we load the package into your R working environment.\nWe can load a R package into our R session using the command library(). With library(ggplot2), R knows the function ggplot and data mpg are from the ggplot2 package, and we don‚Äôt need to add ggplot2:: in front of them anymore.\n\nlibrary(ggplot2)\nggplot(mpg, aes(x = displ, \n                y = hwy, \n                colour = class)) + \n    geom_point()\n\n\n\n\n3.2.2 Operators\n R is a Calculator \nFirst, as many other languages, R is a calculator. We can do basic arithmetic operations using R.\n Arithmetic Operators \nBasic arithmetic operators are shown in the table below. Most symbols are intuitive. To get the remainder of division, we use two percentage symbols. To get the quotient of division, we use percent, slash and percent symbol.\n\n\n\n\n\n\n\nFigure¬†3.16: Table of arithmetic operators. Source: https://www.statmethods.net/management/operators.html\n\n\n\n\n Examples \nHere are some examples. Very simple. But keep the PEMDAS rule in mind, the order of operations. We do operations on parenthesis first, then exponentiation, then multiplication/division and then addition/subtraction.\n\n2 + 3 * 5 + 4\n# [1] 21\n2 + 3 * (5 + 4)\n# [1] 29\n\n\n\n\n\n\n\n\n\nFigure¬†3.17: Order of operations. Source: https://www.fabworldtoday.com/pemdas-rule/\n\n\n\n\n\n R Does Comparisons \n Logical Operators \nHere are some commonly used comparison and logical operators. R uses ! to negate the result, vertical bar | for ‚Äúor‚Äù and & for ‚Äúand‚Äù. There is a R buil-in function isTRUE() to test if an object x is TRUE.\n\n\n\n\n\n\n\nFigure¬†3.18: Table of logical operators. Source: https://www.statmethods.net/management/operators.html\n\n\n\n\n Examples \n\n\n\n5 &lt;= 5\n# [1] TRUE\n5 &lt;= 4\n# [1] FALSE\n# Is 5 is NOT equal to 5?\n5 != 5\n# [1] FALSE\n\n\n\n\n\n## Is TRUE not equal to FALSE?\nTRUE != FALSE\n# [1] TRUE\n## Is not TRUE equal to FALSE?\n!TRUE == FALSE\n# [1] TRUE\n## TRUE if either one is TRUE or both are TRUE\nTRUE | FALSE\n# [1] TRUE\n\n\n\n\n\n\n\n\n\nWhat does TRUE & FALSE return?\n\n\n\n\n\n\n\n Built-in Functions \nR has lots of built-in functions, especially for mathematics, probability and statistics. No importing other modules or libraries is required. These functions are pretty similar to the MATLAB or Python functions. We‚Äôll learn more about them together when we are actually doing probability and statistics.\n\n\n\n\n\n\n\nFigure¬†3.19: R Built-in functions. Source: https://www.statmethods.net/management/functions.html\n\n\n\n\n Examples \n\n\n\nsqrt(144)\n# [1] 12\nexp(1)  ## Euler's number\n# [1] 2.72\nsin(pi/2)\n# [1] 1\nabs(-7)\n# [1] 7\n\n\n\n\n\nfactorial(5)\n# [1] 120\n## without specifying base value\n## it is a natural log with base e\nlog(100)\n# [1] 4.61\n## log function and we specify base = 2\nlog(100, base = 10)\n# [1] 2\n\n\n\n\n Commenting \n\n\n\n\n\n\nYou‚Äôve seen comments a lot! How do we write a comment in R?\n\n\n\n\n\n\nWe use # to add a comment so that the text after # is not read as an R command. Writing (good) comments is highly recommended. Comments help readers, and more importantly yourself, understand what the code is doing. They should explain the why, not the what.\n\n\n\n\n\n\nhttps://www.reddit.com/r/ProgrammerHumor /comments/8w54mx/code_comments_be_like/\n\n\n\n\n\n\n\n\nhttps://juliabirkett.medium.com/in-defense-of-some-code-comments-8e60f2ae0a4\n\n\n\n\n\n\n Objects and Funtions in R \nTo understand computation in R, two slogans are helpful:\n\n Everything that exists is an object.  Everything that happens is a function call. \n‚Äì John Chambers, the creator of the S programming language.\n\nWe have made lots of things happen! Even arithmetic and logical operators are functions!\n\n`+`(x = 2, y = 3)\n# [1] 5\n`&`(TRUE, FALSE)\n# [1] FALSE\n\n\n Creating Variables \nA variable stores a value that can be changed according to our need. In R it is highly recommended üëç using the &lt;- operator to assign a value to the variable. 1\nFor example, we create an object, value 5, and call it x, which is a variable. We type the variable name to see the value of the object stored in the variable.\n\nx &lt;- 5\nx\n# [1] 5\n\n\nWe can reassign any value to the variable we created. We can also perform any operations on variables. Variables can also be used in any built-in functions too.\n\n(x &lt;- x + 6)\n# [1] 11\nx == 5\n# [1] FALSE\nlog(x) \n# [1] 2.4\n\n\n\n\n\nSource: https://irudnyts.github.io/r-coding-style-guide/\n\n\n\n\n Bad Naming \n\n‚ùå Unless you have a very good reason, don‚Äôt create a variable whose name is the same as any R built-in constant or function!\nüòü It causes lots of confusion when your code is long and when others read it.\n\npi is a R built-in constant, the ratio of the circumference of a circle to its diameter. abs(x) is a built-in function for calculating the absolute value of x. Please NO NOT do assignment like the following unless you have a very strong reason to do so. The bad assignment or naming forces pi and abs both to be equal to 20 that causes lots of confusion.\n\n## THIS IS BAD CODING! DON'T DO THIS!\npi\n# [1] 3.14\n(pi &lt;- 20)\n# [1] 20\nabs\n# function (x)  .Primitive(\"abs\")\n(abs &lt;- abs(pi))\n# [1] 20\n\n\n3.2.3 Object Types\n Types of Variables \nWe know we can create R objects, right? And objects may have different types. Object Type plays an important role in data analysis, because it affects which model we should use and which functions we should use to manipulate our data.\nWe use R command typeof() to check which type a variable belongs to. Common types include character, double, integer and logical. To check if it‚Äôs of a specific type, we can use is.character(), is.double(), is.integer(), is.logical().\n\n\n\ntypeof(5)\n# [1] \"double\"\ntypeof(5L)\n# [1] \"integer\"\ntypeof(\"I_love_stats!\")\n# [1] \"character\"\n\n\n\n\n\ntypeof(1 &gt; 3)\n# [1] \"logical\"\nis.double(5L)\n# [1] FALSE\n\n\n\n\n\n Variable Types in R and in Statistics \nType character and logical correspond to categorical variables. Type logical is a special type of categorical variables that has only two categories (binary). We usually call it a binary variable.\nType double and integer correspond to numerical variables with an exception discussed later. Type double is for continuous variables, and type integer is for discrete variables.\n\n3.2.4 R Data Structures\n (Atomic) Vector \nTo create a vector, we use command c(), which is short for concatenate or combine. The key property is that all elements of a vector must be of the same type.\nIn the example, we learn that TRUE and FALSE can be written as T and F. To check how many elements in a vector, we use length(). So length(dbl_vec) returns 3. The command str() is useful when we want to quickly get a compact description of any R object of any data structure. dbl_vec is a numerical vector that has three elements whose values are 1, 2.5, and 4.5.\n\n\n\n(dbl_vec &lt;- c(1, 2.5, 4.5)) \n# [1] 1.0 2.5 4.5\n(int_vec &lt;- c(1L, 6L, 10L))\n# [1]  1  6 10\n(log_vec &lt;- c(TRUE, FALSE, F))  \n# [1]  TRUE FALSE FALSE\n(chr_vec &lt;- c(\"pretty\", \"girl\"))\n# [1] \"pretty\" \"girl\"\n\n\n\n\n\nlength(dbl_vec) \n# [1] 3\nstr(dbl_vec) \n#  num [1:3] 1 2.5 4.5\n\n\n\n Operations on Vectors \nWe can do the same operations on vectors that we do on a scalar variable which can be viewed as a vector of length 1. All operations happen element-wisely. We can also use a function on a vector if the fucntion accepts vectors, for example, log() and sqrt().\n\n\n\n# Create two vectors\nv1 &lt;- c(3, 8)\nv2 &lt;- c(4, 100) \n\n# Vector addition\nv1 + v2\n# [1]   7 108\n# Vector subtraction\nv1 - v2\n# [1]  -1 -92\n\n\n\n\n\n# Vector multiplication\nv1 * v2\n# [1]  12 800\n# Vector division\nv1 / v2\n# [1] 0.75 0.08\nsqrt(v2)\n# [1]  2 10\n\n\n\n Recycling of Vectors \nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.\nIn the example, v1 * 2 means that each element in v1 is multiplied by 2. So it is equivalent to multiply by a vector of 2 of the same length of v1. v3 becomes c(4, 11, 4, 11) when doing the operation v1 + v3.\n\n\n\nv1 &lt;- c(3, 8, 4, 5)\nv1 * 2\n# [1]  6 16  8 10\nv1 * c(2, 2, 2, 2)\n# [1]  6 16  8 10\nv3 &lt;- c(4, 11)\nv1 + v3\n# [1]  7 19  8 16\n\n Subsetting Vectors \nTo extract element(s) in a vector, use a pair of brackets [] with element indexing. The indexing starts with 1. We can put a sequence of numbers inside the brackets to extract multiple elements. For example, We can use the vector c(1, 3) to extract the first and the third element. If we want to extract all but a few elements, just put a negative sign before the vector of indices. For example v1[-c(2, 3)] keeps all the elements except the second and the third one.\n\n\n\nv1\n# [1] 3 8 4 5\nv2\n# [1]   4 100\n## The first element\nv1[1] \n# [1] 3\n## The second element\nv2[2]  \n# [1] 100\n\n\n\n\n\nv1[c(1, 3)]\n# [1] 3 4\nv1[-c(2, 3)] \n# [1] 3 5\n\n\n\n\n Factor \nA vector of type factor can be ordered in a meaningful way, but difference is meaningless. So it is ordinal level of measurement and it is for categorical data.\nWe create a factor by factor(). Be careful! It is a type of integer, not character! üò≤ üôÑ\nBecause a factor is an ordered vector, there is a level associated with it. Each level represents an integer, and by default they are ordered from the vector alphabetically. Here, the level of fac from the lowest to highest is ‚Äúhigh‚Äù ‚Äúlow‚Äù ‚Äúmed‚Äù. R stores a level using integers. So ‚Äúhigh‚Äù is 1, ‚Äúlow‚Äù is 2, and ‚Äúmed‚Äù is 3.\nIf we check its structure using str(), it tells us it is a factor with 3 levels, and the first element is ‚Äúmed‚Äù which is the third level of the factor, so its number is 3. The second element is ‚Äúhigh‚Äù, corresponding to the level level, so its number is 1. The ‚Äúlow‚Äù has the number 2 because it is the second level.\nClearly, ordering a factor alphabetically is not what we want here because the levels and integers do not match the meaning of category in the factor. We can specify the level we want using the argument levels in the factor() function. For example here, we tell R that the level of this factor, from the lowest to highest is ‚Äúlow‚Äù, ‚Äúmed‚Äù, ‚Äúhigh. And now the factor is correctly ordered showing 2, 3, 1 here.\n\nfac &lt;- factor(c(\"med\", \"high\", \"low\"))\ntypeof(fac)\n# [1] \"integer\"\nlevels(fac)\n# [1] \"high\" \"low\"  \"med\"\nstr(fac)\n#  Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\norder_fac &lt;- factor(c(\"med\", \"high\", \"low\"), levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n#  Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1\n\n\n List (Generic Vectors) \nLists are different from vectors. Elements can be of any type, including lists themselves.\nWe construct a list by using list() instead of c(). For example, we create a list of 3 elements of different types, integer, charactor, and logical respectively. Elements are separated by a comma.\nThe first and third elements are vectors, and the second element is a one single charactor. The first element has the name idx, and the second and third elements have no name.\n\n\n\nx_lst &lt;- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\nx_lst\n# $idx\n# [1] 1 2 3\n# \n# [[2]]\n# [1] \"a\"\n# \n# [[3]]\n# [1]  TRUE FALSE\n\n\n\n\n\nstr(x_lst)\n# List of 3\n#  $ idx: int [1:3] 1 2 3\n#  $    : chr \"a\"\n#  $    : logi [1:2] TRUE FALSE\nnames(x_lst)\n# [1] \"idx\" \"\"    \"\"\nlength(x_lst)\n# [1] 3\n\n\n\n Subsetting a List \nThis is where we should pay more attention to. When we subset a list, it may return an element of the list, or it returns a sub-list of the list. Let‚Äôs see how it happens.\n\nx_lst &lt;- list(idx = 1:3,\n             \"a\",\n             c(TRUE, FALSE))\n\nWe can subset a list by name or by indexing. Suppose we want the first element of the list, we can get it by its name using x_lst$idx. We can also obtain it by using indexing like x_lst[[1]] because we want the first element. Notice that the way we subset a list returns an integer vector, the real first element of the list, not a list.\nLet‚Äôs see another case on the right. We can also subset by name using single pair of brackets, and put the name inside the brackets with quotation marks. Or we can subset by indexing, using a single pair of brackets instead. And you see what happened? The way we subset a list here returns a sub-list, not the element itself.\nPlease be careful when subsetting a list. If you want a vector, use $name or [[number]]. If you want to keep it as a list, use [\"name\"] or [number].\n\n\n\nReturn an  element  of a list\n\n\n## subset by name (a vector)\nx_lst$idx  \n# [1] 1 2 3\n## subset by indexing (a vector)\nx_lst[[1]]  \n# [1] 1 2 3\ntypeof(x_lst$idx)\n# [1] \"integer\"\n\n\n\n\n\nReturn a  sub-list  of a list\n\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n# $idx\n# [1] 1 2 3\n## subset by indexing (still a list)\nx_lst[1]  \n# $idx\n# [1] 1 2 3\ntypeof(x_lst[\"idx\"])\n# [1] \"list\"\n\n\n\nWe know images speak louder than words. Here you go.\n\n\n\n\n\n\n\nFigure¬†3.20: Condiment analogy for subsetting lists\n\n\n\n\n\n\nIf list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\n‚Äî @RLangTip, https://twitter.com/RLangTip/status/268375867468681216\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†3.21: Train analogy for subsetting a list\n\n\n\n\n\n Matrix \nA matrix is a two-dimensional analog of a vector. We use command matrix() to create a matrix. By default, R creates a matrix column by column. In other words, R is a column major language.\n\n## Create a 3 by 2 matrix called mat\n(mat &lt;- matrix(data = 1:6, nrow = 3, ncol = 2)) \n#      [,1] [,2]\n# [1,]    1    4\n# [2,]    2    5\n# [3,]    3    6\ndim(mat)  # dimension\n# [1] 3 2\nnrow(mat) # number of rows\n# [1] 3\nncol(mat) # number of columns\n# [1] 2\n\n Subsetting a Matrix \nTo extract a sub-matrix, use the same indexing approach as vectors on rows and columns. Because a matrix has row index and column index, we use comma , to separate row and column index of a matrix. For example, mat[2, 2] extracts the element of the second row and second column.\nWhen we leave row index blank, and specify 2 in column index, mat[, 2] returns all rows and 2nd column, and mat[, 2] automatically becomes a vector.\n\n\n\nmat\n#      [,1] [,2]\n# [1,]    1    4\n# [2,]    2    5\n# [3,]    3    6\nmat[, 2]\n# [1] 4 5 6\n\n\n\n\n\n## 2nd row and all columns\nmat[2, ] \n# [1] 2 5\n## The 1st and 3rd rows\nmat[c(1, 3), ] \n#      [,1] [,2]\n# [1,]    1    4\n# [2,]    3    6\n\n\n\n Binding Matrices \nWe can generalize c() used in vectors to cbind() (binding matrices by adding columns) and rbind() (binding matrices by adding rows) for matrices. When matrices are combined by columns, they should have the same number of rows. When matrices are combined by rows, they should have the same number of columns.\n\n\n\nmat\n#      [,1] [,2]\n# [1,]    1    4\n# [2,]    2    5\n# [3,]    3    6\nmat_c &lt;- matrix(data = c(7, 0, 0, 8, 2, 6), nrow = 3, ncol = 2)\n## should have the same number of rows\ncbind(mat, mat_c)  \n#      [,1] [,2] [,3] [,4]\n# [1,]    1    4    7    8\n# [2,]    2    5    0    2\n# [3,]    3    6    0    6\n\n\n\n\n\n\nmat_r &lt;- matrix(data = 1:4, nrow = 2, ncol = 2)\n## should have the same number of columns\nrbind(mat, mat_r)  \n#      [,1] [,2]\n# [1,]    1    4\n# [2,]    2    5\n# [3,]    3    6\n# [4,]    1    3\n# [5,]    2    4\n\n\n\n\n Data Frame: The Most Common Way of Storing Data \nIn fact, the data matrix we discussed before can be and should be stored as a data frame in R.\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure. It is more general than a matrix in that different columns can have different types. Each column vector is an element of the list.\nTo create a data frame, we use data.frame() that takes named vectors as inputs.\n\n\n\n## data frame w/ an dbl column named age and char column named gender\n(df &lt;- data.frame(age = c(19, 21, 40), gender = c(\"m\", \"f\", \"m\")))\n#   age gender\n# 1  19      m\n# 2  21      f\n# 3  40      m\n\n## a data frame has a list structure\nstr(df)  \n# 'data.frame': 3 obs. of  2 variables:\n#  $ age   : num  19 21 40\n#  $ gender: chr  \"m\" \"f\" \"m\"\n\n\n\n\n\nData frames must set column names, or they are ugly and non-recognizable.\n\ndata.frame(c(19, 21, 40), c(\"m\",\"f\", \"m\")) \n#   c.19..21..40. c..m....f....m..\n# 1            19                m\n# 2            21                f\n# 3            40                m\n\n\n\n Properties of Data Frames \nData frame has properties of matrix and list.\n\n\n\nnames(df)  ## as a list\n# [1] \"age\"    \"gender\"\n\ncolnames(df)  ## as a matrix\n# [1] \"age\"    \"gender\"\n\nlength(df) ## as a list\n# [1] 2\n\nncol(df) ## as a matrix\n# [1] 2\n\ndim(df) ## as a matrix\n# [1] 3 2\n\ntypeof(df)\n# [1] \"list\"\n\nclass(df)\n# [1] \"data.frame\"\n\n\n\n\n\n\n## rbind() and cbind() can be used on df\n\ndf_r &lt;- data.frame(age = 10, gender = \"f\")\nrbind(df, df_r)\n#   age gender\n# 1  19      m\n# 2  21      f\n# 3  40      m\n# 4  10      f\n\ndf_c &lt;- data.frame(col = c(\"red\",\"blue\",\"gray\"))\n(df_new &lt;- cbind(df, df_c))\n#   age gender  col\n# 1  19      m  red\n# 2  21      f blue\n# 3  40      m gray\n\n\n\n Subsetting a Data Frame \nWhen we subset data frames, we can use either list or matrix subsetting methods.\n\n\n\ndf_new\n#   age gender  col\n# 1  19      m  red\n# 2  21      f blue\n# 3  40      m gray\n\n## Subset rows\ndf_new[c(1, 3), ]\n#   age gender  col\n# 1  19      m  red\n# 3  40      m gray\n\n## select the row where age == 21\ndf_new[df_new$age == 21, ]\n#   age gender  col\n# 2  21      f blue\n\n\n\n\n\n\n## Subset columns\n## like a list\ndf_new$age\n# [1] 19 21 40\ndf_new[c(\"age\", \"gender\")]\n#   age gender\n# 1  19      m\n# 2  21      f\n# 3  40      m\n\n## like a matrix\ndf_new[, c(\"age\", \"gender\")]\n#   age gender\n# 1  19      m\n# 2  21      f\n# 3  40      m\n\nstr(df[\"age\"])  ## a data frame with one column\n# 'data.frame': 3 obs. of  1 variable:\n#  $ age: num  19 21 40\n\nstr(df[, \"age\"])  ## becomes a vector by default\n#  num [1:3] 19 21 40\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nCreate a vector object called x that has 5 elements 3, 6, 2, 9, 14.\nCompute the average of elements of x.\nSubset the mtcars data set by selecting variables mpg and disp.\nSelect the cars (rows) in mtcars that have 4 cylinders.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>ready fo*R* data</span>"
    ]
  },
  {
    "objectID": "intro-r.html#exercises",
    "href": "intro-r.html#exercises",
    "title": "3¬† ready foR data",
    "section": "\n3.3 Exercises",
    "text": "3.3 Exercises\n\n# ==============================================================================\n## Vector\n# ==============================================================================\npoker_vec &lt;- c(170, -20, 50, -140, 210)\nroulette_vec &lt;- c(-30, -40, 70, -340, 20)\ndays_vec &lt;- c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\")\nnames(poker_vec) &lt;- days_vec\nnames(roulette_vec) &lt;- days_vec\n\n\nVector\n\nThe code above shows a Marquette student poker and roulette winnings from Monday to Friday. Copy and paste them into your R and complete problem 1.\n\nAssign to the variable total_daily how much you won or lost on each day in total (poker and roulette combined).\nCalculate the winnings overall total_week. Print it out.\n\n\n\n# ==============================================================================\n## Factor\n# ==============================================================================\n# Create speed_vector\nspeed_vec &lt;- c(\"medium\", \"low\", \"low\", \"medium\", \"high\")\n\n\nFactor\n\n\n\nspeed_vec above should be converted to an ordinal factor since its categories have a natural ordering. Create an ordered factor vector speed_fac by completing the code below. Set ordered to TRUE, and set levels to c(\"low\", \"medium\", \"high\"). Print speed_fac.\n\n\n\n# ==============================================================================\n## Data frame\n# ==============================================================================\n# Definition of vectors\nname &lt;- c(\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \n          \"Uranus\", \"Neptune\")\ntype &lt;- c(\"Terrestrial planet\", \"Terrestrial planet\", \"Terrestrial planet\", \n          \"Terrestrial planet\", \"Gas giant\", \"Gas giant\", \n          \"Gas giant\", \"Gas giant\")\ndiameter &lt;- c(0.375, 0.947, 1, 0.537, 11.219, 9.349, 4.018, 3.843)\nrotation &lt;- c(57.63, -242.03, 1, 1.05, 0.42, 0.44, -0.73, 0.65)\nrings &lt;- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\n\n\nData Frame\n\nData frames have properties of lists and matrices, so we skip lists and matrices and focus on data frames. You want to construct a data frame that describes the main characteristics of eight planets in our solar system. You feel confident enough to create the necessary vectors: name, type, diameter, rotation and rings that have already been coded up as above. The first element in each of these vectors corresponds to the first observation.\n\nUse the function data.frame() to construct a data frame. Pass the vectors name, type, diameter, rotation and rings as arguments to data.frame(), in this order. Call the resulting data frame planets_df.\n\n\nUse str() to investigate the structure of the new planets_df variable. Which are categorical (qualitative) variables and which are numerical (quantitative) variables? For those that are categorical, are they nominal or ordinal? For those numerical variables, are they interval or ratio level? discrete or continuous?\nFrom planets_df, select the diameter of Mercury: this is the value at the first row and the third column. Simply print out the result.\nFrom planets_df, select all data on Mars (the fourth row). Simply print out the result.\nSelect and print out the first 5 values in the diameter column of planets_df.\nUse $ to select the rings variable from planets_df.\nUse (f) to select all columns for planets that have rings.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>ready fo*R* data</span>"
    ]
  },
  {
    "objectID": "intro-r.html#footnotes",
    "href": "intro-r.html#footnotes",
    "title": "3¬† ready foR data",
    "section": "",
    "text": "https://colinfay.me/r-assignment/ for more discussion about why we use arrow as an assignment operator in R.‚Ü©Ô∏é",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>ready fo*R* data</span>"
    ]
  },
  {
    "objectID": "intro-py.html",
    "href": "intro-py.html",
    "title": "4¬† Prepare Yourself for data",
    "section": "",
    "text": "4.1 Python in RStudio\nPython Script\nA Python script is a .py file that contains Python code. To create a Python script, go to File &gt; New &gt; Python Script, or click the green-plus icon on the topleft corner and select Python Script. Here I print the string Hello, World!\", create an string object b storing Hello, World!\", and then print the 3rd to 4th letter of the string.\nFigure¬†4.1: Creating a Python script\nRun Python Code\nTo type and run Python code directly in the console, with RStudio we install the R package reticulate (see R package section below for more discussion). Once the reticulate is installed, we use the call library() to load it into into memory for direct use of it in our computing environment. Then the function call repl_python() will turn R console into Python console. The word ‚Äúrepl‚Äù means Read, Evaluate, Print, and Loop. We the type Python code after the prompt &gt;&gt;&gt;, and run it by hitting Enter/return.\nTo return back to R console, just type quit or exitin the Python console, and hit Enter/return.\nAfter we run the Python script from Figure¬†4.1, the following object is stored in the environment:",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>*P*repare *Y*ourself for data</span>"
    ]
  },
  {
    "objectID": "intro-py.html#python-in-rstudio",
    "href": "intro-py.html#python-in-rstudio",
    "title": "4¬† Prepare Yourself for data",
    "section": "",
    "text": "Running Python code may require you to update some packages. Please say YES!\n\n\n\n\nWhen you run the Python code in the script, the console will switch from R to Python.\nType quit in the Python console to switch back to the R console.\n\n\n\n\n\n\n\nObject b storing a string Hello World!",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>*P*repare *Y*ourself for data</span>"
    ]
  },
  {
    "objectID": "intro-py.html#basic-python",
    "href": "intro-py.html#basic-python",
    "title": "4¬† Prepare Yourself for data",
    "section": "\n4.2 Basic Python",
    "text": "4.2 Basic Python\nIn this section, we learn some basic Python syntax by translating the R code in the previous chapter. While their output may be a little different, the R and Python code will provide exactly the same result.\n\n\n4.2.1 Python Packages üì¶\n\nSame as R, there are many Python packages/libraries out there. Most popular python packages by their purposes include:\n\nComputing: NumPy, SciPy\nData manipulation: pandas, Polars\nData visualization: matplotlib, seaborn\nMachine Learning: scikit-learn\nStatistics: statsmodels\n\nWithout those packages, Python cannot or at least very hard to do statistical and data analysis. Again, Python is a general purpose language, but R is built for statistics. R has many built-in functionalities specifically for statistics and data management. But with those packages, Python can also do fancy statistics and data science.\nTo install a Python package in Posit cloud, in your RStudio project, run\n\nlibrary(reticulate)\nvirtualenv_create(\"myenv\")\n\nThen go to Tools &gt; Global Options &gt; Python &gt; Select &gt; Virtual Environments\n\n\n\n\nYou may need to restart R session. Do it, and in the new R session, run the following to install NumPy, pandas, and matplotlib packages.\n\nlibrary(reticulate)\npy_install(c(\"numpy\", \"pandas\", \"matplotlib\"))\n\nRun the following Python code, and make sure everything goes well.\n\nimport numpy as np\nimport pandas as pd\nv1 = np.array([3, 8])\nv1\ndf = pd.DataFrame({\"col\": ['red', 'blue', 'green']})\ndf\n\n\n4.2.2 Operators\nPython and R have very similar arithmetic and logical operator syntax.\n\n## Arithmetic Example\n2 + 3 * 5 + 4\n# 21\n2 + 3 * (5 + 4)\n# 29\n\n\n## Logical Example\n5 &lt;= 5\n# True\n5 &lt;= 4\n# False\n5 != 5  # Is 5 NOT equal to 5?\n# False\n\nTo negate key words True or False in Python, we add not in front of them. Also, instead of using & and |, we use and and or to comparisons.\n\n## Boolean Operations\nTrue != False  # Is TRUE not equal to FALSE?\n# True\nnot True == False  # Is not TRUE equal to FALSE?\n# True\nTrue or False  # TRUE if either one is TRUE or both are TRUE\n# True\n\nTo use mathematical functions in Python, we need to import the math module. Then to call any function in this module, we first type the module name followed by dot, and then the function name. Such syntax is general for using methods in a Python module or library, like libraryname.functionname().\n\n## Built-in Functions\nimport math\nmath.sqrt(144)\n# 12.0\nmath.exp(1)  # Euler's number\n# 2.718281828459045\nmath.sin(math.pi / 2)\n# 1.0\nabs(-7)\n# 7\nmath.factorial(5)\n# 120\nmath.log(100)  # Natural log with base e\n# 4.605170185988092\nmath.log(100, 10)  # Log function with specified base 10\n# 2.0\n\nIn Python, we use = to do assignment.\n\n## Assignment\nx = 5\nx\n# 5\n## Variable Operations\nx = x + 6\nx\n# 11\nx == 5\n# False\nmath.log(x)\n# 2.3978952727983707\n\nAs shown in Basic R, the following code is an example of BAD naming! Never do this!\n\n## Bad Naming (Avoid Doing This in Practice)\nmath.pi = 20  # This is bad coding, avoid overwriting built-in names\nabs\n# &lt;built-in function abs&gt;\nabs = abs(math.pi)\nabs\n\n\n4.2.3 Object types\nTo check object type, we also use type(). Note that in Python 5 itself is an integer with no decimal places. 5.0 instead is of type float corresponding to type double in R. We can turn a float into an integer using int(). The character type in R is the type str short for string in Python.\n\n## Type Checking\ntype(5)\n# &lt;class 'int'&gt;\ntype(5.0)\n# &lt;class 'float'&gt;\ntype(int(5.0))\n# &lt;class 'int'&gt;\ntype(\"I_love_stats!\")\n# &lt;class 'str'&gt;\n\nThe logical type in R is the type bool short for boolean in Python.\n\n## Boolean Type\ntype(1 &gt; 3)\n# &lt;class 'bool'&gt;\nprint(isinstance(5, int))\n# True\n\nWe can use isinstance() function to check whether or not the specified object is of the specified type.\n\nisinstance(5, float)\n# False\nisinstance(5.0, int)\n# False\n\n\n4.2.4 Python data structures\nPython has built-in data structures including lists, tuples, dictionaries, and sets. However, they are not specifically for statistics and data science. We usually use ‚Äúarray‚Äù in the library NumPy and ‚ÄúDataFrames‚Äù in pandas for statistical analysis. There is no exactly one-to-one correspondence of R and Python data structures. Below I show you one Python version of R data structures. We can definitely use other Python structures to represent the same thing in R.\n Vector (One-dimensioanl Array of NumPy) \nPython has numbers and strings, but no built-in vector structure. To create a sequence type of structure, we can use a list that can save several elements in an single object. To create a list in Python, we use []. For details about Python lists, please check Appendix B.\nHere we use one dimensional array structure in NumPy to represent a vector. First we import and give NumPy a shorter name np.\n\nimport numpy as np\n\nThen to create a one dimensional array, we call the function array(), with a list of objects inside.\n\n## Vector Creation\ndbl_vec = np.array([1, 2.5, 4.5])\ndbl_vec\n# array([1. , 2.5, 4.5])\nint_vec = np.array([1, 6, 10])\nint_vec\n# array([ 1,  6, 10])\nlog_vec = np.array([True, False, False])\nlog_vec\n# array([ True, False, False])\nchr_vec = np.array([\"pretty\", \"girl\"])\nchr_vec\n# array(['pretty', 'girl'], dtype='&lt;U6')\n\nThe function len() is used to check the number of elements in the array, and the method .dtype short for data type, can be used to check an NumPY object‚Äôs type or specify or convert data type. Note that when we write dbl_vec.dtype, it in fact gives us the data type of the first element of the array. It is a 64-bit floating-point number, where 64-bit is its size saved in the memory.\n\n## Vector Properties\nlen(dbl_vec)  # Length of the vector\n# 3\ndbl_vec.dtype  # Type of elements in the vector\n# dtype('float64')\n\n\nchr_vec[0].dtype\n# dtype('&lt;U6')\nchr_vec[1].dtype\n# dtype('&lt;U4')\n\n Operations on Vectors (1D Array) \nSame as R, Python array operations happen element-wisely.\n\n\n\n## Vector Arithmetic\nv1 = np.array([3, 8])\nv2 = np.array([4, 100])\n\n# %%\n# Vector addition\nv1 + v2\n# array([  7, 108])\n# Vector subtraction\nv1 - v2\n# array([ -1, -92])\n\n\n\n\n\n# Vector multiplication and division\nv1 * v2\n# array([ 12, 800])\nv1 / v2\n# array([0.75, 0.08])\nnp.sqrt(v2)\n# array([ 2., 10.])\n\n\n\n Recycling of Vectors (1D Array) \nUnlike R, Python Numpy array does not support vector recycling unless scalar operations.\n\n## Recycling in Vector Arithmetic\nv1 = np.array([3, 8, 4, 5])\nv1 * 2  # Element-wise multiplication\n# array([ 6, 16,  8, 10])\nv1 * np.array([2, 2, 2, 2])  # Equivalent to above\n# array([ 6, 16,  8, 10])\nv3 = np.array([4, 11])\n# Use np.resize to automatically resize v3 to match v1's length\nv1.shape\n# (4,)\nv3_resized = np.resize(v3, v1.shape)\nv3_resized\n# array([ 4, 11,  4, 11])\nv1 + v3_resized\n# array([ 7, 19,  8, 16])\n\nIf we do v1 + v3, Python will render an error message below saying that the two vectors are not of the same size.\n# Traceback (most recent call last):\n#   File \"&lt;string&gt;\", line 1, in &lt;module&gt;\n# ValueError: operands could not be broadcast together with shapes (4,) (2,)\nSo in Python, we need to do recycling manually. We can first resize (.resize) the vector v3 so that is has the same size (.shape) as v1.\n Subsetting Vectors (1D Array) \nAlways keep in mind that the indexing of Python starts with 0!!! So we grab the first element with indexing [0], and in general the kth element with [k-1]. If we want to keep multiple elements, we can use a Python list by a pair of square brackets []. So v1[[0, 2]] keeps the first and third element of v1. In Python, we can use np.delete method to remove elements.\n\n\n\n\n\n\nWarning\n\n\n\nIn Python array, we cannot use v1[[-1, -2]] or v1[-[1, 2]] to remove the second and third element.\nv1[[-1, -2]] actually returns the last element of v1 followed by the second last. The negative indexing works in Python, and it means indexing from the last.\n\nv1[[-1, -2]]\n# array([5, 4])\n\nv1[-[1, 2]] instead will render an error. There is no such index rule by adding a negative sign in front of a list.\n\n\n\n\n\n## Subsetting\nv1\n# array([3, 8, 4, 5])\nv2\n# array([  4, 100])\nv1[0]  # First element\n# 3\nv2[1]  # Second element\n# 100\n\n\n\n\n\n# %%\nv1[[0, 2]] # Corresponds to v1[c(1, 3)] in R\n# array([3, 4])\nnp.delete(v1, [1, 2])  # Corresponds to v1[-c(2, 3)] in R\n# array([3, 5])\n\n\n\n\n Factor (pd.Categorical()) \nThere is no default data structure type factor in Python. One similar to factor in Python is the Categorical vector in pandas package. We first import the package into our working session, and call it pd.\nWe can create a pandas categorical vector, we use pd.Categorical(), and inside the call, we provide a list-like object.\n\n# Factor equivalent in Python using pandas\nfac = pd.Categorical([\"med\", \"high\", \"low\"])\nfac\n# ['med', 'high', 'low']\n# Categories (3, object): ['high', 'low', 'med']\ntype(fac)\n# &lt;class 'pandas.core.arrays.categorical.Categorical'&gt;\n\nfac.categories can check the categories or levels, and fac.codes shows how those levels are coded in numbers. You see that by default they are ordered by the length of objects first and then alphabetically. low and med with 3 characters are shorter than high, and ‚Äúl‚Äù comes earlier than m. Therefore we have low = 1, med = 2, and high = 3.\n\nfac.categories\n# Index(['high', 'low', 'med'], dtype='object')\nfac.codes\n# array([2, 0, 1], dtype=int8)\n\nWe can create an ordered categorical vector by adding ordered=True. The order will follow the specification in the argument categories. Notice that now we have ['low' &lt; 'med' &lt; 'high'].\n\norder_fac = pd.Categorical([\"med\", \"high\", \"low\"], categories=[\"low\", \"med\", \"high\"], ordered=True)\norder_fac\n# ['med', 'high', 'low']\n# Categories (3, object): ['low' &lt; 'med' &lt; 'high']\norder_fac.codes\n# array([1, 2, 0], dtype=int8)\n\n\n List \nPython has it own built-in list structure. Unlike R list, Python lists cannot have named elements. To create a Python built-in list, we use []. Check Appendix B for more details.\n\n\n\n# Creating and accessing lists\nx_lst = [[1, 2, 3], \"a\", [True, False]]\nx_lst\n# [[1, 2, 3], 'a', [True, False]]\n\n\n\n\n\ntype(x_lst)\n# &lt;class 'list'&gt;\nlen(x_lst)\n# 3\n\n\n\nExtracting a single element of a Python list is straightforward. Just put the index in the square bracket.\n\n# Subsetting list elements\nx_lst[0] \n# [1, 2, 3]\ntype(x_lst[0])\n# &lt;class 'list'&gt;\n\nIf we would like to extract multiple elements in a Python list, we need to use a slice operator that is represented by colons. It takes at least two arguments: starting index and ending index. The starting index is called inclusive, and the ending index is called exclusive. For example lst[2:4] means we grab the third element and the fourth element of a list lst.\nBy default, it creates a sequence of indices with increment 1. We can add one more colon followed by the specified gap of the indices. For example 2:8:2 will create a sequence of indices (2, 4, 6). Not that 8 is excluded. The followings show some examples.\n\nx_lst[0:2:1]\n# [[1, 2, 3], 'a']\nx_lst[0:2]\n# [[1, 2, 3], 'a']\nx_lst[0:3:2]\n# [[1, 2, 3], [True, False]]\n\n\n Matrix (2-dimensional numpy array) \nThe matrix structure is in fact a 2-dimensional array which can be created by the numpy package.\nTo create a 2D array, we create a list of list in np.array(). The first list element is the the first row of the resulting matrix, and the second list element is the the second row of the resulting matrix, and so on. By default, Python will fill in elements row by row.\n\nmat = np.array([[1, 4], [2, 5], [3, 6]])\nmat\n# array([[1, 4],\n#        [2, 5],\n#        [3, 6]])\n\nWe could also create a list of numbers, then use .reshape method to decide the dimension of the matrix, and how the numbers in the list are filled in the matrix. The argument order = \"F\" means we‚Äôd like to fill elements by columns.\n\nmat = np.array([1, 2, 3, 4, 5, 6]).reshape((3, 2), order = \"F\")\nmat\n# array([[1, 4],\n#        [2, 5],\n#        [3, 6]])\n\n\nmat.shape # Dimension\n# (3, 2)\nmat.shape[0] # Number of rows\n# 3\nmat.shape[1] # Number of columns\n# 2\n\n Subsetting a Matrix \nSubsetting a matrix in Python is similar to that in R. We have two sets of indices for row and column respectively that are separated by comma. However, in Python, if we keep all rows or all columns, we need to add colon : in the row index or column index. Also, remember that indexing in Python starts with 0.\n\n\n\nmat[:, 1] # Second column\n# array([4, 5, 6])\nmat[1, :] # Second row and all columns\n# array([2, 5])\n\n\n\n\n\nmat[[0, 2], :] # First and third rows\n# array([[1, 4],\n#        [3, 6]])\n\n\n\n Stacking Matrices \nTo combine two matrices, we use np.hstack() and np.vstack(). np.hstack() is similar to cbind() in R that stacks arrays in sequence horizontally (column wise). Instead, np.vstack() is similar to rbind()` in R that stacks arrays in sequence vertically (row wise)\n\n# Column binding (cbind in R)\nprint(mat)\n# [[1 4]\n#  [2 5]\n#  [3 6]]\nmat_c = np.array([7, 0, 0, 8, 2, 6]).reshape((3, 2), order='F')\nprint(np.hstack((mat, mat_c)))  # Should have the same number of rows\n# [[1 4 7 8]\n#  [2 5 0 2]\n#  [3 6 0 6]]\n\n\n# Row binding (rbind in R)\nprint(mat)\n# [[1 4]\n#  [2 5]\n#  [3 6]]\nmat_r = np.array([1, 2, 3, 4]).reshape((2, 2), order='F')\nprint(np.vstack((mat, mat_r)))  # Should have the same number of columns\n# [[1 4]\n#  [2 5]\n#  [3 6]\n#  [1 3]\n#  [2 4]]\n\n\n Data Frame \nPython has no built-in data frame structure, and the numpy package does not supply it too. The data frame can be created using the pandas package using the command pd.DataFrame() once we import pandas as pd.\n\nimport pandas as pd\n\nInside pd.DataFrame(), we need to provide a sequence of named objects, where names are column names or variable names of the data frame. To provide such, we can use a Python built-in dictionary that is generated by {} with the key-value structure as key:value. The keys in the dictionary will work as the column names of the resulting data frame. For more details about Python dictionaries, please check Appendix B.\n\n# Creating a DataFrame\ndf = pd.DataFrame({\"age\": [19, 21, 40], \"gender\": [\"m\", \"f\", \"m\"]})\ndf\n#    age gender\n# 0   19      m\n# 1   21      f\n# 2   40      m\n\nWe can check summary of the data frame using the .info() method.\n\n# DataFrame structure\ndf.info()\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 3 entries, 0 to 2\n# Data columns (total 2 columns):\n#  #   Column  Non-Null Count  Dtype \n# ---  ------  --------------  ----- \n#  0   age     3 non-null      int64 \n#  1   gender  3 non-null      object\n# dtypes: int64(1), object(1)\n# memory usage: 180.0+ bytes\n\n Properties of Data Frames \n\n# Accessing DataFrame properties\ndf.columns  # Names of columns\n# Index(['age', 'gender'], dtype='object')\nlen(df)  # Number of observations\n# 3\ndf.shape[1]  # Number of columns\n# 2\ndf.shape  # Dimensions\n# (3, 2)\ntype(df)  # Type of df\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\ndf.__class__.__name__  # Class of df\n# 'DataFrame'\n\nTo combine two data frames, we can use pd.concat() command. Notice the difference between the case with and without ignore_index=True. If True, the resulting axis will be labeled \\(0, . . ., n - 1\\). This is useful if you are concatenating objects where the concatenation axis does not have meaningful indexing information.\n\n# Row binding with DataFrames\ndf_r = pd.DataFrame({\"age\": [10], \"gender\": [\"f\"]})\npd.concat([df, df_r], ignore_index=True)\n#    age gender\n# 0   19      m\n# 1   21      f\n# 2   40      m\n# 3   10      f\npd.concat([df, df_r])\n#    age gender\n# 0   19      m\n# 1   21      f\n# 2   40      m\n# 0   10      f\n\nBy default, pd.concat() combines two data frames by rows (axis=0). If we like to combine data frames by columns, we add axis=1 in the function.\n\n# Column binding with DataFrames\ndf_c = pd.DataFrame({\"col\": [\"red\", \"blue\", \"gray\"]})\ndf_new = pd.concat([df, df_c], axis=1)\ndf_new\n#    age gender   col\n# 0   19      m   red\n# 1   21      f  blue\n# 2   40      m  gray\n\n Subsetting a Data Frame \nTo access a group of rows and columns of a data frame, we can use .loc() or iloc(). loc is short for location, and i stands for index. In the examples, we use iloc() with index [0, 2] to grab the first and the third row. We use .loc() to get the ‚Äòage‚Äô column by its name, which can also be got using df_new[\"age\"]. We can put a selection condition in the brackets similar to what we do in R.\n\n# Subsetting rows\ndf_new.iloc[[0, 2], :]  # Subset rows\n#    age gender   col\n# 0   19      m   red\n# 2   40      m  gray\ndf_new.loc[:, 'age']\n# 0    19\n# 1    21\n# 2    40\n# Name: age, dtype: int64\ndf_new[\"age\"]\n# 0    19\n# 1    21\n# 2    40\n# Name: age, dtype: int64\ndf_new[df_new[\"age\"] == 21]  # Select row where age == 21\n#    age gender   col\n# 1   21      f  blue\n\nLook carefully the difference between one and two brackets subsetting. With one bracket, the data frame becomes a one dimensional pandas Series which is similar to 1D numpy array, and can work as a vector. When two brackets are used for subsetting, the data frame structure is kept. The two outputs are printed differently too.\n\n# Subsetting columns\ndf_new[\"age\"]  # become a pd Series\n# 0    19\n# 1    21\n# 2    40\n# Name: age, dtype: int64\ntype(df_new[\"age\"])\n# &lt;class 'pandas.core.series.Series'&gt;\ndf_new[[\"age\"]]  # Still a pd DataFrame\n#    age\n# 0   19\n# 1   21\n# 2   40\ntype(df_new[[\"age\"]])\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\ndf_new[[\"age\", \"gender\"]]  # Multiple columns like a matrix\n#    age gender\n# 0   19      m\n# 1   21      f\n# 2   40      m\ndf_new.loc[:, [\"age\", \"gender\"]]  # Equivalent to matrix-like subsetting\n#    age gender\n# 0   19      m\n# 1   21      f\n# 2   40      m",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>*P*repare *Y*ourself for data</span>"
    ]
  },
  {
    "objectID": "intro-py.html#exercises",
    "href": "intro-py.html#exercises",
    "title": "4¬† Prepare Yourself for data",
    "section": "\n4.3 Exercises",
    "text": "4.3 Exercises\n\nVector\n\nThe code above shows a Marquette student poker and roulette winnings from Monday to Friday. Copy and paste them into your Python session and complete problem 1.\n\nAssign to the variable total_daily how much you won or lost on each day in total (poker and roulette combined).\nCalculate the winnings overall total_week. Print it out.\n\n\n\n# ==============================================================================\n## Factor\n# ==============================================================================\n# Create speed_vector\nspeed_vec = pd.Categorical([\"medium\", \"low\", \"low\", \"medium\", \"high\"])\n\n\nFactor\n\n\n\nspeed_vec above should be converted to an ordinal factor since its categories have a natural ordering. Create an ordered factor vector speed_fac by completing the code below.\n\n\n# Create speed_vector\n___________ = pd.Categorical(______, categories=___________, ordered=______)\n\n\n\n# ==============================================================================\n## Data frame\n# ==============================================================================\n# Defining vectors for planets\nname = [\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\"]\nplanet_type = [\"Terrestrial planet\", \"Terrestrial planet\", \"Terrestrial planet\", \n         \"Terrestrial planet\", \"Gas giant\", \"Gas giant\", \n         \"Gas giant\", \"Gas giant\"]\ndiameter = [0.375, 0.947, 1, 0.537, 11.219, 9.349, 4.018, 3.843]\nrotation = [57.63, -242.03, 1, 1.05, 0.42, 0.44, -0.73, 0.65]\nrings = [False, False, False, False, True, True, True, True]\n\n\nData Frame\n\nData frames have properties of lists and matrices, so we skip lists and matrices and focus on data frames. You want to construct a data frame that describes the main characteristics of eight planets in our solar system. You feel confident enough to create the necessary vectors: name, planet_type, diameter, rotation and rings that have already been coded up as above. The first element in each of these vectors corresponds to the first observation.\n\nUse the function pd.DataFrame() to construct a data frame. Pass the vectors name, planet_type, diameter, rotation and rings as arguments in this order. Call the resulting data frame planets_df.\n\n\n_________ = pd.___________({____:_____, ____:_____, ____:_____, ____:_____, ____:_____})\n\n\nFrom planets_df, select the diameter of Mercury: this is the value at the first row and the third column. Simply print out the result.\nFrom planets_df, select all data on Mars (the fourth row). Simply print out the result.\nSelect and print out the first 5 values in the diameter column of planets_df.",
    "crumbs": [
      "Statistics and Data",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>*P*repare *Y*ourself for data</span>"
    ]
  },
  {
    "objectID": "datasummary.html",
    "href": "datasummary.html",
    "title": "Summarizing Data",
    "section": "",
    "text": "Statistics can be divided into two parts: descriptive statistics and inferential statistics (statistical inference). Before doing inferential statistics, we usually learn to understand our data because we cannot use appropriate methods, do the analysis and interpret the result correctly without fully understanding our data. Descriptive statistics discusses how to describe and summarize data by frequency/count tables, graphics/visualization or some important numerical measures.\nThis Part is about descriptive statistics. We learn frequency tables and basic data visualization in 5¬† Data Visualization and numerical measures in 6¬† Numerical Measures of Data. You may already learned descriptive statistics in your elementary, middle or high school. If yes, this Part is more like a review. But the difference is that you will learn how to use R to describe and plot your data.\n\n\n\n\n\n\n\nFigure¬†1: Branches of statistics",
    "crumbs": [
      "Summarizing Data"
    ]
  },
  {
    "objectID": "data-graphics.html",
    "href": "data-graphics.html",
    "title": "5¬† Data Visualization",
    "section": "",
    "text": "5.1 Frequency Table for Categorical Variable\nIn this chapter, we talk about how to visualize our data using R. We use basic statistical graphics for displaying categorical and numerical data, and showing their frequency distribution graphically. There are various visualization and chart types out there, and data visualization itself could be an one-semester course. 1 Please google for more online resources.\nThe most popular R package for graphics is ggplot2 that is one of the packages in the tidyverse ecosystem. This book does not use tidyverse syntax, and use the default base R syntax to do all the statistical computations including graphics. While tidyverse is getting popular, and has become the default way of doing data science, the Base R methods still have some advantages, and worth learning for R beginners. 2\nFor a categorical variable data, we usually summarize its information in terms of a frequency table (frequency distribution)3 that lists its variable ‚Äúvalues‚Äù (categories) individually along with their corresponding number of times occurred in the data (frequencies or counts). Below is a general format of a frequency table for categorical data with \\(n\\) being the total number of data values. It is also called one-way count table where one-way refers to one categorical variable being considered in the table. In the table, the variable has \\(k\\) categories, \\(C_1, \\dots, C_k\\), and \\(C_i\\) occurs \\(f_i\\) times in the data, \\(i = 1, \\dots, k\\). Relative frequency on the right is the proportion of the number of times a category occurs to the total number of data values. Relative frequency is closely related to the idea of probability that is discussed in Chapter 7.\nWe can also make a frequency table with Category and Frequency shown in rows as long as every category and its corresponding counts are correctly shown in the table.\nBelow is an concrete example of a categorical variable color that has three categories.\nThe relative frequency of ‚Äúred‚Äù tells us that color red appears 16% of the time in the data that has totally \\(8+26+16 = 50\\) objects.",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-graphics.html#frequency-table-for-categorical-variable",
    "href": "data-graphics.html#frequency-table-for-categorical-variable",
    "title": "5¬† Data Visualization",
    "section": "",
    "text": "Category name\nFrequency\nRelative Frequency\n\n\n\n\\(C_1\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(C_2\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(C_k\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\nRed üî¥\n8\n8/50 = 0.16\n\n\nBlue üîµ\n26\n26/50 = 0.52\n\n\nBlack ‚ö´\n16\n16/50 = 0.32\n\n\n\n\n\n\n\nR\nPython\n\n\n\nBelow is the loan50 data set from the openintro package in R. The data set has 50 subjects with 18 variables. We construct a categorical frequency table using the variable homeownership.\n\n# install.packages(\"openintro\")\nlibrary(openintro)\nstr(loan50)\n# tibble [50 √ó 18] (S3: tbl_df/tbl/data.frame)\n#  $ state                  : Factor w/ 51 levels \"\",\"AK\",\"AL\",\"AR\",..: 32 6 41 6 36 16 35 25 11 11 ...\n#  $ emp_length             : num [1:50] 3 10 NA 0 4 6 2 10 6 3 ...\n#  $ term                   : num [1:50] 60 36 36 36 60 36 36 36 60 60 ...\n#  $ homeownership          : Factor w/ 3 levels \"rent\",\"mortgage\",..: 1 1 2 1 2 2 1 2 1 2 ...\n#  $ annual_income          : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 80000 ...\n#  $ verified_income        : Factor w/ 4 levels \"\",\"Not Verified\",..: 2 2 4 2 2 3 3 2 2 3 ...\n#  $ debt_to_income         : num [1:50] 0.558 1.306 1.056 0.574 0.238 ...\n#  $ total_credit_limit     : int [1:50] 95131 51929 301373 59890 422619 349825 15980 258439 87705 330394 ...\n#  $ total_credit_utilized  : int [1:50] 32894 78341 79221 43076 60490 72162 2872 28073 23715 32036 ...\n#  $ num_cc_carrying_balance: int [1:50] 8 2 14 10 2 4 1 3 10 4 ...\n#  $ loan_purpose           : Factor w/ 14 levels \"\",\"car\",\"credit_card\",..: 4 3 4 3 5 5 4 3 3 4 ...\n#  $ loan_amount            : int [1:50] 22000 6000 25000 6000 25000 6400 3000 14500 10000 18500 ...\n#  $ grade                  : Factor w/ 8 levels \"\",\"A\",\"B\",\"C\",..: 3 3 6 3 3 3 5 2 2 4 ...\n#  $ interest_rate          : num [1:50] 10.9 9.92 26.3 9.92 9.43 ...\n#  $ public_record_bankrupt : int [1:50] 0 1 0 0 0 0 0 0 0 1 ...\n#  $ loan_status            : Factor w/ 7 levels \"\",\"Charged Off\",..: 3 3 3 3 3 3 3 3 3 3 ...\n#  $ has_second_income      : logi [1:50] FALSE FALSE FALSE FALSE FALSE FALSE ...\n#  $ total_income           : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 192000 ...\n\nNote that a categorical variable in R is of either type factor or character. homeownership is of type factor that has three categories ‚Äúrent‚Äù, ‚Äúmortgage‚Äù and ‚Äúown‚Äù. In the code I create another object x that stores homeownership data.\n\n(x &lt;- loan50$homeownership)\n#  [1] rent     rent     mortgage rent     mortgage mortgage rent     mortgage\n#  [9] rent     mortgage rent     mortgage rent     mortgage rent     mortgage\n# [17] rent     rent     rent     mortgage mortgage mortgage mortgage rent    \n# [25] mortgage rent     mortgage own      mortgage mortgage rent     mortgage\n# [33] mortgage rent     rent     own      mortgage rent     mortgage rent    \n# [41] mortgage rent     rent     mortgage mortgage mortgage mortgage rent    \n# [49] own      mortgage\n# Levels: rent mortgage own\n\nWe can simply use the function table() to create the frequency table for the variable homeownership.\n\n## frequency table\ntable(x)\n# x\n#     rent mortgage      own \n#       21       26        3\n\n\n\n\n\n\n\nIf we want to create a frequency table shown in definition, which R data structure we can use?\n\n\n\n\n\n\nThe frequency table can be constructed as R matrix. The relative frequency is computed using the count vector freq divided by the total number of observations, which is sum of counts of each category. Finally we can combine the frequency and relative frequency together by columns using cbind().\n\nfreq &lt;- table(x)\nrel_freq &lt;- freq / sum(freq)\ncbind(freq, rel_freq)\n#          freq rel_freq\n# rent       21     0.42\n# mortgage   26     0.52\n# own         3     0.06\n\n\n\n Bar Chart \nTo visualizing the categorical data, we usually turn its frequency table into a plot. Such plot is called the bar chart. Each bar stands for one category of the variable, and the height of the bar indicates the frequency. Below is a bar chart that visualizes the homeownership frequency table. In R, we use the function barplot() to make a bar chart. In the first argument height, remember to put the frequency table of our data vector, not the data vector itself. As the basic plotting, we can add the title using main argument, and \\(x\\) label using xlab.\n\nbarplot(height = table(x), main = \"Bar Chart\", xlab = \"Homeownership\")\n\n\n\n\n\n\n\n Pie Chart \nThe homeownership frequency table can also be visualized using a pie chart, especially when we want to see the proportion or distribution of the count of category. In R, simply use pie() function with the frequency table as the argument.\n\npie(x = table(x), main = \"Pie Chart\")\n\n\n\n\n\n\n\n\n\nHere we demo how create a categorical frequency table in Python. loan50 data are not in any Python package. It has been saved as a loan50.csv file. The extension ‚Äúcsv‚Äù stands for ‚Äúcomma-separated values‚Äù which is a common text file format that stores data in a table structure.\n\nimport pandas as pd\n\nTo import the loan50.csv data into our working session, in Python we can use pd.read_csv(), and provide the information about where we save the data file, or the file path, so that Python knows where he can read it. The file path needs to be a string with quote. In my example, the file path starts with a dot, which means ‚Äúcurrent working directory‚Äù, whatever it is. The file loan50.csv is saved in the data folder under the current working directory.\n\nloan50 = pd.read_csv(\"./data/loan50.csv\")\nloan50.info()\n# &lt;class 'pandas.core.frame.DataFrame'&gt;\n# RangeIndex: 50 entries, 0 to 49\n# Data columns (total 18 columns):\n#  #   Column                   Non-Null Count  Dtype  \n# ---  ------                   --------------  -----  \n#  0   state                    50 non-null     object \n#  1   emp_length               48 non-null     float64\n#  2   term                     50 non-null     int64  \n#  3   homeownership            50 non-null     object \n#  4   annual_income            50 non-null     int64  \n#  5   verified_income          50 non-null     object \n#  6   debt_to_income           50 non-null     float64\n#  7   total_credit_limit       50 non-null     int64  \n#  8   total_credit_utilized    50 non-null     int64  \n#  9   num_cc_carrying_balance  50 non-null     int64  \n#  10  loan_purpose             50 non-null     object \n#  11  loan_amount              50 non-null     int64  \n#  12  grade                    50 non-null     object \n#  13  interest_rate            50 non-null     float64\n#  14  public_record_bankrupt   50 non-null     int64  \n#  15  loan_status              50 non-null     object \n#  16  has_second_income        50 non-null     bool   \n#  17  total_income             50 non-null     int64  \n# dtypes: bool(1), float64(3), int64(8), object(6)\n# memory usage: 6.8+ KB\n\nThe x that saves the ‚Äòhomeownership‚Äô column is a pd.Series.\n\n# Extract the 'homeownership' column\nx = loan50['homeownership']\nx\n# 0         rent\n# 1         rent\n# 2     mortgage\n# 3         rent\n# 4     mortgage\n# 5     mortgage\n# 6         rent\n# 7     mortgage\n# 8         rent\n# 9     mortgage\n# 10        rent\n# 11    mortgage\n# 12        rent\n# 13    mortgage\n# 14        rent\n# 15    mortgage\n# 16        rent\n# 17        rent\n# 18        rent\n# 19    mortgage\n# 20    mortgage\n# 21    mortgage\n# 22    mortgage\n# 23        rent\n# 24    mortgage\n# 25        rent\n# 26    mortgage\n# 27         own\n# 28    mortgage\n# 29    mortgage\n# 30        rent\n# 31    mortgage\n# 32    mortgage\n# 33        rent\n# 34        rent\n# 35         own\n# 36    mortgage\n# 37        rent\n# 38    mortgage\n# 39        rent\n# 40    mortgage\n# 41        rent\n# 42        rent\n# 43    mortgage\n# 44    mortgage\n# 45    mortgage\n# 46    mortgage\n# 47        rent\n# 48         own\n# 49    mortgage\n# Name: homeownership, dtype: object\n\nIn Python, we use .value_counts() to obtain the frequency table of a pd.Series.\n\nfreq = x.value_counts()\nfreq\n# homeownership\n# mortgage    26\n# rent        21\n# own          3\n# Name: count, dtype: int64\n\n\n# Calculate relative frequency\nrel_freq = freq / freq.sum()\nrel_freq\n# homeownership\n# mortgage    0.52\n# rent        0.42\n# own         0.06\n# Name: count, dtype: float64\n\nI save the final frequency table as a pd.DataFrame.\n\nfreq_table = pd.DataFrame({'Frequency': freq, 'Relative Frequency': rel_freq})\nfreq_table\n#                Frequency  Relative Frequency\n# homeownership                               \n# mortgage              26                0.52\n# rent                  21                0.42\n# own                    3                0.06\n\n Bar Chart \nThere are several packages used for plotting in Python. We use matplotlib for demonstration in the book. Check Appendix B for detailed discussion of plotting using the package. We first import the module pyplot of matplotlib as plt for plotting.\n\n\nimport matplotlib.pyplot as plt\n\nThe function plt.figure() is used to create a new figure with some pre-specified settings. For example, here we set the figure size having width 5 inches and height 4 inches. The we use plt.bar() to generate a bar plot, where argument x shows the categories, and height shows their number of frequencies. Then we add the figure‚Äôs title and axis labels, and show the plot.\n\n# Bar chart for homeownership\nplt.figure(figsize=(5, 4))\nplt.bar(x = freq.index, height = freq.values)\nplt.title('Bar Chart')\nplt.xlabel('Homeownership')\nplt.ylabel('Frequency')\nplt.show()\n\n\n\n\n\n\n\n Pie Chart \nThe function plt.pie() is used to make a pie chart.\n\n# Pie chart for homeownership\nplt.figure(figsize=(5, 4))\nplt.pie(x=freq.values, labels=freq.index)\n# ([&lt;matplotlib.patches.Wedge object at 0x33447dd00&gt;, &lt;matplotlib.patches.Wedge object at 0x334221670&gt;, &lt;matplotlib.patches.Wedge object at 0x33449eae0&gt;], [Text(-0.06906950569907239, 1.0978294054098232, 'mortgage'), Text(-0.13786673266574132, -1.091326149244154, 'rent'), Text(1.0805159332487935, -0.20611966911357438, 'own')])\nplt.title('Pie Chart')\nplt.show()",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-graphics.html#frequency-distribution-for-numerical-variables",
    "href": "data-graphics.html#frequency-distribution-for-numerical-variables",
    "title": "5¬† Data Visualization",
    "section": "\n5.2 Frequency Distribution for Numerical Variables",
    "text": "5.2 Frequency Distribution for Numerical Variables\nIt is not that straightforward as categorical data when we create a frequency distribution for a numerical variable. Categorical variables have natural separated or non-overlapping categories that we can count the number for, but numerical variables don‚Äôt have such natural divisions. We may have the numerical data so that every data value is shown just once, like \\((2.3, 4.5, 4.6, 4.8, 2.8, 5.9)\\), and summarizing the data like\n\n# \n# 2.3 2.8 4.5 4.6 4.8 5.9 \n#   1   1   1   1   1   1\n\ndoes not make sense because it does not give us how the data are generally distributed.\nIn order to create a more meaningful frequency table of numerical data, we first need to divide the data into several groups. Therefore, we are able to see which groups contain more data values than others, and understand the general pattern of the distribution of data.\nWe sort of convert numerical data into categorical one. The procedure is as follows.\n\nDecide the number of non-overlapping groups of intervals \\(k\\), or classes for dividing the data. The non-overlapping property makes sure that one value belongs one and only one class, like categorical data.\nConvert the data into \\(k\\) categories with an associated class interval. We need to know the upper and lower limit of each interval, so that we know which interval or class a data value belongs to.\nCount the number of measurements falling in a given class interval (class frequency). After this we know the number of data points falling in each class interval, and this gives us the frequency distribution for a numerical variable.\n\nThe general format of the frequency distribution of numerical data is shown below.\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\\(1\\)\n\\([a_1, a_2]\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(2\\)\n\\((a_2, a_3]\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n\\(k\\)\n\\((a_k, a_{k+1}]\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\nNote that the interval \\([a_1, a_2]\\) includes \\(a_2\\), but \\((a_2, a_3]\\) does not. So if there is a data point having value \\(a_2\\), it is in Class 2, not Class 1. Same logic for other intervals. Such open and closed intervals guarantees the non-overlapping property.\nNotice that all class widths are the same! That is, \\((a_2 - a_1) = (a_3 - a_2) = \\cdots = (a_{k+1} - a_k)\\). It is not allowed to have some classes have a larger range of numbers, and some have a smaller range.\n\n\n\n\n\n\nCan our grade conversion be used for creating a frequency distribution?\n\n\n\nNo, because the class widths are not all the same as seen in Figure¬†5.1.\n\n\n\n\n\n\n\n\nGrade\n\n\nPercentage\n\n\n\n\n\nA\n\n\n[94, 100]\n\n\n\n\nA-\n\n\n[90, 94)\n\n\n\n\nB+\n\n\n[87, 90)\n\n\n\n\nB\n\n\n[83, 87)\n\n\n\n\nB-\n\n\n[80, 83)\n\n\n\n\nC+\n\n\n[77, 80)\n\n\n\n\nC\n\n\n[73, 77)\n\n\n\n\nC-\n\n\n[70, 73)\n\n\n\n\nD+\n\n\n[65, 70)\n\n\n\n\nD\n\n\n[60, 65)\n\n\n\n\nF\n\n\n[0, 60)\n\n\n\n\n\n\nFigure¬†5.1: Grading scale for this class\n\n\n\n\n Interest Rate Data \nWe use interest rate variable in the loan 50 data set for demonstrating frequency distribution and graphics for numerical data.\n\n\n\n\nR\nPython\n\n\n\n\nint_rate &lt;- round(loan50$interest_rate, 1)\nint_rate\n#  [1] 10.9  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n# [16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n# [31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n# [46] 10.4 21.4 10.9  9.4  6.1\n\n\n\n\n# Round the interest rates to one decimal place\nint_rate = loan50['interest_rate'].round(1)\nint_rate\n# 0     10.9\n# 1      9.9\n# 2     26.3\n# 3      9.9\n# 4      9.4\n# 5      9.9\n# 6     17.1\n# 7      6.1\n# 8      8.0\n# 9     12.6\n# 10    17.1\n# 11     5.3\n# 12     7.4\n# 13     5.3\n# 14     8.0\n# 15    24.8\n# 16    18.1\n# 17    10.4\n# 18     8.0\n# 19    19.4\n# 20    14.1\n# 21    20.0\n# 22     9.4\n# 23     9.9\n# 24    10.9\n# 25     5.3\n# 26     6.7\n# 27    15.0\n# 28    12.0\n# 29    12.6\n# 30    10.9\n# 31     9.4\n# 32     9.9\n# 33     7.4\n# 34    18.4\n# 35    17.1\n# 36     8.0\n# 37     6.1\n# 38     6.7\n# 39     7.3\n# 40    12.6\n# 41    16.0\n# 42    10.9\n# 43     9.9\n# 44     9.4\n# 45    10.4\n# 46    21.4\n# 47    10.9\n# 48     9.4\n# 49     6.1\n# Name: interest_rate, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\nSource: Unsplash-Markus Spiske\n\n\n\n\n\n Frequency Distribution of Interest Rate \n\n\n\n\nFrequency Distribution by R\nR Code\nFrequency Distribution by Python\nPython Code\n\n\n\nA frequency distribution of the interest rate data is created using R and shown below.\n\n#  Class Class_Intvl Freq Rel_Freq\n#      1   5% - 7.5%   11     0.22\n#      2  7.5% - 10%   15     0.30\n#      3 10% - 12.5%    8     0.16\n#      4 12.5% - 15%    5     0.10\n#      5 15% - 17.5%    4     0.08\n#      6 17.5% - 20%    4     0.08\n#      7 20% - 22.5%    1     0.02\n#      8 22.5% - 25%    1     0.02\n#      9 25% - 27.5%    1     0.02\n\n\n\n\n# Define class intervals for histogram\nk &lt;- 9\nclass_width &lt;- 2.5\nlower_limit &lt;- 5\n# Calculate class boundaries\nclass_boundary &lt;- lower_limit + 0:k * class_width\n\n# Create class intervals as strings for display\nclass_int &lt;- paste(paste0(class_boundary[1:k], \"%\"),\n                   paste0(class_boundary[2:(k+1)], \"%\"), \n                   sep = \" - \")\n\n# Calculate frequency distribution without plotting\nfreq_info &lt;- hist(int_rate, \n                  breaks = class_boundary, \n                  plot = FALSE)\n\n# Create a data frame for frequency distribution\nfreq_dist &lt;- data.frame(\n  \"Class\" = as.character(1:k), \n  \"Class_Intvl\" = class_int, \n  \"Freq\" = freq_info$counts, \n  \"Rel_Freq\" = round(freq_info$counts / length(int_rate), 2))\n\nprint(freq_dist, row.names = FALSE)\n\n\n\nA frequency distribution of the interest rate data is created using Python and shown below.\n\n#   Class    Class_Intvl  Freq  Rel_Freq\n# 0     1    5.0% - 7.5%    11      0.22\n# 1     2   7.5% - 10.0%    15      0.30\n# 2     3  10.0% - 12.5%     8      0.16\n# 3     4  12.5% - 15.0%     4      0.08\n# 4     5  15.0% - 17.5%     5      0.10\n# 5     6  17.5% - 20.0%     3      0.06\n# 6     7  20.0% - 22.5%     2      0.04\n# 7     8  22.5% - 25.0%     1      0.02\n# 8     9  25.0% - 27.5%     1      0.02\n\n\n\n\nimport numpy as np\n# Define class intervals for histogram\nk = 9\nclass_width = 2.5\nlower_limit = 5\n# Calculate class boundaries\nclass_boundary = lower_limit + np.arange(0, k+1) * class_width\n\n# Create class intervals as strings for display\nclass_int = [f\"{class_boundary[i]}% - {class_boundary[i+1]}%\" for i in range(k)]\n\n# Calculate frequency distribution without plotting\nfreq_info, bin_edges = np.histogram(int_rate, bins=class_boundary)\n\n# Create a DataFrame for frequency distribution\nfreq_dist = pd.DataFrame({\n    \"Class\": [str(i+1) for i in range(k)],\n    \"Class_Intvl\": class_int,\n    \"Freq\": freq_info,\n    \"Rel_Freq\": (freq_info / len(int_rate)).round(2)\n})\nfreq_dist\n\n\n\n\n\n\nFirst, all class widths are the same with the length 2.5%. Here, the number of classes \\(k = 9\\). \\(k\\) should not be too big or too small, and choosing \\(k\\) is subjective and depends on the research need. Too big \\(k\\) put too many different values together in the same class, and some important features may be masked. On the other hand, when \\(k\\) is too small, there may be many classes having one or few data points only, and general pattern of data becomes unclear. A simple rule is to choose \\(k\\) between 5 and 30, then tune the value a little bit to get the best presentation result. Or the number of classes \\(k\\) can be calculated from a suggested class width \\(\\approx \\frac{\\text{max} - \\text{min}}{k}\\).\n\n\nR\nPython\n\n\n\n\n# min and max value\nrange(int_rate)\n# [1]  5.3 26.3\n\n\n\n\n# min and max value\n(int_rate.min(), int_rate.max())\n# (5.3, 26.3)\n\n\n\n\nThe lower limit of the 1st class should not be greater than the minimum value of the data. Otherwise we are gonna miss some data values because the values are out of the any intervals, and the counts are incorrect. The lower limit of the 1st class is 5%, which is less than the minimum value of 5.3%. The upper limit of the last class should not be smaller than the maximum value of the data. The upper limit of the last class is 27.5%, which is greater than the maximum value of 26.3%. The idea is the class intervals should cover all the data values, so that when we classify or count them, we don‚Äôt miss any value in the table.\n\n\n\n\n\n\n\n\nHow do we choose the number of classes or the class width?\n\n\n\nR decides the number of classes for us when we visualize the frequency distribution by a histogram.\n\n\n\n Visualizing Frequency Distribution by a Histogram \n\n\nTo visualize a frequency distribution of numerical data, we use a histogram. Each bar or column in the plot stands for one class or bin in the frequency distribution, and the height shows the frequency of a column. Unlike bar charts, all the bins in the histogram are connected each other with no gaps.\n\n\nR\nPython\n\n\n\nIn R, we use the command hist() to make a histogram. The only required argument is x at which we put our data. If we don‚Äôt specify how the data are divided into classes, R does it for us. The histogram below is the histogram of the interest rate data using the default class breaks, the boundary values separating class intervals.\n\n\n\n\n\nhist(x = int_rate, \n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Defualt)\")\n\n\n\n\n\n\n\nIf we don‚Äôt like the default one, we can specify customized breaks in the argument breaks. The class_boundary object saves the class boundaries used in the previous frequency distribution. The histogram corresponding the frequency distribution is shown below. Note that when our customized breaks are used, we can see the count in the 7.5%-10% interval is more than the count in the 5%-7.5% interval. This information cannot be obtained when the default breaks are used.\n\nclass_boundary\n#  [1]  5.0  7.5 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5\nhist(x = int_rate, \n     breaks = class_boundary, #&lt;&lt;\n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Ours)\")\n\n\n\n\n\n\n\nSometimes specifying all the lower and upper limits might be tedious. The argument breaks allows a single number too, giving the number of classes \\(k\\) for the histogram, breaks = 10 for example. Sometimes the histogram won‚Äôt have the number of classes exactly the same as the number specified in breaks. It‚Äôs just an approximated one.\n\n\nIn Python, we use the command plt.hist() to make a histogram. The only required argument is x at which we put our data. If we don‚Äôt specify how the data are divided into classes, Python does it for us. The histogram below is the histogram of the interest rate data using the default class breaks, the boundary values separating class intervals.\n\nplt.hist(x = int_rate)\nplt.xlabel('Interest Rate (%)')\nplt.title('Hist. of Int. Rate (Defualt)')\nplt.show()\n\n\n\n\n\n\n\nIf we don‚Äôt like the default one, we can specify customized breaks in the argument bins. The class_boundary object saves the class boundaries used in the previous frequency distribution. The histogram corresponding the frequency distribution is shown below.\n\nclass_boundary\n# array([ 5. ,  7.5, 10. , 12.5, 15. , 17.5, 20. , 22.5, 25. , 27.5])\nplt.hist(int_rate, bins=class_boundary)\nplt.xlabel('Interest Rate (%)')\nplt.title('Hist. of Int. Rate (Ours)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n Skewness \nWe‚Äôve learned to plot a frequency distribution. Let‚Äôs learn to describe it, and see if there is any interesting pattern contained in the data. Some key characteristics of distributions include shape, center and dispersion. Skewness provides a way to summarize the shape of a distribution.\nFigure¬†5.2 illustrates skewness of distribution. When the distribution has a long right (left) tail, we say the distribution is skewed to the right (left), or it is right (left)-skewed. A right (left)-skewed distribution is also called positively (negatively) skewed. The distribution is said to be symmetric when the left side of the distribution mirrors the right side without being skewed. If you fold the distribution along the y axis, the right and left parts of the distribution will coincide.\n\n\n\n\n\n\n\nFigure¬†5.2: Distribution characteristics\n\n\n\n\n\nFind it hard to memorize the right skewed or left skewed? Figure¬†5.4 provides a good trick to learn the skewness that you will never forget. Just think of your feet. The right (left) skewed distribution resembles toes on right (left) foot. I hope the shape of your feet looks like the feet in the picture!\n\n\n\n\n\n\n\n\nIs the interest rate histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.3: Interest Rate Histogram\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†5.4: Trick for remembering skewness (Biostatistics for the Biological and Health Sciences p.53)\n\n\n\n\n\n\n\n Scatterplot for Two Numerical Variables \n\nA scatterplot provides a case-by-case view of data for two numerical variables. \\(X\\) axis represents one variable and \\(Y\\) axis stands for the other. The data value \\((x, y)\\) pair is plotted as a point in the \\(X\\)-\\(Y\\) a Cartesian coordinate plane, where \\(x\\) is the value of one variable, and \\(y\\) is the value of the other for a case. Below is a scatterplot of Loan Amount vs.¬†Total Income from the loan 50 data.\n\n\nR\nPython\n\n\n\nTo create a scatter plot in R, we use the command plot() and put the two variables‚Äô data in the x and y arguments. Their length should be identical.\n\nplot(x = loan50$total_income, y = loan50$loan_amount,\n     xlab = \"Total Income\", ylab = \"Loan Amount\",\n     main = \"Loan Amount vs Total Income\",\n     pch = 16, col = 4)\n\n\n\n\n\n\n\n\n\nTo create a scatter plot in Python, we use the command plt.scatter and put the two variables‚Äô data in the x and y arguments. Their length should be identical.\n\nplt.scatter(x = loan50['total_income'], y = loan50['loan_amount'], \n            color='blue', marker='o')\nplt.xlabel('Total Income')\nplt.ylabel('Loan Amount')\nplt.title('Loan Amount vs Total Income')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMore details about scatterplot and statistical methods for 2 numerical variables are discussed in Chapter 27.",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-graphics.html#exercises",
    "href": "data-graphics.html#exercises",
    "title": "5¬† Data Visualization",
    "section": "\n5.3 Exercises",
    "text": "5.3 Exercises\nIn the following, we will be using the data set mtcars to do some data summary and graphics. First load the data set into your R session by the command data(mtcars). The data set is like\n\n#                    mpg cyl disp  hp drat   wt qsec vs am gear carb\n# Mazda RX4         21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n# Mazda RX4 Wag     21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n# Datsun 710        22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n# Hornet 4 Drive    21.4   6  258 110 3.08 3.21 19.4  1  0    3    1\n# Hornet Sportabout 18.7   8  360 175 3.15 3.44 17.0  0  0    3    2\n# Valiant           18.1   6  225 105 2.76 3.46 20.2  1  0    3    1\n\nPlease see ?mtcars for the description of the data set.\n\nUse the function pie() to create a pie chart for the number of carburetors (carb). What the number of carburetors has the most frequencies in the data?\nUse the function barplot() to create a bar chart for the number of cylinders (cyl). What the number of cylinders has the most frequencies in the data?\nUse the function hist() to generate a histogram of the gross horsepower (hp). Is it right or left-skewed?\nUse the function plot() to create a scatter plot of weight (wt) vs.¬†miles per gallon (mpg). As the weight increases, does the miles per gallon tend to increase or decrease?",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-graphics.html#footnotes",
    "href": "data-graphics.html#footnotes",
    "title": "5¬† Data Visualization",
    "section": "",
    "text": "Some references includes https://r-graph-gallery.com/ggplot2-package.html/ and https://rkabacoff.github.io/datavis//.‚Ü©Ô∏é\nhttps://github.com/matloff/TidyverseSkeptic/‚Ü©Ô∏é\nAlthough they are exchangeable, for categorical data, I tend to call it frequency table, and I prefer using frequency distribution for numerical data.‚Ü©Ô∏é",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Data Visualization</span>"
    ]
  },
  {
    "objectID": "data-numerics.html",
    "href": "data-numerics.html",
    "title": "6¬† Numerical Measures of Data",
    "section": "",
    "text": "6.1 Measures of Center\nMean\nThe (arithmetic) mean or average is calculated by adding up all of the values and then dividing by the total number of them. Let \\(x_1, x_2, \\dots, x_n\\) denote the measurements observed in a sample data of size \\(n\\). The sample mean is defined as\n\\[\\overline{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + x_2 + \\dots + x_n}{n}\\]\nFor the interest rate example, the sample mean interest rate is\n\\[\\overline{x} = \\frac{10.9\\% + 9.9\\% + \\cdots + 6.1\\%}{50} = 11.56\\%\\]\nBalancing Point\nWhy arithmetic mean can be a measure of center? Intuitively we can think of the mean as the balancing point of the distribution. Figure¬†6.1 is the distribution of the interest rate. If you imagine that we put all those data points on a see-saw or lever, the sample mean is the balancing point or fulcrum (pivot) that keeps the see-saw (lever) balanced horizontally.\nMedian\nThe median is the middle value when data values are sorted (from smallest to largest). In the sorted data, half of the values are less than or equal to the median, and the other half are greater than the median. To find the median, we first sort the values.\nMode\nThe mode is the value that occurs most frequently. For continuous numerical data, it is common for there not to be any observations that share the same value, i.e., every data value happened just once. With the definition, mode works better for categorical data. A more practical definition is a bit less specific that a mode is represented by a prominent peak in the distribution.",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Numerical Measures of Data</span>"
    ]
  },
  {
    "objectID": "data-numerics.html#measures-of-center",
    "href": "data-numerics.html#measures-of-center",
    "title": "6¬† Numerical Measures of Data",
    "section": "",
    "text": "R\nPython\n\n\n\n\nIn R, we simply put the interest rate vector int_rate in the function mean() to calculate the arithmetic mean. If the data contain missing values, but we still want to compute the arithmetic mean with the missing values removed, we can set the argument na.rm = TRUE in the mean() function.\n\nmean(int_rate)\n# [1] 11.6\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nnp.mean(int_rate)\n# 11.56\n\n\n\n\n\n\n\nNote\n\n\n\nThere are other Python modules that offer functions for computing arithmetic mean and other statistical functions, for example, Python statistics module, and scipy.stats module.\nWe can do the following to compute the mean.\n\nimport statistics as stats\nstats.mean(int_rate)\n# 11.56\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.1: Mean as a balancing point for interest rate example\n\n\n\n\n\n\n\n\n\n\n\nSource: Unsplash-Markus Winkler\n\n\n\n\n\n\n\n\n\nIf \\(n\\) is odd, the median is located in the exact middle of the ordered values. Check the example below.\n\n Data: (0, 2, 10, 14, 8) \n Sorted Data: (0, 2, 8, 10, 14) \n\n The median is \\(8\\) . There are two numbers on its right and two on its left. It is right on the middle.\n\n\nIf \\(n\\) is even, the median is the average of the two middle numbers. Look at the example below.\n\n Data: (0, 2, 10, 14, 8, 12) \n Sorted Data: (0, 2, 8, 10, 12, 14) \n\n The median is \\(\\frac{8 + 10}{2} = 9\\) . The number 8 or 10 cannot be the median because neither is in the exact middle of the sorted data. Their average value 9 is the middle value because three values are smaller (0, 2, 8) and three larger (10, 12, 14) than the value 9.\n\n\n\n\n\nR\nPython\n\n\n\nIn R we can obtain the median using the definition as follows. We first sort the data using sort(). By default, the sort() function sorts a data vector in an increasing order. Set decreasing = TRUE in the function if we want the sort to be increasing. When median is calculated, increasing or decreasing order does not matter. The sorted interest rate data is the sort_rate. There are 50 data values, and the median is the average of the 25th value and 26th value, which is 9.9%.\n\n## Compute the median using definition\n(sort_rate &lt;- sort(int_rate))  ## sort data\n#  [1]  5.3  5.3  5.3  6.1  6.1  6.1  6.7  6.7  7.3  7.3  7.3  8.0  8.0  8.0  8.0\n# [16]  9.4  9.4  9.4  9.4  9.4  9.9  9.9  9.9  9.9  9.9  9.9 10.4 10.4 10.9 10.9\n# [31] 10.9 10.9 10.9 12.0 12.6 12.6 12.6 14.1 15.0 16.0 17.1 17.1 17.1 18.1 18.4\n# [46] 19.4 20.0 21.4 24.9 26.3\nlength(int_rate)  ## Check sample size is odd or even\n# [1] 50\n(sort_rate[25] + sort_rate[26]) / 2  ## Verify the answer\n# [1] 9.9\n\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to sort the data first if computing the median using the definition. Using un-sorted data leads to a wrong answer! The value below is not the median because\n\n(int_rate[25] + int_rate[26]) / 2\n# [1] 8.1\n\n\n\nA more convenient way of computing the median is using the command median() just as we use mean() for arithmetic mean. We don‚Äôt need to sort the data because R does everything for us.\n\n## Compute the median using command median()\nmedian(int_rate)\n# [1] 9.9\n\n\n\nIn Python we can obtain the median using the definition as follows. We first sort the data using np.sort(). The sort() function sorts a data vector in an increasing order. When median is calculated, increasing or decreasing order does not matter. The sorted interest rate data is the sort_rate. There are 50 data values, and the median is the average of the 25th value and 26th value, which is 9.9%.\n\n## Compute the median using definition\nsort_rate = np.sort(int_rate)  ## sort data\nsort_rate\n# array([ 5.3,  5.3,  5.3,  6.1,  6.1,  6.1,  6.7,  6.7,  7.3,  7.4,  7.4,\n#         8. ,  8. ,  8. ,  8. ,  9.4,  9.4,  9.4,  9.4,  9.4,  9.9,  9.9,\n#         9.9,  9.9,  9.9,  9.9, 10.4, 10.4, 10.9, 10.9, 10.9, 10.9, 10.9,\n#        12. , 12.6, 12.6, 12.6, 14.1, 15. , 16. , 17.1, 17.1, 17.1, 18.1,\n#        18.4, 19.4, 20. , 21.4, 24.8, 26.3])\nlen(int_rate)  ## Check sample size is odd or even\n# 50\n(sort_rate[24] + sort_rate[25]) / 2  ## Verify the answer\n# 9.9\n\nNotice that we use the index 24 and 25 because again Python indexing starts with 1!\n\n\n\n\n\n\nWarning\n\n\n\nBe sure to sort the data first if computing the median using the definition. Using un-sorted data leads to a wrong answer! The value below is not the median because\n\n(int_rate[24] + int_rate[25]) / 2\n# 8.1\n\n\n\nA more convenient way of computing the median is using the command median() just as we use mean() for arithmetic mean. We don‚Äôt need to sort the data because Python does everything for us.\n\n## Compute the median using command median()\nnp.median(int_rate)\n# 9.9\nstats.median(int_rate)\n# 9.9\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nThere is a mode() function in R, but unfortunately it is not used to compute the mode of data. We can still get the mode using its definition. We can first create a count table of the data, then the data value having the most counts will be our mode. After sorting the frequency table, the number that happens most frequently will be the first element, which is 9.9%. It occurs six times in the data.\n\n## Create a frequency table\n(table_data &lt;- table(int_rate))\n# int_rate\n#  5.3  6.1  6.7  7.3    8  9.4  9.9 10.4 10.9   12 12.6 14.1   15   16 17.1 18.1 \n#    3    3    2    3    4    5    6    2    5    1    3    1    1    1    3    1 \n# 18.4 19.4   20 21.4 24.9 26.3 \n#    1    1    1    1    1    1\n\n\nsort(table_data, decreasing = TRUE)\n# int_rate\n#  9.9  9.4 10.9    8  5.3  6.1  7.3 12.6 17.1  6.7 10.4   12 14.1   15   16 18.1 \n#    6    5    5    4    3    3    3    3    3    2    2    1    1    1    1    1 \n# 18.4 19.4   20 21.4 24.9 26.3 \n#    1    1    1    1    1    1\n\n\n\nIn Python, the statistics module has the function mode() that find the value shown the most of the time in the data.\n\nstats.mode(int_rate)\n# 9.9",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Numerical Measures of Data</span>"
    ]
  },
  {
    "objectID": "data-numerics.html#comparison-of-mean-median-and-mode",
    "href": "data-numerics.html#comparison-of-mean-median-and-mode",
    "title": "6¬† Numerical Measures of Data",
    "section": "\n6.2 Comparison of Mean, Median and Mode",
    "text": "6.2 Comparison of Mean, Median and Mode\nThe mode is applicable for both categorical and numerical data, while the median and mean work for numerical data only. What is the average of male and female? It is also possible to have more than one mode because two data values, either categorical or numerical, can occurs the same number of times in the data. However, by definition, there can only be one median and one mean.\nThe mean is more sensitive to extreme values or outliers. In other words, if the data contain one or few values that are very far away from the rest of the data values, the mean will move towards those extreme values, and therefore be away from the rest of the data too. Thank about the seesaw or lever example. If a data point is at very end of the bar, the pivot must be closer to that point to make lever balanced.\nThe median and mode are more robust than the mean, meaning that these measures of center are more resistant to the addition of extreme values to the data. An example is shown below. The data_extreme is the interest rate data with the first value replaced with 90.0 which is extremely higher than all other values. The original interest data has the mean 11.56, but the data_extreme has the higher mean 13.14. The median and the mode, however, stay the same even though a large value 90.0 is in the data.\n\n\nR\nPython\n\n\n\n\ndata_extreme\n#  [1] 90.0  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n# [16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n# [31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n# [46] 10.4 21.4 10.9  9.4  6.1\n\n\nmean(data_extreme)  ## Large mean! Original mean is 11.56\n# [1] 13.1\nmedian(data_extreme)  ## Median does not change!\n# [1] 9.9\nnames(sort(table(data_extreme), decreasing = TRUE))[1] ## Mode does not change either!\n# [1] \"9.9\"\n\n\n\n\ndata_extreme = int_rate.copy()\ndata_extreme.iloc[0] = 90  # Replace the first value with a large value\ndata_extreme\n# 0     90.0\n# 1      9.9\n# 2     26.3\n# 3      9.9\n# 4      9.4\n# 5      9.9\n# 6     17.1\n# 7      6.1\n# 8      8.0\n# 9     12.6\n# 10    17.1\n# 11     5.3\n# 12     7.4\n# 13     5.3\n# 14     8.0\n# 15    24.8\n# 16    18.1\n# 17    10.4\n# 18     8.0\n# 19    19.4\n# 20    14.1\n# 21    20.0\n# 22     9.4\n# 23     9.9\n# 24    10.9\n# 25     5.3\n# 26     6.7\n# 27    15.0\n# 28    12.0\n# 29    12.6\n# 30    10.9\n# 31     9.4\n# 32     9.9\n# 33     7.4\n# 34    18.4\n# 35    17.1\n# 36     8.0\n# 37     6.1\n# 38     6.7\n# 39     7.3\n# 40    12.6\n# 41    16.0\n# 42    10.9\n# 43     9.9\n# 44     9.4\n# 45    10.4\n# 46    21.4\n# 47    10.9\n# 48     9.4\n# 49     6.1\n# Name: interest_rate, dtype: float64\n\n\nstats.mean(data_extreme)\n# 13.142\nstats.median(data_extreme)\n# 9.9\nstats.mode(data_extreme)\n# 9.9\n\n\n\n\nBelow is a figure that shows the differences in where the mean, median, and mode lie for skewed distributions vs.¬†symmetric distributions.\n\nWhen the frequency distribution is right-skewed, mean &gt; median &gt; mode.\nWhen the frequency distribution is left-skewed, mean &lt; median &lt; mode.\nWhen the frequency distribution is symmetric, mean = median = mode.\n\n\n\n\n\n\n\n\nFigure¬†6.2: Comparison of mean, median, and mode for symmetrical vs.¬†skewed distributions",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Numerical Measures of Data</span>"
    ]
  },
  {
    "objectID": "data-numerics.html#measures-of-variation",
    "href": "data-numerics.html#measures-of-variation",
    "title": "6¬† Numerical Measures of Data",
    "section": "\n6.3 Measures of Variation",
    "text": "6.3 Measures of Variation\nDispersion, spread or variation is another important property of a distribution. If the measure of center tells us the ‚Äúlocation‚Äù of the distribution, the measures of variation tells us how much the data values spread out. Measures of variation affect the shape of the distribution (Figure¬†6.3). Here we have three frequency distributions with the same mean, median and mode because their distribution is symmetric. But from top to bottom, the data have small, median and large variation. Basically larger variation means the data values spread out more. When a distribution has large variation, the data values are quite far away from each other and from its mean. Their deviation from the sample mean is large. In this case, using sample mean to represent the entire data set may not be a good idea because data values look very different from it.\n\n\n\n\n\n\n\nFigure¬†6.3: Effects of variation on the shape of distributions\n\n\n\n\n\n p-th percentile \nThe p-th percentile (quantile) is a data value such that\n\nat most \\(p\\%\\) of the values are below it\nat most \\((1-p)\\%\\) of the values are above it\n\n\n\n\n\n\n\n\n\nThere are two data sets with the same mean 20. That is, their frequency distribution is centered at the same value.\n\n\n\n\nData 1 has 99-th percentile = 30, and 1-st percentile = 10.\nData 2 has 99-th percentile = 40, and 1-st percentile = 0.\nWhich data set has larger variation?\n\n\n\nFigure¬†6.4 shows percentiles for ACT math scores. If you want to be in top 10%, i.e., 90th percentile or higher, you must have score at least 28. Back to the question. About 98% of the Data 1 values are in the range from 10 to 30, and Data 2 values are ranging from 0 to 40 98% of the time. Since Data 2 has larger range, its values are more dispersed, and has larger variation than Data 1.\nIn R we use quantile(x, prob) to find any percentile or quantile of the data vector x through specifying the probability in the prob argument.\n\n\n\n\n\n\n\n\nFigure¬†6.4: Percentiles for ACT scores (https://en.wikipedia.org/wiki/ACT_(test))\n\n\n\n\n\n\n\n Interquartile Range (IQR) \nPreviously we use the range of two percentiles to determine the degree of variation of data. With the idea, conventionally we use the interquartile range (IQR) to measure the variation.\nThe first quartile (Q1) is the 25-th percentile of the data. Second quartile (Q2) is the 50-th percentile which in fact is the median of the data. Third Quartile (Q3) is the 75-th percentile of the data. The interquartile Range (IQR) is Q3 - Q1.\n\n\nR\nPython\n\n\n\nTo find the IQR, in R we can follow its definition, or get it directly using the IQR() function.\n\n\n\nquantile(x = int_rate,\n         probs = c(0.25, 0.5, 0.75))\n#  25%  50%  75% \n#  8.0  9.9 13.7\n\n## IQR by definition\nquantile(x = int_rate, probs = 0.75) -\n  quantile(x = int_rate, probs = 0.25)\n#  75% \n# 5.72\n\n\n\n\n\n\nIQR(int_rate)\n# [1] 5.72\n\n\n\nA useful way of quickly getting some important numerical measures is to use the summary() function.\n\nsummary(int_rate)\n#    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#     5.3     8.0     9.9    11.6    13.7    26.3\n\n\n\nTo find the IQR, in Python we can follow its definition using np.quantile() that computes percentiles at any given probability.\n\nnp.quantile(a = int_rate, q = [0.25, 0.5, 0.75])\n# array([ 8.   ,  9.9  , 13.725])\n## IQR by definition\nnp.quantile(a = int_rate, q = 0.75) - np.quantile(a = int_rate, q = 0.25)\n# 5.725\n\nThere is no built-in Python function for calculating IQR directly. But we can use the iqr() function the scipy.stats module.\n\nfrom scipy.stats import iqr\niqr(x = int_rate)\n# 5.725\n\nTo obtain a quick summary of data, pandas provides the describe() method for descriptive statistics of Series and DataFrame.\n\nint_rate.describe()\n# count    50.000000\n# mean     11.560000\n# std       5.045871\n# min       5.300000\n# 25%       8.000000\n# 50%       9.900000\n# 75%      13.725000\n# max      26.300000\n# Name: interest_rate, dtype: float64\n\n\n\n\n\n\n\n\n\n\nDoes a larger IQR means more or less variation?\n\n\n\n\n\n\n\n Variance and Standard Deviation \nThe distance of an observation from its (sample) mean, \\(x_i - \\overline{x}\\), is its deviation. The sample Variance is defined as \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}.\\]\nThe sample standard deviation (SD) is the square root of the variance \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}}.\\] \n(Sample) variance and standard deviation are the most commonly used measures of variation of data. The variance is the average of squared deviation from the sample mean \\(\\overline{x}\\) or the mean squared deviation from the mean. Look at its formula. \\((x_i - \\overline{x})\\) is the deviation from the mean, and \\((x_i - \\overline{x})^2\\) is the squared deviation. If we take the sum of the squared deviations of all data, and divided by the total number of data points, we get the mean squared deviation, just like we get the arithmetic mean \\(\\frac{\\sum_{i=1}^nx_i}{n}\\). The only difference is that the denominator is \\(n-1\\), not \\(n.\\) The standard deviation is the root mean squared deviation from the mean. It measures, on average, how far the data spread out around the average.\n\n\n\n\n\n\nNote\n\n\n\nThe measures of center and variation discussed here including mean and variance are for the sample data. So be more precise they are sample mean and sample variance. If the sample is representative of its population, its sample mean and sample variance should be close to its population mean and population variance. In statistics, we usually use the Greek letter \\(\\mu\\) to stand for the population mean and \\(\\sigma^2\\) for population variance where \\[\\mu = \\frac{\\sum_{j=1}^Nx_j}{N}\\] and \\[\\sigma^2 = \\frac{\\sum_{j=1}^N(x_j-\\mu)^2}{N},\\] where \\(N\\) is the total number of subjects in the population.\nLater when we discuss probability, you will learn how we define \\(\\mu\\) and \\(\\sigma^2\\) using the expected value from a probability distribution (Chapter 10 and Chapter 11), which is related to the frequency distribution of data.\n\n\nBut why divide by \\(n-1\\) instead of \\(n\\)? Intuitively the smaller our sample, the less likely we are to realistically capture the true magnitude of the variation of the population. Although \\(\\overline{x}\\) and \\(s^2\\) measure the location and variation of the data, we hope them to faithfully reflect the center and variation of its population because we are interested in the property of population, not the data. We collect sample to learn the population, not the sample itself.\nUnless our sample size is big, we underestimate the variance that‚Äôs really there in the population. Figure¬†6.5 illustrates this concept. The sample drawn from the population usually has smaller range (maximum - minimum), and leads to smaller variation. To let the sample variance be more consistent with the population variance, we make the estimates of variance and SD a little bigger by dividing by \\(n-1\\) instead of \\(n\\). With this correction, the sample variance is more reliable and useful.\nThe numerator of \\(s^2\\) is a sum of squares. In fact a sum of squares has its corresponding degrees of freedom that is actually \\(n-1\\) in the \\(s^2\\) case. The the sum of squares divided by its degrees of freedom is called the mean square. We don‚Äôt go into the details of sum of squares, degrees of freedom, and their properties, and those topics are usually discussed in advanced probability and statistical inference courses.\n\n\n\n\n\n\n\n\nFigure¬†6.5: Population and sample points.\n\n\n\n\n\n\n\nR\nPython\n\n\n\nIn R, we use var() for computing variance and sd() for standard deviation.\n\nvar(int_rate)\n# [1] 25.5\nsqrt(var(int_rate))\n# [1] 5.05\nsd(int_rate)\n# [1] 5.05\n\n\n\nIn Python, we can use np.var() for computing variance and np.std() for standard deviation.\nBe careful that by default the denominator is \\(n\\), not \\(n-1\\). It can be adjusted by the argument ddof. The denominator is \\(n-\\)ddof when the functions calculate variance and stadard deviation.\n\nnp.var(int_rate, ddof=1)  # ddof=1 for sample variance\n# 25.460816326530615\nnp.sqrt(np.var(int_rate, ddof=1))\n# 5.045871215809081\nnp.std(int_rate, ddof=1)\n# 5.045871215809081",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Numerical Measures of Data</span>"
    ]
  },
  {
    "objectID": "data-numerics.html#visualizing-data-variation",
    "href": "data-numerics.html#visualizing-data-variation",
    "title": "6¬† Numerical Measures of Data",
    "section": "\n6.4 Visualizing Data Variation",
    "text": "6.4 Visualizing Data Variation\n Boxplot \nBoxplot is a good tool for visualizing data variation and general distribution pattern. It is called a box-and-whisker plot because it is made of a box with lines which are called whiskers extending from the box.\nLet‚Äôs look at the box first. We have 3 vertical lines here. The lines from left to right indicate Q1, Q2 or the median, and Q3. So the length of the box shows the IQR. Now let‚Äôs look at the whiskers. The upper limit of the whisker is the smaller one of the maximal data value and Q3 + 1.5 IQR. The lower limit of the whisker on the left is the larger one of the minimal data value and Q1 - 1.5 IQR. For any data values that are greater than Q3 + 1.5 IQR or smaller than Q1 - 1.5 IQR, we show them as a point. Basically those points are far from the center of the data, and we could potentially treat them as extreme values or outliers.\n\n\n\n\n\n\n\n\n\n\nFigure¬†6.6: a boxplot. (https://www.leansigmacorporation.com/box-plot-with-minitab/)\n\n\n\n\n Interest Rate Boxplot \nBelow is the boxplot for the interest rate data (Figure¬†6.7). You can see that there are two very high interest rates in the data. They are 25% or more. And the median interest rate is just about 10%. And so these two data points are suspected outliers. They are way far from the the median or the most data points.\n\n\n\n\n\n\n\nFigure¬†6.7: Boxplot for interest rate example\n\n\n\n\n\n\nR\nPython\n\n\n\nIn R we use boxplot() to generate a boxplot.\n\n\n\nboxplot(int_rate, ylab = \"Interest Rate (%)\")\n\n\n\n\n\n\n\n\n\n\n\n\nsort(int_rate, decreasing = TRUE)[1:5]\n# [1] 26.3 24.9 21.4 20.0 19.4\nsort(int_rate)[1:5]\n# [1] 5.3 5.3 5.3 6.1 6.1\nQ3 &lt;- quantile(int_rate, probs = 0.75,\n               names = FALSE)\nQ1 &lt;- quantile(int_rate, probs = 0.25,\n               names = FALSE)\nIQR &lt;- Q3 - Q1\nQ1 - 1.5 * IQR\n# [1] -0.587\nQ3 + 1.5 * IQR\n# [1] 22.3\n\n\n\n\n\nIn Python, we use plt.boxplot to generate a boxplot.\n\nplt.boxplot(int_rate)\n# {'whiskers': [&lt;matplotlib.lines.Line2D object at 0x381998c50&gt;, &lt;matplotlib.lines.Line2D object at 0x38199b800&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x38199bb00&gt;, &lt;matplotlib.lines.Line2D object at 0x38199bda0&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x38192c740&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x3819e40e0&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x3819e43e0&gt;], 'means': []}\nplt.ylabel(\"Interest Rate (%)\")\nplt.show()\n\n\n\n\n\n\n\n\nnp.sort(int_rate)[-5:] ## top 5\n# array([19.4, 20. , 21.4, 24.8, 26.3])\nnp.sort(int_rate)[:5] ## bottom 5\n# array([5.3, 5.3, 5.3, 6.1, 6.1])\nQ3 = np.quantile(a = int_rate, q = 0.75)\nQ1 = np.quantile(a = int_rate, q = 0.25)\nIQR = Q3 - Q1\nQ1 - 1.5 * IQR\n# -0.5874999999999986\nQ3 + 1.5 * IQR\n# 22.3125\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe R/Python boxplot is a little different from the one in Figure¬†6.6.\n\nThe end of upper whisker is the largest data value that is below Q3 + 1.5 IQR if there are some outliers greater than Q3 + 1.5 IQR. The end of lower whisker is the smallest data value that is below Q1 - 1.5 IQR if there are some outliers smaller than Q1 - 1.5 IQR.\nQ1 and Q3 used in the boxplot generated from R may be different from the Q1 and Q3 computed by the quantile() function. They are pretty close although different. We don‚Äôt quite need to worry about their difference at this moment.",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Numerical Measures of Data</span>"
    ]
  },
  {
    "objectID": "data-numerics.html#exercises",
    "href": "data-numerics.html#exercises",
    "title": "6¬† Numerical Measures of Data",
    "section": "\n6.5 Exercises",
    "text": "6.5 Exercises\n\nIn the following, we will be using the data set mtcars to do some data summary and graphics. First load the data set into your R session by the command data(mtcars). The data set is like\n\n\n#                    mpg cyl disp  hp drat   wt qsec vs am gear carb\n# Mazda RX4         21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n# Mazda RX4 Wag     21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n# Datsun 710        22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n# Hornet 4 Drive    21.4   6  258 110 3.08 3.21 19.4  1  0    3    1\n# Hornet Sportabout 18.7   8  360 175 3.15 3.44 17.0  0  0    3    2\n# Valiant           18.1   6  225 105 2.76 3.46 20.2  1  0    3    1\n\nPlease see ?mtcars for the description of the data set.\n\nUse the function boxplot() to generate a boxplot of 1/4 mile time (qsec). Are there any outliers?\nCompute the mean, median and standard deviation of displacement (disp).\n\n\n\nMean and standard deviation (SD): For each part, compare data (1) and (2) based on their mean and SDs. You don‚Äôt need to calculate these statistics, but compare (1) and (2) by stating which one has a larger mean/SD or they have the same mean/SD. Explain your reasoning.\n\n\n-30, 0, 0, 0, 15, 25, 25\n-50, 0, 0, 0, 15, 20, 25\n\n\n0, 1, 3, 5, 7\n21, 23, 25, 27, 29\n\n\n100, 200, 300, 400, 500\n0, 50, 350, 500, 600\n\n\n\n\nSkewness: Facebook data indicate that \\(50\\%\\) of Facebook users have 130 or more friends, and that the average friend count of users is 115. What do these findings suggest about the shape (right-skewed, left-skewed, symmetric) of the distribution of number of friends of Facebook users? Please explain.",
    "crumbs": [
      "Summarizing Data",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Numerical Measures of Data</span>"
    ]
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Probability",
    "section": "",
    "text": "If we want to describe or quantify the uncertainty about something happening or not, the most formal and rigorous way of doing it is to use probability. Therefore, we can view the probability as the language of uncertainty.\nMost of the time, we cannot do statistical inference or prediction using machine learning without probability because we usually assume our data at hand are random realizations from some targeted population that are described by a probability distribution. In other words, we are doing data analysis in the world of uncertainty. As a result, before jumping into statistical inference or data analysis, we need to have basic understanding of probability definitions, rules, and operations that are frequently used in the data science community.\nIn this part, we are going to learn the following topics:\n\nProbability definitions\nProbability rules\nDiscrete probability distributions\nContinuous probability distributions\nRandom sampling and sampling distributions\nLaw of large numbers and central limit theorem",
    "crumbs": [
      "Probability"
    ]
  },
  {
    "objectID": "prob-define.html",
    "href": "prob-define.html",
    "title": "7¬† Definition of Probability",
    "section": "",
    "text": "7.1 Language of Uncertainty\nWhy Study Probability\nIt goes without saying that we live in a world full of chances and uncertainty. People do care about chances and usually make decisions given the information about some uncertain situations. Figure¬†7.1 shows the Google search result of ‚Äúwhat are chances‚Äù on my personal computer on September 19, 2022. It looks like people are curious about getting pregnant, getting COVID of course at that time, and getting struck by lightning or tornado. And of course, it is the chance or uncertainty that makes so many games so fun, amusing and even additive, for example, the board game monopoly, and the gambling machines like slots and blackjacks.\nFigure¬†7.1: Google search result of ‚Äúwhat are chances‚Äù.\nApparently most people want to quantify uncertainty about something happening, or measure the chances of some event happening for better decision making. The question is, how? We need a consistent way of measuring chances and uncertainty so that our society can be operated in order. Thanks to great enthusiasm for gambling, the mathematical study of chances starts quite early, and nowadays, the most formal and rigorous way of studying chances is to use probability. Therefore, we could probably view probability as the language of uncertainty.\nMost of the time, we cannot do statistical inference or prediction using machine learning without probability because we usually assume our data at hand are random realizations from some targeted population that are described by a probability distribution. In other words, we are doing data analysis in the world of uncertainty. For example, we are interested in the mean height of Marquette female students, which is usually assumed to be bell-shaped distributed. By chances, our collected sample of girls may all have heights below 5‚Äô4‚Äù, which is not representative of the target population, and the sample mean is far from the true population mean height being interested. Examining the chance of getting such biased data set becomes important and helps us quantify the plausibility of the numerical results we obtain from the data.\nAs a result, before jumping into statistical inference or data analysis, we need to have basic understanding of probability definitions, rules, and operations that are frequently used in the data science community.\nWhy Probability Before Statistics?\nAlthough probability and statistics are closely related and usually taught in one single introductory course, they are two distinct subjects. Since the randomness of sampling from the target population, statistical inference and data analysis usually involve uncertainty quantification, and hence probability is used in the analysis.\nIn a typical statistical analysis, we assume the target population, for example the Marquette students‚Äô height follows some probability distribution, and the collected sample data are the realized data points from the distribution. With this, the population probability distribution is a mechanism that generates data. In probability theory, we examine how this mechanism generates data, and how the observed data will behave given the fact that they are from the probabilistic data generating process. For example, we assume the height follows some probability distribution, and we are curious about the probability that the sample mean is larger than 5‚Äô10‚Äù, or that the sample mean is between 5‚Äô6‚Äù and 5‚Äô11‚Äù.\nIn the probability theory, the process generating the data is assumed known and we are interested in properties of observations. However, in reality, the data generating process, or the target population distribution, is unknown to us, and is what we would like to infer to using the sample data we collect. For example, we want to estimate the unknown mean height of Marquette students using the data of 100 Marquette students sampled at random from the unknown target population distribution. This is what statistical inference is all about. For statistics, we observe the data (sample) and are interested in determining what process generates such data (population). These principles are illustrated below in Figure¬†7.5.\nFigure¬†7.5: Relationship between probability and statistical inference\nEven though the data generating process is fixed and unchanged every time a data set is collected, the data replicates are all different due to the random sampling from the population probability distribution. Such randomness creates the uncertainty about how we do the inference about the population properties because a different data set represents only a part of, and probably biased, information about the the whole distribution. As a result, when doing inference, we prefer probabilistic statements to deterministic statements.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Definition of Probability</span>"
    ]
  },
  {
    "objectID": "prob-define.html#language-of-uncertainty",
    "href": "prob-define.html#language-of-uncertainty",
    "title": "7¬† Definition of Probability",
    "section": "",
    "text": "Figure¬†7.2: Monopoly baord game\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†7.3: Slots\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†7.4: Blackjacks",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Definition of Probability</span>"
    ]
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability",
    "href": "prob-define.html#interpretation-of-probability",
    "title": "7¬† Definition of Probability",
    "section": "\n7.2 Interpretation of Probability",
    "text": "7.2 Interpretation of Probability\n Relative Frequency \nThere are several ways of interpreting probability. The first interpretation is relative frequency. Formally speaking, the probability that some outcome of a process will be obtained is interpreted as the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\nThink about the following scenario. Your mom gave you a unfair coin, but she does not know the probability of getting heads when one tosses the coin. How do you obtain, or at least approximate the probability? Well, we can use the concept of relative frequency. First, we decide how many times we want to flip the coin. Each time after flipping the coin, we record either heads or tails shows up. Once we are done all the flips, we count the frequency or the total number of times heads shows up among all the flips. To obtain the probability of getting heads, we calculate the relative frequency, the ratio of the frequency of heads to the number of tosses.\nBelow is an example depicting the relative frequency of flipping a coin and getting heads or tails.\n\n\n\n\n      Frequency Relative Frequency\nHeads         4                0.4\nTails         6                0.6\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       514              0.514\nTails       486              0.486\nTotal      1000              1.000\n---------------------\n\n\n\n\nWhen we flip the coin 10 times, 4 of them end up being heads, and the probability of getting heads is 40%. You may be skeptical of the result, and want to have more replicates. Some day you have lots of spare time, and you decide to flip the coin 1000 times. The relative frequency, or the probability of getting heads, now becomes 51.4%.\n\n\n\n\n\n\nDo you see any issues with relative frequency probability?\n\n\n\n\n\n\nApparently, we don‚Äôt know the true probability if it does exist. And as you learned in the coin-flipping example, we don‚Äôt have one unique answer for that if we use relative frequency as the way of interpreting probability. In fact, there are some issues when we treat relative frequency as probability.\n Issues with Relative Frequency \n\nüòï How large of a number is large enough?\n\nThere is no correct answer for how many replicates of the experiment we should have. We may think 10 times is not enough, but how about 1000 times? one million times? How well the relative frequency approximates the true probability depends on the complexity of the experiments. In general, the larger the number of times of repeating the process, the better the approximation of the relative frequency. This is the result of the so-called law of large numbers that is discussed in Chapter¬†13.\n\nüòï What is the meaning of ‚Äúunder similar conditions‚Äù?\n\nIn the definition, the experiment or process needs to be repeated ‚Äúunder similar conditions‚Äù. What does that mean? The definition itself is not rigorous enough. Do we need to control the airflow when the experiment is conducted? How about temperature? Can your mom and you take turns to flip the coin? Be honest, there is no answer for that.\n\nüòï Is the relative frequency reliable under identical conditions?\n\nCan we trust the relative frequency when the experiment is conducted under identical conditions? If there is a skilled person who can control whether heads or tails shows up every time he flips a coin, should we believe the relative frequency is a good approximation to the true probability?\n\nüëâ We can only obtain an approximation instead of exact value for the probability.\n\nYou may already find out that the relative frequency is only an approximation instead of exact value for the probability. If we want to get the true probability (if it does exist), we need to get the relative frequency whose process is repeated infinitely many of times, which is unrealistic. Such probability stems from the frequentist philosophy that interprets probability as the long-run relative frequency of a repeatable experiment.\n\nüòÇ How do you compute the probability that Chicago Cubs win the World Series next year?\n\nIn the real world and our daily lives, lots of times we want to compute the probability of something happening where the something cannot be a repeatable process or experiment. For example, it is impossible to compute the probability that Chicago Cubs win the World Series next year because we would never to able to obtain the relative frequency of Chicago Cubs winning the World Series next year.\n\n\n\n\nSource: https://media.giphy.com/media/EKURBxKKkw0uY/giphy.gif\n\n\n\n\n Classical Approach \nAnother interpretation of probability follows the classical approach, whose probability is based on the concept of equally likely outcomes. If the outcome of some process must be one of \\(n\\) different outcomes, the probability of each outcome is simply \\(1/n\\). For example, if you toss a fair coin (2 outcomes) ü™ô, the probability of getting heads is 1/2. If you roll a well-balanced die (6 outcomes) üé≤, the probability of each outcome being shown is 1/6. If you draw one from a deck of cards (52 outcomes) üÉè, the probability of each card being drawn is 1/52.\n\n\n\n\n\n\nDo you see any issues with classical probability?\n\n\n\n\n\n\nIt wouldn‚Äôt make sense to say that the probability that [you name it] wins the World Series next year is 1/30. Even though there are 30 teams in the MLB, each team is not equally likely to win the World Series. Don‚Äôt you agree?!\n\n Subjective Approach \nThe last interpretation of probability we discuss here is the subjective approach, whose probability is assigned or estimated using people‚Äôs knowledge, beliefs and information about the data generating process. In this case, it is a person‚Äôs subjective probability of an outcome, rather than the true probability of that outcome. For example, I think ‚Äúthe probability that the Milwaukee Brewers win the World Series this year is 30%‚Äù. My probability that the Milwaukee Brewers win the World Series this year is likely to be different from an ESPN analyst‚Äôs probability.\n\n\n\n\n\n\nSource: https://www.nj.com/yankees/2020/02/mlb-rumors-espns-sunday-night-baseball-will-feature-a-lot-of-ex-yankee-alex-rodriguez-and-not-much-else.html\n\n\n\n\n\n\n\n\nSource: Wiki: ESPN Major League Baseball\n\n\n\n\n\nHere, a probability measures the relative plausibility of some event or outcome, and such probability stems from the so-called Bayesian philosophy. With this, we can claim that candidate A has a 0.9 probability of winning because the probability represents our plausibility or belief about the winning chance of the candidate A. A Bayesian statistician would say based on analysis the candidate A is 9 times more likely to win than to lose. For a statistician with the frequentist philosophy, he might say the statement is wrong or there is no such claim. Or he might weirdly say in long-run hypothetical repetitions of the election, candidate A would win roughly 90% of the time.\n\n\n\n\n\n\n\nNote\n\n\n\nProbability operations and rules do NOT depend on the interpretation of probability!",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Definition of Probability</span>"
    ]
  },
  {
    "objectID": "prob-rule.html",
    "href": "prob-rule.html",
    "title": "8¬† Probability Rules and Bayes Formula",
    "section": "",
    "text": "8.1 Probability Operations and Rules\nExperiments, Events and Sample Space\nProbability starts with the concept of sets. When we calculate the probability of ‚Äúsomething‚Äù, that something is represented as a set, which is a collection of some outcomes generated by an process or experiment associated to the something we are interested. To denote a set, we usually use a pair of curly braces { }, and the elements of the set is put inside the braces, each separated by a comma, for example, {red, green, yellow} is a set with three color elements.\nHere we define some terminology that are commonly used in set and probability concepts.\nThe key words is ahead of time. For example, we know what is the result of flipping a coin, which is heads or tails showing up, before we actually do it. Therefore, flipping a coin is an experiment. Similarly, before we roll a six-sided die, we already know the possible outcome of doing that, which is 1, 2, 3, 4, 5, 6, so rolling a die is also an experiment.\nGenerally there are two or more potential outcomes for some experiment. Any collection of those outcomes is called an event. For example, there are 6 possible outcomes for rolling a die, 1, 2, 3, 4, 5, 6. Then any collection of those 6 numbers is an event. So ‚ÄúAn odd number showing up‚Äù which corresponds to the collection {1, 3, 5} is an event. ‚ÄúAn even number showing up‚Äù that is represented by {2, 4, 6} is also an event.\nBased on the definition, the sample space the largest set associated with an experiment because it collects all possible outcomes. In other words, when an experiment is conducted, no matter what outcome shows up, it is always in the sample space.\nThe table below provides a summary of experiments flipping a coin and rolling a die.\nSet Concept: Example of Rolling a six-side balanced die\nProbability Rules\nWe have learned how to represent an event in terms of sets and set operations. Here we are going to learn several probability rules for events.\nFirst, we denote the probability of an event \\(A\\) on a sample space \\(\\mathcal{S}\\) as \\(P(A)\\).\nIn order to have a coherent and logically consistent set of probability rules, we need some axioms that are self-evident to anyone. The three axioms are as follows.\nIf we treat the sample space as an event, the probability of the entire sample space is always equal to one because this event must happen every time the experiment is conducted. In the Venn diagram, the sample space is the entire rectangle, and in probability, we presume the area of the rectangle is one.\nBecause any event is a collection of some outcomes that could possibly occur or not occur, its probability is greater than or equal to zero. Any probability cannot be negative. It is clearly shown in terms of Venn diagram because an area of any shape of an object is greater than or equal to zero.\nFinally, if the two events are disjoint, the probability of the union of the two is just the sum of their own probability. For example, if the probability of getting a green M&M is 20% and that of getting a blue M&M is 15%, then the probability of getting a green or blue M&M is 20% + 15% = 35%. It is clearly shown in the Venn diagram too because the total area of the two disjoint events in the sample space is the sum of the individual area.\nWith the three axioms, the entire probability operation system can be constructed. Some basic properties are listed here.\nThe empty set does not contain any possible outcomes of an experiment. Because some outcome must be occurred after an experiment is conducted, it is impossible for some event to happen without any outcome involved. Therefore, the probability of the empty set is zero. In terms of Venn diagram, an empty set is a set with area zero because it does not occupy any part (outcome) of the entire sample space.\nSince every event being considered must be a subset of the sample space, the area of any event is smaller than the area of the sample space which is one. Therefore, for any event \\(A\\), \\(P(A) \\le 1\\). \\(P(A) = 1\\) if and only if \\(A = \\mathcal{S}\\).\nThe addition rule can be clearly understood using the Venn diagram. Figure¬†8.1 shows how \\(P(A \\cup B)\\) is expressed by \\(P(A)\\), \\(P(B)\\) and \\(P(A \\cap B)\\). To get the area of \\(A \\cup B\\), we can first consider the sum of the area of \\(A\\) and \\(B\\), the left and right circles. However, the middle part which is \\(A \\cap B\\) is counted twice when we take the sum, so the one piece of area of \\(A \\cap B\\) should be removed from the sum of the area.\nFigure¬†8.1: Venn Diagram depiction of the addition rule\nIn fact, the third axiom is a special case of the addition rule. Figure¬†8.2 illustrates the case. Since the two events are disjoint, \\(A \\cap B = \\emptyset\\), and the area of \\(A \\cap B\\), or \\(P(A \\cap B)\\) is zero.\nFigure¬†8.2: Venn Diagram depiction of the addition rule for the disjoint case\nSince \\(A \\cup A^c = \\mathcal{S}\\) and \\(A \\cap A^c = \\emptyset\\), we have \\[P(A \\cup A^c) = P(\\mathcal{S}) = P(A) + P(A^c)\\] Therefore, \\(P(A) + P(A^c) = 1\\) and \\(P(A^c) = 1 - P(A)\\).\nIf \\(A \\subset B\\), the area of \\(A\\) is smaller or equal to the area of \\(B\\). Therefore, \\(P(A) \\le P(B)\\).\nExample: M&M Colors",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Probability Rules and Bayes Formula</span>"
    ]
  },
  {
    "objectID": "prob-rule.html#probability-operations-and-rules",
    "href": "prob-rule.html#probability-operations-and-rules",
    "title": "8¬† Probability Rules and Bayes Formula",
    "section": "",
    "text": "An experiment is any process in which the possible outcomes can be identified ahead of time.\n\n\n\nAn event is a set of possible outcomes of the experiment.\n\n\n\nThe sample space \\((\\mathcal{S})\\) of an experiment is the collection of ALL possible outcomes of the experiment.\n\n\n\n\n\n\n\n\nIs the sample space also an event?\n\n\n\n\nYes, the sample space itself is an event because it is also a set of possible outcomes of the experiment.\n\n\n\n\n\n\n\n\n\n\n\n\nExperiment\nPossible Outcomes\nSome Events\nSample Space\n\n\n\nFlip a coin ü™ô\nHeads, Tails\n{Heads}, {Heads, Tails}, ‚Ä¶\n{Heads, Tails}\n\n\nRoll a die üé≤\n1, 2, 3, 4, 5, 6\n{1, 3, 5}, {2, 4, 6}, {2}, {3, 4, 5, 6}, ‚Ä¶\n{1, 2, 3, 4, 5, 6}\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nDraw a Venn Diagram every time you get stuck! Venn diagram is a very useful tool for identifying a set, so I encourage you to draw a venn diagram when you get stuck on complicated set operations.\n\n\n\n\n The complement  of an event (set) \\(A\\), is denoted  \\(A^c\\) . It is the set of all outcomes (elements) of \\(\\mathcal{S}\\) in which \\(A\\) does not occur. For the die example, let \\(A\\) be an event that a number greater than 2 is rolled. Then \\(A = \\{3, 4, 5, 6\\}\\) and \\(A^c = \\{1, 2\\}\\).\n\n\n\n\nThe  union \\((A \\cup B)\\)  is the set of all outcomes of \\(\\mathcal{S}\\) in \\(A\\) or \\(B\\).  For the die example, let \\(B\\) be an event that an even number is rolled. Then \\(B = \\{2, 4, 6\\}\\) and \\(A \\cup B = \\{2, 3, 4, 5, 6\\}\\).  Basically, as long as an element is in either \\(A\\) or \\(B\\), not necessarily in both \\(A\\) and \\(B\\), the element is collected in the set \\((A \\cup B)\\).\n\n\n\nThe  intersection \\((A \\cap B)\\)  is the set of all outcomes of \\(\\mathcal{S}\\) in both \\(A\\) and \\(B\\).  For the die example, \\(A \\cap B = \\{4, 6\\}\\). Basically, for any element in the set \\((A \\cap B)\\), it must be an element of \\(A\\) and of \\(B\\).\n\n\n\n\n\\(A\\) and \\(B\\) are disjoint or mutually exclusive if they have no outcomes in common \\((A \\cap B = \\emptyset)\\). \\(\\emptyset\\) means an empty set, \\(\\{\\}\\), i.e., no elements in the set. If the two events are disjoint, it means that they cannot occur at the same time for one single trial of the experiment.  For the die example, let \\(C\\) be an event that an odd number is obtained. Then \\(C = \\{1, 3, 5\\}\\) and \\(B \\cap C = \\emptyset\\).  When we roll a die one time, we cannot get a number that is odd and even at the same time.\n\n\n\nThe containment \\((A \\subset B)\\) means every elements of \\(A\\) also belongs to \\(B\\). In other words, if \\(A\\) occurs, then so does \\(B\\).  For the die example, let \\(B\\) be the event that an even number is obtained and \\(D\\) be the event that a number greater than 1 is obtained. Then \\(B = \\{2, 4, 6\\}\\) and \\(D = \\{2, 3, 4, 5, 6\\}\\).  In this case, every even number is greater than one, therefore \\(B\\) is a subset of \\(D\\), i.e., \\(B \\subset D\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind that every event (set) being considered is a subset of the sample space \\((\\mathcal{S})\\) because it only makes sense to discuss the events that are possibly to be occurred. As a result, for any event \\(A\\), \\(A \\subset \\mathcal{S}\\).\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nTreat the probability of an event as the area of the event in the Venn diagram.\n\n\n\n\n\\(P(\\mathcal{S}) = 1\\)\nFor any event \\(A\\), \\(P(A) \\ge 0\\)\n\nIf \\(A\\) and \\(B\\) are disjoint or mutually exclusive, \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\n\n\n\n\n\n\n\\(P(\\emptyset) = 0\\).\n\\(0 \\le P(A) \\le 1\\)\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\\(P(A^c) = 1 - P(A)\\)\nIf \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe makers of the M&Ms report that their plain M&Ms are composed of\n\n15% Yellow, 10% Red, 20% Orange, 25% Blue, 15% Green and 15% Brown\n\n\n\n\n\n\n\n\n\nSource: Unsplash: Robert Anasch\n\n\n\n\n\n\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\n\n\nIt is brown.\nIt is red or green.\nIt is not blue.\nIt is red and brown.\n\n\n\n\n\n\n\nSolution\n\n\\(P(\\mathrm{Brown}) = 0.15\\)\n\\(\\begin{align} P(\\mathrm{Red} \\cup \\mathrm{Green}) &= P(\\mathrm{Red}) + P(\\mathrm{Green}) - P(\\mathrm{Red} \\cap \\mathrm{Green}) \\\\ &= 0.10 + 0.15 - 0 = 0.25 \\end{align}\\)\n\\(P(\\text{Not Blue}) = 1 - P(\\text{Blue}) = 1 - 0.25 = 0.75\\)\n\\(P(\\text{Red and Brown}) = P(\\emptyset) = 0\\)\n\n\n\n\n\n\n\n\n\nWhich interpretation of probability is used in this question?",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Probability Rules and Bayes Formula</span>"
    ]
  },
  {
    "objectID": "prob-rule.html#conditional-probability-and-independence",
    "href": "prob-rule.html#conditional-probability-and-independence",
    "title": "8¬† Probability Rules and Bayes Formula",
    "section": "\n8.2 Conditional Probability and Independence",
    "text": "8.2 Conditional Probability and Independence\n Conditional Probability \nQuite often people are interested in the probability of something happening given the fact that some other event has been occurred or some information about the experiment or its outcomes have been known. In this case, we can calculate the conditional probability that takes the occurred event or known information into account. The conditional probability would be more appropriate for quantifying uncertainty about what we are interested because knowing some event being occurred is a piece of valuable information that helps us properly adjust the chance of something happening.\nBy definition, the conditional probability of \\(A\\) given \\(B\\) is \\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)}.\\]\nThe vertical bar ‚Äú\\(\\mid\\)‚Äù is read as given or conditional on. Therefore, we consider the event \\(A\\) not in the entire sample space, but the event \\(A\\) that is conditional on the event \\(B\\), or \\(A \\mid  B\\). In other words, we don‚Äôt consider all possible parts of \\(A\\). Instead, we only care about the part of \\(A\\) for which \\(B\\) has already occurred.\nThe formula is well defined when \\(P(B) &gt; 0\\) and undefined if \\(P(B) = 0\\). Intuitively, one cannot calculate the probability of \\(A\\) given \\(B\\) when \\(B\\) is not occurred by any chance. Figure¬†8.3 illustrates the conditional probability of \\(A\\) given \\(B\\) using the Venn diagram. When we compute the probability of \\(A\\), the information about \\(B\\) have been given, and the probability of \\(A\\) is adjusted, according to this information. The conditional probability is the ratio of \\(P(A \\cap B)\\) to \\(P(B)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†8.3: Venn Diagram illustration of the conditional probability of A given B\n\n\n\n\nSo what is the difference between \\(P(A)\\) and \\(P(A \\mid B)\\)? How knowing event \\(B\\) occurred affects the probability of \\(A\\)? Well, Figure¬†8.4 describes the difference. When we calculate \\(P(A)\\), we assume we don‚Äôt have any specific information at hand. What we can base on is the entire sample space because we need to take all possible outcomes into account, and see if any outcome is related to the event \\(A\\). Therefore, although we just write \\(P(A)\\), the probability is actually calculated conditional on the entire sample space, i.e., \\(P(A \\mid \\mathcal{S})\\), which is the ratio of area of \\(A\\) to the area of \\(\\mathcal{S}\\) that is one. Such probability is usually called unconditional or marginal probability because being conditional on \\(\\mathcal{S}\\) is like being not conditional on some specific event, and ‚Äúmarginal‚Äù means all other possible events or outcomes have been ‚Äúmarginalized‚Äù out, and only event \\(A\\) is of our interest.\nNow if we know \\(B\\) has occurred, we don‚Äôt need to consider the entire sample space any more. Instead, we can focus only on \\(B\\) since we know \\(B\\) has occurred, and anything not related to \\(B\\), or \\(B^c\\) becomes irrelevant. We shrink the search pool from \\(\\mathcal{S}\\) to the smaller space \\(B\\). To find \\(P(A \\mid  B)\\), we just need to find how large part of \\(B\\) that also belongs to \\(A\\). Intuitively speaking, the event \\(B\\) has become our new sample space that are smaller than the original \\(\\mathcal{S}\\). We calculate the probability of \\(A\\) based on \\(B\\), not \\(\\mathcal{S}\\). Since \\(B\\) is the new sample space, we can treat \\(P(B)\\) as one. In other words, any probability conditional on \\(B\\) is scaled up by \\(1 / P(B)\\) so that \\(P(B) \\times \\frac{1}{P(B)} = 1\\). This is why \\(P(A \\cap B)\\) is multiplied by \\(1 / P(B)\\) in the conditional probability formula.\nHere is an example of new sample space or search pool. Suppose we would like to calculate the probability of a woman who is greater than 20 years old in a certain area. When we don‚Äôt have any background information about the woman, to compute the probability, we need to base on the entire female population of interest. But if we do know that the woman has two children, we shrink our focus on the pool of women who have two children, and compute the proportion of the women pool that is over 20 years of age.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†8.4: Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)\n\n\n\n\nThe conditional probability formula lead to the multiplication rule: \\[P(A \\cap B) = P(A \\mid  B)P(B) = P(B \\mid  A)P(A)\\] The rule is a rearranged form of the formula by multiplying both sides by \\(P(B)\\). Notice that \\(P(B \\mid  A) = \\frac{P(B \\cap A)}{P(A)}\\) and hence \\(P(B \\mid  A)P(A) = P(B \\cap A) = P(A \\cap B)\\).\n Example: Peanut Butter and Jelly\n\nSuppose 80% of people like peanut butter, 89% like jelly and 78% like both. Given that a randomly sampled person likes peanut butter, what is the probability that she also likes jelly?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want \\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)}\\).\nFrom the problem we have \\(P(PB) = 0.8\\), \\(P(J) = 0.89\\) and \\(P(PB \\cap J) = 0.78\\)\n\n\\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)} = \\frac{0.78}{0.8} = 0.975\\)\nIf we don‚Äôt know if the person loves peanut butter, the probability that he or she loves jelly is 89%.\nIf we do know she loves peanut butter, the probability that he or she loves jelly is going up to 97.5%.\n\n\n\n\n Independence \nIn the previous example, we learn that whether a person loves peanut butter affects the probability that she loves jelly. This piece of information is relevant, and the two events ‚Äúlove peanut butter‚Äù and ‚Äúlove jelly‚Äù are dependent each other because the one event will affect the chance of the other event happening. Uncovering the association or dependence is important for statistical inference because it helps statisticians better pin down the probability of being interest.\nFormally speaking, event \\(A\\) and \\(B\\) are independent if \\(\\begin{align} P(A \\mid  B) &= P(A) \\text{ or }\\\\ P(B \\mid  A) &= P(B) \\text{ or } \\\\P(A\\cap B) &= P(A)P(B)\\end{align}\\) \\(\\text{ for } P(A) &gt; 0 \\text{ and } P(B) &gt; 0.\\)\nIntuitively, this means that knowing \\(B\\) occurs does not change the probability that \\(A\\) occurs and vice versa. The information about \\(B\\) is irrelevant to probability of \\(A\\).\nHere is a question. Can we compute \\(P(A \\cap B)\\) if we only know \\(P(A)\\) and \\(P(B)\\) with no information other than sample space? The answer is no. We cannot compute \\(P(A \\cap B)\\) because we don‚Äôt know if \\(A\\) and \\(B\\) are independent. We can only do so if \\(A\\) and \\(B\\) are independent. In general, we need to consider the dependence of two events and use the multiplication rule \\(P(A \\cap B) = P(A \\mid B)P(B)\\).\nFigure¬†8.5 explain independence using the Venn diagram. Independence means that the ratio of area of \\(A\\) to area of \\(\\mathcal{S}\\) is the same as the ratio of area of \\(A \\cap B\\) to area of \\(B\\). Look at the case of non-independence on the right, the two events (circles) in particular. The area of \\(A \\cap B\\) is very close to the area of \\(B\\) because the two events are quite overlapped each other. It means that the ratio of area of \\(A \\cap B\\) to area of \\(B\\) is pretty close to one. So if we know \\(B\\) has occurred, there will be a very high chance that \\(A\\) would happen as well. In this case, the information about \\(B\\) does matter, and affect probability of \\(A\\). The idea is that the two events describe pretty much similar set of outcomes for an experiment. As a result, when one occurs, we are pretty sure the other happens as well.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†8.5: Venn Diagram Explanation of Independence\n\n\n\n\n Independence Example \n\n\n\n\n\n\nAssuming that events \\(A\\) and \\(B\\) are independent and that \\(P(A) = 0.3\\) and \\(P(B) = 0.7\\).\n\n\n\n\n\n\\(P(A \\cap B)\\)?\n\n\\(P(A \\cup B)\\)?\n\n\\(P(A \\mid B)\\)?\n\n\n\nSolution\n\n\\(P(A \\cap B) = P(A)P(B)=0.21\\)\n\\(P(A \\cup B) = P(A)+P(B)-P(A\\cap B) = 0.3+0.7-0.21=0.79\\)\n\\(P(A \\mid B) = P(A) = 0.3\\)",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Probability Rules and Bayes Formula</span>"
    ]
  },
  {
    "objectID": "prob-rule.html#bayes-formula",
    "href": "prob-rule.html#bayes-formula",
    "title": "8¬† Probability Rules and Bayes Formula",
    "section": "\n8.3 Bayes‚Äô Formula",
    "text": "8.3 Bayes‚Äô Formula\nBayes‚Äô formula is one of the most important theorems in the probability and statistical theory. It is the basis of Bayesian inference and Bayesian machine learning discussed in Chapter 22, which is getting more and more popular these days due to fast computation technology. In this section, we learn why we need the Bayes‚Äôs formula, and how we can use it to obtain the probability we are interested.\n Why Bayes‚Äô Formula? \nQuite often, we know \\(P(B \\mid A)\\), but we are much more interested in \\(P(A \\mid B)\\). Let‚Äôs take COVID diagnostic test as an example. Suppose you are having a sore throat and muscle and headache, and you decide to test if you got COVID. Of course, any test may not be 100% accurate. A person who does get COVID may be tested negative, which is called false negative, and a person who doesn‚Äôt get COVID may get a positive testing result, false positive in this case. With lots of trials, we can have false negative rate \\(P(\\text{test negative}  \\mid \\text{COVID})\\) and false positive rate \\(P(\\text{test positive}  \\mid \\text{not COVID})\\) of some COVID test.\nLet me ask you a question. Suppose that during a doctor‚Äôs visit, you tested positive for COVID. If you only get to ask the doctor one question, which would it be?\n\nWhat‚Äôs the chance that I actually have COVID?\nIf in fact I don‚Äôt have COVID, what‚Äôs the chance that I would‚Äôve gotten this positive test result?\n\nIf I were you, I would choose a. because I care more about whether I got COVID or not, not the efficacy of the test! In fact, diagnostic tests provide \\(P(\\text{test positive}  \\mid \\text{COVID})\\) and \\(P(\\text{test positive}  \\mid \\text{not COVID})\\), but what we are really interested is \\(P(\\text{COVID} \\mid \\text{test positive})\\)!\nSo, how can we utilize the diagnostic test efficacy to get the chance that one actually gets COVID? Bayes‚Äô formula is the answer. It provides a way to find \\(P(A \\mid B)\\) from \\(P(B \\mid A)\\).\n\n\n\n\n\n\nSource: Unsplash Martin Sanchez\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Formula \nThe Bayes‚Äô formula comes from the conditional probability. If \\(A\\) and \\(B\\) are events whose probability is not zero or one, then the Bayes‚Äô formula is derived as below. \\[\\begin{align*} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\quad ( \\text{def. of cond. prob.}) \\\\ &= \\frac{P(A \\cap B)}{P((B \\cap A) \\cup (B \\cap A^c))} \\quad ( \\text{partition } B) \\\\ &= \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}  \\quad ( \\text{multiplication rule}) \\end{align*}\\]\n\n\n\n\n\n\n\n\nFigure¬†8.6: Venn Diagram illustration for Bayes‚Äô formula\n\n\n\n\nThe first equality is just the definition of conditional probability. For the second equality, we partition \\(B\\) into \\((B \\cap A)\\) and \\((B \\cap A^c)\\) as \\((B \\cap A) \\cup (B \\cap A^c) = B\\) and \\((B \\cap A)\\) and \\((B \\cap A^c)\\) are disjoint, as illustrated in Figure¬†8.6. 1 Since \\((B \\cap A)\\) and \\((B \\cap A^c)\\) are disjoint, \\(P((B \\cap A) \\cup (B \\cap A^c)) = P(B \\cap A) + P(B \\cap A^c)\\). Finally, we can use the multiplication rule to write \\(P(B \\cap A)\\) as \\(P(B \\mid A)P(A)\\) and \\(P(B \\cap A^c)\\) as \\(P(B \\mid A^c)P(A^c)\\). The derivation of the Bayes‚Äô formula is complete.\nNotice that we use the information about \\(P(B \\mid A)\\), \\(P(B \\mid A^c)\\), \\(P(A)\\) and \\(P(A^c)\\) to obtain \\(P(A \\mid B)\\). In general, \\(P(A \\mid B) \\ne P(A \\mid B)\\), and they are completely different probabilities. Interpret your probability with additional care.\n Example: Passing Rate \n\n\nAfter taking MATH 4720, \\(80\\%\\) of students understand the Bayes‚Äô formula.\n\nOf those who understood the Bayes‚Äô formula, \\(95\\%\\) passed.\nOf those who did not understand the Bayes‚Äô formula, only \\(60\\%\\) passed.\n\n\n\n\n\n\nSource: Unsplash JESHOOTS.COM\n\n\n\n\n\n\n\n\n\n\n\nCalculate the probability that a student understands the Bayes‚Äô formula given the fact that she passed.\n\n\n\n\n\n\n\n\n Step 1: Formulate what we would like to compute \n\n\\(P(\\text{understood} \\mid \\text{passed})\\)\n\n\n\n Step 2: Define relevant events in the formula: \\(A\\), \\(A^c\\) and \\(B\\) \n\nLet \\(A =\\) understood and \\(B =\\) passed. Then \\(A^c =\\) didn‚Äôt understand and \\(P(\\text{understood} \\mid \\text{passed}) = P(A \\mid B)\\).\n\n\n\n Step 3: Find probabilities in the Bayes‚Äô formula using provided information. \n\n\\(P(B \\mid A) = P(\\text{passed} \\mid \\text{understood}) = 0.95\\)\n\n\\(P(B \\mid A^c) = P(\\text{passed} \\mid \\text{didn't understand}) = 0.6\\)\n\n\\(P(A) = P(\\text{understood}) = 0.8\\)\n\\(P(A^c) = 1 - P(A) = 0.2\\)\n\n\n\n Step 4: Apply Bayes‚Äô formula. \n\n\\(\\small P(\\text{understood} \\mid \\text{passed}) = P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)} = \\frac{(0.95)(0.8)}{(0.95)(0.8) + (0.6)(0.2)} = 0.86\\)\n\n\n\n Tree Diagram Illustration \n\n\n\\(80\\%\\) of students understand the Bayes‚Äô formula.\nOf those who understood the Bayes‚Äô formula, \\(95\\%\\) passed (\\(5\\%\\) failed).\nOf those who did not understand the formula, \\(60\\%\\) passed (\\(40\\%\\) failed).\n\n\n\n\n\n\n\n\n\n\nFigure¬†8.7: Tree Diagram illustration of Passing Rate example\n\n\n\n\n\n\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{P(\\text{pass | yes})P(\\text{yes})}{P(\\text{pass | yes})P(\\text{yes}) + P(\\text{pass | no})P(\\text{no})} \\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]\n\n\n\n Law of total probability* \nIf in general \\(n\\) events \\(A_1 \\dots, A_n\\) are disjoint, each event has positive probability and one of the events must occur, then \\[ P(B) = \\sum_{i=1}^n P(B\\mid A_i)P(A_i).\\]\nThis is the law of total probability. We‚Äôve seen this with \\(n = 2\\) in the denominator of Bayes‚Äô formula! Let \\(A_1 = A\\) and \\(A_2 = A^c\\), \\[\n\\begin{align*}\nP(B) &= P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c) \\\\\n&= P(B \\mid A_1)P(A_1) + P(B \\mid A_2)P(A_2) \\\\\n&= \\sum_{i=1}^2 P(B\\mid A_i)P(A_i)\n\\end{align*}\n\\]\nHow about \\(n = 3\\)? Check Figure¬†8.8 to learn the idea. Notice that \\(B = (B \\cap A_1) \\cup (B \\cap A_2) \\cup (B\\cap A_3)\\) and this can be generalized to arbitrary \\(n\\).\n\n\n\n\n\n\n\nFigure¬†8.8: Venn diagram of illustraing the law of total probability when \\(n = 3\\).\n\n\n\n\n\n\nIf \\(P(B) &gt; 0\\), the previous general \\(n\\) case implies\n\\[ P(A_k \\mid B) = \\frac{P(B \\mid A_k)P(A_k)}{\\sum_{i=1}^n P(B \\mid A_i)P(A_i)} \\]\nThis is the general Bayes‚Äô formula for arbitrary integer \\(n\\).",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Probability Rules and Bayes Formula</span>"
    ]
  },
  {
    "objectID": "prob-rule.html#exercises",
    "href": "prob-rule.html#exercises",
    "title": "8¬† Probability Rules and Bayes Formula",
    "section": "\n8.4 Exercises",
    "text": "8.4 Exercises\n\nA Pew Research survey asked 2,422 randomly sampled registered voters their political affiliation (Republican, Democrat, or Independent) and whether or not they identify as swing voters. 38% of respondents identified as Independent, 25% identified as swing voters, and 13% identified as both.\n\nAre being Independent and being a swing voter disjoint, i.e.¬†mutually exclusive?\nWhat percent of voters are Independent but not swing voters?\nWhat percent of voters are Independent or swing voters?\nWhat percent of voters are neither Independent nor swing voters?\nIs the event that someone is a swing voter independent of the event that someone is a political Independent?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarth is warming\nNot warming\nDon‚Äôt Know/Refuse\nTotal\n\n\n\nConservative Republican\n0.11\n0.20\n0.02\n0.33\n\n\nMod/Lib Republican\n0.06\n0.06\n0.01\n0.13\n\n\nMod/Cons Democrat\n0.25\n0.07\n0.02\n0.34\n\n\nLiberal Democrat\n0.18\n0.01\n0.01\n0.20\n\n\nTotal\n0.60\n0.34\n0.06\n1.00\n\n\n\n\n\nA Pew Research poll asked 1,423 Americans, ‚ÄúFrom what you‚Äôve read and heard, is there solid evidence that the average temperature on earth has been getting warmer over the past few decades, or not?‚Äù. The table above shows the distribution of responses by party and ideology, where the counts have been replaced with relative frequencies.\n\nAre believing that the earth is warming and being a liberal Democrat mutually exclusive?\nWhat is the probability that a randomly chosen respondent believes the earth is warming or is a Mod/Cons Democrat?\nWhat is the probability that a randomly chosen respondent believes the earth is warming given that he is a Mod/Cons Democrat?\nWhat is the probability that a randomly chosen respondent believes the earth is warming given that he is a Mod/Lib Republican?\nDoes it appear that whether or not a respondent believes the earth is warming is independent of their party and ideology? Explain your reasoning.\n\n\nAfter an MATH 4740/MSSC 5740 course, 73% of students could successfully construct scatter plots using R. Of those who could construct scatter plots, 84% passed, while only 62% of those students who could not construct scatter plots passed. Calculate the probability that a student is able to construct a scatter plot if it is known that she passed.\nOf all homes having wood-burning furnaces, 30% own a type 1 furnace, 25% a type 2 furnace, 15% a type 3, and 30% other types. Over 3 years, 5% of type 1 furnaces, 3% of type 2, 2% of type 3, and 4% of other types have resulted in fires. If a fire occurs in a particular home, what is the probability that a type 1 furnace is in the home?",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Probability Rules and Bayes Formula</span>"
    ]
  },
  {
    "objectID": "prob-rule.html#footnotes",
    "href": "prob-rule.html#footnotes",
    "title": "8¬† Probability Rules and Bayes Formula",
    "section": "",
    "text": "We say sets \\(A_1, \\dots, A_n\\) form a partition of set \\(A\\) if \\(A_1 \\cup \\cdots \\cup A_n = A\\) and \\(A_1, \\dots, A_n\\) are disjoint.‚Ü©Ô∏é",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>8</span>¬† <span class='chapter-title'>Probability Rules and Bayes Formula</span>"
    ]
  },
  {
    "objectID": "prob-rv.html",
    "href": "prob-rv.html",
    "title": "9¬† Random Variables",
    "section": "",
    "text": "9.1 Random Variables\nIn Chapter 2, we learn that a variable in a data set is a characteristic or attribute that varies from one object to another. A variable can be either categorical or numerical. For example, gender is categorical and height is numerical. In addition, numerical variables can be either discrete or continuous. A discrete variable takes on values of a finite or countable number, while a continuous variable takes on values anywhere over a particular range without gaps or jumps. So the number of conferences Dr.¬†Yu attended last year is discrete, and college GPA is continuous.\nWe know variables in a data set vary from one to another. If a variable takes numerical values, and its value is determined by some chance or randomness of a procedure or experiment, then we say the variable is a random variable, usually denoted as \\(X\\) or other capital English letters, \\(Y\\), \\(Z\\) for example.\nUsually a capital letter represents a random variable and a small letter represents a realized value of that random variable. For example, \\(X\\) is weight which is a random variable, and \\(x\\) is a realized value of \\(X\\) that can be any possible value of \\(X\\), for example, \\(X = 170\\) means the realized value \\(x\\) is 170.\nOther random variable examples are\nNote that \\(X\\) must follow some randomness pattern, and its realized value cannot be known in advance unless we collect the data or realize it.\nTo accounting for its randomness, a random variable has a probability distribution associated with it. The probability distribution governs the behavior of the random variable, indicating the range of the possible values, and what values are more probable to be realized than others. Probability distribution is key to uncertainty quantification in statistical inference and machine learning prediction. Figure¬†9.1 below shows the many different types of probability distributions, and we‚Äôll discuss a few basic but important probability distributions in Chapter 10 and Chapter 11.\nFigure¬†9.1: Source: https://github.com/rasmusab/distribution_diagrams\nWhen a random variable is defined, \\(X = x\\) or \\(a &lt; X &lt; b\\) or any specification about the values of \\(X\\) represents some event of some experiment. Consider the experiment that one tosses a fair coin twice independently, and define the random variable \\(X\\) as the number of heads. Then \\(\\{X = 0\\}\\) is the event \\(\\{\\text{tails}, \\text{tails}\\}\\) meaning that the first toss ends up with tails, so does the second toss. \\(\\{X = 1\\}\\) is the event \\(\\{\\text{tails}, \\text{heads}\\}\\) or \\(\\{\\text{heads}, \\text{tails}\\}\\) because heads can be shown up in the first toss or the second toss. \\(\\{X = 2\\}\\) corresponds to the event \\(\\{\\text{heads}, \\text{heads}\\}\\).",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob-rv.html#random-variables",
    "href": "prob-rv.html#random-variables",
    "title": "9¬† Random Variables",
    "section": "",
    "text": "\\(X\\) = # of heads after flipping a coin twice.\n\\(X\\) = # of accidents in W. Wisconsin Ave. per day",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob-rv.html#discrete-and-continuous-random-variables",
    "href": "prob-rv.html#discrete-and-continuous-random-variables",
    "title": "9¬† Random Variables",
    "section": "\n9.2 Discrete and Continuous Random Variables",
    "text": "9.2 Discrete and Continuous Random Variables\nAs variables, since a random variable takes numerical values, it can be discrete or continuous.\nA discrete random variable takes on a finite or countable number of values.\n\nThe number of relationships you‚Äôve ever had is discrete variable because we can count the number and it is finite.\nIf we can further determine the probability that the number is 0, 1, 2 or any possible number, then it is a discrete random variable.\n\nA continuous random variable has infinitely many values, and the collection of values is uncountable.\n\nHeight is a continuous variable because it can be any number within a range.\nIf we have a way to quantify the probability that the height is from any value \\(a\\) to any value \\(b\\), it is a continuous random variable.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>9</span>¬† <span class='chapter-title'>Random Variables</span>"
    ]
  },
  {
    "objectID": "prob-disc.html",
    "href": "prob-disc.html",
    "title": "10¬† Discrete Probability Distributions",
    "section": "",
    "text": "10.1 Probability Mass Function\nFor this and next chapters, we learn about probability distributions. In this chapter, we focus on discrete probability distributions for discrete random variables. We first learn what is a discrete probability distribution, followed by the two popular discrete distributions, binomial and Poisson distributions. We will learn how to use R to calculate probabilities from the distributions.\nA discrete random variable uses a probability function or probability mass function (pmf) to describe its randomness, and the pmf is its associated discrete probability distribution. The probability mass function of a discrete random variable \\(X\\) is a function of \\(X\\), denoted \\(P(X = x)\\) or \\(p(x)\\) for short, that assigns a probability to every possible number \\(x\\) of \\(X\\).\nThe probability distribution for a discrete \\(X\\) displays its probability function. The display can be a table, graph or mathematical formula of \\(P(X = x)\\) as long as every possible value of \\(X\\) has its own corresponding probability. We illustrate the probability function using an example of tossing a coin.\nExample:ü™ôü™ô Toss a fair coin twice independently where \\(X\\) is the number of heads.\nWith the example, first the possible values of \\(X\\) by its definition are 0, 1, and 2. Both results can be tails and \\(X = 0\\). We can have one heads and one tails, or \\(X=1\\), and we can have two heads showing up \\(X = 2\\). We cannot have \\(X=3\\) or any number greater than 2 because the coin is tossed twice only. Also, the possible value of \\(X\\) cannot be any non-integer values, like 1.5 or 0.8 because \\(X\\) is the number times, and it is discrete.\nTo present its probability distribution in a table, for each possible value of \\(X\\), we get to compute its probability, then list all probabilities as the table below.\nx\n0\n1\n2\n\n\nP(X = x)\n0.25\n0.5\n0.25\nLet me show the calculation of the probabilities.\nWe can also display the probability distribution using graphs. We put possible values of \\(X\\) in the x-axis, and the y-axis stands for the probability \\(P(X=x)\\). Then for each \\(x\\), we draw a vertical bar at \\(X=x\\) from \\(y = 0\\) to \\(y = P(X=x)\\). Figure¬†10.1 shows the discrete probability distribution of two coin flips as a graph.\nFigure¬†10.1: Discrete probability distribution of two coin flips as a graph\nIf you love math, you can specify the probability distribution using mathematical formula:\n\\[\nP(X = x)=\n\\begin{cases}\n{2 \\choose x}\\left( \\frac{1}{4}\\right), ~~ x = 0, 1, 2,\\\\\n0,  ~~ \\text{otherwise},\n\\end{cases}\n\\] where \\({2 \\choose x} = \\frac{2!}{x!(2-x)!}\\). Keep in mind that where \\(x\\) takes values or the support of \\(x\\) should be clearly specified.\nNow let‚Äôs talk a little bit about the properties of pmf.\nMean\nRemember that a probability distribution describes the randomness pattern of a random variable \\(X\\). Given its distribution, people are usually interested in its average value or central tendency and how uncertain it is or its dispersion.\nFor the central tendency, we consider the mean of a discrete variable \\(X\\). Suppose \\(X\\) takes values \\(x_1, \\dots, x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\). The mean or expected value of \\(X\\), written as \\(E(X)\\), is the sum of each outcome multiplied by its corresponding probability: \\[E(X) := x_1 \\times P(X = x_1) + \\dots + x_k \\times P(X = x_k) = \\sum_{i=1}^kx_iP(X=x_i)\\]\nThe Greek letter \\(\\mu\\) may also be used in place of the notation \\(E(X)\\).\nThe mean of a discrete random variable \\(X\\) is actually a weighted average of \\(x\\) weighted by their corresponding probability.\nIf you calculate it correctly, your answer should be one. It tells us that if we toss a fair coin twice independently, on average we‚Äôll see one heads showing up.\nVariance\nAs sample variance calculated from the sample data, we calculate the variance of the random variable \\(X\\) to quantify its variability and dispersion. Suppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\). The variance of \\(X\\), denoted by \\(Var(X)\\) or \\(\\sigma^2\\), is\n\\[\\small \\begin{align*} Var(X) &:= (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) \\\\ &= \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i) \\\\ &= E[(X-\\mu)^2] \\end{align*}.\\]\nThe standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance.\nIntuitively, the variance of a discrete random variable \\(X\\) is the weighted sum of squared deviation from the mean weighted by probability values. It is the mean squared distance from the mean.\nWe have learned the general discrete probability distributions. Next we are going to learn two popular discrete probability distributions, binomial and Poisson distribution.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-disc.html#probability-mass-function",
    "href": "prob-disc.html#probability-mass-function",
    "title": "10¬† Discrete Probability Distributions",
    "section": "",
    "text": "\\(P(X = 0) = P(\\{\\text{tails}, \\text{tails}\\}) = P(\\text{tails})P(\\text{tails}) = (0.5)(0.5) = 0.25.\\)\n\\(\\begin{align*} P(X = 1) &= P(\\{\\text{heads}, \\text{tails}\\} \\cup \\{\\text{tails}, \\text{heads}\\})\\\\ &= P(\\{\\text{heads}, \\text{tails}\\}) + P(\\{\\text{tails}, \\text{heads}\\}) \\\\ &= (0.5)(0.5) + (0.5)(0.5) = 0.5. \\end{align*}\\)\n\\(P(X = 2) = P(\\{\\text{heads}, \\text{heads}\\}) = P(\\text{heads})P(\\text{heads}) = (0.5)(0.5) = 0.25.\\)\n\n\n\n\n\n\n\n\\(0 \\le P(X = x) \\le 1\\) for every value \\(x\\) of \\(X\\). Remember that \\(P(X = x)\\) is a probability of some event, for example \\(P(X = 0) = P(\\{\\text{tails}, \\text{tails}\\})\\), and we‚Äôve learned that \\(0 \\le P(A) \\le 1\\) for any event \\(A\\). \nThe probabilities for a discrete \\(X\\) are additive because \\(\\{X = a\\}\\) and \\(\\{X = b\\}\\) are disjoint for any possible values \\(a \\ne b\\). In our coin tossing example, \\(\\{X = 0\\}\\) and \\(\\{X = 1\\}\\) are disjoint because when tossing a fair coin two times independently, we cannot have no heads and one heads results at the same time. Therefore, \\[P(X = 0 \\text{ or } 1) = P(\\{X = 0\\} \\cup \\{X = 1\\}) = P(X = 0) + P(X = 1).\\]\n\\(\\sum_{x}P(X=x) = 1\\), where \\(x\\) assumes all possible values. In our coin tossing example, \\(P(X = 0) + P(X = 1) + P(X = 2) = 0.25 + 0.5 + 0.25 = 1\\). The reason is that \\(\\{X = 0\\}\\), \\(\\{X = 1\\}\\), and \\(\\{X = 2\\}\\) form a partition of the entire sample space, where in the example \\(\\mathcal{S} = \\left\\{ \\{\\text{tails}, \\text{tails}\\}, \\{\\text{heads}, \\text{tails}\\}, \\{\\text{tails}, \\text{heads}\\}, \\{\\text{heads}, \\text{heads}\\} \\right\\}\\). Therefore, \\(\\sum_{x}P(X=x) = P(\\mathcal{S}) = 1\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the mean of \\(X\\) (the number of heads) in the coin example?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the variance of \\(X\\) (the number of heads) in the previous example?\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nThe mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\)) of a random variable or probability distribution are not the same as the sample mean (\\(\\overline{x}\\)) and sample variance (\\(s^2\\)) calculated from the sample data. Their main difference will be discussed in the Part Inference.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-disc.html#binomial-distribution",
    "href": "prob-disc.html#binomial-distribution",
    "title": "10¬† Discrete Probability Distributions",
    "section": "\n10.2 Binomial Distribution",
    "text": "10.2 Binomial Distribution\n Binomial Experiment and Random Variable \nBinomial distribution is generated from the so-called binomial experiment that has the following properties:\n\nüëâ The experiment consists of a fixed number of identical trials, say \\(n\\). That is, \\(n\\) is pre-specified before the experiment is conducted, and remains unchanged while the experiment is in progress. Also, all the trials or repetitions in the experiment should be performed with exactly the same condition or procedure.\nüëâ Each trial results in one of exactly two outcomes. In practice, we use success (S) and failure (F) to represent the two outcomes. The word success just means one of the two outcomes and does not necessarily mean something good. üò≤ Depending on your research question, you could define Drug abuse as success and No drug abuse as failure.\nüëâ Trials are independent, meaning that the outcome of one trial does not affect the outcome of any other trial.\nüëâ The probability of success, say \\(\\pi\\), is constant for all trials.\n\nIf a binomial experiment is conducted, and \\(X\\) is defined as  the number of successes observed in \\(n\\) trials , then \\(X\\) is a binomial random variable.\n\n\n\n\n\n Distribution \nThe probability function \\(P(X = x)\\) of a binomial variable \\(X\\) can be fully determined by the number of trials \\(n\\) and the probability of success \\(\\pi\\). Once \\(n\\) and \\(\\pi\\) are fixed and known, we know exactly what the distribution looks like, and we can form a table, graph, and provide a mathematical formula of the binomial probability function. A different \\((n, \\pi)\\) pair generates a different binomial probability distribution. The value(s) that determines an entire probability distribution is called the parameter of the distribution. Therefore, \\(X\\) is said to follow a binomial distribution with parameters \\(n\\) and \\(\\pi\\), written as \\(\\color{blue}{X \\sim binomial(n, \\pi)}\\).\nThe binomial probability function is \\[ \\color{blue}{P(X = x \\mid n, \\pi) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x}, \\quad x = 0, 1, 2, \\dots, n}\\]\nIt is not that important to memorize the formula as nowadays we use computing software to obtain \\(P(X = x)\\) for any \\(x\\). Notice that the maximal possible number of \\(x\\) is \\(n\\), the number of trials. We cannot have 5 successes when there are only 4 trials in the experiment. Second, when both \\(n\\) and \\(pi\\) are known and fixed, with a value of \\(x\\), everything in the formula is known, and the probability can be calculated.\nIt can be shown that the binomial distribution has mean \\(\\mu = E(X) = n\\pi\\) and variance \\(\\sigma^2 = Var(X) = n\\pi(1-\\pi)\\).\n\nviewof params = Inputs.form([\n      Inputs.range([1, 200], {value: 10, step: 1, label: tex`n`}),\n      Inputs.range([0, 1], {value: 0.5, step: 0.01, label: tex`\\pi`}),\n    ])\n\n\n\n\n\n\n\nviewof kvantil = Inputs.range([1, params[0]], {value: 3, step: 1, label: \"quantile\"})\n\n\n\n\n\n\n\nviewof approx = Inputs.toggle({label: \"show normal approximation\", value: false})\n\n\n\n\n\n\n\njstat = require('jstat')\n\n\n\n\n\n\n\nbinomcdf = jstat.binomial.cdf(kvantil, params[0], params[1]);\n\n\n\n\n\n\n\nnormalapproxpdf = {\n  const x = d3.range(0, params[0], 1);\n  const normalapproxpdf = x.map(x =&gt; ({x: x, pdf: jstat.normal.pdf(x, params[0]*params[1], Math.sqrt(params[0]*params[1]*(1-params[1])))}));\n  return normalapproxpdf\n}\n\n\n\n\n\n\n\nnormalapproxcdf = jstat.normal.cdf(kvantil, params[0]*params[1], Math.sqrt(params[0]*params[1]*(1-params[1])))\n\n\n\n\n\n\n\nnormalapproxcdf_corrected = jstat.normal.cdf(kvantil+0.5, params[0]*params[1], Math.sqrt(params[0]*params[1]*(1-params[1])))\n\n\n\n\n\n\n\nbinompdf = {\n  const x = d3.range(0, params[0]+1, 1);\n  const binompdf = x.map(x =&gt; ({x: x, pdf: jstat.binomial.pdf(x, params[0], params[1]), type: \"exact\"}));\n  const normalapproxpdf = x.map(x =&gt; ({x: x, pdf: jstat.normal.pdf(x, params[0]*params[1], Math.sqrt(params[0]*params[1]*(1-params[1]))),type: \"approx\"}));\n  const data = binompdf.concat(normalapproxpdf)\n  return data\n}\n\n\n\n\n\n\n\nmoments = tex`\n\\text{If } X \\sim \\operatorname{Binom}(${params[0]},${params[1]}) \\text{ then }\\\\[0.5em]\n\nE( X) =n  \\pi = ${(params[0]*params[1]).toPrecision(3)} \\\\[0.5em]\nVar( X) =n  \\pi (1-\\pi) =${(params[0]*params[1]*(1-params[1])).toPrecision(3)} \\\\[0.5em]\n\n`\n\n\n\n\n\n\n\ncdfs = \n{\n  var cdf;\n  if (approx){\n  cdf = tex`\n  \\begin{array}{ll}\n  \\text{Exact binomial:} & P( X\\leq ${kvantil}) =${binomcdf.toPrecision(4)} \\\\[0.25em]\n  \\text{Normal approx:} & P( X\\leq ${kvantil}) =${normalapproxcdf.toPrecision(4)} \\\\[0.25em]\n  \\text{Normal approx - corr:} & P( X\\leq ${kvantil})  =${normalapproxcdf_corrected.toPrecision(4)} \\\\[1em]\n  \\end{array}\n  `}\n  else{\n   cdf = tex`\n  \\begin{array}{ll}\n  \\text{Exact:} & P( X\\leq ${kvantil}) =${binomcdf.toPrecision(4)} \\\\[1em]\n  \\end{array}\n  ` \n  }\n  return cdf;\n}\n\n\n\n\n\n\n\nplt_binom = Plot.plot({\n    color: { \n      legend: approx\n    },\n    x: {\n      label: \"x\",\n      axis: true,\n      domain: d3.range(0, params[0]+1, 1)\n    },\n    y: {\n      label: \"P(X = x)\",\n      domain: [0,1.1*jstat.binomial.pdf(params[0]*params[1], params[0],params[1])]\n    },\n  tooltip: {\n    fill: \"#C04000\",\n    stroke: \"#C04000\",\n    opacity: 1,\n  },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.barY(binompdf,{filter: d =&gt; d.type == \"exact\", x: \"x\", y: \"pdf\", fill : \"orange\", strokeWidth: 0, opacity: 0.8, \n        title: (d) =&gt; `P(X=${d.x}) = ${(d.pdf).toPrecision(4)}`}),\n      Plot.barY(binompdf, {filter: d =&gt; d.type == \"exact\" && d.x &lt;= kvantil, x: \"x\", y: \"pdf\", fill: \"darkorange\", opacity: 1,\n        title: (d) =&gt; `P(X=${d.x}) = ${(d.pdf).toPrecision(4)}`}),\n      Plot.line(normalapproxpdf, {filter: d =&gt; approx, x: \"x\", y: \"pdf\", stroke : \"steelblue\", strokeWidth: 2})    \n    ]\n  })\n\n\n\n\n\n\n\nFigure¬†10.2: Source: https://observablehq.com/@mattiasvillani/binomial-distribution\n\n\n\n\n\n\n\n\n\nIf we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial random variable?\n\n\n\n\n\n\nTo answer this question, we need to check if the experiment satisfies the four properties.\n\nThe number of trials is 2, and the two trials, tossing a fair coin are identical (if you are not nitpicking).\nEach trial results in one of exactly two outcomes, heads or tails.\nTrials are independent. Well it‚Äôs hard to say, but at least they are nearly independent.\nThe probability of heads is 1/2 for all trials because we got a fair coin.\n\nSo, is \\(X\\) a binomial random variable? You tell me.\n Example \n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose it‚Äôs a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\). Let \\(X\\) be the number of drivers exceeding limit. Then \\(X \\sim binomial(15, 0.2)\\). Therefore,\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, 15.}\\]\nSince we know the value of parameter \\(n\\) and \\(\\pi\\), the entire binomial distribution can be described, as shown in Figure¬†10.3.\n\n\n\n\n\n\n\nFigure¬†10.3: Binomial Distribution Example \\(X \\sim binomial(15, 0.2).\\)\n\n\n\n\nTo answer the first question, we just need to calculate \\(P(X = 6)\\) using the formula: \\[\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\]\nThe second question asks for \\(P(X \\ge 6)\\) because of ‚Äú6 or more will exceed the legal limit‚Äù. One can calculate the probability like \\(\\small P(X \\ge 6) = P(X = 6 \\cup X = 7 \\cup \\cdots \\cup X = 15) = p(6) + \\dots + p(15)\\). It requires 10 probability calculations. One simpler way of obtaining the probability is to consider the complement set of \\(\\{X \\ge 6\\}\\) which is \\(\\{X \\le 5\\}\\). Then use the property that \\(P(A) = 1 - P(A^c)\\) to get the answer. So we can do the following \\(P(X \\ge 6) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\). In this case, we only need to calculate 6 probabilities from the formula.\nWell I believe you‚Äôve seen how tedious calculating a binomial probability is! It is so true especially when \\(n\\) is relatively large. No worries. We are living in the 21th century with lots of advancing computing technology. We‚Äôll never do such calculation by hand, and we‚Äôll learn to compute them using R!\n\n\n\n\n\n\n\nR\nPython\n\n\n\nIn practice, we are not gonna calculate probabilities of binomial or other commonly used probability distributions. Instead, we use computing software. In R we can use dpqr family functions to calculate probabilities or generate values from the distributions. In general, for some distribution, short for dist, R has the following functions\n\n\nddist(x, ...): calculate density value \\(f(x)\\) or probability value \\(P(X = x)\\).\n\npdist(q, ...): calculate \\(P(X \\le q)\\).\n\nqdist(p, ...): obtain quantile of probability \\(p\\).\n\nrdist(n, ...): generate \\(n\\) random numbers.\n\nWhen we use these functions, the dist is replaced with the shortened name of the distribution we consider. For example, we use dbinom(x, ...) to do the calculation for the binomial distribution.\nThe function ddist(x, ...) gives us the probability density value \\(f(x)\\) if the distribution is continuous, and it is why the function starts with d. The continuous probability distribution will be discussed in Chapter 11 in detail. If the distribution is discrete, like binomial, it gives us the value \\(P(X = x)\\) where \\(X\\) follows the distribution being considered, and \\(x\\) is the value specified in the first argument of the function.\nFor the binomial distribution, we use dbinom(x, size, prob) to compute \\(P(X = x)\\), where size is the number of trials and prob is the probability of success. Besides \\(x\\), we need to provide the values of size and prob because remember that they are the parameters of the binomial distribution. Without them, we cannot have a specific binomial distribution, and its probability cannot be calculated.\nTo obtain \\(P(X = 6)\\) where \\(X \\sim binomial(n=15, \\pi=0.2)\\), in R, we do\n\n\n[1] 0.04299262\n\n\nTo answer the second question \\(P(X \\ge 6)\\), we can use the function pbinom(q, size, prob) that calculates \\(P(X \\le q)\\) for \\(X \\sim bionomial(n = \\texttt{size}, \\pi = \\texttt{prob})\\). Notice that \\(P(X &gt;= 6) = 1 - P(X &lt;= 5)\\), so in R, we can do\n\n\n[1] 0.06105143\n\n\nBy default, the function pbinom(q, size, prob) calculates the probability \\(P(X \\le q)\\) which is the left or lower tail part of the distribution. The function provides an argument lower.tail that is logical, and is TRUE by default. When we set lower.tail = FALSE in the function, it will instead calculate \\(P(X &gt; q)\\) which is the right or upper tail part of the distribution. Since \\(P(X \\ge 6) = P(X &gt; 5)\\), in R we can do\n\n\n[1] 0.06105143\n\n\nNotice that we use q = 5 instead of q = 6 because we want \\(P(X &gt; 5)\\). Also, lower.tail = FALSE is added into the function, and the probability value is the same as the value before.\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is an example of how to generate the binomial probability distribution as a graph.\n\nplot(x = 0:15, y = dbinom(0:15, size = 15, prob = 0.2), \n     type = 'h', xlab = \"x\", ylab = \"P(X = x)\", \n     lwd = 5, main = \"Binomial(15, 0.2)\")\n\n\n\n\n\n\n\nHere a sequence of integers 0 to 15 are created and put in the x-axis. Then dbinom(0:15, size = 15, prob = 0.2) is used to create probabilities of \\(binomial(15, 0.2)\\) for each integer. The vertical bar geometry comes from the argument type = 'h' standing for ‚Äúhistogram‚Äù.1\nSince \\(n = 15\\) and \\(\\pi = 0.2\\), the mean is \\((15)(0.2) = 3\\). For the binomial distribution, it means that the number of success is more likely to be happened around \\(x = 3\\). It is very uncommon to see that more than 10 drivers have alcohol level above the legal limit.\n\n\nIn practice, we are not gonna calculate probabilities of binomial or other commonly used probability distributions. Instead, we use computing software. In Python we can use the methods in the scipy.stats.binom module to calculate probabilities or generate values from the distributions. In general, for some distribution, short for dist, Python has the following functions\n\n\ndist.pmf(k, ...): calculate probability value \\(P(X = k)\\).\n\n\npmf means probability mass function.\n\n\n\ndist.pdf(x, ...): calculate probability density \\(f(x)\\).\n\n\npdf means probability density function.\n\n\n\ndist.cdf(k, ...): calculate \\(P(X \\le k)\\).\n\n\ncdf means cumulative distribution function.\n\n\n\ndist.sf(k, ...): calculate \\(P(X &gt; k)\\).\n\n\nsf means survival function.\n\n\n\ndist.ppf(q, ...): obtain quantile of probability \\(q\\).\n\n\nppf means percent point function.\n\n\n\ndist.rvs(n, ...): generate \\(n\\) random numbers.\n\n\nrvs means random variables.\n\n\n\nWhen we use these functions, the dist is replaced with the shortened name of the distribution we consider. For example, we use binom.pmf(k, ...) to do the calculation for the binomial distribution.\nFor the binomial distribution, we use binom.pmf(k, n, p) to compute \\(P(X = k)\\), where n is the number of trials and p is the probability of success. Besides k, we need to provide the values of n and p because remember that they are the parameters of the binomial distribution. Without them, we cannot have a specific binomial distribution, and its probability cannot be calculated.\nTo obtain \\(P(X = 6)\\) where \\(X \\sim binomial(n=15, \\pi=0.2)\\), in Python, we do\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n\n# Binomial distribution calculations\n# 1. P(X = 6)\nbinom.pmf(k=6, n=15, p=0.2)\n\n0.04299262263296005\n\n\nTo answer the second question \\(P(X \\ge 6)\\), we can use the function binom.cdf(k, n, p) that calculates \\(P(X \\le k)\\) for \\(X \\sim bionomial(n = \\texttt{n}, \\pi = \\texttt{p})\\). Notice that \\(P(X &gt;= 6) = 1 - P(X &lt;= 5)\\), so in Python, we can do\n\n# 2. P(X &gt;= 6) = 1 - P(X &lt;= 5)\n1 - binom.cdf(k=5, n=15, p=0.2)\n\n0.06105142961766408\n\n\nThe function binom.cdf(q, n, p) calculates the probability \\(P(X \\le k)\\) which is the left or lower tail part of the distribution. We can use binom.sf(q, n, p) to calculate \\(P(X &gt; q)\\) which is the right or upper tail part of the distribution. Since \\(P(X \\ge 6) = P(X &gt; 5)\\), in Python we can do\n\n# Alternatively, using the upper tail probability\nbinom.sf(5, n=15, p=0.2)\n\n0.061051429617664056\n\n\nNotice that we use k=5 instead of k=6 because we want \\(P(X &gt; 5)\\). The probability value is the same as the value before.\nBelow is an example of how to generate the binomial probability distribution as a graph. We use the stem plot plt.stem().\n\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n\n\n\n\n\n\n\n\nHere a sequence of integers 0 to 15 are created and put in the x-axis. np.arange(0, 16) is a way to generate a sequence of numbers from 0 to 15. Again 0 is included but 16 is not. Then binom.pmf(np.arange(0, 16), n=15, p=0.2) is used to create probabilities of \\(binomial(15, 0.2)\\) for each integer.\nSince \\(n = 15\\) and \\(\\pi = 0.2\\), the mean is \\((15)(0.2) = 3\\). For the binomial distribution, it means that the number of success is more likely to be happened around \\(x = 3\\). It is very uncommon to see that more than 10 drivers have alcohol level above the legal limit.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-disc.html#poisson-distribution",
    "href": "prob-disc.html#poisson-distribution",
    "title": "10¬† Discrete Probability Distributions",
    "section": "\n10.3 Poisson Distribution",
    "text": "10.3 Poisson Distribution\n Poisson Random Variables \nIf we want to count the number of occurrences of some event2 over a unit of time or space/region/volume and observe its associated probability, we could consider the Poisson distribution. For example,\n\nThe number of COVID patients arriving at ICU in one hour\nThe number of Marquette students logging onto D2L in one day\nThe number of dandelions per square meter on Marquette‚Äôs campus\n\nLet \\(X\\) be a Poisson random variable. Then \\(\\color{blue}{X \\sim Poisson(\\lambda)}\\), where \\(\\lambda\\) is the parameter representing the mean number of occurrences of the event in some time interval or region. The Poisson probability function is\n\\[\\color{blue}{P(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0, 1, 2, \\dots}\\]\nAgain, the parameter \\(\\lambda\\) determines the shape of the distribution. The constant \\(e\\) is the Euler‚Äôs number that is approximately \\(2.7182818284\\). \\(x! = x \\times (x-1) \\times \\cdots \\times 2 \\times 1\\) and \\(0! = 1\\). Note that the possible value of \\(x\\) could be any positive integer. Theoretically speaking, there is no upper limit for the number of occurrences of any event. No worries. You don‚Äôt need to memorize the formula, but it‚Äôs good to recognize it.\nOne interesting property of the Poisson distribution is that its mean and variance are both equal to its parameter \\(\\lambda\\). So a \\(Poisson(5)\\) distribution has mean and variance being equal to 5.\n\n\nviewof params_pois = Inputs.form([\n      Inputs.range([0.1, 20], {value: 2, step: 0.1, label: tex`\\lambda:`}),\n      Inputs.range([1, 40], {value: 1, step: 1, label: \"quantile:\"})\n    ])\n\n\n\n\n\n\n\ndist_quantile = tex`\\text{If } X \\sim \\operatorname{Poisson}(${params_pois[0]}) \\text{ then } `\n\n\n\n\n\n\n\nmoments_pois = tex`\n\\begin{aligned}\n&\\mathrm{E}( X) =  \\lambda = ${params_pois[0].toPrecision(3)} \\\\[0.5em]\n&\\mathrm{Var}( X) =  \\lambda  = ${params_pois[0].toPrecision(3)} \\\\[0.5em]\n&\\mathrm{P}( X\\leq ${params_pois[1]}) =${poiscdf.toPrecision(4)}\n\\end{aligned}\n`\n\n\n\n\n\n\n\nplt_pois = Plot.plot({\n    color: {\n      legend: false\n    },\n    x: {\n      label: \"x\",\n      axis: true\n    },\n    y: {\n      label: \"P(X = x)\",\n      axis: true\n    },\n  tooltip: {\n    fill: \"#097969\",\n    stroke: \"#097969\",\n    opacity: 1,\n  },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.barY(poispdf,{x: \"x\", y: \"pdf\", fill: \"#C1E1C1\", strokeWidth: 0, opacity: 1,\n                title: (d) =&gt; `P(X=${d.x}) = ${(d.pdf).toPrecision(4)}`}),\n      Plot.barY(poispdf, {filter: d =&gt; d.x &lt;= params_pois[1], x: \"x\", y: \"pdf\", fill: \"#50C878\", opacity: 1,\n                title: (d) =&gt; `P(X=${d.x}) = ${(d.pdf).toPrecision(4)}`})\n    ]\n  })\n\n\n\n\n\n\n\nFigure¬†10.4: Source: https://observablehq.com/@mattiasvillani/poisson-distribution\n\n\n\n\npoispdf = {\n  const x = d3.range(0, params_pois[0] + 4*Math.sqrt(params_pois[0]), 1);\n  const data = x.map(x =&gt; ({x: x, pdf: jstat.poisson.pdf(x, params_pois[0])}));\n  return data\n}\n\n\n\n\n\n\n\npoiscdf = jstat.poisson.cdf(params_pois[1], params_pois[0]);\n\n\n\n\n\n\n\n Assumptions and Properties of Poisson Variables \nAs the binomial distribution, the Poisson distribution comes from the Poisson experiment having the following properties and assumptions:\n\nüëâ Events occur one at a time; two or more events do not occur at the same time or in the same space or spot. For example, one cannot say two patients arrived at ICU at the same time. There must be a way to separate one event from the other, and one can always know which patient arrives at ICU earlier.\nüëâ The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a nonoverlapping time period or region of space. For example, number of patients arriving at ICU between 2 PM and 3 PM has nothing to do with the number of patients arriving at ICU between 8 PM and 9 PM because the two time periods have no overlap.\nüëâ \\(\\lambda\\) is constant for any period or region. For example, the mean number of patients arriving at ICU between 2 PM and 3 PM is the same as the mean number of patients arriving at ICU between 8 PM and 9 PM. This assumption is pretty strong, and usually violated in reality. If you want to use Poisson distribution to build your statistical model, use it with additional care.\n\n\n\n\n\n\n\nWhat are the differences between Binomial and Poisson distributions?\n\n\n\nThe Poisson distribution\n\nis determined by one single parameter \\(\\lambda\\)\nhas possible values \\(x = 0, 1, 2, \\dots\\) with no upper limit (countable), while a binomial variable has possible values \\(0, 1, 2, \\dots, n\\) (finite)\n\n\n\n\n Example \n\n\nLast year there were 4200 births at the University of Wisconsin Hospital. Let \\(X\\) be the number of births in a given day at the center, and assume \\(X \\sim Poisson(\\lambda)\\). Find\n\n\\(\\lambda\\), the mean number of births per day.\nthe probability that on a randomly selected day, there are exactly 10 births.\n\\(P(X &gt; 10)\\)?\n\n\n\n\n\n\nSource: Unsplash kaushal mishra\n\n\n\n\n\n\n\\(\\small \\lambda = \\frac{\\text{Number of birth in a year}}{\\text{Number of days}} = \\frac{4200}{365} = 11.5.\\) There are totally 4200 births in one year, so on average there are 11.5 per day. According to how we define \\(X\\), the time unit is a day, not a year. The parameter \\(\\lambda\\) and \\(X\\) should have the same time unit.\n\\(\\small P(X = 10 \\mid \\lambda = 11.5) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = \\frac{11.5^{10} e^{-11.5}}{10!} = 0.113.\\)\n\\(\\small P(X &gt; 10) = p(11) + p(12) + \\dots + p(20) + \\dots\\) (No end!) \\(\\small P(X &gt; 10) = 1 - P(X \\le 10) = 1 - (p(1) + p(2) + \\dots + p(10))\\).\n\nDid you see how tedious it is to calculate the Poisson probabilities even using a calculator? I know you are waiting for R/Python implementation!\n\n\n\n\n\n\nR\nPython\n\n\n\nInstead of using dbinom() and pbinom(), for Poisson distribution, we replace binom with pois, and use dpois(x, lambda) and ppois(q, lambda) to calculate the probabilities.\nWith lambda being the mean of Poisson distribution, and \\(X\\sim Poisson(\\lambda)\\), we use\n\ndpois(x, lambda) to compute \\(P(X = x \\mid \\lambda)\\)\nppois(q, lambda) to compute \\(P(X \\le q \\mid \\lambda)\\)\nppois(q, lambda, lower.tail = FALSE) to compute \\(P(X &gt; q \\mid \\lambda)\\)\n\n\n\n[1] 11.50685\n\n\n[1] 0.112834\n\n\n\n\n[1] 0.5990436\n\n\n[1] 0.5990436\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is an example of how to generate the Poisson probability distribution as a graph.\n\nplot(0:24, dpois(0:24, lambda = lam), type = 'h', \n     lwd = 5, ylab = \"P(X = x)\", xlab = \"x\", \n     main = \"Poisson(11.5)\")\n\n\n\n\n\n\n\nBe careful that the Poisson \\(X\\) has no upper limit; the graph is truncated at \\(x = 24\\). Strictly speaking, the graph does not accurately display the \\(Poisson(11.5)\\) distribution. Since the mean is 11.5, we know that it is very unlikely to have a very large \\(x\\), that is \\(P(X = x)\\) is very close to zero for \\(x &gt; 25\\). As the binomial distribution, the number of occurrences is more likely to be around its mean number 11.5, and the chance is decaying as the number is away from the mean.\n\n\nInstead of using binom.pnf() and binom.cdf(), for Poisson distribution, we replace binom with poisson, and use poisson.pmf(k, mu) and poisson.cdf(k, mu) to calculate the probabilities.\nWith lambda being the mean of Poisson distribution, and \\(X\\sim Poisson(\\lambda)\\), we use\n\npoisson.pmf(k, mu = lambda) to compute \\(P(X = k \\mid \\lambda)\\)\npoisson.cdf(k, mu = lambda) to compute \\(P(X \\le k \\mid \\lambda)\\)\npoisson.sf(k, mu = lambda) to compute \\(P(X &gt; k \\mid \\lambda)\\)\n\n\nfrom scipy.stats import poisson\n\n\n\n11.506849315068493\n\n\n0.1128340209466802\n\n\n\n\n0.5990435715682069\n\n\n0.5990435715682069\n\n\nBelow is an example of how to generate the Poisson probability distribution as a graph.\n\nplt.stem(np.arange(0, 25), poisson.pmf(np.arange(0, 25), mu=lam), \n         basefmt=\" \")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(X = x)\")\nplt.title(\"Poisson(11.5)\")\nplt.show()\n\n\n\n\n\n\n\nBe careful that the Poisson \\(X\\) has no upper limit; the graph is truncated at \\(x = 24\\). Strictly speaking, the graph does not accurately display the \\(Poisson(11.5)\\) distribution. Since the mean is 11.5, we know that it is very unlikely to have a very large \\(x\\), that is \\(P(X = x)\\) is very close to zero for \\(x &gt; 25\\). As the binomial distribution, the number of occurrences is more likely to be around its mean number 11.5, and the chance is decaying as the number is away from the mean.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-disc.html#relationship-between-binomial-and-poisson",
    "href": "prob-disc.html#relationship-between-binomial-and-poisson",
    "title": "10¬† Discrete Probability Distributions",
    "section": "\n10.4 Relationship between Binomial and Poisson*",
    "text": "10.4 Relationship between Binomial and Poisson*\nActually binomial and Poisson distributions are somewhat related. Let‚Äôs consider the following example.\nSuppose a store owner believes that customers arrive at his store at a rate of 3.6 customers per hour on average. He wants to find the distribution of the number customers who will arrive during a particular one-hour period. And here is what he plans to do.\nHe models customer arrivals in different time periods as independent. Then he divides the one-hour period into 3600 seconds and thinks of the arrival rate as being \\(3.6/3600 = 0.001\\) per second.During each second either 0 or 1 customer will arrive, and the probability of an arrival during any single second is \\(0.001\\).\n\n\n\nThink about it. what he does is actually an binomial experiment because his experiment has\n\nA fixed number of identical trials: 3600 seconds (each second is one trial)\n\nEach trial results in one of two outcomes (No customer or 1 customer)\n\nTrials are independent. (customer arrivals in different time periods are independent)\n\nThe probability of success is constant. (the probability of an arrival during any single second is 0.001)\n\n\nIf we let \\(X\\) be the number of customers arrived during one-hour period or 3600 seconds, then \\(X \\sim binomial(n=3600, \\pi = 0.001)\\). However, since he‚Äôs actually counting the number of occurrences over a unit of time, we know that he can model \\(X\\) as a Poisson random variable with mean 3.6 customers per hour which is happened to be equal to \\(n\\pi\\) in the binomial distribution: \\(X \\sim Poisson(\\lambda = 3.6 = n\\pi)\\).\nSo what do we learn from this example? ‚ÄúAll models are wrong, but some are useful‚Äù ‚Äì George E. Box (1919 - 2013). We can deal with a problem using different approaches or models. Also, the binomial and Poisson distributions are somehow related with each other.\n\nSo in fact, when \\(n\\) is large and \\(\\pi\\) is small, like the ones in our example, we can use Poisson distribution as approximation to binomial distribution with parameters \\(n\\) and \\(\\pi\\), where the \\(\\lambda\\) parameter in Poisson distribution equals \\(n\\pi\\).\nLet‚Äôs see the two probability distributions with different size of \\(n\\) and \\(\\pi\\). On the left, \\(n\\) is not large, just 10, and \\(\\pi\\) is not small, just \\(0.5\\). We can see that the two distributions are not alike. The binomial distribution is symmetric and the Poisson is a little bit right-skewed, and more spread out.\nWhen \\(n\\) is large, say 100, and \\(\\pi\\) is small like \\(0.01\\), the two distributions are almost the same! It is hard to distinguish them by our eyes. If you want to learn formal proof about how a binomial distribution converges to Poisson as \\(n\\) goes to infinity, and \\(\\pi\\) goes to zero while \\(n\\pi\\) stays constant, you can take a probability theory course.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nviewof params_binom_pois = Inputs.form([\n      Inputs.range([1, 200], {value: 10, step: 1, label: tex`n`}),\n      Inputs.range([0.001, 1], {value: 0.5, step: 0.001, label: tex`\\pi`}),\n    ])\n\n\n\n\n\n\n\nviewof kvantil_binom_pois = Inputs.range([0, params_binom_pois[0]], {value: 3, step: 1, label: \"quantile\"})\n\n\n\n\n\n\n\nmoments_binom_pois = tex`\n\\text{If } X \\sim \\operatorname{Binom}(${params_binom_pois[0]},${params_binom_pois[1]}) \\text{ then }\\\\[0.5em]\n\n\nE( X) =n  \\pi = ${(params_binom_pois[0]*params_binom_pois[1]).toPrecision(3)} \\\\[0.5em]\nVar( X) =n  \\pi (1-\\pi) =${(params_binom_pois[0]*params_binom_pois[1]*(1-params_binom_pois[1])).toPrecision(3)} \\\\[0.5em]\n\n`\n\n\n\n\n\n\n\ncdfs_poisapprox = \n{\n  var cdf;\n  if (poisapprox){\n      cdf = tex`\n        \\begin{array}{ll}\n        \\text{Exact:} & P( X\\leq ${kvantil_binom_pois}) =${binomcdf_binom_pois.toPrecision(3)} \\\\[0.25em]\n        \\text{Poisson approx} & P( X\\leq ${kvantil_binom_pois}) =${poissonapproxcdf.toPrecision(3)} \n        \\end{array}\n        `\n  }\n  else{\n      cdf = tex`\n        \\begin{array}{ll}\n        \\text{Exact:} & P( X\\leq ${kvantil_binom_pois}) =${binomcdf_binom_pois.toPrecision(3)}\n        \\end{array}\n        ` \n  }\n  return cdf;\n}\n\n\n\n\n\n\n\nviewof plotselector = Inputs.checkbox([\"poisson\"], {label: \"plot approximation:\", value: [\"poisson approximation\"]})\n\n\n\n\n\n\n\nplt_pdf = Plot.plot({\n    color: {\n          legend: true,\n          domain: legenddomain.filter( (d,i) =&gt; boolselect[i]),\n          range: legendcolor.filter( (d,i) =&gt; boolselect[i])\n        },\n    x: {\n      label: \"x\",\n      axis: true,\n      domain: xrange\n    },\n    y: {\n      label: \"P(X = x)\",\n      domain: [0,1.1*jstat.binomial.pdf(params_binom_pois[0]*params_binom_pois[1], params_binom_pois[0],params_binom_pois[1])]\n    },\n  tooltip: {\n    fill: \"b39640\",\n    stroke: \"b39640\",\n    opacity: 1,\n  },\n    marks: [\n      Plot.ruleY([0]),\n      Plot.barY(approxpdf,{x: \"x\", y: \"binompdf\", fill : \"#C1CFE4\", strokeWidth: 0, opacity: 0.8, \n        title: (d) =&gt; `P(X=${d.x}) = ${(d.binompdf).toPrecision(4)}`}),\n      Plot.barY(approxpdf, {filter: d =&gt; d.x &lt;= kvantil_binom_pois, x: \"x\", y: \"binompdf\", fill: \"#98afd2\", opacity: 1,\n        title: (d) =&gt; `P(X=${d.x}) = ${(d.binompdf).toPrecision(4)}`}),\n      \n      // Plot.line(approxpdf, {filter: d =&gt; normalapprox, x: \"x\", y: \"normalpdf\", stroke : \"#AB8D61\", strokeWidth: 3}),\n      // Plot.dot(approxpdf, {filter: d =&gt; normalapprox, x: \"x\", y: \"normalpdf\", stroke : \"#AB8D61\", strokeWidth: 3}),\n      \n      // Plot.line(approxpdf, {filter: d =&gt; normalapproxcorr, x: \"x\", y: \"normalpdfcorr\", stroke : \"#007878\", strokeWidth: 3}),\n      // Plot.dot(approxpdf, {filter: d =&gt; normalapproxcorr, x: \"x\", y: \"normalpdfcorr\", stroke : \"#007878\", strokeWidth: 3}),\n      \n      Plot.ruleX(approxpdf, {filter: d =&gt; poisapprox && d.x &gt;= 0, x: \"x\", y: \"poissonpdf\", stroke: \"#780000\", strokeWidth: 2}),  \n      Plot.dot(approxpdf, {filter: d =&gt; poisapprox && d.x &gt;= 0, x: \"x\", y: \"poissonpdf\", fill: \"#780000\", r: 5}),\n    ]\n  })\n\n\n\n\n\n\n\napproxpdf = {\n  const x = xrange;\n  const approxpdf = x.map(x =&gt; ({\n    x: x, \n    binompdf: jstat.binomial.pdf(x, params_binom_pois[0], params_binom_pois[1]),\n    // normalpdf: jstat.normal.pdf(x, params[0]*params[1], Math.sqrt(params[0]*params[1]*(1-params[1]))), \n    // normalpdfcorr: jstat.normal.pdf(x + 0.5, params[0]*params[1], Math.sqrt(params[0]*params[1]*(1-params[1]))),\n    poissonpdf: jstat.poisson.pdf(x, params_binom_pois[0]*params_binom_pois[1])\n  }));\n  return approxpdf\n}\n\n\n\n\n\n\n\nxrange = d3.range(0, high + 1, 1);\n\n\n\n\n\n\n\nhigh = Math.ceil(jstat.normal.inv(0.9999, params_binom_pois[0]*params_binom_pois[1], Math.sqrt(params_binom_pois[0]*params_binom_pois[1]*(1-params_binom_pois[1]))))\n\n\n\n\n\n\n\n\n\n\nlegendcolor = [\"#C1CFE4\", \"#AB8D61\", \"#007878\", \"#780000\"]\n\n\n\n\n\n\n\nboolselect = [true, poisapprox]\n\n\n\n\n\n\n\npoisapprox = plotselector.includes(\"poisson\")\n\n\n\n\n\n\n\nlegenddomain = [\"binomial\", \"poisson\"]\n\n\n\n\n\n\n\npoissonapproxcdf = jstat.poisson.cdf(kvantil_binom_pois, params_binom_pois[0]*params_binom_pois[1])\n\n\n\n\n\n\n\nbinomcdf_binom_pois = jstat.binomial.cdf(kvantil_binom_pois, params_binom_pois[0], params_binom_pois[1]);",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-disc.html#exercises",
    "href": "prob-disc.html#exercises",
    "title": "10¬† Discrete Probability Distributions",
    "section": "\n10.5 Exercises",
    "text": "10.5 Exercises\n\nData collected by the Substance Abuse and Mental Health Services Administration (SAMSHA) suggests that 65% of 18-20 year olds consumed alcoholic beverages in any given year.\n\nSuppose a random sample of twelve 18-20 year olds is taken. When does it make sense to use binomial distribution for calculating the probability that exactly five consumed alcoholic beverages?\nWhat is the probability that exactly five out of twelve 18-20 year olds have consumed an alcoholic beverage?\nWhat is the probability that at most 3 out of 7 randomly sampled 18-20 year olds have consumed alcoholic beverages?\n\n\nA Dunkin‚Äô Donuts in Milwaukee serves an average of 65 customers per hour during the morning rush.\n\nWhich distribution have we studied that is most appropriate for calculating the probability of a given number of customers arriving within one hour during this time of day?\nWhat are the mean and the standard deviation of the number of customers this Starbucks serves in one hour during this time of day?\nCalculate the probability that this Dunkin‚Äô Donuts serves 55 customers in one hour during this time of day.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-disc.html#footnotes",
    "href": "prob-disc.html#footnotes",
    "title": "10¬† Discrete Probability Distributions",
    "section": "",
    "text": "It is a little misleading. The 'h' type here is just a notation for this bar-like plotting type, not really the histogram we discussed in Chapter 5. To draw a histogram, we use the function hist().‚Ü©Ô∏é\nIn this Poisson distribution section, the word event means an incident or happening. It is not the event used in probability, which is a set conceptually.‚Ü©Ô∏é",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html",
    "href": "prob-cont.html",
    "title": "11¬† Continuous Probability Distributions",
    "section": "",
    "text": "11.1 Introduction\nIn this chapter, we discuss continuous probability distributions. We first learn the idea and properties of continuous distributions, then talk about probably the most important and commonly used distribution in probability and statistics, the normal distribution.\nUnlike a discrete random variable taking finite or countable values, a continuous random variable takes on any values from an interval of the real number line. For example, a continuous random variable \\(X\\) could take any value in the unit interval \\([0, 1]\\). Its possible values are uncountable.\nInstead of probability functions, a continuous random variable \\(X\\) has the probability density function (pdf) denoted \\(f(x)\\) such that for any real value \\(a &lt; b\\), \\[P(a &lt; X &lt; b) = \\int_{a}^b f(x) dx.\\]\nThe probability that \\(X\\) is in some interval is computed from the integral of the density function with respect to \\(x\\). Keep in mind that the density function \\(f(x)\\) itself is not the probability that \\(X = x\\). The probability of continuous random variables is defined through the integral of \\(f(x)\\).\nThe cumulative distribution function (cdf) of \\(X\\) is defined as \\[F(x) := P(X \\le x) = \\int_{-\\infty}^x f(t)dt.\\]\nüòé Luckily, we don‚Äôt calculate integrals in this course. You just need to remember that for continuous random variables,\nEvery probability density function must satisfy the two properties:\n\\(f(x)\\) is a density value. For property 1, like density used in Physics, it cannot be negative. The density here represents how much likely the random variable \\(X\\) is around the value \\(x\\). When \\(f(x) = 0\\), it means that it is not possible to have \\(X\\) having value in the tiny neighborhood around \\(x\\). On the other hand, when \\(f(x)\\) is large, it is pretty likely to have \\(X\\) having values around \\(x\\). Because \\(f(x)\\) is the integrand, a larger value of \\(f(x)\\) in the interval \\([a, b]\\) will lead to a larger probability \\(P(a &lt; X &lt; b)\\).\nThe second property tells us that \\(P(-\\infty &lt; X &lt; \\infty) = 1\\). Remember that a random variable, whether it is discrete or continuous, must take a real value. Therefore the probability that \\(X\\) lives on the entire real line \\((-\\infty, \\infty)\\) is one.\nIn fact, any function satisfying the two properties can be served as a probability density function for some random variable.\nDensity Curve\nA probability density function generates a graph called a density curve that shows the likelihood of a random variable at all possible values. Figure¬†11.1 shows an example of density curve colored in blue. From Calculus 101, we have two important findings:\nFigure¬†11.1: Density curve for a random variable\nCommonly Used Continuous Distributions\nThere are tons of continuous distributions out there, and we won‚Äôt be able to discuss all of them. In this book, we will touch on normal (Gaussian), student‚Äôs t, chi-square, and F distributions. Some other popular distributions include uniform, exponential, gamma, beta, inverse gamma, Cauchy, etc. If you are interested in learning more distributions and their properties, please take a calculus-based probability theory course.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#introduction",
    "href": "prob-cont.html#introduction",
    "title": "11¬† Continuous Probability Distributions",
    "section": "",
    "text": "Important\n\n\n\n\nThe pdf does not represent a probability.\nThe integral of pdf represents a probability.\nThe cdf itself by definition is a probability that is also from the integral of pdf.\n\n\n\n\n\n\\(f(x) \\ge 0\\) for all \\(x\\) on the real line\n\\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\)\n\n\n\n\n\n\n\n\nThe integral of \\(f(x)\\) between \\(a\\) and \\(b\\) is actually the area under the density curve between \\(a\\) and \\(b\\). Therefore, the area under the density curve represents the probability \\(P(a &lt; X &lt; b) = \\int_{a}^b f(x) dx\\), the density value \\(f(x)\\), or the height of the density curve does not.\nThe total area under any density curve is equal to 1: \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that the area under the density curve represents probability, not the density value or the height of the density curve at some value of \\(x\\).\nOne question is for a continuous random variable \\(X\\), what is the probability that \\(X\\) equals any real number, or \\(P(X = a)\\) for any \\(a \\in \\mathbf{R}\\)? Since \\(P(X = a) = P(a \\le X \\le a) = \\int_{a}^a f(x) dx = 0\\), we know that \\(P(X = a) = 0\\) for any real number \\(a\\). Graphically speaking, it means that there is no area under the density curve between \\(a\\) and \\(a\\).\nTherefore, for a continuous random variable \\(X\\), \\(P(a \\le X\\le b) = P(a &lt; X &lt; b)\\) for any real value \\(a\\) and \\(b\\) because there is no probability mass on \\(x = a\\) and \\(x = b\\).",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#normal-gaussian-distribution",
    "href": "prob-cont.html#normal-gaussian-distribution",
    "title": "11¬† Continuous Probability Distributions",
    "section": "\n11.2 Normal (Gaussian) Distribution",
    "text": "11.2 Normal (Gaussian) Distribution\nWe now discuss the most important distribution in probability and statistics, the normal distribution or Gaussian distribution.1\nThe normal distribution, referred to as \\(N(\\mu, \\sigma^2\\)), has the probability density function given by \\[\\small f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty &lt; x &lt; \\infty,\\] where the two parameters \\(\\mu\\) and \\(\\sigma^2\\) (\\(\\sigma\\)) are the mean and variance (standard deviation) of the distribution respectively, i.e., \\(E(X) = \\mu\\), and \\(Var(X) = \\sigma^2\\) if \\(X \\sim N(\\mu, \\sigma^2)\\). The normal variable \\(X\\) lives on the entire real line. The normal density value \\(f(x)\\) is not exactly equal to zero although it is tiny for extremely large \\(x\\) in absolute value. When \\(\\mu = 0\\) and \\(\\sigma = 1\\), \\(N(0, 1)\\) is called standard normal.\nFigure¬†11.2 are examples of normal density curves and how they change with different means and standard deviations. The normal distribution is always bell-shaped and symmetric about the mean \\(\\mu\\), regardless of the value of \\(\\mu\\) and \\(\\sigma\\). The parameter \\(\\mu\\) is the location parameter that controls the ‚Äúlocation‚Äù of the distribution. The navy \\(N(100, 10^2)\\) is 80 units right of the red \\(N(20, 10^2)\\). The parameter \\(\\sigma\\) is the scale parameter that determines the variability or spreadness of the distribution. The navy \\(N(100, 10^2)\\) and the yellow \\(N(100, 15^2)\\) are at the same location, but \\(N(100, 15^2)\\) has more variation than \\(N(100, 10^2)\\). Since the total area under the density curve is always one, to account for large variation, the density curve of \\(N(100, 15^2)\\) has heavier tails and lower density values around the mean. Heavier tails means it is more probable to have extreme values like 130 or 70 that are away from the mean 100, comparing to \\(N(100, 10^2)\\) with smaller variation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11.2: Normal density curves with varying means and standard deviations",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#standardization-and-z-scores",
    "href": "prob-cont.html#standardization-and-z-scores",
    "title": "11¬† Continuous Probability Distributions",
    "section": "\n11.3 Standardization and Z-Scores",
    "text": "11.3 Standardization and Z-Scores\nStandardization is a transformation that allows us to convert any normal distribution \\(N(\\mu, \\sigma^2)\\) to \\(N(0, 1)\\), the standard normal distribution.\nWhy do we want to perform standardization? We want to put data on a standardized scale, because it helps us make comparisons more easily. Later we will see why. Let‚Äôs first see how we can do standardization.\nIf \\(x\\) is an observation from a distribution, not necessarily normal, with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the standardized value of \\(x\\) is called \\(z\\)-score: \\[z = \\frac{x - \\mu}{\\sigma}\\]\nThe \\(z\\)-score tells us how many standard deviations \\(x\\) falls away from its mean and in which direction.\n\nObservations larger than the mean have positive \\(z\\)-scores.\nObservations smaller than the mean have negative \\(z\\)-scores.\nA \\(z\\)-score -1.2 means that \\(x\\) is 1.2 standard deviations to the left of or below the mean.\nA \\(z\\)-score 1.8 means that \\(x\\) is 1.8 standard deviations to the right of or above the mean.\n\n If \\(X \\sim N(\\mu, \\sigma^2)\\), then \\(Z = \\frac{X - \\mu}{\\sigma}\\), the transformed random variable, follows the standard normal distribution \\(Z \\sim N(0, 1)\\). \n\n\n\n\n\n\nNote\n\n\n\nAny transformation of a random variable is still a random variable but with a different probability distribution.\n\n\n\n Graphical Illustration \nWhat does the standardization really do? Well it first subtracts the original variable value \\(x\\) from the mean \\(\\mu\\), then divided by its standard deviation \\(\\sigma\\).\n\nFirst, \\(X - \\mu\\) shifts the mean from \\(\\mu\\) to 0. Figure¬†11.3 illustrates this. The original distribution is \\(X \\sim N(3, 4)\\) (navy). Then the new variable \\(Y = X-\\mu\\) is \\(Y = X - 3\\) that follows \\(N(0, 4)\\) distribution (blue). \\(X - \\mu\\) means the distribution is shifted to the left 3 units, so that the new location or center becomes zero.\n\n\n\n\n\n\n\n\nFigure¬†11.3: Standardization shifts mean from 3 to 0\n\n\n\n\n\nSecond, \\(\\frac{X - \\mu}{\\sigma}\\) scales the variation from 4 to 1. \\(Y = X-3 \\sim N(0, 4).\\) Because \\(\\sigma = 2\\), \\(Z = \\frac{X - 3}{2} \\sim N(0, 1)\\). The idea is that one unit change in \\(X\\) is one unit change in \\(Y\\), but just 1/2 unit change in \\(Z\\). Through dividing by \\(\\sigma\\), the variation measured in the new scale becomes smaller, and the new variance is one. For any normal variable with an arbitrary finite value of \\(\\mu\\) and \\(\\sigma\\), the variable after standardization will always follow \\(N(0, 1)\\) (red).\n\n\n\n\n\n\n\n\nFigure¬†11.4: Standardization scales variance from 4 to 1\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf \\(\\sigma &lt; 1\\), then the variation measured in the new scale becomes larger, because the new variance is one.\n\n\nA value of \\(x\\) that is 2 standard deviation below the mean, \\(\\mu\\), corresponds to \\(z = -2\\). For any \\(\\mu\\) and \\(\\sigma\\), \\(x\\) and \\(z\\) have a one-to-one correspondence relationship: \\(z = \\frac{x  -\\mu}{\\sigma} \\iff x = \\mu + z\\sigma\\). So if \\(z = -2\\), \\(x = \\mu - 2\\sigma\\). Figure¬†11.5 depicts how the values on the x-axis change when standardization is performed.\n\n\n\n\n\n\n\nFigure¬†11.5: Standardized Normal Distribution\n\n\n\n\n\n SAT and ACT Example (OS Example 4.2) \nStandardization can help us compare the performance of students on the SAT and ACT, which both have nearly normal distributions. The table below lists the parameters for each distribution.\n\n\nMeasure\nSAT\nACT\n\n\n\nMean\n1100\n21\n\n\nSD\n200\n6\n\n\n\n\n\n\n\n\n\n\n\nSuppose Anna scored a 1300 on her SAT and Tommy scored a 24 on his ACT. We want to determine whether Anna or Tommy performed better on their respective tests.\n Standardization \nSince SAT and ACT are measured on a different scale, we are not able to compare the two scores unless we measure them using the same scale. What we do is standardization. Both SAT and ACT are normally distributed but with different mean and variance. We first transform the two distributions into the standard normal distribution, then examining Anna and Tommys‚Äô performance by checking the location of their score on the standard normal distribution.\nThe idea is that we first measure the two scores using the same scale and unit. The new transformed score in both cases is how many standard deviations the original score is away from its original mean. That is, both SAT and ACT are measured using the z-score. Then if A‚Äôs z-score is larger than B‚Äô z-score, we know that A performs better than B because A has a relatively higher score than B.\nThe z-score of Anna and Tommy is \\(z_{A} = \\frac{x_{A} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1300-1100}{200} = 1\\); \\(z_{T} = \\frac{x_{T} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24-21}{6} = 0.5\\).\n\n\n\n\n\n\n\nFigure¬†11.6: SAT and ACT distribution\n\n\n\n\nThis standardization tells us that Anna scored 1 standard deviation above the mean and Tommy scored 0.5 standard deviations above the mean. From this information, we can conclude that Anna performed better on the SAT than Tommy performed on the ACT.\nFigure¬†11.6 shows the SAT and ACT distributions. Note that the two distributions are depicted using the same density curve, as if they are measured on the same scale or standard normal distribution. Clearly we can see that Anna tends to do better with respect to everyone else than Tommy did.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#tail-areas-and-normal-percentiles",
    "href": "prob-cont.html#tail-areas-and-normal-percentiles",
    "title": "11¬† Continuous Probability Distributions",
    "section": "\n11.4 Tail Areas and Normal Percentiles",
    "text": "11.4 Tail Areas and Normal Percentiles\n Finding Tail Areas \\(P(X &lt; x)\\) \nFinding tail areas allows us to determine the percentage of cases that are above or below a certain score. Going back to the SAT and ACT example, this can help us determine the fraction of students have an SAT score below Anna‚Äôs score of 1300. This is the same as determining what percentile Anna scored at, which is the percentage of cases that had lower scores than Anna. Therefore, we are looking for \\(P(X &lt; 1300 \\mid \\mu = 1100, \\sigma = 200)\\) or \\(P(Z &lt; 1 \\mid \\mu = 0, \\sigma = 1)\\) that corresponds to the blue area size shown in Figure¬†11.7. How? We can calculate this value using R.\n\n\n\n\n\n\n\nFigure¬†11.7: Tail area for scores below 1300\n\n\n\n\n\n\nR\nPython\n\n\n\nWith mean and sd representing the mean and standard deviation of a normal distribution, we use\n\npnorm(q, mean, sd) to compute \\(P(X \\le q)\\)\npnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X &gt; q)\\)\n\n\npnorm(1, mean = 0, sd = 1)\n\n[1] 0.8413447\n\npnorm(1300, mean = 1100, sd = 200)\n\n[1] 0.8413447\n\n\n\n\nWith loc and scale representing the mean and standard deviation of a normal distribution, we use\n\nnorm.cdf(x, loc, scale) to compute \\(P(X \\le x)\\)\nnorm.sf(x, loc, scale) to compute \\(P(X &gt; x)\\)\n\n\nfrom scipy.stats import norm, binom\n\n\nnorm.cdf(x=1, loc=0, scale=1)\n\n0.8413447460685429\n\nnorm.cdf(1300, loc=1100, scale=200)\n\n0.8413447460685429\n\n\n\n\n\nNotice that the z-score 1 in standard normal is equivalent to 1300 in \\(N(1100, 200^2)\\). The shaded area represents the 84.1% of SAT test takers who had z-score below 1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Second ACT and SAT Example \nShannon is an SAT taker, and nothing else is known about her SAT aptitude. With SAT score following \\(N(1100, 200^2)\\), what is the probability Shannon SAT score is at least 1190?\n\n\n\n\n\nLet‚Äôs get the probability step by step. The first step is to figure out the probability we want to compute from the description of the question.\n\n\n Step 1: State the problem \n\n We want to compute \\(P(X \\ge 1190)\\). \n\n\n\nIf you are an expert like me, you may already know how to get the probability using R once you know what you want to compute. But if you are a beginner, I strongly recommend you drawing a normal picture, and figure out which area is your goal.\n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\nFigure¬†11.8: Tail area for scores greater than 1190\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†11.9: Method to determine right tail areas\n\n\n\n\nNote that Figure¬†11.9 reflects the fact that \\(P(X \\ge 1190) = 1 - P(X &lt; 1190).\\) The area on the right is equal to the whole area which is one minus the area on the left.\nThe next step, which is not necessary, is to find the z-score. Using z-scores help us write shorter R code to compute the wanted probability.\n\n Step 3: Find \\(z\\)-score \n\n \\(z = \\frac{1190 - 1100}{200} = 0.45\\) and we want to compute \\(\\begin{align*} P(X &gt; 1190) &= P\\left( \\frac{X - \\mu}{\\sigma} &gt; \\frac{1190 - 1000}{200} \\right) \\\\&= P(Z &gt; 0.45) = 1 - P(Z \\le 0.45) \\end{align*}\\) \nAt this point, we obtain the target probability once we get \\(P(Z \\le 0.45)\\). The last step is to use pnorm() function to get it done.\n\n\nR\nPython\n\n\n\n\n Step 4: Find the area using pnorm() \n\n\n1 - pnorm(0.45)\n\n[1] 0.3263552\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we use R pnorm() to compute normal probabilities, standardization is not a must. However, if we don‚Äôt use z-scores, we must specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma). Otherwise, R does not know which normal distribution we are considering. For example,\n\n1 - pnorm(1190, mean = 1100, sd = 200)\n\n[1] 0.3263552\n\n\nBy default, pnorm() uses the standard normal distribution assuming mean = 0 and sd = 1. So if we use z-scores to compute probabilities, we don‚Äôt need to specify the value of mean and standard deviation, and our code is shorter:\n\n1 - pnorm(0.45)\n\n[1] 0.3263552\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAny probability can be computed using the ‚Äúless than‚Äù form (lower or left tail). In the previous example, we use \\(P(X \\ge 1190) = 1 - P(X &lt; 1190)\\) expression, and we find \\(P(X &lt; 1190)\\) that has the ‚Äúless than‚Äù form.\nThis step is not necessary too, and we can directly compute \\(P(X \\ge 1190)\\) using pnorm(). However, if the calculation involves the ‚Äúgreater than‚Äù form, or we focus on upper or right tail part of the distribution, we need to add lower.tail = FALSE in pnorm(). For example,\n\npnorm(1190, mean = 1100, sd = 200, lower.tail = FALSE)\n\n[1] 0.3263552\n\n\nBy default, lower.tail = TRUE, and pnorm(q, ...) finds a probability \\(P(X &lt; q)\\), the lower tail part of the distribution.\n\n\n\n\n\n Step 4: Find the area using norm.cdf() \n\n\n1 - norm.cdf(0.45)\n\n0.32635522028791997\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhen we use Python norm.cdf() to compute normal probabilities, standardization is not a must. However, if we don‚Äôt use z-scores, we must specify the mean and SD of the original distribution of \\(X\\), like norm.cdf(x, loc= mu, scale=sigma). Otherwise, Python does not know which normal distribution we are considering. For example,\n\n1 - norm.cdf(1190, loc=1100, scale=200)\n\n0.32635522028791997\n\n\nBy default, norm.cdf() uses the standard normal distribution assuming loc=0 and scale=1. So if we use z-scores to compute probabilities, we don‚Äôt need to specify the value of mean and standard deviation, and our code is shorter:\n\n1 - norm.cdf(0.45)\n\n0.32635522028791997\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAny probability can be computed using the ‚Äúless than‚Äù form (lower or left tail). In the previous example, we use \\(P(X \\ge 1190) = 1 - P(X &lt; 1190)\\) expression, and we find \\(P(X &lt; 1190)\\) that has the ‚Äúless than‚Äù form.\nThis step is not necessary too, and we can directly compute \\(P(X \\ge 1190)\\) using norm.sf(). For example,\n\nnorm.sf(1190, loc=1100, scale=200)\n\n0.32635522028791997\n\n\n\n\n\n\n\n\n Normal Percentiles in R \nQuite often we want to know what score we need to get in order to be in the top 10% of the test takers, or the minimal score we should get to be not at the bottom 20%. To answer such questions, we need to find the percentile or quantile of the underlying distribution.\n\n\nTo get the \\(100p\\)-th percentile (or the \\(p\\) quantile denoted as \\(q\\) ) of a normal distribution, given probability \\(p\\), we use\n\n\nR\nPython\n\n\n\n\nqnorm(p, mean, sd) to get a value of \\(X\\), \\(q\\), such that \\(P(X \\le q) = p\\)\nqnorm(p, mean, sd, lower.tail = FALSE) to get \\(q\\) such that \\(P(X \\ge q) = p\\)\n\n\n\n\nnorm.ppf(p, loc, scale) to get a value of \\(X\\), \\(x\\), such that \\(P(X \\le x) = p\\)\nnorm.isf(p, loc, scale) to get \\(x\\) such that \\(P(X \\ge x) = p\\). isf is short for inverse survival function.\n\n\n\n\n SAT and ACT Example \nBack to our SAT example. What is the 95th percentile for SAT scores?\nKeep in mind that a percentile or quantile is a value of random variable \\(x\\), not a probability. When we find the quantile, its associated probability is given because the probability is the required information to obtain the quantile.\nThe first step again is to figure out what we want. If we want to find the 95th percentile, it means that we want to find the variable value \\(q\\) so that \\(P(X &lt; q) = 0.95\\). In other words, we want to find an \\(x\\) value of the normal distribution, which is greater than 95% of all other cases.\n\n\n Step 1: State the problem \n\n We want to find \\(q\\) s.t \\(P(X &lt; q) = 0.95\\). \n\n\n\nThe whole idea is shown graphically in Figure¬†11.10. We already know the percentage 95%. All we need to do is to find the value \\(q\\) so that the area left to it is 95%.\n\n Step 2: Draw a picture\n\n\n\n\n\n\n\n\nFigure¬†11.10: Picture for the 95th percentile of SAT scores\n\n\n\n\n\n\n\nLike we do in finding probabilities, we can first do the standardization for finding quantiles although it is not necessary. So we use qnorm() to find the z-score \\(z^*\\), or the value of a standard normal variable that is the 95th percentile, i.e., \\(P(Z &lt; z^*) = 0.95\\).\n\n\nR\nPython\n\n\n\n\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z &lt; z^*) = 0.95\\) using qnorm():\n\n\n(z_95 &lt;- qnorm(0.95))\n\n[1] 1.644854\n\n\nNow, since we are interested in the 95th percentile of SAT, not the z-score, we need to transform the 95th percentile of \\(N(0, 1)\\) back to the 95th percentile of \\(N(1100, 200^2)\\), the original SAT distribution.\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma = 1100+(1.645)\\times200 = 1429\\). \n\n\n\n\n(x_95 &lt;- 1100 + z_95 * 200)\n\n[1] 1428.971\n\n\nTherefore, the 95th percentile for SAT scores is 1429.\nNote that we can directly find the 95th percentile of SAT without standardization. We just need to remember to stay in the original SAT distribution by explicitly specifying mean = 1100 and sd = 200 in the qnorm() function, as we do for pnorm().\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1428.971\n\n\n\n\n\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z &lt; z^*) = 0.95\\) using norm.ppf():\n\n\nz_95 = norm.ppf(0.95)\nz_95\n\n1.6448536269514722\n\n\nNow, since we are interested in the 95th percentile of SAT, not the z-score, we need to transform the 95th percentile of \\(N(0, 1)\\) back to the 95th percentile of \\(N(1100, 200^2)\\), the original SAT distribution.\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma = 1100+(1.645)\\times200 = 1429\\). \n\n\n\n\nx_95 = 1100 + z_95 * 200\nx_95\n\n1428.9707253902943\n\n\nTherefore, the 95th percentile for SAT scores is 1429.\nNote that we can directly find the 95th percentile of SAT without standardization. We just need to remember to stay in the original SAT distribution by explicitly specifying loc=1100 and scale=200 in the norm.ppf() function, as we do for norm.cdf().\n\nnorm.ppf(0.95, loc=1100, scale=200)\n\n1428.9707253902943",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#finding-probabilties",
    "href": "prob-cont.html#finding-probabilties",
    "title": "11¬† Continuous Probability Distributions",
    "section": "\n11.5 Finding Probabilties",
    "text": "11.5 Finding Probabilties\n\n\nR\nPython\n\n\n\nüëâ To find a probability, if you are a beginner, it is always good to draw and label the normal curve and shade the area of interest. Below is a summary of how we can use pnorm() to compute various kinds of probabilities.\n\nüëâ Less than\n\n\\(\\small P(X &lt; x) = P(Z &lt; z)\\)\npnorm(z)\npnorm(x, mean = mu, sd = sigma)\n\n\nüëâ Greater than\n\n\\(\\small P(X &gt; x) = P(Z &gt; z) = 1 - P(Z \\le z)\\)\n1 - pnorm(z)\n1 - pnorm(x, mean = mu, sd = sigma)\npnorm(x, mean = mu, sd = sigma, lower.tail = FALSE)\n\n\n\n\n\nüëâ Between two numbers\n\n\\(\\small P(a &lt; X &lt; b) = P(z_a &lt; Z &lt; z_b) = P(Z &lt; z_b) - P(Z &lt; z_a)\\)\npnorm(z_b) - pnorm(z_a)\npnorm(b, mean = mu, sd = sigma) - pnorm(a, mean = mu, sd = sigma)\n\n\n\nüëâ Outside of two numbers \\((a &lt; b)\\) \\[\\small \\begin{align} P(X &lt; a \\text{ or } X &gt; b) &= P(Z &lt; z_a \\text{ or } Z &gt; z_b) \\\\ &= P(Z &lt; z_a) + P(Z &gt; z_b) \\\\ &= P(Z &lt; z_a) + 1 - P(Z &lt; z_b) \\end{align}\\]\n\npnorm(z_a) + pnorm(z_b, lower.tail = FALSE)\npnorm(z_a) + 1 - pnorm(z_b)\npnorm(a, mean = mu, sd = sigma) + pnorm(b, mean = mu, sd = sigma, lower.tail = FALSE)\npnorm(a, mean = mu, sd = sigma) + 1 - pnorm(b, mean = mu, sd = sigma)\n\n\n\n\n\n\nüëâ To find a probability, if you are a beginner, it is always good to draw and label the normal curve and shade the area of interest. Below is a summary of how we can use norm.cdf() and norm.sf() to compute various kinds of probabilities.\n\nüëâ Less than\n\n\\(\\small P(X &lt; x) = P(Z &lt; z)\\)\nnorm.cdf(z)\nnorm.cdf(x, loc=mu, scale=sigma)\n\n\nüëâ Greater than\n\n\\(\\small P(X &gt; x) = P(Z &gt; z) = 1 - P(Z \\le z)\\)\n1 - norm.cdf(z)\n1 - norm.cdf(x, loc=mu, scale=sigma)\nnorm.sf(x, loc=mu, scale=sigma)\n\n\n\n\n\nüëâ Between two numbers\n\n\\(\\small P(a &lt; X &lt; b) = P(z_a &lt; Z &lt; z_b) = P(Z &lt; z_b) - P(Z &lt; z_a)\\)\nnorm.cdf(z_b) - pnorm.cdf(z_a)\nnorm.cdf(b, loc=mu, scale=sigma) - norm.cdf(a, loc=mu, scale=sigma)\n\n\n\nüëâ Outside of two numbers \\((a &lt; b)\\) \\[\\small \\begin{align} P(X &lt; a \\text{ or } X &gt; b) &= P(Z &lt; z_a \\text{ or } Z &gt; z_b) \\\\ &= P(Z &lt; z_a) + P(Z &gt; z_b) \\\\ &= P(Z &lt; z_a) + 1 - P(Z &lt; z_b) \\end{align}\\]\n\nnorm.cdf(z_a) + norm.sf(z_b)\nnorm.cdf(z_a) + 1 - norm.cdf(z_b)\nnorm.cdf(a, loc=mu, scale=sigma) + norm.sf(b, loc=mu, scale=sigma)\nnorm.cdf(a, loc=mu, scale=sigma) + 1 - norm.cdf(b, loc=mu, scale=sigma)",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#checking-normality",
    "href": "prob-cont.html#checking-normality",
    "title": "11¬† Continuous Probability Distributions",
    "section": "\n11.6 Checking Normality",
    "text": "11.6 Checking Normality\n\n11.6.1 Normal quantile plot\nIf we use a statistical method with its assumption being violated, the analysis results and conclusion made by the method will be worthless. Many statistical methods assume variables are normally distributed. Therefore, testing the appropriateness of the normal assumption is a key step.\nWe can check this normality assumption using a so-called normal quantile plot (normal probability plot) or a Quantile-Quantile plot (QQ plot).\nThe construction of the QQ plot is technical, and we don‚Äôt need to dig into that at this moment. The bottom line is if the data are (nearly) normally distributed, the points on the QQ plot will lie close to a straight line.\nIf the data are right-skewed, the points on the QQ plot will be convex-shaped. If the data are left-skewed, the points on the QQ plot will be concave-shaped.\n\n\n\n\n\n\n\nR\nPython\n\n\n\nTo generate a QQ-plot for checking normality in R, we can use qqnorm() and qqline(), where the first argument in the functions is the sample data we would like to check. Figure¬†11.11 shows QQ plots at the bottom for normal and right-skewed data samples. Since the data normal_sample actually come from a normal distribution, its histogram looks like normal, and its QQ plot look like a perfect straight line. On the other hand, on the right hand side we have a right skewed data set right_skewed_sample. Clearly, its QQ plot is a upward curve, and definitely not linear, indicating that the sample data are not normally distributed.\n\nqqnorm(normal_sample, main = \"Normal data\", col = 4)\nqqline(normal_sample)\nqqnorm(right_skewed_sample, main = \"Right-skewed data\", col = 6)\nqqline(right_skewed_sample)\n\n\n\n\n\n\nFigure¬†11.11: QQ plots for normal and right-skewed data samples\n\n\n\n\n\n\nTo generate a QQ-plot for checking normality in R, we can use stats.probplot(), where the first argument in the functions is the sample data we would like to check. Figure¬†11.12 shows QQ plots at the bottom for normal and right-skewed data samples. Since the data normal_sample actually come from a normal distribution, its histogram looks like normal, and its QQ plot look like a perfect straight line. On the other hand, on the right hand side we have a right skewed data set right_skewed_sample. Clearly, its QQ plot is a upward curve, and definitely not linear, indicating that the sample data are not normally distributed.\n\nimport numpy as np\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n\n# First plot: Histogram of normal data\nnor_hist = axs[0, 0].hist(normal_sample, bins=20, color='blue', edgecolor='white')\naxs[0, 0].set_title(\"Normal data\")\naxs[0, 0].set_xlabel(\"Value\")\naxs[0, 0].set_ylabel(\"Frequency\")\n\n# Second plot: QQ plot for normal data\nnor_qq = stats.probplot(normal_sample, dist=\"norm\", plot=axs[1, 0])\naxs[1, 0].set_title(\"Normal data (QQ Plot)\")\n\n# Third plot: Histogram of right-skewed data\nskew_hist = axs[0, 1].hist(right_skewed_sample, bins=20, color='magenta', edgecolor='white');\naxs[0, 1].set_title(\"Right-skewed data\")\naxs[0, 1].set_xlabel(\"Value\")\naxs[0, 1].set_ylabel(\"Frequency\")\n\n# Fourth plot: QQ plot for right-skewed data\nskew_qq = stats.probplot(right_skewed_sample, dist=\"norm\", plot=axs[1, 1])\naxs[1, 1].get_lines()[0].set_markerfacecolor('magenta')\naxs[1, 1].set_title(\"Right-skewed data (QQ Plot)\")\n\n# Display the plots\nplt.show()\n\n\n\n\n\n\n\n\nFigure¬†11.12: QQ plots for normal and right-skewed data samples\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe code involves doing subplots in one single figure, and it makes the code a bit complex and long. No need to worry that much.\n\n\n\n\n\n\n11.6.2 Normality test\nIf visualization is not enough for you to tell whether the data are far from normally distributed, we can use some formal procedures to make such conclusion. Since the methods are about hypothesis testing which will be first introduced in Chapter 16, we will take about normality test in later chapters after we learn hypothesis testing.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#normal-approximation-to-binomial-distribution",
    "href": "prob-cont.html#normal-approximation-to-binomial-distribution",
    "title": "11¬† Continuous Probability Distributions",
    "section": "\n11.7 Normal Approximation to Binomial Distribution*",
    "text": "11.7 Normal Approximation to Binomial Distribution*\nIn Chapter 10, we learn that a binomial distribution can be approximated by a Poisson distribution. In this section we learn that a binomial distribution can also be approximated by a normal distribution. How good the approximation is again depends on the size of \\(n\\) and \\(\\pi\\). Centuries ago, with no computing technology, calculating binomial probabilities is a pain in the neck, especially when \\(n\\) is large. This motivates mathematicians to find another way to calculate the probabilities, or at least approximate them well. Not only binomial distributions, many other distributions are related to the normal distribution. This will be discussed in a probability theory course.\nTo determine a normal distribution, we need parameters \\(\\mu\\) and \\(\\sigma\\). We know that if \\(X \\sim Binomial(n, \\pi)\\), then the mean and standard deviation of \\(X\\) are \\[ \\begin{align}\n\\mu &= n\\pi,\\\\\n\\sigma &= \\sqrt{n\\pi(1-\\pi)}.\n\\end{align}\\]\nWhen \\(n\\) is large, we can approximate \\(X\\) with a normal distribution \\(N\\left(\\mu = n\\pi, \\sigma^2 = n\\pi(1-\\pi)\\right)\\). When this normal approximation does a pretty good job? Usually, the normal approximation performs well when \\(n\\) is so large that \\(n\\pi \\ge 5\\) and \\(n(1-\\pi) \\ge 5\\). As you can see, when \\(\\pi\\) is near 0 or 1, \\(n\\) needs to be much larger to satisfy the two conditions. The intuition is that normal distributions are symmetric, but binomial distributions are generally asymmetric. The more \\(\\pi\\) is close to 0 or 1, the more asymmetric the binomial distribution is, unless \\(n\\) gets larger.\nCan you see any potential issue of using normal to approximate binomial? In fact, we are using a continuous normal distribution to approximate a discrete binomial distribution. In order to have a good approximation, especially when \\(n\\) is not that large, we need something called continuity correction.\nContinuity correction is made to transform discrete binomial values \\(0, 1, 2, \\dots, n\\) to a continuous interval from 0 to \\(n\\) by adding and subtracting 0.5 from the whole number \\(0, 1, 2, \\dots, n\\).\n\n\n\n\n\n\n\nFigure¬†11.13\n\n\n\n\nBy doing so, \\(P_{Binomial}(X = k) \\approx\\) integral of normal from \\(k-0.5\\) to \\(k+0.5\\). The idea is that in normal distribution, we treat any value between 0.5 and 1.5 as an integer 1 of the value of the discrete binomial distribution. The followings are some examples of continuity correction.\n\n\\(P_{Binomial}(X \\ge 1) \\approx P_{Normal}(X &gt; 0.5)\\).\n\\(P_{Binomial}(X &gt; 1) \\approx P_{Normal}(X &gt; 1.5)\\)\n\\(P_{Binomial}(X \\le 4) \\approx P_{Normal}(X &lt; 4.5)\\)\n\\(P_{Binomial}(X &lt; 4) \\approx P_{Normal}(X &lt; 3.5)\\)\n\\(P_{Binomial}(X = 4) \\approx P_{Normal}(3.5 &lt; X &lt; 4.5)\\)\n\nFigure¬†11.13 illustrate that \\(P_{Binomial}(X = 4) \\approx P_{Normal}(3.5 &lt; X &lt; 4.5)\\). The height of the black bar at \\(x = 4\\) is 0.187 showing \\(P_{Binomial}(X = 4)\\) with \\(X \\sim binomial(15, 0.2)\\). This can be approximated by the integral of the normal density curve between 3.5 and 4.5, the red shaded area, which is \\(P_{Normal}(3.5 &lt; X &lt; 4.5)\\) where \\(X \\sim N(15(0.2), 15(0.2)(0.8))\\).\n\n Example of Normal Approximation (4.26 of OTT) \nA large drug company has 100 potential new prescription drugs under clinical test. About 20% of all drugs that reach this stage are eventually licensed for sale. What is the probability that at least 15 of the 100 drugs are eventually licensed?\n\n\nR\nPython\n\n\n\n\nn &lt;- 100  # number of trials\np &lt;- 0.2  # probability of being licensed for sale\n\n## 1. Exact Binomial Probability P(X &gt;= 15) = 1 - P(X &lt; 14)\n1 - pbinom(q = 14, size = n, prob = p)\n\n[1] 0.9195563\n\n## 2. Normal approximation with Continuity Correction \n## P(X &gt;= 14.5) = 1 - P(X &lt; 14.5)\n1 - pnorm(q = 14.5, mean = n * p, sd = sqrt(n * p * (1 - p)))\n\n[1] 0.9154343\n\n## 3. Normal approximation with NO Continuity Correction \n## P(X &gt;= 15) = 1 - P(X &lt; 15)\n1 - pnorm(q = 15, mean = n * p, sd = sqrt(n * p * (1 - p)))\n\n[1] 0.8943502\n\n\n\n\n\nn = 100  # number of trials\np = 0.2  # probability of success\n\nfrom scipy.stats import norm, binom\n\n# 1. Exact Binomial Probability P(X &gt;= 15) = 1 - P(X &lt; 15)\n1 - binom.cdf(14, n, p)\n\n0.9195562788619489\n\n# 2. Normal approximation with continuity correction\n1 - norm.cdf(14.5, loc=n * p, scale=np.sqrt(n * p * (1 - p)))\n\n0.9154342776486644\n\n# 3. Normal approximation without continuity correction\n1 - norm.cdf(15, loc=n * p, scale=np.sqrt(n * p * (1 - p)))\n\n0.8943502263331446",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#exercises",
    "href": "prob-cont.html#exercises",
    "title": "11¬† Continuous Probability Distributions",
    "section": "\n11.8 Exercises",
    "text": "11.8 Exercises\n\nWhat percentage of data that follow a standard normal distribution \\(N(\\mu=0, \\sigma=1)\\) is found in each region? Drawing a normal graph may help.\n\n\\(Z &lt; -1.75\\)\n\\(-0.7 &lt; Z &lt; 1.3\\)\n\\(|Z| &gt; 1\\)\n\n\nThe average daily high temperature in June in Chicago is 74\\(^{\\circ}\\)F with a standard deviation of 4\\(^{\\circ}\\)F. Suppose that the temperatures in June closely follows a normal distribution.\n\nWhat is the probability of observing an 81\\(^{\\circ}\\) F temperature or higher in Chicago during a randomly chosen day in June?\nHow cool are the coldest 15% of the days (days with lowest average high temperature) during June in Chicago?\n\n\nHead lengths of Virginia opossums follow a normal distribution with mean 104 mm and standard deviation 6 mm.\n\nCompute the \\(z\\)-scores for opossums with head lengths of 97 mm and 108 mm.\nWhich observation (97 mm or 108 mm) is more unusual or less likely to happen than another observation? Why?",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-cont.html#footnotes",
    "href": "prob-cont.html#footnotes",
    "title": "11¬† Continuous Probability Distributions",
    "section": "",
    "text": "Personally I prefer call it Gaussian to normal distribution. Every distribution is unique and has its own properties. Why the Gaussian distribution is normal?‚Ü©Ô∏é",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>11</span>¬† <span class='chapter-title'>Continuous Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html",
    "href": "prob-sampling.html",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "",
    "text": "12.1 Monte Carlo Simulation\nFrom Wikipedia, there is no consensus on how Monte Carlo should be defined. In short, Monte Carlo simulation is a repeated random sampling method. Therefore, when using Monte Carlo method, we do not just simulate one single value, but generate numerous realized values of random variables. Also these values are generated with randomness.\nMonte Carlo methods can be used to approximate the probability of some event happening. Take categorical data for example, once we collect the Monte Carlo samples, we can calculate the relative frequency of each category in the data, and use their relative frequency as the probability that a specific category happens when a sample is drawn from some target distribution.\nSuppose there are 2 red balls and 3 blue ones in a bag. Apparently, if each ball is equally likely to be chosen (picking at random), and if we Do know in advance that 2 reds and 3 blues are in the bag, then the probability that a red ball is drawn is 2/5. The question is\nThe idea of Monte Carlo simulation is that we repeat the experiment (drawing a ball) a large number of times, then obtain the relative frequency of red ball to approximate the probability of getting a red ball.\nLet me first create a vector bag_balls that saves the information about the color of ball in the bag. It‚Äôs there, but let‚Äôs just don‚Äôt see it at this moment.\nIt produces one random outcome. This time we get a red ball. We can (put the red ball back in to the bag) draw a ball again by running the code:\nSo if there are 5 balls in the bag, how many red balls in the bag? Well I would guess there are 2 red balls and 3 blue balls in the bag because the chance of getting a red ball is about 40%. We just demo a Monte Carlo simulation for categorical data.\nIt is important to know that Monte Carlo works better when the number of repetitions is large. When we repeatedly sample the value only few times, we may unluckily not be able see the entire picture of the target distribution, and get stuck in some part of the distribution. As a result, the Monte Carlo method cannot faithfully show the whole picture, leading to a biased approximation. In our example, notice that the first 6 random draws are all blue. If we just stop there, we may make a conclusion that the balls in the bag only have one color blue because we ‚Äúnever‚Äù see any other colors in our experiment. In this case, 6 samples are too small, and the ‚Äútrue color‚Äù has not shown. Again, generally it needs sufficiently large Monte Carlo samples to uncover the target distribution. When we repeatedly draw a ball 10,000 times, we have\nNote that the relative frequency of red ball is now so close to the true probability 0.4. Now we are more sure that exactly two red balls in the bag!",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html#monte-carlo-simulation",
    "href": "prob-sampling.html#monte-carlo-simulation",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "",
    "text": "Without peeking at the bag, how do we approximate the probability of getting a red/blue ball?\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nIn R, to randomly sample an element of a vector, we can use the function sample(). For example, the following code shows how to randomly draw a ball from that bag:\n\nsample(x = bag_balls, size = 1)\n# [1] \"red\"\n\n\n\nIn Python, to randomly sample an element of a vector, we can use the function np.random.choice(). For example, the following code shows how to randomly draw a ball from that bag:\n\nnp.random.choice(bag_balls, size=1)\n# array(['red'], dtype='&lt;U4')\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nsample(x = bag_balls, size = 1)\n# [1] \"blue\"\n\nThis time we get a blue ball. To do a Monte Carlo simulation, we can use the replicate() function, which allows us to repeat the same job a number of times\n\nmc_sim &lt;- replicate(n = 100, expr = sample(bag_balls, 1))\n\nThe argument n is an integer indicating the number of replications. The argument expr is the expression (usually a function call) to evaluate repeatedly. Here I repeatedly draw a ball 100 times. The object mc_sim contains 100 colors collected from the bag of colored balls.\n\nstr(mc_sim)\n#  chr [1:100] \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"red\" \"blue\" \"red\" ...\n\nHow do we get the relative frequency of category? We can first check the frequency table or frequency distribution using table() function:\n\n# mc_sim\n# blue  red \n#   58   42\n\nTo get the relative frequency, just divided by the total number of repetitions.\n\n# mc_sim\n# blue  red \n# 0.58 0.42\n\n\n\n\nnp.random.choice(bag_balls, size=1)\n# array(['blue'], dtype='&lt;U4')\n\nThis time we get a blue ball. To do a Monte Carlo simulation, we repeatedly draw one ball from the bag using a for loop within a list, so that the every draw is saved in a list.\nIn the code, range(100) creates a sequence of 100 integers from 0 to 99. Then for each integer, we call it i, we do the sampling np.random.choice(bag_balls). Every sampling result is wrapped up in a list [].\n\nmc_sim = [np.random.choice(bag_balls) for i in range(100)]\nmc_sim[0:10]\n# ['blue', 'red', 'blue', 'blue', 'red', 'red', 'red', 'red', 'red', 'blue']\n\nHow do we get the relative frequency of category? We can first turn a list into a pd.Series so that we can use the method value_count() to generate the frequency table or frequency distribution:\n\nimport pandas as pd\nfreq_table = pd.Series(mc_sim).value_counts()\n\nTo get the relative frequency, just divided by the total number of repetitions.\n\nfreq_table / 100\n# blue    0.63\n# red     0.37\n# Name: count, dtype: float64\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nmc_sim &lt;- replicate(n = 10000, expr = sample(bag_balls, 1))\ntable(mc_sim)/10000\n# mc_sim\n#  blue   red \n# 0.603 0.397\n\n\n\n\nmc_sim = [np.random.choice(bag_balls) for i in range(10000)]\npd.Series(mc_sim).value_counts() / 10000\n# blue    0.5962\n# red     0.4038\n# Name: count, dtype: float64",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html#random-number-generation",
    "href": "prob-sampling.html#random-number-generation",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "\n12.2 Random Number Generation",
    "text": "12.2 Random Number Generation\nWhen we do sampling using a computer, we use a so-called random number generator to generate a random outcome, and sampling results vary from sample to sample. This is why this time you get red, next time you get blue, and you don‚Äôt know you will get a blue or red next time you do the random sampling.\nInterestingly, a computer cannot generate a pure random outcome, and a pseudorandom number generator is usually used to generate a sequence of numbers whose properties approximate the properties of sequences of truly random numbers. Because of this, we can actually control random generation results using a random seed. For example, if you set the seed at 2024, then every time you do random sampling, you always get the same sampling result.\n\n\nR\nPython\n\n\n\nTo ensure the results are exactly the same every time you do sampling, in R we set the random seed to a specific number using the function set.seed().\n\nset.seed(2024)\nsample(x = bag_balls, size = 3)\n# [1] \"red\"  \"red\"  \"blue\"\n\n\n## same result!\nset.seed(2024)\nsample(x = bag_balls, size = 3)\n# [1] \"red\"  \"red\"  \"blue\"\n\nHere we draw three balls at random. Because the two random seeds are identical, the two ‚Äúrandom‚Äù sampling results are identical too.\nThe sample() function can actually draw lots of balls. But be careful about the with and without replacement issue. The default setting is draw without replacement. So when we try to draw 6 balls at once, it renders an error, saying cannot take a sample larger than the population when replace = FALSE.\n\nsample(bag_balls, 6)\n# Error in sample.int(length(x), size, replace, prob): \n# cannot take a sample larger than the population when 'replace = FALSE'\n\nBasically we only 5 balls, without replacement, once a ball is drawn, the ball is out of bag, and only 4 balls remain in the bag. So without replacement, we can at most draw 5 balls, which is the ‚Äúentire population‚Äù. If we wanna draw more than 5 times from the same bag, we got to specify replace = TRUE in the sample() function.\n\nsample(bag_balls, 6, replace = TRUE)\n# [1] \"blue\" \"red\"  \"blue\" \"red\"  \"red\"  \"red\"\n\nW can actually do the Monte Carlo simulation using just the sample() function like sample(balls, size = B, replace = TRUE). This means that we return the ball back to the bag after drawing it, and we repeatedly draw a ball continually B times basically under the same conditions.\n\nmc_sim_rep &lt;- sample(x = bag_balls, size = 100, replace = TRUE)\ntable(mc_sim_rep) / 100\n# mc_sim_rep\n# blue  red \n# 0.59 0.41\n\nNot surprisingly, we get results very similar to those previously obtained using the replicate() function. Moreover, if we use the same random seed on sample() and replicate(), their sampling results will be the same:\n\nset.seed(2025)\nmc1 &lt;- replicate(n = 100, expr = sample(bag_balls, 1))\ntable(mc1)/100\n# mc1\n# blue  red \n# 0.62 0.38\nset.seed(2025)\nmc2 &lt;- sample(x = bag_balls, size = 100, replace = TRUE)\ntable(mc2) / 100\n# mc2\n# blue  red \n# 0.62 0.38\n\n\n\nTo ensure the results are exactly the same every time you do sampling, in R we set the random seed to a specific number using the function np.random.seed().\nNote that by default np.random.choice() does sampling with replacement. We need to set replace=False.\n\n# Sampling three balls without replacement\nnp.random.seed(2024)\nnp.random.choice(bag_balls, size=3, replace=False)\n# array(['red', 'blue', 'blue'], dtype='&lt;U4')\n\n\n## same result!\nnp.random.seed(2024)\nnp.random.choice(bag_balls, size=3, replace=False)\n# array(['red', 'blue', 'blue'], dtype='&lt;U4')\n\nHere we draw three balls at random. Because the two random seeds are identical, the two ‚Äúrandom‚Äù sampling results are identical too.\nThe np.random.choice() function can actually draw lots of balls. We only 5 balls, without replacement, once a ball is drawn, the ball is out of bag, and only 4 balls remain in the bag. So without replacement, we can at most draw 5 balls, which is the ‚Äúentire population‚Äù. If we wanna draw more than 5 times from the same bag, we got to specify replace should be True (by default) in the np.random.choice() function.\n\n# Sampling six balls with replacement\nnp.random.choice(bag_balls, size=6, replace=True)\n# array(['blue', 'blue', 'red', 'red', 'blue', 'blue'], dtype='&lt;U4')\n\n\n# Another Monte Carlo simulation with replacement\nnp.random.seed(1000)\nmc_sim_rep = np.random.choice(bag_balls, size=100, replace=True)\npd.Series(mc_sim_rep).value_counts()\n# blue    63\n# red     37\n# Name: count, dtype: int64\n\nNot surprisingly, we get results very similar to those previously obtained using the for loop within a list. Moreover, if we use the same random seed on np.random.choice(), their sampling results will be the same:\n\n# Compare two methods\nnp.random.seed(2025)\nmc1 = [np.random.choice(bag_balls) for i in range(100)]\npd.Series(mc1).value_counts()\n# blue    58\n# red     42\n# Name: count, dtype: int64\n\nnp.random.seed(2025)\nmc2 = np.random.choice(bag_balls, size=100, replace=True)\npd.Series(mc2).value_counts()\n# blue    58\n# red     42\n# Name: count, dtype: int64",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html#random-number-generation-from-probability-distributions",
    "href": "prob-sampling.html#random-number-generation-from-probability-distributions",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "\n12.3 Random Number Generation from Probability Distributions",
    "text": "12.3 Random Number Generation from Probability Distributions\n\n\nR\nPython\n\n\n\nRemember from the previous two chapters that we learn the dpqr family of functions for calculating probabilities, densities, and generating values from a probability distribution. To generate random values, we use rdist(n, ...). \\(n\\) is the number of observations we would like to generate. The ... denotes the required parameter values for the distribution being drawn from. For example,\n\nrbinom(n, size, prob) generates \\(n\\) observations from the binomial distribution with the number of trials specified in size and the probability of success specified in prob.\nrpois(n, lambda) generates \\(n\\) observations from the Poisson distribution with the parameter lambda.\nrnorm(n, mean = 0, sd = 1) generates \\(n\\) observations from the normal distribution with mean and standard deviation specified in mean and sd. By default, the mean is zero, and the variance is one. So rnorm(5) draws five numbers from the standard normal distribution.\n\n\n## the default mean = 0 and sd = 1 (standard normal)\nrnorm(5)\n# [1]  0.983 -0.409 -0.669  1.319 -1.085\n\n\n\nTo generate random values, in Python we can use dist.rvs(..., size) from the scipy.stats module. size is the number of observations we would like to generate. The ... denotes the required parameter values for the distribution being drawn from. For example,\n\n\nnorm.rvs(loc, scale, size) generates size observations from the normal distribution with mean and standard deviation specified in loc and scale. By default, the mean is zero, and the variance is one. So norm.rvs(size=5) draws five numbers from the standard normal distribution. Note that the word size cannot be skipped because it is the third argument. If we write norm.rvs(5), the function will draw a number from a normal distribution with mean 5 and variance 1 \\(N(5, 1)\\).\n\n\nfrom scipy.stats import norm\nnorm.rvs(size=5)\n# array([-1.20602816,  1.30241414, -0.5761093 ,  1.07786143, -0.64792117])\n\n\n\n\n\n\n\nNote\n\n\n\nNumPy offers np.random.randn() to sample from the standard normal distribution.\n\nnp.random.randn(5)\n# array([-0.06048719, -1.09012056,  1.47339158, -1.9039995 ,  1.81507773])\n\n\n\n\n\n\nAll the 100 red points below are random draws from the standard normal distribution. You can see that most of the points are around the mean because the density around the mean is higher. Also, we can see when we draw normal samples, it is very difficult to get a sample with a very extreme value because its corresponding density value is quite small. Therefore, we tend to underestimate the population variance if we use the sample data to estimate it. And that‚Äôs one of the reason why we divided by \\(n - 1\\) in the sample variance formula to sort of correct this underestimation.\n\n\n\n\n\n\n\n\nIn statistics, we hope the sampled data to be as representative of population as possible. Suppose the population is normal. If the sample is a random sample, we hope the sample size to be large. The larger the sample size, the more representative of population the sample is.\nWhen the sample size is just 20, you can see the sample data does not look very normal. When \\(n = 200\\), the sample start looking like a normal distribution. When \\(n = 5000\\), the sample is very representative of its population which follows standard normal distribution.\n\n\nn = 20\nn = 200\nn = 5000",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html#why-use-sampling-distribution",
    "href": "prob-sampling.html#why-use-sampling-distribution",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "\n12.4 Why Use Sampling Distribution",
    "text": "12.4 Why Use Sampling Distribution\nWhen we do statistical inference, we assume each data point in the sample is drawn from the target population whose characteristic we focus is assumed to follow some probability distribution that is unknown to us. For example, suppose we would like to do inference about Marquette students‚Äô mean height. We assume the student height follows a normal distribution \\(N(\\mu, \\sigma^2)\\) with \\(\\mu\\) being unknown to us (\\(\\mu\\) is what we would like to infer for). We collect our sample data, and assume each data point comes from the assumed distribution \\(N(\\mu, \\sigma^2)\\).\nThe sampling distribution is an important concept that connects probability and statistics together. We use the sampling distribution quite often, at least at the introductory level, to do statistical inference, and that‚Äôs why we need to learn what it is before doing statistical inference.\n Parameter \nParameters in a probability distribution are the values describing the entire distribution. For example,\n\n Binomial: two parameters, \\(n\\) and \\(\\pi\\) \n Poisson: one parameter, \\(\\lambda\\) \n Normal: two parameters, \\(\\mu\\) and \\(\\sigma\\) \n\nAs long as we know the values of the parameters of some distribution, we are able to calculate any probability of the distribution, and describe the distribution exactly. The entire distribution is controlled solely by the few parameters.\nIn statistics, we usually assume our target population follows some distribution, but its parameters are unknown to us. For example, we may assume human weight follows \\(N(\\mu, \\sigma^2)\\) although we are not quite sure what its mean and/or variance is. We may think the number of snowstorms in one year in the US follows \\(Poisson(\\lambda)\\) although we have no idea of the mean number of occurrences.\n\n\n\n Human weight \\(\\sim N(\\mu, \\sigma^2)\\) \n\n\n\n\n\n\n\n\n\n\n\n # of snowstorms \\(\\sim Poisson(\\lambda)\\) \n\n\n\n\n\n\n\n\n\n\n\n\n\n Treat Each Data Point as a Random Variable \n\nA statistical data analysis more or less involves some probability. How do we bring probability into the analysis? How is the data related to probability? Here we are going to learn some insight about it. Suppose in order to do a data analysis and inference about some population characteristic, the population mean for example, we collect a sample data of size \\(n\\), a data set having \\(n\\) data points or values.\nHere is how the probability comes into play. First, we assume the target population follows some probability distribution, say \\(N(\\mu, \\sigma^2)\\). Then we treat each data point as a random variable whose realized value, the value shown in our collected data set, is drawn from the population distribution.\nTake Marquette students weight for example. Suppose the weight follows \\(N(\\mu, \\sigma^2)\\). Suppose we decide to collect ten data points, so \\(n = 10\\). Now before we actually collect the data, the ten data points are all random variables that follow \\(N(\\mu, \\sigma^2)\\). If we write the ten variables as \\(X_1\\), \\(X_2, \\dots, X_{10}\\), then we have \\(X_i \\sim N(\\mu, \\sigma^2), i = 1, 2, \\dots, 10.\\) Notice that \\(X_1, X_2, \\dots, X_{10}\\) all follow the same distribution because they all come from the same population. Now, after we collect our data, we have the realized value of those ten random variables. For example, out data set may look like\n\n# 134 110 177 183 144 150 95 200 145 189\n\nSo the realized value of \\(X_1\\) is 134, the realized value of \\(X_2\\) is 110, and so on. Each data value is drawn from the population.\n\n\n\n\n\n\n\n\n\n\nFigure¬†12.1: Illustration of sampling from a population\n\n\n\n\nIn statistics, we usually assume that \\(X_1, X_2, \\dots, X_n\\) are independent, meaning that the value of \\(X_i\\) is not affected by any other \\(X_j, j\\ne i\\). With the same distribution, \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed , or iid, denoted as\n\n \\(X_1, X_2, \\dots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2).\\) \n\nThen we call such sample data \\((X_1, X_2, \\dots, X_n)\\) a random sample of size \\(n\\) from the population.\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBefore we actually collect the data, the data \\(X_1, X_2, \\dots, X_n\\) are random variables from the population distribution \\(N(\\mu, \\sigma^2)\\).\nOnce we collect the data, we know the realized value of these random variables: \\(x_1, x_2, \\dots, x_n\\).",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html#sampling-distribution",
    "href": "prob-sampling.html#sampling-distribution",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "\n12.5 Sampling Distribution",
    "text": "12.5 Sampling Distribution\nAny value computed from a sample \\((X_1, X_2, \\dots, X_n)\\) is called a (sample) statistic.\n\n The sample mean \\(\\frac{1}{n}\\sum_{i=1}^n X_i\\) is a statistic. \n Sample variance \\(\\frac{\\sum_{i=1}^n \\left(X_i -             \\overline{X}\\right)^2}{n-1}\\) is also a statistic. \n\nSince \\(X_1, X_2, \\dots, X_n\\) are random variables, any transformation or function of \\((X_1, X_2, \\dots, X_n)\\) or its statistics is also a random variable. The probability distribution of a statistic is called the sampling distribution of that statistic. It is the probability distribution of that statistic if we were to repeatedly draw samples of the same size from the population.\n\n\n\n\n\n\nDoes the sample mean \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) have a sampling distribution?\n\n\n\nYes, because sample mean is a statistic, and it is a random variable! Keep in mind that before we get the realized values of data, data points are random variables. Because \\(X_1, X_2, \\dots, X_n\\) are random, \\(\\frac{1}{n}\\sum_{i=1}^n X_i\\) is random too. The reason why we can calculate sample mean and same variance in Chapter 6 is because we get the realized values of data. They are actual numbers, so we can do calculations.\n\n\n\n Sampling Distribution of Sample Mean \nSince \\(X_1, X_2, \\dots, X_n\\) are random, if we repeat the sampling couple of times, every time we will be getting different realized values.\nThe following table shows 5 replicates of sample data of size 10. The first data set has 10 realized values 117 169 111 190  98  94 127 105  93 187. If we were to collect another data set, the first realized value of \\(X_1\\) could be any other number from the population, not necessarily to be 117 because again \\(X_1\\) is a random variable. In our example \\(x_1 = 192\\) in the second data set. The idea applies to \\(X_2\\) to \\(X_{10}\\). Now because every time we collect a new data set we get different realized values \\(x_1, x_2, \\dots, x_{10}\\), the realized value of sample mean will vary from sample to sample as well. The first data set gets the sample mean 129.1, the second one 153.2, and so on. This shows why the sample mean is random by nature, and its value varies according to a distribution that is its sampling distribution.\n\n#            x1  x2  x3  x4  x5  x6  x7  x8  x9 x10 mean\n# Dataset-1 117 169 111 190  98  94 127 105  93 187  129\n# Dataset-2 192 175 179 159 168 167 103 145 151  93  153\n# Dataset-3  93 110 129 173 145 156 189 184 182  94  146\n# Dataset-4 155 136 129 173 137  92 176 130 189 161  148\n# Dataset-5 121 131 132  91 168 143 138 191 145 140  140\n\nFigure¬†12.2 illustrates how we collect the sample of sample means that represents its sampling distribution. In short, if we were able to collect lots of samples of size \\(n\\), and get the sample mean for each sample of size \\(n\\), the histogram of those sample means gives us a pretty good understanding of what the sampling distribution of the sample mean looks like.\n\n\n\n\n\n\n\nFigure¬†12.2: Sampling distribution of sample means (Biostatistics for the Biological and Health Sciences p.241)\n\n\n\n\nThe following histogram shows the sampling distribution of the sample mean for the sample of size 10 when the sampling are repeated 1000 times. That is, we have 1000 \\(\\overline{x}\\)s, each being computed from \\(\\frac{1}{10}\\sum_{i=1}^{10}x_{i}\\).\n\n\n\n\n\n\n\n\nThe concept is a little abstract, and you may need time to digest it. The applet Sampling Distribution Applet provides animation of how the sampling distribution is formed. I highly recommend that you play with it, and figure out the entire building process.\n\n\n\n\n\n\nWhat are the differences between the sampling distribution of \\(\\overline{X}\\) and the population distribution each individual random variable, \\(X_i\\), is drawn from?\n\n\n\n\n\n\nThis is an important question. So far we know each data point or random variable \\(X_i, i = 1, \\dots, n\\) is drawn from the population distribution. The sample mean \\(\\overline{X}\\) is also a random variable following its sampling distribution. Fo r any population distribution, not necessarily normal, \\(\\overline{X}\\) has the following two properties:\n\n\nThe sample mean \\((\\overline{X})\\) is  less variable  than an individual observation \\(X_i\\). Although \\(\\overline{X}\\) and \\(X_i\\) are both random variables, the sampling distribution of \\(\\overline{X}\\) has smaller variance than \\(X_i\\). Intuitively, \\(\\overline{X}\\) is the average of bunch of \\(X_i\\)s. Averaging is washing the extreme values out, resulting in values similar to each other.\n\n\n# data 1: 30 40 50 60 70\n# data 2: 0 5 50 95 100\n# data 3: 0 5 10 15 220\n\nThe three data sets all have \\(x_1, \\dots, x_5\\). Clearly, \\(X_i\\) could generate pretty small or large values. However, when all five \\(x_1, \\dots, x_5\\) are averaged, the extreme values are combined together, moving toward to some value in between. In this example, all three data sets have the sample mean \\(50\\) with even no variation at all. The sample mean is much more stable than individual \\(X_i\\), especially when the sample size \\(n\\) is large.\n\nThe sample mean \\((\\overline{X})\\) is  more normal  than an individual observation \\(X_i\\).\n\nThe population distribution \\(X_i\\) is drawn from is not necessarily a normal distribution, and it can be any distribution that is not bell-shaped or not unimodal. However, the sampling distribution will always look more like a normal distribution than the assumed population distribution. This sounds unreal, but it is true. The important central limit theorem proves this, and we will talk about it in Chapter 13.\nSuppose \\((X_1, \\dots, X_n)\\) is the random sample from a population distribution with mean \\(\\mu\\), and standard deviation \\(\\sigma\\). Can we know the mean and variance of the sampling distribution of the sample mean \\(\\overline{X} = \\frac{\\sum_{i=1}^nX_i}{n}\\), denoted by \\(\\mu_{\\overline{X}}\\) and \\(\\sigma_{\\overline{X}}\\) respectively? The answer is yes. In fact,\n\n \\(\\mu_{\\overline{X}} = \\mu\\) . The mean of \\(\\mu_{\\overline{X}}\\) is equal to the population mean \\(\\mu\\), i.e., \\(E(\\overline{X}) = \\mu\\).\n \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) . The standard deviation of \\(\\overline{X}\\) is not equal to the population standard deviation \\(\\sigma\\). It is actually \\(\\frac{\\sigma}{\\sqrt{n}}\\) that is smaller than \\(\\sigma\\). This is consistent with the property that \\(\\overline{X}\\) is less variable than an individual variable \\(X_i\\) we learned before. Notice that the variation of \\(\\overline{X}\\) is getting smaller as the sample size \\(n\\) get large. \\(\\sigma_{\\overline{X}}\\) is also known as the standard error of \\(\\overline{X}\\).\n\nIf the population distribution is  \\(N(\\mu, \\sigma^2)\\) , then the sampling distribution of \\(\\overline{X}\\) is also normally distributed:  \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\) .\nFigure¬†12.3 depicts that the sampling distributions of the sample mean are less variable than the population distribution (black colored). As the sample size \\(n\\) increases from 2 (red), 4 (blue), to 8 (green), its corresponding sampling distribution is getting less variable, with smaller chance to have extreme values. Since the population distribution is normally distributed, so is the sampling distribution. The population distribution and the sampling distribution have the same mean, both centered at one.\n\n\n\n\n\n\n\nFigure¬†12.3: Comparison between sampling distributions and the population distribution.\n\n\n\n\n Example: Rolling a Die \nLet‚Äôs see how we get a sampling distribution through an example. Suppose one rolls a fair die 3 times üé≤üé≤ üé≤ independently to obtain 3 values from the ‚Äúpopulation‚Äù \\(\\{1, 2, 3, 4, 5, 6\\}\\). Well if we let \\(X_i, i = 1, 2, 3\\) be the number showing up for the \\(i\\)th roll, then each \\(X_i\\) follows the discrete uniform distribution \\(P(X_i = 1) = P(X_i = 2) = \\cdots = P(X_i = 6) = 1/6\\) because \\(X_i\\) is a discrete random variable and a fair die is rolled. The population mean is \\((1+2+3+4+5+6)/6 = 3.5.\\) Figure¬†12.4 shows the population distribution.\nTo obtain the sampling distribution of the sample mean, we first repeat the process 10,000 times, and get 10,000 corresponding sample means.\n\n#           x1 x2 x3 mean\n# Dataset-1  6  3  2 3.67\n# Dataset-2  4  2  5 3.67\n# Dataset-3  3  2  6 3.67\n# . . . . . .\n#               x1 x2 x3 mean\n# Dataset-9998   5  4  6 5.00\n# Dataset-9999   4  1  1 2.00\n# Dataset-10000  4  6  1 3.67\n\nThen plot the histogram of those sampling means. Figure¬†12.5 shows the histogram of those 10000 sample means which can be treated as the sampling distribution of the sample mean. What do we see from the plots? First, since the population distribution is discrete, so is the sampling distribution. Second, both have the identical mean 3.5. 1 Third, the sampling distribution look more like a normal distribution.\n\n\n\n\n\n\n\n\n\nFigure¬†12.4: Population distribution is discrete uniformly distributed.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†12.5: Sampling distribution is more normal-like.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html#standardization-of-sample-mean",
    "href": "prob-sampling.html#standardization-of-sample-mean",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "\n12.6 Standardization of Sample Mean",
    "text": "12.6 Standardization of Sample Mean\nAny random variable can be standardized. For a single random variable \\(X \\sim N(\\mu, \\sigma^2)\\), we have \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\). For the sample mean of \\(n\\) variables, we know \\(\\overline{X} \\sim N(\\mu_{\\overline{X}}, \\sigma^2_{\\overline{X}}) = N(\\mu, \\frac{\\sigma^2}{n})\\). To standardize \\(\\overline{X}\\), and make a new standard normal variable from it, we just subtract it from its own mean and divided by it own standard deviation:\n \\[Z = \\frac{\\overline{X} - \\mu_{\\overline{X}}}{\\sigma_{\\overline{X}}} = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\]\nAgain, since \\(\\overline{X}\\) is a random variable, its transformation is also a random variable. \\(\\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}}\\) is a standard normal variable.\n Example: Psychomotor Retardation \n\n\nSuppose psychomotor retardation scores for a group of patients have a normal distribution with a mean of 930 and a standard deviation of 130.\n\nWhat is the probability that the mean retardation score of a random sample of 20 patients was between 900 and 960?\n\n\n\n\n\n\n\n\n\n\n\n\nFirst, from the question assume that \\((X_1, \\dots, X_{20})\\) forms a random sample, and \\(X_1, \\dots, X_{20}  \\stackrel{iid}{\\sim} N(930, 130^2)\\). Then \\(\\overline{X} = \\frac{\\sum_{i=1}^{20}X_i}{20} \\sim N\\left(930, \\frac{130^2}{20} \\right)\\).\nWhat we want to compute is \\(P(900 &lt; \\overline{X} &lt; 960)\\). We can first standardize \\(\\overline{X}\\) and represent the probability using standard normal \\(Z\\): \\[\\small \\begin{align}\nP(900 &lt; \\overline{X} &lt; 960) &= P\\left( \\frac{900-930}{130/\\sqrt{20}} &lt; \\frac{\\overline{X}-930}{130/\\sqrt{20}} &lt; \\frac{960-930}{130/\\sqrt{20}}\\right)=P(-1.03 &lt; Z &lt; 1.03)\\\\\n&=P(Z &lt; 1.03) - P(Z &lt; -1.03)\n  \\end{align}\\]\nFinally we just need to find \\(P(Z &lt; 1.03)\\) and \\(P(Z &lt; -1.03)\\) using R/Python.\n\n\nR\nPython\n\n\n\n\n(z1 &lt;- (960-930)/(130/sqrt(20)))\n# [1] 1.03\n(z2 &lt;- (900-930)/(130/sqrt(20)))\n# [1] -1.03\npnorm(z1) - pnorm(z2)\n# [1] 0.698\n\nIf we don‚Äôt do standardization, remember to use values in the original scale, and specify the mean and standard deviation. Keep in mind the standard deviation is \\(\\sigma_{\\overline{X}} = 130/\\sqrt{20}\\), not \\(130\\).\n\n## P(Xbar &lt; 960) - P(Xbar &lt; 900)\npnorm(960, mean = 930, sd = 130/sqrt(20)) - \n  pnorm(900, mean = 930, sd = 130/sqrt(20))\n# [1] 0.698\n\n\n\n\n# Z-scores and probabilities\nz1 = (960-930)/(130/np.sqrt(20))\nz1\n# 1.0320313742306721\nz2 = (900-930)/(130/np.sqrt(20))\nz2\n# -1.0320313742306721\nnorm.cdf(z1) - norm.cdf(z2)\n# 0.6979425798488381\n\nIf we don‚Äôt do standardization, remember to use values in the original scale, and specify the mean and standard deviation. Keep in mind the standard deviation is \\(\\sigma_{\\overline{X}} = 130/\\sqrt{20}\\), not \\(130\\).\n\n# Direct probability calculation\nnorm.cdf(960, loc=930, scale=130/np.sqrt(20)) - norm.cdf(900, loc=930, scale=130/np.sqrt(20))\n# 0.6979425798488381\n\n\n\n\nThe probability that the mean psychomotor retardation score of a random sample of 20 patients is between 900 and 960 is about 70%.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html#exercises",
    "href": "prob-sampling.html#exercises",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "\n12.7 Exercises",
    "text": "12.7 Exercises\n\nHead lengths of Virginia possums follow a normal distribution with mean 104 mm and standard deviation 6 mm.\n\nWhat is the sampling distribution of the sample mean of the head length when the sample size \\(n = 18\\)?\n\n\nAssume that females have pulse rates that are normally distributed with a mean of 76.0 beats per minute and a standard deviation of 11.5 beats per minute.\n\nIf 1 adult female is randomly selected, find the probability that her pulse rate is less than 81 beats per minute.\nIf 18 adult female are randomly selected, find the probability that their mean pulse rate is less than 81 beats per minute.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-sampling.html#footnotes",
    "href": "prob-sampling.html#footnotes",
    "title": "12¬† Random Sampling and Sampling Distribution",
    "section": "",
    "text": "The average or the empirical mean of those 10000 \\(\\overline{x}\\)s would not be exactly equal to, but very close to the population mean 3.5. In fact, the average is 3.5001. As we discussed, theoretically it is true that \\(E(\\overline{X}) = \\mu = 3.5\\).‚Ü©Ô∏é",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>12</span>¬† <span class='chapter-title'>Random Sampling and Sampling Distribution</span>"
    ]
  },
  {
    "objectID": "prob-llnclt.html",
    "href": "prob-llnclt.html",
    "title": "13¬† Law of Large Numbers and Central Limit Theorem",
    "section": "",
    "text": "13.1 Law of Large Numbers\nIn sampling distribution Chapter 12, we learn that as the sample size \\(n\\) grows, the sampling distribution of sample mean\nYou may be wondering if it is always true. The answer is yes if the sample mean is the average of independent and identically distributed random variables. These properties are proved by the two theorems: the Law of Large Numbers and the Central Limit Theorem.\nThe Law of Large Numbers (LLN) says that\nIf we keep on taking larger and larger random or iid samples (larger \\(n\\)), the statistic sample mean \\(\\overline{X}\\) is guaranteed to get closer and closer to the population mean \\(\\mu = E(X)\\) if the mean exists.\nTherefore, with LLN, if \\(E(X) = \\mu\\), \\(\\overline{X} \\approx \\mu\\) when \\(n\\) is sufficiently large. It is always true, and it does not matter whether we know the value of \\(\\mu\\) or not, as long as \\(\\mu\\) exists or it is finite.\nFigure¬†13.1: Illustration of Law of Large Numbers. The population mean is assumed having value three. The simulation is run twice.\nFigure¬†13.1 illustrates LLN using a simulation. We consider the sample size \\(n = 1, 2, \\dots, 1000\\). For each \\(n\\), we draw a random sample \\(x_1, \\dots, x_n\\) from a normal distribution \\(N(3, 5^2)\\), and the corresponding sample mean is \\(\\bar{x} = (x_1+ \\cdots + x_n)/n\\). When \\(n\\) is small, \\(\\bar{x}\\) is quite from away from the population mean value \\(\\mu = 3\\) due to the randomness of the samples. For example, in this simulation, \\(x_1\\), \\(x_2\\), and \\(x_3\\) are 7.91, 5.34, and 2.46 respectively, and hence the sample mean \\(\\bar{x}_3 =\\) 5.24. Each \\(x_i\\) comes from \\(N(3, 5^2)\\). Although their expected value is 3, keep in mind that theoretically its realized value can be any real value. With larger variance, it is more likely to have a value away from its expected value. When the sample size is small, such randomness generally will not be washed out simply by averaging. Therefore, the sample mean is not that close to the population.\nviewof samplesize = Inputs.range([1, 10000], {value: 10, label: tex`n`, step: 1})\nviewof param_lln = Inputs.form([\n  Inputs.range([-5, 10], {value: 1, label: tex`\\mu`, step: 0.01}),\n  Inputs.range([0.01, 10], {value: 1, label: tex`\\sigma`, step: 0.01})\n])\npltkonv = {\n  return Plot.plot({\n    y: { label: \"sample mean\"},\n    x: { label: \"number of observations\"},\n  marks: [\n    Plot.ruleX([0]),\n    Plot.lineY([{x: 0, y: param_lln[0]},{x: samplesize, y: param_lln[0]}], {x: \"x\", y: \"y\", stroke: \"orange\"}),\n    Plot.lineY(running_means, {x: \"x\", y: \"y\", stroke: \"steelblue\"})\n  ]\n})\n}\n\n\n\n\n\n\n\nFigure¬†13.2: Source: https://observablehq.com/@mattiasvillani/law-large-numbers\nrunning_means = xall.slice(0, samplesize).map(function(x, i) {\n    return { x: i + 1, y: d3.mean(xall.slice(0, i + 1)) };\n  });\nxall = d3.range(10000).map( x =&gt; jstat.normal.sample(param_lln[0],param_lln[1]))\njstat = require('jstat')\nfunction toc(selector = \"h2,h3\", heading = \"&lt;b&gt;Table of Contents&lt;/b&gt;\") {\n  return Generators.observe(notify =&gt; {\n    let headings = [];\n\n    function observed() {\n      const h = Array.from(document.querySelectorAll(selector));\n      if (h.length !== headings.length || h.some((h, i) =&gt; headings[i] !== h)) {\n        notify(html`${heading}&lt;ul&gt;${Array.from(headings = h, h =&gt; {\n          return Object.assign(\n            html`&lt;li&gt;&lt;a href=#${h.id}&gt;${DOM.text(h.textContent)}`,\n            {onclick: e =&gt; (e.preventDefault(), h.scrollIntoView())}\n          );\n        })}`);\n      }\n    }\n\n    const observer = new MutationObserver(observed);\n    observer.observe(document.body, {childList: true, subtree: true});\n    observed();\n    return () =&gt; observer.disconnect();\n  });\n}\nThe marvelous part is that when the sample size is large, such randomness or the deviation from the mean can be washed out via averaging, and the sample mean will get closer to the population mean. In the figure, two simulations are run, and both show that when the sample size is over 200, the sample mean is quite close to the population mean, and it does not fluctuate much as when the sample size is small. This is probably one of the reasons why we love large sample size. Heard Big Data before? When we don‚Äôt know but want to sort of guess or learn the value of \\(\\mu\\), we can draw a larger sample from the target population, calculate the sample mean, then use it as a proxy or representative of the population mean. The entire process of using sample statistics to learn unknown population properties or quantities is called statistical inference which will be discussed in detail starting next part.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Law of Large Numbers and Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob-llnclt.html#law-of-large-numbers",
    "href": "prob-llnclt.html#law-of-large-numbers",
    "title": "13¬† Law of Large Numbers and Central Limit Theorem",
    "section": "",
    "text": "Note\n\n\n\n\nRemember that sine \\(\\overline{X}\\) is a transformation of random variables \\(X_1, \\dots, X_n\\), \\(\\overline{X}\\) itself is also a random variable with some randomness and variation. Therefore, when the simulation (random sampling) is run twice (independently), for each size of \\(n\\), we will get two different sets of values of \\(x_1, \\dots, x_n\\), and two different \\(\\bar{x}\\)s, resulting in two lines in Figure¬†13.1.\nAdditionally, as shown in the figure, the black and green values of \\(\\overline{X}\\) are quite different when \\(n\\) is small, showing a larger variation. Why? It comes from the fact that \\(\\Var(\\overline{X}) = \\sigma^2/n\\). It also tells us why \\(\\overline{X}\\) converges to \\(\\mu\\). When \\(n\\) gets large, its variation gets small, and eventually this random variable behaves like a constant with no variation. That constant is \\(\\mu\\).\n\n\n\n\n\nAlgorithm\nR Simulation Result\nR Code\nPython Simulation Result\nPython Code\n\n\n\n\nSimulation of LLN (Normal distribution example)\n\nConsider different sample size \\(n\\).\n\nFor each size of \\(n\\), do:\n\n\n[1] Draw values \\(x_1, x_2, \\dots, x_{n}\\) from \\(N(\\mu, \\sigma^2)\\) for some values of \\(\\mu\\) and \\(\\sigma^2\\).\n\n[2] Compute \\(\\overline{x}_n = \\frac{1}{n}\\sum_{i=1}^n x_i\\).\n\n\nPlot \\(\\overline{x}_n\\) (y-axis) vs.¬†\\(n\\) (x-axis).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu &lt;- 3\nsd &lt;- 5\nnn &lt;- 1000\n\n## Algorithm\n# [1] Generate normal distribution samples\nset.seed(1000)\nx &lt;- rnorm(nn, mean = mu, sd = sd)\n# [2] Calculate cumulative mean\nx_bar &lt;- cumsum(x) / seq_len(nn)\n\n## Plotting\nplot(x_bar, type = \"l\", \n     main = \"Law of Large Numbers w/ mu = 3\", \n     xlab = \"sample size (n)\", \n     ylab = \"sample mean\", las = 1)\nabline(h = 3, col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\nmu = 3\nsd = 5\nnn = 1000\n\n## Algorithm\n# [1] Generate normal distribution samples\nnp.random.seed(1000)\nx = np.random.normal(mu, sd, nn)\n# [2] Calculate cumulative mean\nx_bar = np.cumsum(x) / np.arange(1, nn + 1)\n\n# Plotting\nplt.figure(figsize=(8, 6))\nplt.plot(x_bar, label=\"Sample Mean\")\nplt.axhline(y=mu, color='red', linestyle='--', linewidth=2, label=\"True Mean (mu=3)\")\nplt.title(\"Law of Large Numbers w/ mu = 3\")\nplt.xlabel(\"Sample size (n)\")\nplt.ylabel(\"Sample mean\")\nplt.legend()\nplt.show()",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Law of Large Numbers and Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob-llnclt.html#central-limit-theorem",
    "href": "prob-llnclt.html#central-limit-theorem",
    "title": "13¬† Law of Large Numbers and Central Limit Theorem",
    "section": "\n13.2 Central Limit Theorem",
    "text": "13.2 Central Limit Theorem\nIt‚Äôs time to talk about the most important theorem in probability and statistics, at least in my opinion, the central limit theorem (CLT).\nIn sampling distribution Chapter 12, we learned that if \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\), then \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\). But the question is  what if the population distribution is NOT normal?  What does the sampling distribution of the sample mean look like if the population distribution is multimodal? or skewed? or not bell-shaped? Well, the central limit theorem gives us the answer!\n\n\n\n\nCentral Limit Theorem (CLT):\nSuppose \\(\\overline{X}\\) is the sample mean from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma &lt; \\infty\\). As \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\) regardless of the distribution from which we are sampling!\n\n\n\n\n\n\n\nFigure¬†13.3: Illustration of Central Limit Theorem. Source: Wiki.\n\n\n\n\nFigure¬†13.3 illustrates the CLT. First, the random sample \\((X_1, \\dots, X_n)\\) can be collected from any population distribution, whether it is normal or not. The magic part is that the sampling distribution of the sample mean \\(\\overline{X}\\) always looks like normal distribution \\(N(\\mu, \\sigma^2/n)\\) as long as the sample size \\(n\\) is sufficiently large. The larger \\(n\\) is, the more normal-like the sampling distribution of \\(\\overline{X}\\) is. One question is how large is enough for \\(n\\). Amazingly the normal approximation is quite well when \\(n \\ge 30\\). The variance of the sampling distribution which is \\(\\sigma^2/n\\) is decreasing with \\(n\\) as well.\nPlease try the app and see how the shape of the sampling distribution changes with the sample size \\(n\\) and with the shape of the population distribution. You will find that it requires larger \\(n\\) to get a more normal-like sampling distribution if the population distribution is very skewed. You can also see how the CLT works in Figure¬†13.4 and Figure¬†13.5. The population distribution can be discrete, like binomial or Poisson distribution. Their sampling distribution of \\(\\overline{X}\\) will still look like normal although the sampling distribution is not continuous.\n\nviewof dist_type = Inputs.select(['beta', 'bimodal', 'binomial', 'cauchy', 'chi2', 'lognormal', 'poisson', 'studentt', 'uniform',], {value: \"uniform\", label: \"data distribution:\"})\n\n\n\n\n\n\n\n// Returns a normal deviate (mu=0, sigma=1).\nfunction randn() {\n  let u, v, x, y, q;\n  do {\n    u = Math.random();\n    v = 1.7156 * (Math.random() - 0.5);\n    x = u - 0.449871;\n    y = Math.abs(v) + 0.386595;\n    q = x * x + y * (0.19600 * y - 0.25472 * x);\n  } while (q &gt; 0.27597 && (q &gt; 0.27846 || v * v &gt; -4 * Math.log(u) * u * u));\n  return v / u;\n}\n\n\n\n\n\n\n\nviewof params_clt = {\n  var form;\n    if (dist_type == 'binomial'){\n    form = Inputs.form([\n      Inputs.range([1, 30], {value: 10, step: 1, label: tex`\\text{# of trials }n`}),\n      Inputs.range([Number.EPSILON, 1 - Number.EPSILON], {value: 0.3, step: 0.01, label: tex`\\text{success prob }\\pi`})\n    ])\n  }\n  if (dist_type == 'poisson'){\n    form = Inputs.form([\n      Inputs.range([0, 10], {value: 1, step: 0.1, label: tex`\\text{mean }\\lambda`})\n    ])\n  }\n  if (dist_type == 'beta'){\n    form = Inputs.form([\n      Inputs.range([0.1, 20], {value: 3, step: 0.1, label: tex`\\text{shape }\\alpha`}),\n      Inputs.range([0.1, 20], {value: 5, step: 0.1, label: tex`\\text{shape }\\beta`})\n    ])\n  }\n  if (dist_type == 'cauchy'){\n    form = Inputs.form([\n      Inputs.range([-5, 5], {value: 0, step: 0.1, label: tex`\\text{location }m`}),\n      Inputs.range([0.1, 5], {value: 1, step: 0.1, label: tex`\\text{scale }\\gamma`}),\n    ])\n  }\n  else if (dist_type == 'uniform'){\n    form = Inputs.form([\n      Inputs.range([-5, 5], {value: 0, step: 0.1, label: tex`\\text{lower limit }a`}),\n      Inputs.range([-5, 5], {value: 1, step: 0.1, label: tex`\\text{upper limit }b`})\n    ])\n  }\n  else if (dist_type == 'studentt'){\n    form = Inputs.form([\n      Inputs.range([-5, 5], {value: 0, step: 0.1, label: tex`\\text{location }\\mu`}),\n      Inputs.range([0.1, 5], {value: 1, step: 0.1, label: tex`\\text{scale }\\sigma`}),\n      Inputs.range([1, 50], {value: 4, step: 1, label: tex`\\text{df }\\nu`})\n    ])\n  }\n  else if (dist_type == 'chi2'){\n    form = Inputs.form([\n      Inputs.range([1, 50], {value: 3, step: 1, label: tex`\\text{df }\\nu`})\n    ])\n  }\n  else if (dist_type == 'lognormal'){\n    form = Inputs.form([\n      Inputs.range([-5,5], {value: 0, step: 0.1, label: tex`\\text{location }\\mu`}),\n      Inputs.range([0.1,5], {value: 1, step: 0.1, label: tex`\\text{scale }\\sigma`})\n    ])\n  }\n  else if (dist_type == 'bimodal'){\n    form = Inputs.form([\n      Inputs.range([-5,5], {value: -2, step: 0.1, label: tex`\\text{location }\\mu_1`}),\n      Inputs.range([-5,5], {value: 2, step: 0.1, label: tex`\\text{location }\\mu_2`}),\n      Inputs.range([0.1,5], {value: 1, step: 0.1, label: tex`\\text{scale }\\sigma_1`}),\n      Inputs.range([0.1,5], {value: 1, step: 0.1, label: tex`\\text{scale }\\sigma_2`}),\n      Inputs.range([0,1], {value: 0.5, step: 0.01, label: tex`\\text{weight }w_1`})\n    ])\n  }\n  return form;\n}\n\n\n\n\n\n\n\nviewof sample_size = Inputs.range([1, 1000], {value: 2, step: 1, label: tex`\\text{sample size }n:`})\n\n\n\n\n\n\n\njStat = require('jstat')\n\n\n\n\n\n\n\nnRep = 5000\n\n\n\n\n\n\n\nfunction rand_bimodal(mu1,mu2,sigma1,sigma2,vikt){\n  const comp = Math.random() &lt; vikt\n  return comp ?  jStat.normal.sample(mu1,sigma1) : jStat.normal.sample(mu2,sigma2)\n}\n\n\n\n\n\n\n\nfunction rand_lognormal(mu, sigma){\n  return jStat.lognormal.sample(mu, sigma)\n}\n\n\n\n\n\n\n\nfunction rand_chi2(df){\n  return jStat.chisquare.sample(df)\n}\n\n\n\n\n\n\n\nfunction rand_studentt(location, scale, df){\n  return location + scale * jStat.studentt.sample(df)\n}\n\n\n\n\n\n\n\n// Returns a gamma deviate by the method of Marsaglia and Tsang.\nfunction randg(shape) {\n  let oalph = shape, a1, a2, u, v, x, mat;\n  if (!shape) shape = 1;\n  if (shape &lt; 1) shape += 1;\n  a1 = shape - 1 / 3;\n  a2 = 1 / Math.sqrt(9 * a1);\n  do {\n    do {\n      x = randn();\n      v = 1 + a2 * x;\n    } while (v &lt;= 0);\n    v = v * v * v;\n    u = Math.random();\n  } while (\n    u &gt; 1 - 0.331 * Math.pow(x, 4) &&\n    Math.log(u) &gt; 0.5 * x * x + a1 * (1 - v + Math.log(v))\n  );\n  if (shape === oalph) return a1 * v; // alpha &gt; 1\n  do u = Math.random(); while (u === 0); // alpha &lt; 1\n  return Math.pow(u, 1 / oalph) * a1 * v;\n}\n\n\n\n\n\n\n\nfunction randb(alpha, beta) {\n  const u = randg(alpha);\n  return u / (u + randg(beta));\n}\n\n\n\n\n\n\n\nfunction binomialSample(n,p){\n  return d3.sum(d3.range(n).map(d =&gt; Math.random()&lt;p))\n}\n\n\n\n\n\n\n\ndata = {\n  const size = 1000;\n  if (dist_type == 'binomial'){\n    const n = params_clt[0];\n    const p = params_clt[1];\n    return d3.range(size).map(d =&gt; ({x: binomialSample(n, p)})) \n  }\n  if (dist_type == 'poisson'){\n    const lambda = params_clt[0];\n    return d3.range(size).map(d =&gt; ({x: jStat.poisson.sample(lambda)}))\n  }\n  if (dist_type == 'beta'){\n    const alpha = params_clt[0];\n    const beta = params_clt[1];\n    return d3.range(size).map(d =&gt; ({x: randb(alpha, beta)}))\n  }\n  if (dist_type == 'cauchy'){\n    const location = params_clt[0];\n    const scale = params_clt[1];\n    const df = 1;\n    return d3.range(size).map(d =&gt; ({x: rand_studentt(location, scale, df)}))\n  }\n  else if (dist_type == 'uniform'){\n    const min = params_clt[0];\n    const max = params_clt[1];\n    return d3.range(size).map(d =&gt; ({x: Math.random() * (max-min) + min}))\n  }\n  else if (dist_type == 'studentt'){\n    const location = params_clt[0];\n    const scale = params_clt[1];\n    const df = params_clt[2];\n    return d3.range(size).map(d =&gt; ({x: rand_studentt(location, scale, df)}))\n  }\n  else if (dist_type == 'chi2'){\n    const df = params_clt[0];\n    return d3.range(size).map(d =&gt; ({x: rand_chi2(df)}))\n  }\n  else if (dist_type == 'lognormal'){\n    const mu = params_clt[0];\n    const sigma = params_clt[1];\n    return d3.range(size).map(d =&gt; ({x: rand_lognormal(mu, sigma)}))\n  }\n  else if (dist_type == 'bimodal'){\n    const mu1 = params_clt[0];\n    const mu2 = params_clt[1];\n    const sigma1 = params_clt[2];\n    const sigma2 = params_clt[3];\n    const vikt = params_clt[4];\n    return d3.range(size).map(d =&gt; ({x: rand_bimodal(mu1, mu2, sigma1, sigma1, vikt)}))\n  }\n  \n}\n\n\n\n\n\n\n\npltdata = {\n  if (discrete){\n    return Plot.plot({\n      caption:html`&lt;span style=\"color:#6C8EBF\"&gt;a single dataset simulated from the data distribution.&lt;/span&gt; `,\n      marks: [\n        Plot.barY(data, Plot.groupX({y: \"count\"}, {x: \"x\", fill: \"colors[0]\"})),\n        Plot.ruleY([0])\n      ]\n    }) \n  }\n  else {\n    \n    return Plot.plot({\n      caption:html`&lt;span style=\"color:#6C8EBF\"&gt;a single dataset simulated from the data distribution.&lt;/span&gt; `,\n      marks: [\n        Plot.rectY(data.filter(d =&gt; Math.abs(d.x) &lt;=50), Plot.binX({y: \"count\"}, {x: \"x\", fill: colors[0]})),\n        Plot.ruleY([0])\n      ]\n    })\n  }\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmeans = sample_finite_pop(data, sample_size, nRep)\n\n\n\n\n\n\n\npltmeans = Plot.plot({\n  caption:html`&lt;span style=\"color:#bb989a\"&gt;distribution of the sample mean from samples with n = ${sample_size} observations.&lt;/span&gt; `,\n  x: {\n    label: \"sample means\"\n  },\n  y:{\n    axis: false\n  },\n  marks: [\n    Plot.rectY(means, Plot.binX({y: \"count\"}, {x: \"mean\", fill: colors[8]})),\n    //Plot.areaY(means, Plot.binX({y: \"count\", filter: null}, {x: \"mean\", fillOpacity: 0.2})),\n    //Plot.lineY(means, Plot.binX({y: \"count\", filter: null}, {x: \"mean\"})),\n    Plot.ruleY([0])\n  ]\n})\n\n\n\n\n\n\n\nimport {colors} from \"@mattiasvillani/statistics-tools\"\n\n\n\n\n\n\n\nfunction sample_finite_pop(data, n, nRep){\n  // Sample nRep samples of size n from the data and return the mean in each sample\n  var samplemeans = [];\n  for (let i = 0; i &lt; nRep; ++i) {\n    samplemeans.push({sample: i+1, mean: d3.mean(ss.sample(data, n).map(d =&gt; d.x))})\n  }\n  return samplemeans;\n}\n\n\n\n\n\n\n\nss = require('simple-statistics')\n\n\n\n\n\n\n\ndiscrete = ([\"poisson\", \"binomial\"].includes(dist_type))\n\n\n\n\n\n\n\nplt = html`&lt;html&gt;\n &lt;head&gt;\n &lt;/head&gt;\n &lt;body&gt;\n    &lt;div class=\"container\" style=\"display: flex; height: 400px;\"&gt;\n        &lt;div style=\"width: 44%;\"&gt;\n            ${pltdata}\n        &lt;/div&gt;\n        &lt;div style=\"width: 2%;\"&gt;\n           \n        &lt;/div&gt;\n        &lt;div style=\"width: 44%;\"&gt;\n            ${pltmeans}\n        &lt;/div&gt;\n &lt;/body&gt;\n&lt;/html&gt;`\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.4: CLT Illustration: A Right-Skewed Distribution.\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†13.5: CLT Illustration: A U-shaped Distribution.\n\n\n\n\nIn sum, for a random sample \\((X_1, \\dots, X_n)\\), if the population distribution is normally distributed, then of course with no surprise the sampling distribution of the sample mean is also exactly normally distributed. If the population distribution is not normally distributed, as long as its mean and variance exist, its sampling distribution of the sample mean will still look like a normal distribution when the sample size \\(n\\) is large enough.\n\n\\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\). \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\)\n\\(X_i \\stackrel{iid}{\\sim}\\) any distribution (\\(\\mu, \\sigma^2\\)). \\(\\overline{X}\\) looks like \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\) (for \\(n\\) sufficiently large)\n\nWhy is the central limit theorem Important? Many well-developed statistical methods are based on the normal distribution assumption. With the central limit theorem, we can use these methods even if we are sampling from a non-normal distribution or if we have no idea what the population distribution is, provided that the sample size is large enough.\n\n\nAlgorithm\nR Simulation Result\nR Code\nPython Simulation Result\nPython Code\n\n\n\nTo show the distribution of \\(\\overline{X}\\) looks like Gaussian when \\(n\\) is large, we need lots of values of \\(\\overline{X}\\) and draw a histogram.\n\nSimulation of CLT (Poisson distribution example)\n\nConsider different sample size \\(n\\). Set the sample size of \\(\\overline{X}\\) be \\(M\\).\n\nFor each size of \\(n\\), do:\n\n\n[1] For each \\(m = 1, 2, \\dots, M\\), draw sample \\((x_1^m, x_2^m, \\dots, x_n^m)\\) from some distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\).\n\n[2] For each \\(m = 1, 2, \\dots, M\\), compute \\(\\overline{x}^m_n = \\frac{1}{n}\\sum_{i=1}^n x_i^m\\).\n\n[3] Draw a histogram of the sample \\((\\overline{x}^1_n, \\overline{x}^2_n, \\dots, \\overline{x}^M_n)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM &lt;- 10000\nlambda_par &lt;- 1\n\nfor (k in n_vec) {\n    ## [1]\n    x_sam &lt;- matrix(0, nrow = k, ncol = M)\n    for (m in seq_len(M)) {\n        x_sam[, m] &lt;- rpois(k, lambda = lambda_par)\n    }\n    ## [2]\n    x_bar_sam &lt;- apply(x_sam, 2, mean)\n    \n    ## [3]\n    hist(x_bar_sam, main = paste(\"size =\", k))\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe np.random.poisson function is used to generate Poisson-distributed random samples. The shape (k, M) ensures that you generate k samples for each of the M simulations.\nThe np.mean(x_sam, axis=0) computes the mean of the k samples for each of the M simulations.\nf\"Sample size = {k}\" uses a feature in Python called f-strings (formatted string literals). The f before the opening quote marks indicates that this is a formatted string. It tells Python that the string may contain placeholders (expressions) inside curly braces {} that should be evaluated. Anything inside the curly braces {} is treated as an expression, and its value will be evaluated and inserted into the string. In this case, k is a variable, so its value will be converted to a string and placed inside the output.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nn_vec = [2, 8, 100]  # Vector of sample sizes\nM = 10000            # Number of simulations\nlambda_par = 1       # Poisson parameter\n\nfor k in n_vec:\n    # [1] Generate Poisson samples of size k, repeated M times\n    x_sam = np.random.poisson(lambda_par, (k, M))\n    \n    # [2] Compute the sample means for each column m = 1, ..., M\n    x_bar_sam = np.mean(x_sam, axis=0)\n    \n    # [3] Plot histogram of sample means\n    plt.hist(x_bar_sam)\n    plt.title(f\"Size = {k}\")\n    plt.show()",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Law of Large Numbers and Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob-llnclt.html#central-limit-theorem-example",
    "href": "prob-llnclt.html#central-limit-theorem-example",
    "title": "13¬† Law of Large Numbers and Central Limit Theorem",
    "section": "\n13.3 Central Limit Theorem Example",
    "text": "13.3 Central Limit Theorem Example\n\n\nSuppose that the selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000.\nIn 100 randomly selected sales, what is the probability the average selling price is more than $400,000?\n\n\n\n\n\n\n\n\n\n\n\nSince the sample size is fairly large \\((n = 100)\\), by the central limit theorem, the sampling distribution of the average selling price is approximately normal with a mean of $382,000 and a standard deviation of \\(150,000 / \\sqrt{100}\\).\nThen \\(P(\\overline{X} &gt; 400000) = P\\left(\\frac{\\overline{X} - 382000}{150000/\\sqrt{100}}  &gt; \\frac{400000 - 382000}{150000/\\sqrt{100}}\\right) \\approx P(Z &gt; 1.2)\\) where \\(Z \\sim N(0, 1)\\).\n\nTherefore using R/Python we get the probability\n\n\nR\nPython\n\n\n\n\npnorm(1.2, lower.tail = FALSE)\n# [1] 0.115\npnorm(400000, mean = 382000, \n      sd = 150000/sqrt(100), lower.tail = FALSE)\n# [1] 0.115\n\n\n\n\nfrom scipy.stats import norm\nnorm.sf(1.2)\n# 0.11506967022170822\nnorm.sf(400000, loc=382000, scale=150000/np.sqrt(100))\n# 0.11506967022170822",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Law of Large Numbers and Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "prob-llnclt.html#further-readings-and-references",
    "href": "prob-llnclt.html#further-readings-and-references",
    "title": "13¬† Law of Large Numbers and Central Limit Theorem",
    "section": "\n13.4 Further Readings and References",
    "text": "13.4 Further Readings and References",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>13</span>¬† <span class='chapter-title'>Law of Large Numbers and Central Limit Theorem</span>"
    ]
  },
  {
    "objectID": "infer.html",
    "href": "infer.html",
    "title": "Statistical Inference",
    "section": "",
    "text": "Statistics can be divided into two parts: descriptive statistics and inferential statistics. The descriptive statistics part has been discussed in Part Summarizing Data including 5¬† Data Visualization and 6¬† Numerical Measures of Data, and many of you probably already learned lots of it in your middle or high school. The core and why statistics is that useful in every part of our life is its inferential techniques.\nWe have been equipped with sufficient probability tools for learning basic statistical inference. We‚Äôll start with the very basic concept of statistical inference followed by inferential methods for various kinds of data and research questions.\nInferential statistics uses the information contained in the sample data to learn about some unknown target population characteristic of our interest. For example, we are interested in the mean height of the adults in the United States. The budget and/or time constraint, however, keep us from taking the census and knowing the height of every adult in the United States. Instead, we collected a sample data, and hopefully with a fair statistical method, we are able to use the height data in the sample to estimate the mean height of the adults in the United States as precisely as possible.\n\nTo be honest, statistical inference is a huge and difficult task. To simply our work, we usually assume the target population follows some distribution but with unknown parameters. Then our goal is to learn or uncover the unknown parameters of the assumed population distribution. For example, we can assume the height of the adults in the United States is normally distributed but with its mean \\(\\mu\\) unknown. Then our goal is to estimate the mean height \\(\\mu\\) from our data set.\n\n\n\n\n\n\n\n\n\nSince we only collect a small part of the entire population as our sample, we never see the entire picture and only have partial information, and the conclusion we made based on the sample, with pretty high chance, may be different or even far away from the unknown truth. Also, as we learned in ?sec-prob-samdist, our sample data vary due to its randomness nature. In statistics, we not only learn what we learn about the unknown parameter, but also learn how uncertain we are about what we learn.\nIn statistical inference, there are two main approaches in parameter learning: estimation and hypothesis testing. We are going to learn how to estimate and test on the unknown parameters for various types of data and questions. Get ready and let‚Äôs go!",
    "crumbs": [
      "Statistical Inference"
    ]
  },
  {
    "objectID": "infer-ci.html",
    "href": "infer-ci.html",
    "title": "14¬† Point and Interval Estimation",
    "section": "",
    "text": "14.1 Point Estimator\nLet me ask you a question.\nIf the single number you use can be computed from of sample data \\((X_1, X_2, \\dots, X_n)\\), then you use a point estimator to estimate the unknown parameter \\(\\mu\\). Previously we learned that a sample statistic is any transformation or function of \\((X_1, X_2, \\dots, X_n)\\). Therefore, any statistic is considered a point estimator if it is used to estimate a population parameter.\nA point estimate is a value of a point estimator used to estimate a population parameter. So here is the subtle difference. A point estimator is a random variable which is a function of sample data \\((X_1, X_2, \\dots, X_n)\\) (before actually being collected), and a point estimate is the realized value a point estimator, which is a value calculated from the collected data. For example, \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}^nX_i\\) is a point estimator, and with the sample data \\((x_1, x_2, x_3) = (2, 3, 7)\\), the point estimate is \\(\\overline{x} = \\frac{1}{3}\\sum_{i=1}^3x_i = \\frac{1}{3}(2+3+7) = 4.\\)\nBack to the question. If we want to estimate the unknown population mean, which number we use to estimate it? We now have an intuitive answer. The sample mean \\(\\overline{X}\\) is a statistic and a point estimator for the population mean \\(\\mu\\).\nSample Mean as an Point Estimator\nLet‚Äôs see how the sample mean is used as an point estimator for \\(\\mu.\\) Suppose the true population distribution is \\(N(2, 1)\\). Here the population mean \\(\\mu\\) is two, but let‚Äôs pretend we don‚Äôt know its value and see how the sample mean performs. Such analysis is called simulation study.\nWe are going to collect a sample of size five, \\((x_1, x_2, x_3, x_4, x_5)\\). In the simulation, we draw five values from \\(N(2, 1)\\). The five drawn values are treated as our sample data, and \\(N(2, 1)\\) is the population distribution. In R, we use rnorm() to generate random numbers from a normal distribution, where the first argument is the number of observations to be generated.\n## Generate data x1, x2, x3, x4, x5, each from distribution N(2, 1)\nset.seed(1234)\nx_data_1 &lt;- rnorm(n = 5, mean = 2, sd = 1)\nThe following shows the realized five data points and the sample mean.\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n0.79\n2.28\n3.08\n-0.35\n2.43\n1.65\nHere we use the sample mean \\(\\overline{X} = \\frac{1}{5}\\sum_{i=1}^5X_i\\) as our point estimator for \\(\\mu\\), and given the sample, the point estimate is \\(\\overline{x}=\\) 1.65. You can see that the true \\(\\mu\\) is two, but the point estimate \\(\\overline{x}\\) is not equal to \\(\\mu\\). Why?\nAs we discussed in ?sec-prob-samdist, due to the randomness nature of drawing a sample value from the population distribution, we do not expect the statistic to be the same as the corresponding parameter. It is possible that most of our sample values happen to be larger or smaller than the true mean, or we may unluckily get an outlier sample value that distorts and drags the sample value toward it. In such cases, the sample mean will be not close to the true population mean. You can think this way. One data point represents one piece of information about the unknown population distribution. With a small sample size, our sample only represents a small part of the unknown distribution. The gap between sample mean and the true population mean is kind of like information lost because of not being able to collect the rest of the subjects in the population.\nFigure¬†14.1 shows the sample data and the population distribution \\(N(2, 1)\\). Notice that we have an extreme value \\(-0.35\\) that is two standard deviations below the mean, and this causes the sample mean to be small.\nFigure¬†14.1: Population distribution and sample data.\nWell, we could collect a sample again if resources are permitted. In simulation, another sample of size five is drawn from the same population \\(N(2, 1)\\), and the result is shown below.\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n2.59\n2.71\n1.89\n1.55\n2.61\n2.27\nThe second sample mean, \\(\\overline{x} =\\) 2.27, is different from the first one. Why do the first sample and the second sample give us different sample means? Now you see why we want to learn sampling distribution. We use sample mean as the point estimator for \\(\\mu\\), and the sample mean has its own sampling distribution. Therefore, the sample mean varies from sample to sample due to its randomness nature.\nWe have connected sampling distribution to statistical inference, in particular the point estimation together. Figure¬†14.2 shows the sampling distribution of \\(\\overline{X}\\) which is \\(N(2, 1/5)\\) and the two sample mean values calculated from the two data sets. Now can you see why we want to use \\(\\overline{X}\\) as the point estimator for \\(\\mu\\)? It is because the expected value of \\(\\overline{X}\\), \\(E(\\overline{X})\\) is exactly equal to \\(\\mu\\), meaning that if we were able to produces a lot of \\(\\overline{x}\\)s, the average of these \\(\\overline{x}\\)s will be very close to the true unknown \\(\\mu\\) although single one \\(\\overline{x}\\) may still be distant from \\(\\mu\\). When the expected value of a point estimator is equal to the parameter it estimates, we say it is an unbiased estimator. Therefore, the sample mean \\(\\overline{X}\\) is an unbiased estimator for the population mean \\(\\mu\\) because \\(E(\\overline{X}) = \\mu\\).\nFigure¬†14.2: Sampling Distribution of Sampling Mean.\nWhy Point Estimates Are Not Enough\nSince \\(\\overline{X}\\) is random and has its own distribution, its value varies from sample to sample. However, in reality we usually have only one data set, and one realized sample mean, and we are not able to replicate other data sets due to limited resources. We don‚Äôt know the sample mean we got is close to the true unknown population mean or not. First, the sample mean can go anywhere of its distribution, and the one we got may be far away from \\(\\mu\\). Moreover, we don‚Äôt know the value of \\(\\mu\\)! It does not make much sense to use just one single number when those uncertainty are there.\nIf you want to catch a fish, would you prefer to use a spear or a net? I would use a net because I‚Äôm not a sharpshooter, and using a net covers a large range of possible locations where the fish can be. Due to the variation of \\(\\overline{X}\\), if we report a point estimate, we probably won‚Äôt hit the exact population parameter. If we report a range of plausible values, we have a good shot at capturing the parameter.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Point and Interval Estimation</span>"
    ]
  },
  {
    "objectID": "infer-ci.html#point-estimator",
    "href": "infer-ci.html#point-estimator",
    "title": "14¬† Point and Interval Estimation",
    "section": "",
    "text": "If you could only use a single number to guess the unknown population mean, \\(\\mu\\), what number would you like to use?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to estimate \\(\\mu\\), would you prefer to report a range of values the parameter might be in or a single estimate like \\(\\overline{x}\\)?",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Point and Interval Estimation</span>"
    ]
  },
  {
    "objectID": "infer-ci.html#confidence-intervals",
    "href": "infer-ci.html#confidence-intervals",
    "title": "14¬† Point and Interval Estimation",
    "section": "\n14.2 Confidence Intervals",
    "text": "14.2 Confidence Intervals\nIn statistics, a plausible range of values for \\(\\mu\\) is called a confidence interval (CI). This range depends on how precise and reliable our statistic is as an estimate of the parameter. To construct a CI for \\(\\mu\\), we first need to quantify the variability of our sample mean. Quantifying this uncertainty requires a measurement of how much we would expect the sample statistic to vary from sample to sample. This is in fact the variance of the sampling distribution of the sample mean! Intuitively speaking, if \\(\\overline{x}\\) varies a lot, we are more uncertain about whether the \\(\\overline{x}\\) we got is close to the \\(\\mu\\) or not. In other words, the precision of the estimation is not that good. In order to make sure that the plausible range of values does capture \\(\\mu\\), we need to include more possible values, and make the range larger. Do we know the variance of \\(\\overline{X}\\)? Absolutely. Thanks to CLT, \\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\) regardless of what the population distribution is.\nHow confident we are about the CI covering the parameter is called the level of confidence. The higher the confidence level is, the more reliable the CI is because the CI is more likely to capture the parameter.\n\n\n\n\n\n\n\nNote\n\n\n\nGiven the same level of confidence, the larger the variation of \\(\\overline{X}\\) is, the wider the CI for \\(\\mu\\) will be.\n\n\n\n Precision vs.¬†Reliability \nWith a fixed sample size, the precision and reliability of a confidence interval are trading off. Here is a question.\n\n\n\n\n\n\nIf we want to be very certain that we capture \\(\\mu\\), should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\nWe use a wider interval because a wider interval is more likely to capture the population parameter value. So a more reliable confidence interval is wider than a less reliable confidence interval. But What drawbacks are associated with using a wider interval?\nThe precision and reliability trade-off is clearly explained in the cute comic in Figure¬†14.3. I can say I am 100% confident that your exam 1 score is between 0 and 100. Am I right? Yes. But do I provide helpful information? Absolutely not, the interval includes every possible score of the exam. The interval is too wide to be helpful. Such interval is 100% reliable but with no precision at all.\n\n\n\n\n\n\n\nFigure¬†14.3: Balance between precision and reliability. Source: https://thestatsninja.com/2019/02/19/how-to-navigate-confidence-intervals-with-confidence/\n\n\n\n\n\nNarrower intervals are more precise but less reliable, while wider intervals are more reliable but less precise. How can we get best of both worlds ‚Äì high precision and high reliability/accuracy, meaning short interval with high level of confidence? What we need is larger sample size, given that the sample quality is good. It is a quite easy statement, but sometimes it‚Äôs hard to collect more samples.\n\n A Confidence Interval Is for a Parameter \nA confidence interval is for a parameter, NOT a statistic. Remember, a confidence interval is a way of doing estimation for a unknown parameter. For example, we use the sample mean to form a confidence interval for the population mean.\nWe NEVER say ‚ÄúThe confidence interval of the sample mean, \\(\\overline{X}\\), is ‚Ä¶.‚Äù We SAY ‚ÄúThe confidence interval for the true population mean \\(\\mu\\), is ‚Ä¶‚Äù\nIn general, a confidence interval for \\(\\mu\\) has the form\n\n\\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\nThe \\(m\\) is called the margin of error. It controls the width of the interval \\(2m\\). The CI is centered at the sample mean, and \\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval. The point estimate, \\(\\overline{x}\\), and margin of error, \\(m\\), can be obtained from known quantities and our data once sampled.\n\n \\((1 - \\alpha)100\\%\\) Confidence Intervals \nFormally, for \\(0 \\le \\alpha \\le 1\\), we define the confidence level \\(1-\\alpha\\) as the proportion of times that the CI contains the population parameter, assuming that the estimation process is repeated a large number of times.\nThe confidence level can be any number between zero and one. Common choices for the confidence level include 90% \\((\\alpha = 0.10)\\), 95% \\((\\alpha = 0.05)\\) and 99% \\((\\alpha = 0.01)\\). Keep in mind that confidence level tells us the reliability of the interval. Because precision and reliability have a trade-off relationship, a CI with very high confidence level (high reliability) will have less precision, i.e., larger margin of error and with width of the interval. 95% is the most common level because it has a good balance between precision (width of the CI) and reliability (confidence level).\n\n\n High reliability and Low precision: I am 100% confident that the mean height of Marquette students is between 3‚Äô0‚Äù and 8‚Äô0‚Äù. \n\nDuh‚Ä¶ü§∑\n\n\n\n Low reliability and High precision: I am 20% confident that mean height of Marquette students is between 5‚Äô6‚Äù and 5‚Äô7‚Äù. \n\nThis is far from the truth‚Ä¶ üôÖ\n\n\n\n\n \\(95\\%\\) Confidence Intervals for \\(\\mu\\) \nWe‚Äôve learned the general form of a confidence interval for \\(\\mu\\) and defined the confidence level. We now formally derive the form of the \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\). For simplicity, here we assume \\(\\sigma\\) is known to us when the interval is constructed. A confidence interval can be derived from the sampling distribution of the point estimator. Such approach is called the distribution-based approach. A confidence interval can also be derived using simulation, and such approach is called simulation-based approach, bootstraping method for example. This chapter we build a CI based on the sampling distribution of \\(\\overline{X}\\). We discuss bootstraping in Chapter 15.\n\nSuppose we want to obtain the \\(95\\%\\) confidence interval for \\(\\mu\\). So \\(\\alpha = 0.05\\). We start with the sampling distribution of \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\) shown in Figure¬†14.4. The sampling distribution tells us that the realized value \\(\\overline{x}\\) will be within 1.96 SDs of the population mean, \\(\\mu\\), \\(95\\%\\) of the time. In other words,\n\\[P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} &lt; \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\]\nHere the \\(z\\)-score of 1.96 is the 97.5% percentile of the standard normal distribution and -1.96 is the 2.5% percentile. The \\(z\\)-score 1.96 is associated with 2.5% area to the right and is called a critical value denoted as \\(z_{0.025} = z_{\\alpha/2}\\) . The \\(z\\)-score -1.96 is associated with 2.5% area to the left, and it happens to be the negative value of \\(z_{0.025}\\) because of the symmetry of normal distribution.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†14.4: Sampling distribution of the sample mean with 95% probability in the middle.\n\n\n\n\n\n\n\n\n\nWe learned that \\[P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} &lt; \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95.\\] The probability that the variable \\(\\overline{X}\\) is in the interval \\(\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right)\\) is 95%. But is the interval \\(\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right)\\) our confidence interval?\nThe answer is No ‚ùå! Remember that we don‚Äôt know \\(\\mu\\) and we are estimating it. The interval cannot be determined because it involves the unknown quantity \\(\\mu\\). But don‚Äôt be too disappointed. We are almost there.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can arrange the inequality in the probability so that \\(\\mu\\) is in the middle and the probability remains unchanged.\n\n\\[ \\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} \\iff \\mu &lt; \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\]\n\\[ \\overline{X}  &lt; \\mu+ 1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\iff \\overline{X} - 1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu\\]\n\\[\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} &lt; \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\iff \\overline{X}-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\]\n\n\\[\\small \\begin{align} P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\overline{X} &lt; \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) =\nP\\left( \\boxed{\\overline{X}-1.96\\frac{\\sigma}{\\sqrt{n}} &lt; \\mu &lt; \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}} \\right) = 0.95\n\\end{align}\\]\nWe are done!  With sample data of size \\(n\\), \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) is our \\(95\\%\\) CI for \\(\\mu\\).  Note that if \\(\\sigma\\) is known to us, the interval can be computed from our data because we know the sample size, and we can get the sample mean. The margin of error \\(m = 1.96\\frac{\\sigma}{\\sqrt{n}}\\).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Point and Interval Estimation</span>"
    ]
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "title": "14¬† Point and Interval Estimation",
    "section": "\n14.3 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "14.3 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\nWe just obtained the 95% confident interval for \\(\\mu\\). How about the general \\((1-\\alpha)100%\\) confident interval for \\(\\mu\\) (when \\(\\sigma\\) is known)? We first introduce the requirements for constructing the interval, then provide the interval formula.\nThe requirements for estimating \\(\\mu\\) when \\(\\sigma\\) is known include\n\nüëâ The sample should be a random sample, such that all data \\(X_i\\) are drawn from the same population and \\(X_i\\) and \\(X_j\\) are independent. In fact,  any methods in this course are based on the assumption of a random sample \n\nüëâ The population standard deviation, \\(\\sigma\\), is known.\nüëâ The population is either normally distributed, \\(n &gt; 30\\) or both, i.e., \\(X_i \\sim N(\\mu, \\sigma^2)\\).  The sample size \\(n &gt; 30\\) allows the central limit theorem to be applied and hence normality is satisfied. \n\n\nThe general \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) can be borrowed from the \\(95\\%\\) confidence interval for \\(\\mu\\), \\(\\left(\\overline{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{n}},  \\overline{x} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right)\\). The 95% confidence level means \\(\\alpha = 0.05\\). For the general \\((1-\\alpha)100\\%\\) confidence interval, we just replace \\(z_{0.025}\\) with \\(z_{\\alpha/2}\\) for any \\(\\alpha\\) between zero and one. Therefore, the general \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) is\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo sum up, we provide procedures for constructing a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is known:\n\nCheck that the requirements are satisfied.\nDecide \\(\\alpha\\) or the confidence level \\((1 - \\alpha)\\).\nFind the critical value, \\(z_{\\alpha/2}\\).\nEvaluate margin of error, \\(m = z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\).\nConstruct the \\((1 - \\alpha)100\\%\\) CI for \\(\\mu\\) using the sample mean, \\(\\overline{x}\\), and margin of error, \\(m\\):\n\n\n \\[\\boxed{\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\text{  or  } \\left( \\overline{x} -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\, \\overline{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)}\\]\n\n\n Example \n\n\nSuppose we want to know the mean systolic blood pressure (SBP) of a population. Assume that the population distribution is normal and has a standard deviation of 5 mmHg. We have a random sample of 16 subjects from this population with a mean of 121.5 mmHg. Estimate the mean SBP with a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\n\nWe construct the confidence interval step by step using the procedure.\n\nRequirements:\n\n Normality is assumed, \\(\\sigma = 5\\) is known and a random sample is collected.\n\n\nDecide \\(\\alpha\\):\n\n \\(\\alpha = 0.05\\) \n\n\nFind the critical value \\(z_{\\alpha/2}\\):\n\n \\(z_{\\alpha/2} = z_{0.025} = 1.96\\) \n\n\nEvaluate margin of error \\(m = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\):\n\n \\(m = (1.96) \\frac{5}{\\sqrt{16}} = 2.45\\) \n\n\nConstruct the \\((1 - \\alpha)100\\%\\) CI:\n\n The 95% CI for the mean SBP is \\(\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = (121.5 -2.45, 121.5 + 2.45) = (119.05, 123.95)\\) \n\n\n\nBelow is a demonstration of how to find the 95% CI for SBP using R/Python\n\n\nR\nPython\n\n\n\n\n## save all information we have\nalpha &lt;- 0.05\nn &lt;- 16\nx_bar &lt;- 121.5\nsig &lt;- 5\n\n## 95% CI\n## z-critical value\n(cri_z &lt;- qnorm(p = alpha / 2, lower.tail = FALSE))  \n# [1] 1.96\n\n## margin of error\n(m_z &lt;- cri_z * (sig / sqrt(n)))  \n# [1] 2.45\n\n## 95% CI for mu when sigma is known\nx_bar + c(-1, 1) * m_z  \n# [1] 119 124\n\n\n\n\nimport numpy as np\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\n\n# Given values\nalpha = 0.05\nn = 16\nx_bar = 121.5\nsig = 5  # Population standard deviation\n\n# z-critical value\ncri_z = norm.ppf(1 - alpha / 2)\ncri_z\n# 1.959963984540054\n\ncri_z = norm.isf(alpha / 2) ## also works\ncri_z\n# 1.9599639845400545\n\n# Margin of error\nm_z = cri_z * (sig / np.sqrt(n))\nm_z\n# 2.4499549806750682\n\n# 95% Confidence Interval for mu\nx_bar + np.array([-1, 1]) * m_z\n# array([119.05004502, 123.94995498])\n\n\n\n\n\n\n\n\n\n\nConstruct a 99% CI for the mean SBP. Do you expect it to have a wider or narrower interval than the 95% CI? Why?\n\n\n\n\n\n\n Interpreting the Confidence Interval\nWe have known how to construct a confidence interval. But what on earth is that? How do we interpret the interval correctly? This is pretty important because the interval is usually misinterpreted and inappropriately used in statistical analysis. Don‚Äôt blame yourself if you find it hard to understand the meaning. The confidence interval concept is not intuitive, and it does not really answer what we care about the unknown parameter. The confidence interval is a concept in the classical or frequestist point of view. Another way of interval estimation is to use the so called credible interval that uses Bayesian philosophy. We will discuss their difference in detail in Chapter 22.\nBack to a 95% confidence interval. The following statements and interpretations are wrong. Please do not interpret the interval this way.\n\nWRONG ‚ùå ‚ÄúThere is a 95% chance/probability that the true population mean will fall between 119.1 mm and 123.9 mm.‚Äù\nWRONG ‚ùå ‚ÄúThe probability that the true population mean falls between 119.1 mm and 123.9 mm is 95%.‚Äù\n\nAlthough those statements are often what we want, they are completely wrong. Let‚Äôs learn why. The sample mean is a random variable with a sampling distribution, so it makes sense to compute a probability of it being in some interval. The population mean is unknown and FIXED, so we cannot assign or compute any probability of it. If we were using Bayesian inference Chapter 22, a different inference method, we could compute a probability associated with \\(\\mu\\) because in Bayesian statistics \\(\\mu\\) is treated as a random variable.\nSo how do we correctly interpret a confidence interval? Here is the answer.\n ‚ÄúWe are 95% confident that the mean SBP lies between 119.1 mm and 123.9 mm.‚Äù \nBut still what does ‚Äú95% confident‚Äù really mean? This means if we were able to collect our data many times and build the corresponding CIs, we would expect that about 95% of those intervals would contain the true population parameter, which, in this case, is the mean systolic blood pressure.\nRemember that \\(\\overline{x}\\) varies from sample to sample, so does its corresponding CI because the CI is a function of \\(\\overline{X}\\) given \\(n\\) and \\(\\sigma\\). This idea is shown in Figure¬†14.5. Here we do a simulation assuming \\(\\mu\\) is known at 120, and \\(\\sigma = 5\\). Also, assume we were able to repeatedly collect our sample of the same size \\(n = 16\\). Here 100 data sets are generated, and for each data set, the sample mean and its corresponding 95% CI are computed. Since the confidence level is 95%, 95% of those intervals would contain the true population parameter, 120 in this example. The 36th, 45th, 52nd, 82nd, and 99th data sets have the interval not capturing the true parameter.\n\n\n\n\n\n\n\nFigure¬†14.5: Illustration of 100 95% confidence intervals.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease keep the following ideas in mind.\n\nA 95% CI does not mean that if 100 data sets are collected, there will be exactly 95 intervals capturing \\(\\mu\\). It is a long-term sampling idea.\nWe never know with certainty that 95% of the intervals, or any single interval for that matter, contains the true population parameter because again we never know what the true value of the parameter is.\nIn reality, we usually have only one data set, and we are not able to collect more data. We have no idea of whether our 95% confidence interval capture the unknown parameter or not. We are only ‚Äú95% confident‚Äù.\n\n\n\nThe procedure of generating 100 confidence intervals for \\(\\mu\\) when \\(\\sigma\\) is known is shown in the algorithm below.\n\n\nAlgorithm\nR Simulation Result\nR Code\nPython Simulation Result\nPython Code\n\n\n\n\nAlgorithm\n\nGenerate 100 sampled data of size \\(n\\): \\((x_1^1, x_2^1, \\dots, x_n^1), \\dots (x_1^{100}, x_2^{100}, \\dots, x_n^{100})\\), where \\(x_i^m \\sim N(\\mu, \\sigma^2)\\).\nObtain 100 sample means \\((\\overline{x}^1, \\dots, \\overline{x}^{100})\\).\nFor each \\(m = 1, 2, \\dots, 100\\), compute the corresponding confidence interval \\[\\left(\\overline{x}^m - z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}, \\overline{x}^m + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmu &lt;- 120; sig &lt;- 5 \nal &lt;- 0.05; M &lt;- 100; n &lt;- 16\n\nset.seed(2024)\nx_rep &lt;- replicate(M, rnorm(n, mu, sig))\nxbar_rep &lt;- apply(x_rep, 2, mean)\nE &lt;- qnorm(p = 1 - al / 2) * sig / sqrt(n)\nci_lwr &lt;- xbar_rep - E\nci_upr &lt;- xbar_rep + E\n\nplot(NULL, xlim = range(c(ci_lwr, ci_upr)), ylim = c(0, M), \n     xlab = \"95% CI\", ylab = \"Sample\", las = 1)\nmu_out &lt;- (mu &lt; ci_lwr | mu &gt; ci_upr)\nsegments(x0 = ci_lwr, y0 = 1:M, x1 = ci_upr, col = \"navy\", lwd = 2)\nsegments(x0 = ci_lwr[mu_out], y0 = (1:M)[mu_out], x1 = ci_upr[mu_out], \n         col = 2, lwd = 2)\nabline(v = mu, col = \"#FFCC00\", lwd = 2)\n\n\n\n\n# (114.22947438766174, 125.53817008417028)\n# (0.0, 100.0)\n\n\n\n\n\n\n\n\n\n\nmu = 120\nsig = 5\nal = 0.05\nM = 100\nn = 16\n\nnp.random.seed(2024)\nx_rep = np.random.normal(loc=mu, scale=sig, size=(M, n))\nxbar_rep = np.mean(x_rep, axis=1)\nE = norm.ppf(1 - alpha / 2) * sig / np.sqrt(n)\nci_lwr = xbar_rep - E\nci_upr = xbar_rep + E\n\nfor i in range(M):\n    col = 'red' if mu &lt; ci_lwr[i] or mu &gt; ci_upr[i] else 'navy'\n    plt.plot([ci_lwr[i], ci_upr[i]], [i, i], color=col, lw=1.5)\nplt.xlim([min(ci_lwr), max(ci_upr)])\nplt.ylim([0, M])\nplt.xlabel(\"95% CI\")\nplt.ylabel(\"Sample\")\nplt.axvline(mu, color=\"#FFCC00\", lw=2)\nplt.show()\n\n\n\n\n\n14.3.1 Reducing margin of error and determining sample size*\nWe learn that the margin of error is \\(E = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\). Of course we prefer small margin of error for more estimation accuracy and precision. From its formula, we can see that there are three ways to reduce the margin of error:\n\nReduce \\(\\sigma\\)\n\nIncrease \\(n\\)\n\nReduce \\(z_{\\alpha/2}\\)\n\n\nHowever, given a confidence level \\(1 - \\alpha\\) and known \\(\\sigma\\), we can reduce the margin of error only by increasing sample size \\(n\\). Due to high sampling costs, we like to find the minimum sample size needed to get a desired margin of error.\nWhat we can do is to rewrite the margin of error, and represent \\(n\\) as a function of \\(E\\), \\(\\alpha\\), and \\(\\sigma\\). Once all three are given, we know the how large the sample size is.\n\\(E = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}} \\iff \\frac{1}{\\sqrt{n}} = \\frac{E}{z_{\\alpha/2}\\sigma}\n\\iff \\sqrt{n} = \\frac{z_{\\alpha/2}\\sigma}{E} \\iff n = \\left(\\frac{z_{\\alpha/2}\\sigma}{E}\\right)^2\\)\nClearly, to get the desired margin of error, say \\(E_d\\) the sample size should be at least \\(\\left(\\frac{z_{\\alpha/2}\\sigma}{E_d}\\right)^2\\) because \\(E\\) and \\(n\\) inversely proportional to each other.\n\n Example \nState tax advisory board wants to estimate the mean household income with a margin of error of $1,000 with 99% confidence level. Assume that the population standard deviation is $10,000. How many households they need to sample?\n\n\\(E = 1000\\), \\(\\alpha = 0.01\\) and \\(z_{0.005} = 2.58\\), \\(\\sigma = 10000\\)\n\\(n = \\left(\\frac{z_{\\alpha/2}\\sigma}{E}\\right)^2 = \\left(\\frac{(2.58)(10000)}{1000}\\right)^2 = 656.7\\)\nThey need sample size 657.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Point and Interval Estimation</span>"
    ]
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "title": "14¬† Point and Interval Estimation",
    "section": "\n14.4 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "14.4 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown\nWe complete the discussion of confidence intervals for \\(\\mu\\) when \\(\\sigma\\) is known. Do you see anything unreasonable? Do you think that assuming \\(\\sigma^2\\) is known is reasonable? In fact, the population variance is calculated as \\(\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}\\), where \\(N\\) is the population size. The formula involves \\(\\mu\\), the unknown parameter we‚Äôd like to estimate. It‚Äôs rare that we don‚Äôt know \\(\\mu\\) but know \\(\\sigma\\). What do we do if \\(\\sigma\\) is unknown?\nWhen \\(\\sigma\\) is unknown to us, we cannot use normal distribution anymore. Instead, we use the Student‚Äôs t distribution (or \\(t\\)-distribution) to construct a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown. To construct these confidence intervals we still need\n\nA random sample\nA population that is normally distributed and/or \\(n &gt; 30\\).\n\nThe confidence interval when \\(\\sigma\\) is known includes \\(\\sigma\\) in the formula. When \\(\\sigma\\) is unknown, we cannot use the formula and need to find \\(\\sigma\\)‚Äôs surrogate.\n\n\n\n\n\n\nWhat is a natural estimator for the unknown \\(\\sigma\\)?\n\n\n\n\n\n\nWhen \\(\\sigma\\) is unknown, we use the sample standard deviation, \\(S = \\sqrt{\\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{n-1}}\\), instead when constructing the CI.\n\n Student‚Äôs t Distribution \nIf the population is normally distributed or \\(n &gt; 30\\), we know \\(\\overline{X}\\) is exactly or approximately \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\). Therefore \\(Z = \\frac{\\overline{X} - \\mu}{\\color{red}\\sigma/\\sqrt{n}} \\sim N(0, 1)\\). Now if \\(\\sigma\\) is replaced with its surrogate \\(S\\), then the new random variable say \\(T\\) will be student‚Äôs t distributed with the degrees of freedom (df) \\(n-1\\):\n \\[T = \\frac{\\overline{X} - \\mu}{\\color{red}S/\\sqrt{n}} \\sim t_{n-1}\\]  \nHere the degrees of freedom is the parameter of the student‚Äôs t distribution.\n Properties \nThe student‚Äôs t distribution, as shown in Figure¬†14.6, looks pretty similar to the standard normal distribution, but they are different. Some of the properties of the student‚Äôs t distribution are listed below.\n\nFor any degrees of freedom, the student‚Äôs t distribution is symmetric about the mean 0 and bell-shaped like \\(N(0, 1)\\).\nFor any degrees of freedom, the student‚Äôs t distribution has more variability than \\(N(0, 1)\\), meaning that the distribution has heavier tails and lower peak.\nThe the student‚Äôs t distribution has less variability for larger degrees of freedom (sample size).\nAs \\(n \\rightarrow \\infty\\) \\((df \\rightarrow \\infty)\\), the student‚Äôs t distribution approaches \\(N(0, 1)\\).\n\n\n\n\n\n\n\n\nFigure¬†14.6: Student t distributions with various degrees of freedom.\n\n\n\n\n Critical Values of \\(t_{\\alpha/2, n-1}\\) \nIn the CI formula with known \\(\\sigma\\), we use the critical value \\(z_{\\alpha/2}\\), the standard normal value so that \\(P(Z &gt; z_{\\alpha/2}) = \\alpha/2\\). When \\(\\sigma\\) is unknown, we use \\(t_{\\alpha/2, n - 1}\\) as the critical value, instead of \\(z_{\\alpha/2}\\). Notice that the standard normal has nothing to do with \\(\\mu\\) and \\(\\sigma\\) of a general \\(N(\\mu, \\sigma^2)\\) distribution. 1 Therefore no parameter is attached to \\(z_{\\alpha/2}\\). However, the \\(t\\) critical value \\(t_{\\alpha/2, n - 1}\\) changes with the degrees of freedom \\(n - 1\\). With the same logic, the critical value \\(t_{\\alpha/2, n - 1}\\) is a Student‚Äôs t value with degrees of freedom \\(n - 1\\) so that \\(P(T_{n-1} &gt; t_{\\alpha/2, n - 1}) = \\alpha/2\\) as shown in Figure¬†14.7.\n\n\n\n\n\n\n\nFigure¬†14.7: Illustration of critical value for Student t distribution.\n\n\n\n\n\n\n\n\n\n\nWith the same \\(\\alpha\\), is \\(t_{\\alpha, n-1}\\) or \\(z_{\\alpha}\\) larger?\n\n\n\n\n\n\nYou should be able to answer this question based on the fact that for any degrees of freedom, the student‚Äôs t distribution has more variability than \\(N(0, 1)\\), or heavier tails. The heavier tail forces \\(t_{\\alpha/2, n-1}\\) to be more extreme than \\(z_{\\alpha/2}\\). Figure¬†14.8 illustrates this fact. The red \\(t_{df = 2}\\) distribution has heavier tails than the black standard normal distribution.\n\n\n\n\n\n\n\nFigure¬†14.8: z and t critical values.\n\n\n\n\nThe table below shows \\(z\\) and \\(t\\) critical values at confidence level 90%, 95% and 99%. The \\(t\\) values are getting closer to the \\(z\\) values as the degree of freedom increases. When the degree of freedom goes to infinity, the \\(t\\) values converge to \\(z\\) values.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58\n\n\n\n\n\n\n \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown \nWe have been equipped with everything we need for constructing \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown. The interval is  \\[\\left(\\overline{x} - t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}, \\overline{x} + t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\right)\\] \nThe interval form is the same as before. We have the sample mean plus and minus the margin of error. Comparing to the interval with known \\(\\sigma\\), the difference is that \\(z_{\\alpha/2}\\) is replaced with \\(t_{\\alpha/2, n-1}\\), and \\(\\sigma\\) is replaced with \\(s\\).\nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} &gt; z_{\\alpha/2}\\), leading to a wider interval if \\(s\\) is not too smaller than the true \\(\\sigma\\). The intuition is that we are more uncertain when doing inference about \\(\\mu\\) because we don‚Äôt have information about both \\(\\mu\\) and \\(\\sigma\\), and replacing \\(\\sigma\\) with \\(s\\) adds additional uncertainty.\n\n\n\n\n\n\n\nR\nPython\n\n\n\nBack to the systolic blood pressure (SBP) example. We have \\(n=16\\) and \\(\\overline{x} = 121.5\\). Estimate the mean SBP with a 95% confidence interval with unknown \\(\\sigma\\) and \\(s = 5\\). The code for the \\(t\\) interval is pretty similar to the \\(z\\) interval. The main difference is that we are gonna use qt() to find a quantile or critical value from the Student‚Äôs t distribution. In the function, the first argument is still the given probability, then we must specify the degrees of freedom, otherwise R cannot determine which \\(t\\)-distribution is being considered, and will render an error message.\n\nalpha &lt;- 0.05\nn &lt;- 16\nx_bar &lt;- 121.5\ns &lt;- 5  ## sigma is unknown and s = 5\n\n## t-critical value\n(cri_t &lt;- qt(p = alpha / 2, df = n - 1, lower.tail = FALSE)) \n# [1] 2.13\n\n## margin of error\n(m_t &lt;- cri_t * (s / sqrt(n)))  \n# [1] 2.66\n\n## 95% CI for mu when sigma is unknown\nx_bar + c(-1, 1) * m_t  \n# [1] 119 124\n\n\n\nBack to the systolic blood pressure (SBP) example. We have \\(n=16\\) and \\(\\overline{x} = 121.5\\). Estimate the mean SBP with a 95% confidence interval with unknown \\(\\sigma\\) and \\(s = 5\\). The code for the \\(t\\) interval is pretty similar to the \\(z\\) interval. The main difference is that we are gonna use t.ppf() (or t.isf()) to find a quantile or critical value from the Student‚Äôs t distribution. In the function, the first argument is still the given probability, then we must specify the degrees of freedom, otherwise Python cannot determine which \\(t\\)-distribution is being considered, and will render an error message.\n\nalpha = 0.05\nn = 16\nx_bar = 121.5\ns = 5  # Sample standard deviation (sigma unknown)\n\nfrom scipy.stats import t\n## t-critical value\ncri_t = t.ppf(1 - alpha/2, df=n-1)\ncri_t\n# 2.131449545559323\n\n## margin of error\nm_t = cri_t * (s / np.sqrt(n))\nm_t\n# 2.664311931949154\n\n## 95% CI for mu when sigma is unknown\nx_bar + np.array([-1, 1]) * m_t  \n# array([118.83568807, 124.16431193])\n\n\n\n\n\\(z_{0.025} = 1.96 &lt; t_{0.025, 15} = 2.13\\). The interval is wider with \\(s = 5\\).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Point and Interval Estimation</span>"
    ]
  },
  {
    "objectID": "infer-ci.html#summary",
    "href": "infer-ci.html#summary",
    "title": "14¬† Point and Interval Estimation",
    "section": "\n14.5 Summary",
    "text": "14.5 Summary\nTo conclude this chapter, a table that summarizes the confidence interval for \\(\\mu\\) is provided.\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\) unknown\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\n\nPopulation Mean \\(\\mu\\)\n\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\nRemember to check if the population is normally distributed and/or \\(n&gt;30\\). What if the population is not normal and \\(n \\le 30\\)? We could use a simulation-based approach, for example bootstrapping discussed in Chapter 15.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Point and Interval Estimation</span>"
    ]
  },
  {
    "objectID": "infer-ci.html#exercises",
    "href": "infer-ci.html#exercises",
    "title": "14¬† Point and Interval Estimation",
    "section": "\n14.6 Exercises",
    "text": "14.6 Exercises\n\n\nHere are summary statistics for randomly selected weights of newborn boys: \\(n =207\\), \\(\\bar{x} = 30.2\\)hg (1hg = 100 grams), \\(s = 7.3\\)hg.\n\nCompute a 95% confidence interval for \\(\\mu\\), the mean weight of newborn boys.\nIs the result in (a) very different from the 95% confidence interval if \\(\\sigma = 7.3\\)?\n\n\nA 95% confidence interval for a population mean \\(\\mu\\) is given as (18.635, 21.125). This confidence interval is based on a simple random sample of 32 observations. Calculate the sample mean and standard deviation. Assume that all conditions necessary for inference are satisfied. Use the t-distribution in any calculations.\nA market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation of savings is $95. He wants to collect data such that he can get a margin of error of no more than $12 at a 95% confidence level. How large of a sample should he collect?\n\nThe 95% confidence interval for the mean rent of one bedroom apartments in Chicago was calculated as ($2400, $3200).\n\nInterpret the meaning of the 95% interval.\nFind the sample mean rent from the interval.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Point and Interval Estimation</span>"
    ]
  },
  {
    "objectID": "infer-ci.html#footnotes",
    "href": "infer-ci.html#footnotes",
    "title": "14¬† Point and Interval Estimation",
    "section": "",
    "text": "The standard normal random variable \\(Z \\sim N(0, 1)\\) is a pivotal quantity (or pivot) because it is independent of parameters \\(\\mu\\) and \\(\\sigma\\).‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>14</span>¬† <span class='chapter-title'>Point and Interval Estimation</span>"
    ]
  },
  {
    "objectID": "infer-bt.html",
    "href": "infer-bt.html",
    "title": "15¬† Bootstrapping",
    "section": "",
    "text": "15.1 Idea of Bootstrapping\nBootstrapping is a resampling method that repeatedly generates the bootstrap samples from the data at hand with replacement. Each bootstrap sample data are of the same size of the original observed data. For each bootstrap sample, we calculate our interested statistic, for example the (bootstrap) sample mean. Then the collection of the sample means calculated from the bootstrap samples forms the sampling distribution of the sample mean. Therefore, bootstrapping models how a statistic varies from one sample to another taken from the population. This approach provides insights into how much the statistic deviates from the parameter of interest.\nThe main idea of bootstrapping is that\nLet‚Äôs figure out the idea behind it step by step. Suppose there is a target population, and we want to estimate its mean \\(\\mu\\) possibly with uncertainty quantification. Usually our first step is to collect our sample data from the population, and use the sample mean \\(\\bar{x}\\) to estimate \\(\\mu\\). Due to resources constraint, we may only be able to collect one single data set, and the sample size can‚Äôt be large. We hope the data set is quite representative of the population, so that \\(\\bar{x}\\) is quite close to \\(\\mu\\). In other words, if we were expanding the sample data proportionally, having the sample size same as the population size, the population and the expanded sample data should look very alike. Figure¬†15.1 illustrate this idea.\nFigure¬†15.1: Source: Figure 12.1 of Introduction to Modern Statistics\nHowever, due to the sampling variability, we never know whether or not the one we collect is representative enough. Even we know the data at hand looks pretty similar to the population, we are not sure how much we are confident about the estimation, unless we have lots of sample data replicates.\nFigure¬†15.2: Repeatedly generating sample data from the target population. This is often not feasible in many scientific experiments and studies.\nBy taking repeated samples from the population, the variability from sample to sample can be observed. In Figure¬†15.2 the repeated samples are obviously different both from each other and from the original population. The samples were taken from the same population, and the differences are due entirely to natural variability in the sampling procedure. By summarizing each of the samples (sample mean), we see, directly, the variability of the sample mean from sample to sample. This is exactly the idea of sampling distribution we learn in Chapter 12 illustrated in Figure¬†12.2. The problem again is that quite often it is difficult to obtain sample replicates, and quantify the variability. How do we solve this problem? Smart statisticians came up with a solution: apply exactly the same idea just mentioned to the one single sample data set we have at hand!\nWe can think that our sample data is a small version of the target population. The illustration of bootstrapping is shown in Figure¬†15.3. Suppose the collected sample data from some population has 10 data points. To have sample data replicates, the data work as if our population, and the new sample replicates are generated from the original data set. These new data replicates are called bootstrap samples. The bootstrap samples must have the same sample size as the original sample data. So they all have 10 data points too. Additionally, these samples are drawn from the original data with replacement. Think about it, if they are drawn without replacement, all the bootstrap samples will be exactly the same as the original sample, which does not work for accounting sampling variability. Note that the elements in the bootstrap samples are already collected and saved in our original data. The resampling step does not really conduct any new experiments, perform any new trials, or do any new survey as sampling from the target population does that may be too costly for our study. We save our study by our bootstraps.\nFigure¬†15.3: Illustration of boostrapping: Resampling from the sample data with replacement. Each sample has 15 data points, same as the original sample.\nThe original sample in Figure¬†15.3 has 1 blacks, 2 reds, 3 greens, 4 blues, and 5 yellows. If the sample is representative of the target population, the color distribution in the population should more or less follow the color distribution in the data, although not exactly the same with pretty high chance. To mimic the sampling variability found by collecting data from a population, sampling with replacement produces some variations while keeping the general distribution pattern shown in the data. As a results, the bootstrap samples tend to have more yellows and blues, but could have more reds or greens. It is such resampling variation that creates the sampling variability in the computed bootstrap statistics \\(\\bar{x}_b^1, \\dots, \\bar{x}_b^M\\).\nThe entire workflow of sampling and resampling is illustrated in Figure¬†15.4. First, the original sample is collected from the target population. Usually we only have one data set. To ensure that the data look similar to the population, we hope the sample is a random sample. Next, in the resampling step, we draw samples with replacement, replicating several bootstrap samples. Then for each bootstrap sample, the desired statistic is computed. Finally, the collection of all the bootstrap statistics forms a bootstrap distribution that is used to approximate the sampling distribution of the desired statistic. With the distribution, we now can create the bootstrap confidence interval for the target population parameter.\nFigure¬†15.4: Source: Bootstrapping (statistics) Wiki.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "infer-bt.html#idea-of-bootstrapping",
    "href": "infer-bt.html#idea-of-bootstrapping",
    "title": "15¬† Bootstrapping",
    "section": "",
    "text": "We view the observed data as the target population\n\n\nWe treat the bootstrap samples as the replicates of the samples from the population\n\n\nWe use the collection of bootstrap samples to mimic the collection of samples generated from the target population, which is the sampling distribution of the sample statistic.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "infer-bt.html#bootstrap-confidence-interval",
    "href": "infer-bt.html#bootstrap-confidence-interval",
    "title": "15¬† Bootstrapping",
    "section": "\n15.2 Bootstrap Confidence Interval",
    "text": "15.2 Bootstrap Confidence Interval\nIn this section we learn to create a bootstrap confidence interval using a real data set. This example is borrowed from STA 199 by Dr.¬†√áetinkaya-Rundel.\nHow much do you think it costs to rent a typical one bedroom apartment in Manhattan NYC? Well if you have no idea, at least we can say we are 100% confident that the average one-bedroom rent in Manhattan is between 0 to one million dollars. It‚Äôs not helping though. Let‚Äôs have some data, and get an idea of the rent in Manhattan NYC. The data manhattan.csv consider 20 one-bedroom apartments that were randomly selected on Craigslist Manhattan from apartments listed as ‚Äúby owner‚Äù.\n\n\nR\nPython\n\n\n\n\nmanhattan &lt;- read.csv(\"./data/manhattan.csv\")\nmanhattan$rent\n#  [1] 3850 3800 2350 3200 2150 3267 2495 2349 3950 1795 2145 2300 1775 2000 2175\n# [16] 2350 2550 4195 1470 2350\nrange(manhattan)\n# [1] 1470 4195\n\nThe minimum rent is $1470 and the rent can be as high as near $4200. 1\n\n\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nmanhattan = pd.read_csv(\"./data/manhattan.csv\")\nmanhattan['rent']\n# 0     3850\n# 1     3800\n# 2     2350\n# 3     3200\n# 4     2150\n# 5     3267\n# 6     2495\n# 7     2349\n# 8     3950\n# 9     1795\n# 10    2145\n# 11    2300\n# 12    1775\n# 13    2000\n# 14    2175\n# 15    2350\n# 16    2550\n# 17    4195\n# 18    1470\n# 19    2350\n# Name: rent, dtype: int64\nmanhattan.min()\n# rent    1470\n# dtype: int64\nmanhattan.max()\n# rent    4195\n# dtype: int64\n\nThe minimum rent is $1470 and the rent can be as high as near $4200. 2\n\n\n\n\n\n\n\n\n\n\n\nIf we want to estimate the (population) mean one-bedroom rent in Manhattan, an easy point estimate is the sample mean, which is about $ 2625.8. However, considering the entire rental housing market in Manhattan, this sample is so small, so we are not that convinced of that number, and want a range of plausible mean one-bedroom rent for us to decide whether we rent a bedroom there.\n\n\n\n\n\n\n\n\n\n\nSample median = $2625.8\n\n\n\n\n\n\n\n\n\nPopulation median = ‚ùì\n\n\nThe question is what‚Äôs our bootstrap population look like that we can sample from? Well the idea is that we assume that there are probably more apartments like the ones in our observed sample in the population as well. So here, basically the bootstrap artificial population is made from our sample data, and the population is like so many replicates of our given sample. Again, this is an artificial population, not the real population. If I had the real population, no inference or estimation is needed. We know the truth. Right?\n\n Bootstrapping using R/Python \n\n\nR\nPython\n\n\n\nThere are several R packages out there to help us do bootstrapping without the need to write long lines of code. Here we use the boot() function in the boot package for demonstration. Before calling boot, we still need to write a function that returns the statistic(s) that you would like to bootstrap. The first argument passed to the function should be your dataset. The second argument can be an index vector of the observations in your dataset to use or a frequency or weight vector that informs the sampling probabilities. For example, if our statistic is sample mean, we can create a function mean_fcn() as follows.\n\nmean_fcn &lt;- function(data, index) {\n    data_ind &lt;- data[index]\n    return(mean(data_ind))\n}\n\nThe function compute the mean of resampled data. The index argument is for resampling our data. It tells us which data points in the original data will be chosen to create a bootstrap sample. We don‚Äôt need to generate the index vector ourselves, and the boot() function will use the function like sample() to generate it for us.\nWe can now use the boot() command, providing our dataset name, our function, and the number of bootstrap samples to be drawn.\n\nlibrary(boot)\nset.seed(2024)\nboot_mean &lt;- boot(data = manhattan$rent, statistic = mean_fcn, R = 10000)\n\nThe three required arguments of boot() are data, statistic, and R. We specify the data as a vector, matrix or data frame. Here manhattan$rent is a vector. The argument statistic is a function which when applied to data returns a vector containing the statistic(s) of interest. This is what mean_fcn is doing. The argument R is the number of bootstrap replicates. We generate 10000 bootstrap samples, and 10000 corresponding bootstrap sample means. The boostrapping result is saved in the object boot_mean which is a list. boot_mean$t0 saves the sample mean from the original data, and boot_mean$t saves the 10000 bootstrap sample means. The bias shown in the printed output comes from mean(boot_mean$t) - boot$t0, i.e., the difference between the mean of bootstrap sample means and the original sample mean. This is an estimate of the difference between the original sample mean and the population mean. The std. error is derived from sd(boot_mean$t) that estimates the variance of the sample mean.\n\nboot_mean\n# \n# ORDINARY NONPARAMETRIC BOOTSTRAP\n# \n# \n# Call:\n# boot(data = manhattan$rent, statistic = mean_fcn, R = 10000)\n# \n# \n# Bootstrap Statistics :\n#     original  bias    std. error\n# t1*     2626   -1.76         175\n\n\nmean(boot_mean$t) - boot_mean$t0\n# [1] -1.76\n\n\nsd(boot_mean$t)\n# [1] 175\n\nTo show the bootstrap sampling distribution of sample mean (one-bedroom rent in Manhattan), we can simply use plot() command:\n\nplot(x = boot_mean)\n\n\n\n\n\n\n\nUsing the boot.ci command, we can generate several types of confidence intervals from our bootstrap samples. By default, the function provides four different 95% confidence intervals.\n\nboot.ci(boot.out = boot_mean)\n# BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n# Based on 10000 bootstrap replicates\n# \n# CALL : \n# boot.ci(boot.out = boot_mean)\n# \n# Intervals : \n# Level      Normal              Basic         \n# 95%   (2285, 2970 )   (2273, 2955 )  \n# \n# Level     Percentile            BCa          \n# 95%   (2297, 2978 )   (2326, 3015 )  \n# Calculations and Intervals on Original Scale\n\nWe don‚Äôt need to worry too much about which interval we should use at this moment. When the bootstrap distribution is symmetric and looks like Gaussian, those intervals are pretty similar. Second, those intervals are not exact but approximate by its nature. They vary every time we run a new bootstrapping (unless we use the same random seed). We accept this random variation. The point is, this range of plausible values gives us a general idea of how much the mean one-bedroom rent is.\nCheck UCLA tutorial for more details about implementing bootstrapping using the boot package.\n\n\nIn Python we use the function bootstrap() in the scipy.stats module to do bootstrapping without the need to write long lines of code.\n\nfrom scipy.stats import bootstrap\n\nThe first argument passed to the function should be our data. Note that # the samples must be in a sequence. A way to create such sequence, as suggested in the documentation, is creating a tuple like data = (manhattan['rent'].values,).\nThe second argument is the statistic for which the confidence interval is to be calculated. It is a callable or function. We use np.mean function to construct the bootstrapped confidence interval for the mean.\nn_resamples is the number of resamples performed to form the bootstrap distribution of the statistic.\nThere are three methods to be considered: ‚Äòpercentile‚Äô, ‚Äòbasic‚Äô, ‚Äòbca‚Äô.\nWe set the random number generator with the random seed 2024.\nThe boostrapping result is saved in the object boot_mean. boot_mean.bootstrap_distribution saves the 10000 bootstrap sample means.\n\nrng = np.random.default_rng(2024)\n# samples must be in a sequence\ndata = (manhattan['rent'].values,)\nboot_mean = bootstrap(data=data, statistic=np.mean, \n                n_resamples=10000, method='percentile', \n                random_state=rng)\nboot_mean\n# BootstrapResult(confidence_interval=ConfidenceInterval(low=2303.3475, high=2974.3762499999993), bootstrap_distribution=array([2678.05, 2336.  , 2974.35, ..., 2426.75, 2393.15, 2400.75]), standard_error=172.1321792859602)\n\nThe bias, the difference between the mean of bootstrap sample means and the original sample mean, can be calculated:\n\n## bias\nnp.mean(boot_mean.bootstrap_distribution) - np.mean(manhattan['rent'])\n# 0.6565749999999753\n\nThis is an estimate of the difference between the original sample mean and the population mean.\nThe boot_mean.standard_error is derived from np.std(boot_mean.bootstrap_distribution) that estimates the variance of the sample mean.\n\n## standard error\nboot_mean.standard_error\n# 172.1321792859602\nnp.std(boot_mean.bootstrap_distribution, ddof=1)\n# 172.1321792859602\n\nTo show the bootstrap sampling distribution of sample mean (one-bedroom rent in Manhattan), we can simply use plt.hist() command:\n\nplt.hist(boot_mean.bootstrap_distribution)\n\n\n\n\n\n\n\nThe confidence interval results is saved in boot_mean.confidence_interval. By default, the confidence level is 95%.\n\n# Display the 95% confidence interval\nboot_mean.confidence_interval\n# ConfidenceInterval(low=2303.3475, high=2974.3762499999993)\n\n\n\n\n\n\nAlgorithm\nR Simulation Result\nR Code\nPython Simulation Result\nPython Code\n\n\n\n\nBootstrapped confidence interval\n\n\nTake a bootstrap sample\n\na random sample taken with replacement from the original sample, of the same size as the original sample.\n\n\n\nCalculate the bootstrap statistic\n\na statistic such as mean, median, proportion, slope, etc. computed on the bootstrap samples.\n\n\n\nRepeat steps (1) and (2) many times to create a bootstrap distribution\n\na distribution of bootstrap statistics.\n\n\n\nCalculate the bounds of the \\((1-\\alpha)100\\%\\) confidence interval\n\nthe middle \\((1-\\alpha)100\\%\\) of the bootstrap distribution\n\n\n\n\n\n\n\n# \n# ORDINARY NONPARAMETRIC BOOTSTRAP\n# \n# \n# Call:\n# boot(data = manhattan$rent, statistic = mean_fcn, R = 10000)\n# \n# \n# Bootstrap Statistics :\n#     original  bias    std. error\n# t1*     2626   -1.76         175\n# BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n# Based on 10000 bootstrap replicates\n# \n# CALL : \n# boot.ci(boot.out = boot_mean)\n# \n# Intervals : \n# Level      Normal              Basic         \n# 95%   (2285, 2970 )   (2273, 2955 )  \n# \n# Level     Percentile            BCa          \n# 95%   (2297, 2978 )   (2326, 3015 )  \n# Calculations and Intervals on Original Scale\n\n\n\n\nmean_fcn &lt;- function(data, index) {\n    data_ind &lt;- data[index]\n    return(mean(data_ind))\n}\nlibrary(boot)\nset.seed(2024)\nboot_mean &lt;- boot(data = manhattan$rent, statistic = mean_fcn, R = 10000)\nboot_mean\nboot.ci(boot.out = boot_mean)\n\n\n\n\n# BootstrapResult(confidence_interval=ConfidenceInterval(low=2303.3475, high=2974.3762499999993), bootstrap_distribution=array([2678.05, 2336.  , 2974.35, ..., 2426.75, 2393.15, 2400.75]), standard_error=172.1321792859602)\n# ConfidenceInterval(low=2303.3475, high=2974.3762499999993)\n\n\n\n\n\nrng = np.random.default_rng(2024)\nboot_mean = bootstrap((manhattan['rent'].values,), np.mean,\n                       n_resamples=10000, method='percentile',\n                       random_state=rng)\nboot_mean\nboot_mean.confidence_interval\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are two sources of error in bootstrap inference:\n\nThe error induced by using a particular sample to represent the population. If the sample data do not represent the population well, any bias caused by that will remain in bootstrapping. Unfortunately this error cannot be corrected.\nThe sampling error produced by failing to enumerate all bootstrap samples. When sample size is \\(n\\), if we distinguish the order of elements in the bootstrap samples and treat all of the elements of the original sample as distinct (even when some have the same values) then there are \\(n^n\\) bootstrap samples, each occurring with probability \\(1/n^n\\). This source of error can be controlled by making the number of bootstrap replications sufficiently large.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "infer-bt.html#further-readings-and-references",
    "href": "infer-bt.html#further-readings-and-references",
    "title": "15¬† Bootstrapping",
    "section": "\n15.3 Further Readings and References",
    "text": "15.3 Further Readings and References\n\nIMS chapter 12\nSTA 199 course website\nBootstrapping regression models\nUCLA tutorial",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "infer-bt.html#footnotes",
    "href": "infer-bt.html#footnotes",
    "title": "15¬† Bootstrapping",
    "section": "",
    "text": "The housing market in the U.S. has been going crazy, and the rent this year of course is much higher than the rent shown in this data set.‚Ü©Ô∏é\nThe housing market in the U.S. has been going crazy, and the rent this year of course is much higher than the rent shown in this data set.‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>15</span>¬† <span class='chapter-title'>Bootstrapping</span>"
    ]
  },
  {
    "objectID": "infer-ht.html",
    "href": "infer-ht.html",
    "title": "16¬† Hypothesis Testing",
    "section": "",
    "text": "16.1 Introduction\nWhat is Hypothesis Testing?\nIn statistics, a hypothesis is a claim or statement about a property of a population, often the value of a population distribution parameter. For example,\nYou can see that we usually focus on claims about a population distribution parameter.\nThe null hypothesis, denoted \\(H_0\\), is a statement that the value of a parameter is equal to some claim value, or the negation of the alternative hypothesis that will be discussed in a minute. Often \\(H_0\\) represents a skeptical perspective or a claim to be tested, or the current status of the parameter. For example, the claim ‚Äúthe percentage of Marquette female students loving Japanese food is equal to 80%‚Äù is a \\(H_0\\) claim because of the key word ‚Äúequal‚Äù. Usually we are not very convinced that the \\(H_0\\) claim is true, and in our analysis we want to test the claim, and see whether the evidence and information we collect is strong enough to make a conclusion that the percentage is not equal to 80%.\nThe alternative hypothesis, denoted \\(H_1\\) or \\(H_a\\), is a claim that the parameter is less than, greater than or not equal to some value. It is usually our research hypothesis of some new scientific theory or finding. If we think the percentage of Marquette female students loving Japanese food is greater than 80%, this hypothesis is the \\(H_1\\) claim. If after a formal testing procedure, we conclude that the percentage is greater than 80%, we sort of make a new research discovery that overturns the previous claim or status quo that the percentage is equal to 80%.\nLet‚Äôs do one more exercise. Is the statement ‚ÄúOn average, Marquette students consume less than 3 drinks per week.‚Äù a \\(H_0\\) or \\(H_1\\) claim? Because of the key word ‚Äúless than‚Äù, it is a \\(H_1\\) claim.\nSo what is hypothesis testing? Hypothesis testing 1 is a procedure to decide whether or not to reject \\(H_0\\) based on how much evidence there is against \\(H_0\\). If the evidence is strong enough, we reject \\(H_0\\) in favor of \\(H_1\\).\nExample\nBefore we jump into the formal hypothesis testing procedure, let‚Äôs talk about a criminal charge example. How a criminal is convicted is similar to the formal testing procedure.\nIf we want to make a claim about whether the person is guilty or not, what are our \\(H_0\\) and \\(H_1\\)? Remember that the null hypothesis represents a skeptical perspective or a claim to be tested, or the current status of the parameter, so we have\nThis is how we write a hypothesis: start with \\(H_0:\\) followed by the statement. Being not guilty is the default status quo of anyone, although the jury may doubt or be skeptical of the person being not guilty. The prosecutors and police detectives are trying their best the collect enough strong evidence to proof beyond a reasonable doubt to the jury. Therefore the alternative hypothesis is\nIn the example, the evidence could be  photos, videos, witnesses, fingerprints, DNA, and so on . How do we decide to keep \\(H_0\\) or to accept \\(H_1\\)? After all evidence including defense attorney and prosecutor‚Äôs arguments are presented to the jury, the decision rule is the  jury‚Äôs voting . Finally, to close the case, we need a conclusion that is the verdict  ‚Äúguilty‚Äù  or  ‚ÄúNot enough evidence to convict‚Äù .\nPlease go through the entire criminal charge process again:\n\\(H_0\\) and \\(H_a\\) =&gt; Evidence =&gt; Decision rule =&gt; Conclusion\nThe process is quite similar to the formal procedure for a hypothesis testing.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-ht.html#introduction",
    "href": "infer-ht.html#introduction",
    "title": "16¬† Hypothesis Testing",
    "section": "",
    "text": "The mean body temperature of humans is less than \\(98.6^{\\circ}\\) F.  Here the mean body temperature is a property or characteristic of target population human beings. We can turn the verbal claim into a brief mathematical expression \\(\\mu &lt; 98.6\\).\n Marquette students‚Äô IQ scores has standard deviation equal to 15.  The IQ score standard deviation is a characteristic of the population Marquette students. Mathematically, we can write the claim as \\(\\sigma = 15\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose a person is charged with a crime, and a jury will decide whether the person is guilty or not. We all know the rule: Even though the person is charged with the crime, at the beginning of the trial, the accuse is assumed to be innocent until the jury declares otherwise. Only if overwhelming evidence of the person‚Äôs guilt can be shown is the jury expected to declare the person guilty, otherwise the person is considered not guilty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(H_0:\\) The person is  not guilty  üôÇ\n\n\n\n\n\\(H_1:\\) The person is  guilty  üòü",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "href": "infer-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "title": "16¬† Hypothesis Testing",
    "section": "\n16.2 How to Formally Do a Statistical Hypothesis Testing",
    "text": "16.2 How to Formally Do a Statistical Hypothesis Testing\nThe entire hypothesis testing can be wrapped up in the following six steps. No worries if you don‚Äôt have any idea of it. We will learn this step by step using a test for the population mean \\(\\mu\\).\n\nStep 0: Check Method Assumptions\nStep 1: Set the \\(H_0\\) and \\(H_a\\) in Symbolic Form from a Claim\nStep 2: Set the Significance Level \\(\\alpha\\)\nStep 3: Calculate the Test Statistic (Evidence)\n\n\n\nDecision Rule I: Critical Value Method\n\n Step 4-c: Find the Critical Value \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n\nDecision Rule II: P-Value Method\n\n Step 4-p: Find the P-Value \n Step 5-p: Draw a Conclusion Using P-Value Method \n\n\n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\n\nLet‚Äôs look at this example: Is the New Treatment Effective?\n\n\nA population of patients with hypertension is normal and has mean blood pressure (BP) of 150. After 6 months of treatment, the BP of 25 patients from this population was recorded. The sample mean BP is \\(\\overline{x} = 147.2\\) and the sample standard deviation is \\(s = 5.5\\).\n\n\n\n\n\n\nSource: https://unsplash.com/photos/i1iqQRLULlg\n\n\n\n\n\n\nOur goal is to determine whether a new treatment is effective in reducing BP. Let‚Äôs learn the testing procedure step by step using this example.\n\n\n Step 0: Check Method Assumptions \nAny statistical method is based on some assumptions. To use the method, and analyze our data appropriately, we have to make sure that the assumptions are satisfied. In this book, most of the distribution-based methods require\n\nRandom sample\nThe population is normally distributed and/or the sample size \\(n &gt; 30\\).\n\n\n\n\n\nSource: https://www.pinterest.ph/pin/633387417082544/\n\n\n\n\n Example Step 0: Check Method Assumptions \n\nFrom the question description,  A population of hypertension group is normal .\n\n\n Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \nThe first step of testing is to understand the \\(H_0\\) and \\(H_1\\) claims, and express them using mathematically using population parameters. The followings provdie three examples.\n\nüßë‚Äçüè´ The mean IQ score of statistics professors is higher than 120.\n\n \\(\\begin{align}&H_0: \\mu \\le 120 \\\\ &H_1: \\mu &gt; 120 \\end{align}\\) \n\n\nüíµ The mean starting salary for Marquette graduates who didn‚Äôt take MATH 4720 is less than $60,000.\n\n \\(\\begin{align} &H_0: \\mu \\ge 60000 \\\\ &H_1: \\mu &lt; 60000 \\end{align}\\) \n\n\nüì∫ The mean time between uses of a TV remote control by males during commercials equals 5 sec.¬†\n\n \\(\\begin{align} &H_0: \\mu = 5 \\\\ &H_1: \\mu \\ne 5 \\end{align}\\) \n\n\n\nKeep in mind that the equality sign is always put in \\(H_0\\), and \\(H_0\\) and \\(H_1\\) are mutually exclusive. Also, the claims are for population parameters, not sample statistics. We are not sure the value of the parameter being tested, but we want to collect evidence, and see which claim about the parameter is supported by the evidence.\n Example Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \nThe claim that the new treatment is effective in reducing BP means the mean BP is less than 150, which is an \\(H_1\\) claim. So we can write our \\(H_0\\) and \\(H_1\\) as\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu &lt; 150 \\end{align}\\) \nwhere \\(\\mu\\) is the mean blood pressure.\n\n Step 2: Set the Significance Level \\(\\alpha\\) \nNext, we set the significance level \\(\\alpha\\) that determines how rare or unlikely our evidence must be in order to represent sufficient evidence against \\(H_0\\). It tells us how strong the collected evidence must be in order to overturn the current claim. An \\(\\alpha\\) level of 0.05 implies that evidence occurring with probability lower than 5% will be considered sufficient evidence to reject \\(H_0\\). Mathematically, \\[\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true})\\] As a result, \\(\\alpha = 0.05\\) means that we incorrectly reject \\(H_0\\) 5 out of every 100 times we collect a sample and run the test.\nHere is the idea. When we want to see if what we care about (the population parameter) is not as described as in the null hypothesis \\(H_0\\), we first assume or believe \\(H_0\\) is right, then based on this, we see if there is sufficient and strong evidence to conclude that it is probably not the case, and find the alternative hypothesis more reasonable.\nLet‚Äôs explain \\(\\alpha\\) by an example. Suppose we would like to test the claim that ‚ÄúThe mean IQ of statistics professors is greater than 120.‚Äù Or in short \\(H_0: \\mu = 120\\) vs.¬†\\(H_1: \\mu &gt; 120\\). With large sample size, we can assume \\(\\overline{X}\\) follows a normal distribution. Now, to test whether the mean IQ is greater than 120, we need to first treat the mean not being greater than 120 unless later we have sufficient evidence to say it is greater than 120. In particular, we need to do the test and analysis on the basis that the mean is under \\(H_0\\). That is, we first assume the mean is 120, or \\(\\mu = 120\\), then see if the assumption really makes sense.\nBecause \\(\\overline{X}\\) is normal, we do the test under the assumption that \\(\\overline{X} \\sim N(120, \\sigma^2)\\) for some \\(\\sigma^2\\), say \\(9\\). (Let‚Äôs focus on \\(\\mu\\) and ignore \\(\\sigma\\) at this moment). If \\(\\overline{X}\\) has mean 120, and from our sample data we got the sample mean \\(\\overline{x} = 121\\), do you think the claim \\(H_0: \\mu = 120\\) make sense? How about you got \\(\\overline{x} = 127\\)? Now \\(\\alpha\\) comes into play. Let me ask you a question. What is the threshold or value of the sample mean \\(\\overline{x}\\) that you think it is too large to believe that \\(H_0: \\mu = 120\\) is a reasonable assumption or data generating mechanism? What is the threshold that makes you start to believe that \\(H_1: \\mu &gt; 120\\) makes more sense than \\(H_0: \\mu = 120\\)? The significance level \\(\\alpha\\) is such threshold value. With \\(\\alpha\\) being specified, we know what is the corresponding sample mean threshold \\(\\overline{x}^*\\), which is the one such that \\(P(\\overline{X} &gt; \\overline{x}^*) = \\alpha\\).\nFigure¬†16.1 illustrates the significance level \\(\\alpha\\). Once we decide \\(\\alpha\\), we determines how rare or unlikely our sample mean \\(\\overline{x}\\) must be in order to represent sufficient evidence against \\(H_0: \\mu = 120\\). In this example, if our \\(\\overline{x}\\) is greater than 125, we would think the evidence is strong enough to conclude that \\(\\mu = 120\\) is not so reasonable because the chance of such value happening is no larger than \\(\\alpha\\). We instead think \\(H_1: \\mu &gt; 120\\) makes more sense.\n\n\n\n\n\n\n\n\n\nFigure¬†16.1: Illustration of significance level, alpha.\n\n\n\n\n\n\nThe entire rationale is the rare event rule.\n\n\n\n\n\n\nRare Event Rule\n\n\n\nIf, under a given assumption, the probability of a particular observed event is exceptionally small, we conclude that the assumption is probably not correct.\n\n\n\n\nThe level \\(\\alpha\\) is related to the \\(\\alpha\\) used in confidence intervals for defining a ‚Äúcritical value‚Äù.\n Example Step 2: Set the Significance Level \\(\\alpha\\) \nThere is no \\(\\alpha\\) mentioned in the question description. Usually \\(\\alpha\\) is set by researchers themselves. Let‚Äôs set \\(\\alpha= 0.05\\). This means we are asking, ‚ÄúIs there a sufficient evidence at \\(\\alpha= 0.05\\) that the new treatment is effective?‚Äù\n\n Step 3: Calculate the Test Statistic \nSetting \\(\\alpha\\) is kind of setting the threshold for determining whether our collected evidence is sufficient or strong enough or not. In this step, we are collecting our evidence. The evidence is collected from the information we have, which is the sample data. Sample data is the only source we have for the inference about the unknown parameter. So to do a test about the parameter, or decide whether a statement about the parameter makes sense, we let the data and evidence speak up.\nThe evidence used in the hypothesis testing is called test statistic: a sample statistic value used in making a decision about the \\(H_0\\). Suppose  the test we are interested is \\(H_0: \\mu = \\mu_0\\) and \\(\\quad H_1: \\mu &lt; \\mu_0\\)  where \\(\\mu_0\\) is some population mean value that could be 150 lbs, 175 cm, 50 ounces, etc. When computing a test statistic, we assume \\(H_0\\) is true. Remember, we are trying to see if there is any strong evidence that is against \\(H_0\\). We should do our analysis in the world of \\(H_0\\) or the status quo. If the evidence is not sufficient, we stay in our current situation.\nWhen \\(\\sigma\\) is known, the test statistic for testings about \\(\\mu\\) is\n\\[\\boxed{ z_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{\\sigma/\\sqrt{n}} }\\]\nWhen \\(\\sigma\\) is unknown, the test statistic for testings about \\(\\mu\\) is\n\\[\\boxed{ t_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}} }\\]\nFamiliar with them? Those are \\(z\\) score and \\(t\\) score. Those are the sample statistics used for testing. When we calculate the test statistics, we need the value of \\(\\mu\\). What value we should use? You are right if you use the value assumed in \\(H_0\\)! The test statistics are the evidence we use in testing. The evidence is collected under the assumption that \\(\\mu = \\mu_0\\). We collect any evidence to prove that a suspect committed a crime under the assumption that he is innocent, right? We shouldn‚Äôt look at any person through colored spectacles, or frame anyone by treating someone as criminal, then make up a fake story for what he‚Äôs never done.\n Example Step 3: Calculate the Test Statistic \nSince we don‚Äôt know the true \\(\\sigma\\), and only know \\(s\\), we use \\(t\\) distribution and the test statistic is  \\[\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} =  \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55.\\]  So if the true mean blood pressure is 150, our test statistic or evidence is about 2.55 standard deviations below the mean. Is this number too weird or uncommon to believe that the mean blood pressure is really 150? We need a decision rule, and that is what we are going to learn in the step 4.\n\n Step 4-c: Find the Critical Value \nIn this step, we set the decision rule. There are two methods in testing, the critical-value method and the p-value method. The two methods are equivalent, leading to the same decision and conclusion. Let‚Äôs first talk about the critical-value method.\nIn step 2, we set the \\(\\alpha\\), and in step 3, we collect the evidence. Now we need a way to decide whether the collected evidence is sufficient or not to reject the \\(H_0\\) claim. The critical value(s) is a value determined by the significance level \\(\\alpha\\) that separates the rejection region or critical region, where we reject \\(H_0\\), from the values of the test statistic that do not lead to the rejection of \\(H_0\\).\n\nWhich critical value to be used depends on whether our test is a right-tailed, left-tailed or two-tailed. The right-tailed test, or right-sided test is the test with \\(H_1: \\mu &gt; \\mu_0\\). When we are interested of \\(\\mu\\) is greater than some value, say \\(\\mu_0\\), in the sampling distribution, we will focus on the right hand side of the distribution, because the evidence, the test statistic calculated in the step 3, will usually be on the right hand side of the distribution, so is the critical value used in the decision rule. Similarly, The left-tailed test, or left-sided test is the test with \\(H_1: \\mu &lt; \\mu_0\\). For a two-tailed or two-sided test, we have \\(H_1: \\mu \\ne \\mu_0\\). In this case, we wonder \\(\\mu\\) is larger or smaller than the assumed \\(\\mu_0\\). So we need to pay attention to both sides of the sampling distribution.\nFigure¬†16.2 illustrates rejection regions for the different types of hypothesis tests. Let‚Äôs assume \\(\\sigma\\) is known as the unknown \\(\\sigma\\) case is similar and we just replace the \\(z\\) score with the \\(t\\) score. Given the significance level \\(\\alpha\\), for a right-tailed test, the critical value is \\(z_{\\alpha}\\), the standard normal quantile so that \\(P(Z &gt; z_{\\alpha}) = \\alpha\\), where \\(Z \\sim N(0, 1)\\). For a right-tailed test, the critical value is \\(-z_{\\alpha}\\), or in fact \\(z_{1-\\alpha}\\), the standard normal quantile so that \\(P(Z &lt; -z_{\\alpha}) = \\alpha\\) or \\(P(Z &gt; -z_{\\alpha}) = 1-\\alpha\\). When the test is a two-tailed test, there are two critical values, one at the right-hand side, the other at the left-hand side of the distribution. Here, we need to split \\(\\alpha\\) equally into \\(\\alpha/2\\), and the critical value at the right-hand side is \\(z_{\\alpha/2}\\) such that \\(P(Z &gt; z_{\\alpha/2}) = \\alpha\\) and the critical value at the left-hand side is \\(-z_{\\alpha/2}\\) such that \\(P(Z &lt; -z_{\\alpha/2}) = \\alpha/2\\). Note that by definition, \\(z_{\\alpha}\\) and \\(t_{\\alpha, n-1}\\) are always positive and on the right hand side of the distribution.\n\n\n\n\n\n\n\nFigure¬†16.2: Rejection regions for the different types of hypothesis tests. Source: https://towardsdatascience.com/everything-you-need-to-know-about-hypothesis-testing-part-i-4de9abebbc8a\n\n\n\n\n\n\nThe following table is the summary of the critical values under different cases. When \\(\\sigma\\) is known, we use \\(z\\) scores, and when \\(\\sigma\\) is unknown, we use \\(t\\) scores.\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{\\alpha}\\)\n\\(-z_{\\alpha}\\)\n\n\\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\)\n\n\n\n\n\\(\\sigma\\) unknown\n\\(t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha, n-1}\\)\n\n\\(-t_{\\alpha/2, n-1}\\) and \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n Example Step 4-c: Find the Critical Value \nSince the test is a left-tailed test, and \\(\\sigma\\) is unknown and \\(n = 25\\), the critical value is \\(-t_{\\alpha, n-1}\\) that is  \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711.\\) \n\n Step 5-c: Draw a Conclusion Using Critical Value \nThe critical value separates the the standard normal values into the rejection region and non-rejection region. For a right-tailed test, the rejection region is any \\(z\\) value greater than \\(z_{\\alpha}\\), and the non-rejection region is any \\(z\\) value smaller than or equal to \\(z_{\\alpha}\\). For a left-tailed test, the rejection region is any \\(z\\) value small than \\(-z_{\\alpha}\\), and the non-rejection region is any \\(z\\) value greater than or equal to \\(-z_{\\alpha}\\). For a two-tailed test, the rejection region is the union of any \\(z\\) value smaller than \\(-z_{\\alpha/2}\\) and any \\(z\\) value greater than \\(z_{\\alpha/2}\\).\nIf the test statistic \\(z_{test}\\) is in the rejection region, we reject \\(H_0\\). If \\(z_{test}\\) is not in the rejection region, we do not or fail to reject \\(H_0\\). Figure¬†16.3 is an example that we reject \\(H_0\\) in a right-tailed test. The test statistic is 2.5 which is greater than the critical value 1.645, so the test statistic falls in the rejection region.\n\n\n\n\n\n\n\nFigure¬†16.3: Test statistic inside of critical region. Source: https://www.thoughtco.com/example-of-a-hypothesis-test-3126398\n\n\n\n\nThe rejection region for any type of tests is shown in the table below.\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{test} &gt; z_{\\alpha}\\)\n\\(z_{test} &lt; -z_{\\alpha}\\)\n\\(\\mid z_{test}\\mid \\, &gt; z_{\\alpha/2}\\)\n\n\n\n\\(\\sigma\\) unknown\n\\(t_{test} &gt; t_{\\alpha, n-1}\\)\n\\(t_{test} &lt; -t_{\\alpha, n-1}\\)\n\\(\\mid t_{test}\\mid \\, &gt; t_{\\alpha/2, n-1}\\)\n\n\n\nRemember that a test statistic works as our evidence, and the critical value is a threshold to determine whether the evidence is strong enough. When the test statistic is more extreme than the critical value, it means that from our point of view, the chance of our evidence happening is way too small given the current rules of the game or under \\(H_0\\). Therefore, we don‚Äôt think we live in the world of \\(H_0\\), and it probably makes more sense to think we live in the world of \\(H_1\\), and it is commonplace to see these evidence happening.\n Example Step 5-c: Draw a Conclusion Using Critical Value \nWe reject \\(H_0\\) if \\(t_{test} &lt; -t_{\\alpha, n-1}\\). Since  \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} =  \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)  and  \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) , we have \\(\\small t_{test} = -2.55 &lt; -1.711 = -t_{\\alpha, n-1}\\), so we reject \\(H_0\\).\n\n Step 4-p: Find the P-Value \nAnother decision rule is the p-value method. The \\(p\\)-value measures the strength of the evidence against \\(H_0\\) provided by the data. The smaller the \\(p\\)-value, the greater the evidence against \\(H_0\\). As the name implies, the \\(p\\)-value is the probability of getting a test statistic value that is at least as extreme as the one obtained from the data, assuming that \\(H_0\\) is true. \\((\\mu = \\mu_0)\\). For example, \\(p\\)-value \\(= P(Z \\ge z_{test} \\mid H_0)\\) for a right-tailed test. We are more likely to get a \\(p\\)-value near 0 when \\(H_0\\) is false than when \\(H_0\\) is true. Because when \\(H_0\\) is true, \\(z_{test}\\) will be closer to zero or located around the center of the distribution (the \\(\\mu_0\\) value assumed in \\(H_0\\)), and its p-value will be around 0.5. On the other hand, when \\(H_0\\) is false, or the true \\(\\mu\\) is not \\(\\mu_0\\), the test statistic \\(z_{test}\\) will be farther away from \\(\\mu_0\\) and located at the either tail of the distribution. Therefore, its p-value will be small.\n P-Value Illustration \nSince p-value is a probability, in the distribution, it represents the area under the density curve for values that are at least as extreme as the test statistic‚Äôs value. Figure¬†16.4 shows the p-value for different tests. Note that the p-value for a two-tailed test depends on whether the test statistic is positive or negative. If the calculated test statistic is on the right (left) hand side, the p-value will be the right (left) tail area times two.\n\n\n\n\n\n\n\nFigure¬†16.4: Illustration of p-values for different types of hypothesis tests\n\n\n\n\nMathematically, the p-value for any type of tests is shown in the table below.\n\n\n\n\n\n\n\n\nCondition ¬† ¬†\n\nRight-tailed \\((H_1: \\mu &gt; \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu &lt; \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(P(Z &gt; z_{test} \\mid H_0)\\)\n\\(P(Z &lt; z_{test} \\mid H_0)\\)\n\\(2P(Z &gt; \\,\\mid z_{test} \\mid \\, \\mid H_0)\\)\n\n\n\n\\(\\sigma\\) unknown\n\\(P(T &gt; t_{test} \\mid H_0)\\)\n\\(P(T &lt; t_{test} \\mid H_0)\\)\n\\(2P(T &gt; \\, \\mid t_{test} \\mid  \\, \\mid H_0)\\)\n\n\n\n Example Step 4-p: Find the P-Value \nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T &lt; t_{test})=P(T &lt; -2.55) =\\) 0.01 \n\n Step 5-p: Draw a Conclusion Using P-Value Method \nHow do we use the p-value to make the decision? Well, here the p-value is like our evidence, and the significance level \\(\\alpha\\) is the cut-off for measuring the strength of the evidence. If the \\(p\\)-value \\(\\le \\alpha\\) , we reject \\(H_0\\). If instead the \\(p\\)-value \\(&gt; \\alpha\\), we do not reject or fail to reject \\(H_0\\).\nYes, it is a pretty simple decision rule, but the \\(p\\)-value has been misinterpreted and misused for a long time. When we do a hypothesis testing, it is dangerous to simply compare the size of \\(p\\)-value and \\(\\alpha\\), then jump into the conclusion. You can find more issues of p-value at XXX.\n Example Step 5-p: Draw a Conclusion Using P-Value Method \n We reject \\(H_0\\) if the \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(= 0.01 &lt; 0.05 = \\alpha\\), we reject \\(H_0\\).\n\n Both Methods Lead to the Same Conclusion \nRemember I say both critical-value method and \\(p\\)-value method lead to the same conclusion? Figure¬†16.5 shows why. Test statistic and critical value are variable values, either \\(z\\) or \\(t\\) scores. The p-value and significance level \\(\\alpha\\) are probabilities, either \\(z\\) or \\(t\\) probabilities. The p-value is computed from the test statistic, and \\(\\alpha\\) defines the critical value. The more extreme test statistic implies the smaller p-value, and smaller \\(\\alpha\\) means more extreme critical value. When we reject \\(H_0\\), the following three statements are equivalent:\n\ntest statistic is in the rejection region.\nthe test statistic is more extreme than the critical value\nthe p-value is smaller than \\(\\alpha\\).\n\n\n\n\n\n\n\n\nFigure¬†16.5: The conclusion is the same regardless of the method used (Critical Value or P-Value).\n\n\n\n\n\nThe following distribution shows the equivalence of the critical-value method and the p-value method in the blood pressure example.\n\n\n\n\n\n\n\n\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \nThe final step in the entire hypothesis testing procedure is to make a verbal conclusion, and address the original claim. Figure¬†16.6 gives you a guideline of how we make a conclusion.\n\n\n\n\n\n\n\nFigure¬†16.6: Conclusions based on testing results. Source: https://www.drdawnwright.com/category/statistics/\n\n\n\n\nHere is a reminder. We never say we accept \\(H_0\\). Why can‚Äôt we say we ‚Äúaccept the null‚Äù? The reason is that we are assuming the null hypothesis is true or the situation we are currently in. We are trying to see if there is evidence against it. Therefore, the conclusion should be in terms of rejecting the null. We don‚Äôt accept \\(H_0\\) when we don‚Äôt have evidence against it because we are already in the world of \\(H_0\\).\n\n\n\n\n\n\n\nFigure¬†16.7: Meme about hypothesis testing conclusions. Source: https://www.pinterest.com/pin/287878601159173631/\n\n\n\n\n\n Example Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \nWe have a \\(H_1\\) claim and we reject \\(H_0\\), so we conclude that  There is sufficient evidence to support the claim that the new treatment is effective. \n\n\n\n\nR\nPython\n\n\n\nBelow is a demonstration of how to work through the blood pressure example using R.\n\n## create objects for any information we have\nalpha &lt;- 0.05; mu_0 &lt;- 150\nx_bar &lt;- 147.2; s &lt;- 5.5; n &lt;- 25\n\n## Test statistic\n(t_test &lt;- (x_bar - mu_0) / (s / sqrt(n))) \n\n[1] -2.545455\n\n## Critical value\n(t_cri &lt;- qt(alpha, df = n - 1)) \n\n[1] -1.710882\n\n## p-value\n(p_val &lt;- pt(t_test, df = n - 1)) \n\n[1] 0.008878158\n\n\nThe critical value is \\(-t_{\\alpha, n-1}\\), or the quantile such that \\(P(T_{n-1} &lt; -t_{\\alpha, n-1}) = \\alpha\\). Therefore, we use qt() to get the \\(t\\) value. Notice that the p-value is a probability that the Student‚Äôs t variable with degrees of freedom \\(n-1\\) is smaller (more extreme) than the test statistic. In R, we use pt() to get the probability. Without specifying the lower.tail argument in the function, by default, both qt() and pt() function focuses on the lower tail or left tail, which is what we need in this left-tail test.\n\n\nBelow is a demonstration of how to work through the blood pressure example using Python.\n\nimport numpy as np\nfrom scipy.stats import t\n\n\n## create objects to be used\nalpha = 0.05; mu_0 = 150\nx_bar = 147.2; s = 5.5; n = 25\n\n## Calculate the t-test statistic\nt_test = (x_bar - mu_0) / (s / np.sqrt(n))\nt_test\n\n-2.5454545454545556\n\n## Calculate the critical t value\nt_crit = t.ppf(alpha, df=n-1)\nt_crit\n\n-1.7108820799094282\n\n## Calculate the p-value\np_val = t.cdf(t_test, df=n-1)\np_val\n\n0.008878157746280955\n\n\nThe critical value is \\(-t_{\\alpha, n-1}\\), or the quantile such that \\(P(T_{n-1} &lt; -t_{\\alpha, n-1}) = \\alpha\\). Therefore, we use t.ppf() to get the \\(t\\) value. Notice that the p-value is a probability that the Student‚Äôs t variable with degrees of freedom \\(n-1\\) is smaller (more extreme) than the test statistic. In Python, we use t.cdf() to get the probability. Both t.ppf() and t.cdf() function focuses on the lower tail or left tail, which is what we need in this left-tail test.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-ht.html#sec-ex-two-tailed",
    "href": "infer-ht.html#sec-ex-two-tailed",
    "title": "16¬† Hypothesis Testing",
    "section": "\n16.3 Example: Two-tailed z-test",
    "text": "16.3 Example: Two-tailed z-test\n\n\nThe milk price of a gallon of 2% milk is normally distributed with standard deviation of $0.10. Last week the mean price of a gallon of milk was 2.78. This week, based on a sample of size 25, the sample mean price of a gallon of milk was \\(\\overline{x} = 2.80\\). Under \\(\\alpha = 0.05\\), determine if the mean price is different this week.\n\n\n\n\n\nSource: https://unsplash.com/photos/BYlHH_1j2GA\n\n\n\n\n\n\n Step-by-Step \n Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \nForm the sentence ‚Äúdetermine if the mean price is different this week‚Äù, we know the claim or what we are interested is an \\(H_1\\) claim. If we let \\(\\mu\\) be the mean milk price this week, we have the test  \\(\\small \\begin{align}&H_0: \\mu = 2.78 \\\\ &H_1: \\mu \\ne 2.78 \\end{align}\\)  where 2.78 is the mean milk price last week.\n Step 2: Set the Significance Level \\(\\alpha\\) \n \\(\\small \\alpha = 0.05\\) \n Step 3: Calculate the Test Statistic \nFrom the question we know that the population is normally distributed, and \\(\\sigma\\) is known. So we use the \\(z\\)-test, and the test statistic is  \\(\\small z_{test} = \\frac{\\overline{x} - \\mu_0}{\\sigma/\\sqrt{n}} =  \\frac{2.8 - 2.78}{0.1/\\sqrt{25}} = 1.00\\) \n Step 4-c: Find the Critical Value \nSince it is a two-tailed test, we have two potential critical values. Because \\(z_{test} &gt; 0\\) and on the right hand side of the standard normal distribution, we compare it with the critical value on the right, which is  \\(\\small z_{0.05/2} = 1.96\\). \n Step 5-c: Draw a Conclusion Using Critical Value \nThis is a two-tailed test, and we reject \\(H_0\\) if \\(|z_{test}| &gt; z_{\\alpha/2}\\). Since \\(\\small |z_{test}| = 1 &lt; 1.96 = z_{\\alpha/2}\\), we DO NOT reject \\(H_0\\).\n Step 4-p: Find the P-Value \nThis is a two-tailed test, and the test statistic is on the right \\((&gt; 0)\\), so the \\(p\\)-value is \\(2P(Z &gt; z_{test})=\\) 0.317 .\n Step 5-p: Draw a Conclusion Using P-Value Method \n We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(= 0.317 &gt; 0.05 = \\alpha\\), we DO NOT reject \\(H_0\\).\nThe critical-value and p-value method are illustrated in Figure¬†16.8.\n\n\n\n\n\n\n\nFigure¬†16.8: Illustration of Critical Value and P-Value methods\n\n\n\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n There is insufficient evidence to support the claim that this week the mean price of milk is different from the price last week. \n\n\n\n\nR\nPython\n\n\n\nBelow is an example of how to perform the two-tailed \\(z\\)-test in R.\n\n## create objects to be used\nalpha &lt;- 0.05; mu_0 &lt;- 2.78; \nx_bar &lt;- 2.8; sigma &lt;- 0.1; n &lt;- 25\n\n## Test statistic\n(z_test &lt;- (x_bar - mu_0) / (sigma / sqrt(n))) \n\n[1] 1\n\n## Critical value\n(z_crit &lt;- qnorm(alpha/2, lower.tail = FALSE)) \n\n[1] 1.959964\n\n## p-value\n(p_val &lt;- 2 * pnorm(z_test, lower.tail = FALSE)) \n\n[1] 0.3173105\n\n\n\n\nBelow is an example of how to perform the two-tailed \\(z\\)-test in Python.\n\n## create objects to be used\nalpha = 0.05; mu_0 = 2.78\nx_bar = 2.8; sigma = 0.1; n = 25\n\n\n## Calculate the z-test statistic\nz_test = (x_bar - mu_0) / (sigma / np.sqrt(n))\nz_test\n\n1.0000000000000009\n\nfrom scipy.stats import norm\n## Calculate the critical z value\n# z_crit = norm.isf(alpha/2)\nz_crit = norm.ppf(1 - alpha/2)\nz_crit\n\n1.959963984540054\n\n## Calculate the p-value\np_val = 2 * norm.sf(z_test)\np_val\n\n0.3173105078629137",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-ht.html#testing-summary",
    "href": "infer-ht.html#testing-summary",
    "title": "16¬† Hypothesis Testing",
    "section": "\n16.4 Testing Summary",
    "text": "16.4 Testing Summary\nBelow is a table that summarizes what we have learned about hypothesis testing in this chapter.\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\)  unknown \n\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\n\nPopulation Mean \\(\\mu\\)\n\n\n\nTest Type\nOne sample \\(\\color{blue}{z}\\) test \\(H_0: \\mu = \\mu_0\\)\n\nOne sample \\(\\color{blue}{t}\\) test \\(H_0: \\mu = \\mu_0\\)\n\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{\\color{blue}{s}}{\\sqrt{n}}\\)\n\n\nTest Stat under \\(H_0\\) \n\\(z_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\\(t_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\color{blue}{s}}{\\sqrt{n}}}\\)\n\n\n\\(p\\)-value under \\(H_0\\)\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(Z \\le z_{test})\\)\n\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(T_{n-1} \\le t_{test})\\)\n\n\n\n\n\n\\(H_1: \\mu &gt; \\mu_0\\) \\(p\\)-value \\(=P(Z \\ge z_{test})\\)\n\n\n\\(H_1: \\mu &lt; \\mu_0\\) \\(p\\)-value \\(=P(T_{n-1} \\ge t_{test})\\)\n\n\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\) \\(p\\)-value \\(=2P(Z \\ge \\, \\mid z_{test}\\mid)\\)\n\n\n\\(H_1: \\mu \\ne \\mu_0\\) \\(p\\)-value \\(=2P(T_{n-1} \\ge  \\, \\mid t_{test} \\mid)\\)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-ht.html#type-i-and-type-ii-errors",
    "href": "infer-ht.html#type-i-and-type-ii-errors",
    "title": "16¬† Hypothesis Testing",
    "section": "\n16.5 Type I and Type II Errors",
    "text": "16.5 Type I and Type II Errors\nIt is important to remember that hypothesis testing is not perfect, meaning that we may make a wrong decision or conclusion. After all, the collected evidence may not be able to present the full picture of what the true population distribution is. There are two types of errors we may commit when doing hypothesis testing: Type I error and Type II error.\nIf in fact \\(H_0\\) is true, but we wrongly reject it, we commit the type I error. We shouldn‚Äôt reject it but we did. If \\(H_0\\) is false, but we don‚Äôt reject it, we make the type II error. We should have figured out that \\(H_0\\) does not make sense. The following table tells us when we make a correct decision and when we don‚Äôt. In practice, we will not know for certain if we made the correct decision or if we made one of these two errors because we never know the truth!\n\n\n\nDecision\n\n\\(H_0\\) is true\n\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\n\nType I error\nCorrect decision\n\n\nDo not reject \\(H_0\\)\n\nCorrect decision\nType II error\n\n\n\nBack to the crime example that \\(H_0:\\) The person is  not guilty  v.s. \\(H_1:\\) The person is  guilty . We can have a decision table like\n\n\n\n\n\n\n\nDecision\nTruth is the person innocent\nTruth is the person guilty\n\n\n\nJury decides the person guilty\nType I error\nCorrect decision\n\n\nJury decides the person not guilty\nCorrect decision\nType II error\n\n\n\nIs it worse to wrongly convict an innocent person (Type I error) or to let a perpetrator free (Type II error)? Both hugely negatively impact our society, and if possible, we should make the two errors as rarely as possible.\n\n\n\n\n\n\n\nFigure¬†16.9: Example of Type I and Type II errors (https://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg)\n\n\n\n\nIt you still don‚Äôt get the idea of type I and type II errors, Figure¬†16.9 is a classical example of the two errors. Of course the null hypothesis is ‚Äúnot pregnant‚Äù, and the alternative hypothesis is ‚Äúpregnant‚Äù. Claiming that a old man is expecting a baby is a type I error, and saying a pregnant woman not having a baby is a type II error.\nIn statistics, the probability of committing the type I error is in fact the significance level \\(\\alpha\\).\n\\[\\alpha = P(\\text{type I error}) = P(\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true})\\]\nIf the evidence occurring with probability lower than 5%, it will be considered sufficient evidence to reject \\(H_0\\), even though \\(H_0\\) is actually the true mechanism giving rise to such evidence.\nWhat is the probability of committing the type II error, the probability that we fail to reject \\(H_0\\) when \\(H_0\\) is a false statement? We call the probability \\(\\beta\\):\n\\[\\beta = P(\\text{type II error}) = P(\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false})\\]\n\\(\\alpha\\), \\(\\beta\\) and sample size \\(n\\) are related. If we choose any two of them, the third is automatically determined. We would of course prefer \\(\\alpha\\) to be small since we would not like to conclude in favor of the research hypothesis falsely. But given the sample size, small \\(\\alpha\\) leads to a large \\(\\beta\\). On the other hand, too small \\(\\alpha\\) would most likely result in no discovery because we are gonna be conservative, set the threshold too high, and do not reject lots of \\(H_0\\) that should be rejected. In practice, we specify \\(\\alpha\\) beforehand, and then select \\(n\\) that is practical, so the \\(\\beta\\) is determined.\nIt would be great if we correctly reject \\(H_0\\) when \\(H_0\\) is actually false. We hope the probability of having this to be large. The probability is actually \\(1-\\beta\\), which is called the power of a test. The power depends on the same factors as \\(\\beta\\) does, including the size of \\(\\alpha\\), the sample size, and the true value of parameter.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-ht.html#statistical-power-and-choosing-sample-size",
    "href": "infer-ht.html#statistical-power-and-choosing-sample-size",
    "title": "16¬† Hypothesis Testing",
    "section": "\n16.6 Statistical Power and Choosing Sample Size*",
    "text": "16.6 Statistical Power and Choosing Sample Size*\n\n16.6.1 Power of a Hypothesis Test\n\\(\\beta = P(\\text{type II error}) = P(\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false})\\). In statistics, \\(1 - \\beta =  P(\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is false})\\) is called the (statistical) power of the test. We hope a test has high power because it means the test is able to correctly reject \\(H_0\\) when \\(H_0\\) is false, like we hope all criminals get arrested! The power depends on the same factors as \\(\\beta\\) does, including the size of \\(\\alpha\\), the sample size, and the true value of parameter.\n\nNow let‚Äôs learn type I error, type II error, and power through distributions. Here a right-tailed example \\(H_0: \\mu = \\mu_0\\) v.s. \\(H_1: \\mu &gt; \\mu_0\\) is illustrated in Figure¬†16.10.\nThe distribution on the top is the distribution under \\(H_0\\), that is, the distribution has the mean \\(\\mu_0\\). Since it is a right-tailed test, we reject \\(H_0\\) in favor of \\(H_1\\) if the test statistic is greater than the critical value on the right of the distribution that is indicated at the green vertical line. If we reject \\(H_0\\) when \\(H_0\\) is actually true, we commit a type I error, and the probability of committing a type I error, which is \\(\\alpha\\), is the green area on the right tail of the distribution.\n\n\n\n\n\n\n\nFigure¬†16.10: Example of Power. Source\n\n\n\n\n\nNow, suppose the true population mean is \\(\\mu_a &gt; \\mu_0\\). That is, \\(H_0\\) is false and \\(H_1\\) is true, then the true distribution is the blue curve one centered at \\(\\mu_a\\) at the bottom. However, in reality most of the time we do not know the exact value of \\(\\mu_a\\). Remember that we make our decision under the mechanism or assumption of \\(H_0\\). So we actually do our hypothesis testing under the distribution depicted by the purple-dashed curve. And we know this distribution because \\(mu_0\\) is the hypothesized value of \\(\\mu\\) we specify when we do the testing. If we fail to reject the false \\(H_0\\), we commit the type II error. The yellow area is the probability of committing the type II error. This yellow area is the probability that we fail to reject \\(H_0\\) when \\(H_0\\) is false and \\(H_1\\) is true \\((\\mu = \\mu_a)\\), which is \\(\\beta\\). Although we don‚Äôt know the true value \\(\\mu_a\\), for this right-tailed test, the left-tailed area whose value is smaller than the critical value is always the rejection region. The size of the yellow area, the chance of committing type II error, and the rejection region is smaller (larger) if \\(\\mu_a\\) is much (less) greater than \\(\\mu_0\\).\nWhich part represents the power in the figure? The power is \\(1 - \\beta\\) which is the green area under the blue density curve at the bottom. Remember that the total area under a probability density curve is 1, and the power is the total area minus \\(\\beta\\), the yellow area.\n\n16.6.2 Power Calculation\nIn this section we illustrate how to calculate the type II error rate and power. Suppose we randomly sampled 36 values from a normally distributed population with \\(\\sigma = 18\\) and unknown \\(\\mu\\). At \\(\\alpha = 0.05\\), we are going to test a right-tailed test  \\(\\begin{align}\n  &H_0: \\mu = \\mu_0 = 50 \\\\\n  &H_1: \\mu &gt; 50\n  \\end{align}\\) \nSuppose we are sampling from a normal distribution and \\(\\sigma\\) is known. Then the test statistic is \\(Z_{test} = \\frac{\\overline{X} - \\mu_0}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\) under \\(H_0\\). Since it is a right-tailed test, we reject \\(H_0\\) if \\(z_{test} &gt; z_{\\alpha} = z_{0.05} = 1.645\\).\nNow we are going to re-express the rejection region in terms of \\(\\overline{X}\\) because it is going to help us calculate \\(\\beta\\). The question is, for what values of \\(\\overline{X}\\) will we reject \\(H_0\\)? Remember, we reject \\(H_0\\) if the observed test statistic \\(z_{test} = \\frac{\\overline{x} - \\mu_0}{\\sigma/\\sqrt{n}} &gt; 1.645\\). We can isolate \\(\\overline{x}\\) in the expression by multiplying \\(\\sigma/\\sqrt{n}\\) and adding \\(\\mu_0\\) on both sides. Then we have \\(\\overline{x} &gt; \\mu_0 + 1.645\\frac{\\sigma}{\\sqrt{n}} = 50 + 1.645\\frac{18}{\\sqrt{36}} = 54.94\\).\nTherefore, having evidence \\(z_{test} &gt; 1.645\\) is equivalent to having evidence \\(\\overline{x} &gt; 54.94\\). Both represent the same rejection region. One represents the evidence using standard normal distribution as the benchmark, and the other uses the original hypothesized distribution \\(N(50, 18^2)\\) as the benchmark. The figure below illustrates this idea. We use different scales to measure the same thing.\n\n\n\n\n\n\n\n\n\nWe are not able to calculate \\(\\beta\\) or power without knowing or assuming the true value of \\(\\mu\\) which is not \\(\\mu_0 = 50\\) because \\(\\beta\\) is based on the fact that \\(H_0\\) is false. Suppose the true mean is \\(\\mu = 56\\). Let‚Äôs calculate \\(P(\\text{Type II error}) = \\beta\\).\n\nNow \\(\\beta = P(\\text{Do not reject } H_0 \\mid \\mu = 56)\\).\nBecause we reject \\(H_0\\) if \\(\\overline{X} &gt; 54.94\\), it means that we do not reject \\(H_0\\) if \\(\\overline{X} &lt; 54.94\\). Plug into information into the probability, we have\n\\(\\begin{align*}\n\\beta = P(\\text{Do not reject } H_0 \\mid \\mu = 56) &= P(\\overline{X} &lt; 54.94 \\mid \\mu = 56)\\\\\n&=P\\left(\\frac{\\overline{X} - 56}{18/\\sqrt{36}} &lt; \\frac{54.94 - 56}{18/\\sqrt{36}} \\, \\middle| \\, \\mu = 56 \\right)\\\\\n&= P(Z &lt; -0.355) = 0.361\n\\end{align*}\\)\n\n\nR\nPython\n\n\n\n\npnorm(-0.355)\n\n[1] 0.3612948\n\npnorm(54.94, mean = 56, sd = 18/sqrt(36))\n\n[1] 0.3619193\n\n\n\n\n\nnorm.cdf(-0.355)\n\n0.36129479561631284\n\nnorm.cdf(54.94, loc=56, scale=18/np.sqrt(36))\n\n0.3619192793326037\n\n\n\n\n\nTherefore, Power \\(= P(\\text{Reject } H_0 \\mid \\mu = 56) = 1 - P(\\text{Do not reject } H_0 \\mid \\mu = 56) = 1 - \\beta = 0.639\\)\n\n\n\n\n\n\n\n\n\n\n\nIn general, if the hypothesized mean value is \\(\\mu_0\\), and the true mean value is \\(\\mu_1\\), for one sample \\(z\\) test, we have the formula for calculating \\(\\beta\\) as follows. The derivation is left as an exercise.\n\nFor one-tailed tests (either left-tailed or right-tailed),\n\n\\[ \\beta(\\mu_1) = P\\left( Z \\le \\color{blue}{z_{\\alpha}} - \\frac{|\\mu_0 - \\mu_1|}{\\sigma/\\sqrt{n}}\\right) \\]\n\nFor two-tailed tests,\n\n\\[ \\beta(\\mu_1) = P\\left( Z \\le \\color{blue}{z_{\\alpha/2}} - \\frac{|\\mu_0 - \\mu_1|}{\\sigma/\\sqrt{n}}\\right) \\]\nNote that again to compute \\(\\beta\\), we need the value \\(\\mu_1\\) of the true \\(H_1\\), which in reality is usually unknown. But we could definitely check how the value of \\(\\mu_1\\) affects \\(\\beta\\) through this formula.\n\n16.6.3 Power Analysis\nBack to the milk price example in Section 16.3. we have the test\n \\(\\begin{align}\n  &H_0: \\mu = 2.78 \\\\\n  &H_1: \\mu \\ne 2.78\n  \\end{align}\\)  and we do not reject \\(H_0\\).\nThe question here is\n\nIf \\(|\\mu_0 - \\mu_1| = |2.78 - \\mu_1| \\ge 0.05\\), is the conclusion that price has not changed reasonable or acceptable? Let‚Äôs see the chance of making the wrong decision.\n\nWe check the probability that we do not reject \\(H_0\\) when \\(H_0\\) is false, i.e., the probability that we conclude that the mean milk price has not changed, but it actually did.\n\nSince it is a two-tailed test, and \\(|\\mu_0 - \\mu_1| \\ge 0.05\\), the type II error rate can be as high as \\[\n\\begin{align}\n\\beta &= P\\left( Z \\le \\color{blue}{z_{\\alpha/2}} - \\frac{|\\mu_0 - \\mu_1|}{\\sigma/\\sqrt{n}}\\right) \\\\\n&\\le P\\left( Z \\le 1.96 - \\frac{0.05}{0.1/\\sqrt{25}}\\right) = P\\left(Z &lt; -0.54 \\right) = 0.2946.\n\\end{align}\n\\] If the actual mean milk price this week is \\(\\mu_1 = 2.83\\) or \\(\\mu_1 = 2.73\\), (since \\(|\\mu_0 - \\mu_1| = 0.05\\)), there is about 3 in 10 chance of making a wrong conclusion. If we think the risk is too high, we need to collect more than 25 samples. With a fixed \\(\\alpha\\), we can increase the testing power or decrease type II error rate by increasing the sample size. This leaves a question: How do we choose a sample size that keeps \\(\\beta\\) at some low level?\n\nLet‚Äôs go back to the formula of \\(\\beta\\).\n\\[ \\beta(\\mu_1) = P\\left( Z \\le \\color{blue}{z_{\\alpha}} - \\frac{|\\mu_0 - \\mu_1|}{\\sigma/\\sqrt{n}}\\right) \\]\nWhat will increase the power (decrease \\(\\beta\\))?\n\n\n\\(\\mu_0\\) and \\(\\mu_1\\) are further away: When \\(\\mu_0\\) and \\(\\mu_1\\) are far apart, the evidence that \\(\\mu\\) is not \\(\\mu_0\\) is stronger, and it‚Äôs more likely to reject \\(H_0\\) when it is false. The chance of making type II error decreases. Look how \\(\\beta\\) decreases from the blue area to the red area when the true mean value increases from 56 to 60.\n\n\n\n\n\n\n\n\n\n\n\nLarger \\(\\alpha\\): \\(\\alpha\\) and \\(\\beta\\) are trading off. Everything held constant, if you increases \\(\\alpha\\), it means you use smaller critical values, and allows more type I errors or false discoveries. But at the same time, more rejecting \\(H_0\\) decreases the cases that we don‚Äôt reject \\(H_0\\) when it is false. In other words, type II error rate goes down. The figure below shows how \\(\\beta\\) changes when \\(\\alpha\\) is increased from 0.05 to 0.2.\n\n\n\n\n\n\n\n\n\n\n\nSmaller \\(\\sigma\\): When \\(\\sigma\\) is small, given the same location of the distributions of \\(H_0\\) and \\(H_1\\), or the same \\(| \\mu_1 - \\mu_0|\\), the two distributions are more separated apart, and have smaller overlapped regions. The figure below shows how \\(\\beta\\) shrinks from the blue area to the red area when \\(\\sigma\\) becomes smaller due to the fact that the two distributions become peaky and thin-tailed.\n\n\n\n\n\n\n\n\n\n\n\nLarger sample size \\(n\\): When sample size gets large, more information is collected, and therefore the sampling distribution of the sample mean becomes more certain about its possible values. This results in the same effect of having a smaller \\(\\sigma\\).\n\nTo keep \\(\\alpha\\) and \\(\\beta\\) at a specified level, the only way is to increase the sample size. With \\(\\Delta = |\\mu_0-\\mu_1|\\), the formula for required sample size is shown as below. The derivation is left as an exercise.\n\nOne-tailed test (either left-tailed or right-tailed): \\[n \\ge \\sigma^2 \\frac{(\\color{blue}{z_{\\alpha}} + z_{\\beta})^2}{\\Delta^2}\\]\nTwo-tailed test: \\[n \\ge \\sigma^2 \\frac{( \\color{blue}{z_{\\alpha/2}} + z_{\\beta})^2}{\\Delta^2}\\]\n\n Example: Sample size \n\nA cereal company sell boxes of cereal with the labeled weight of 16 oz. The production is based on the mean weight of 16.37 oz. So that only small portion of boxes have weight less than 16 oz. The box weight is normally distributed with \\(\\sigma = 0.225\\). The percentage of boxes weighting less than 16 oz. is 5%. They suspect that due to some production defect the weight filled in the boxes have mean less than 16.37, and like to conduct a test under \\(\\alpha = 0.05\\).\nThis is a \\(H_1\\) claim and the test is \\(\\begin{align}\n  &H_0: \\mu = 16.37 \\\\\n  &H_1: \\mu &lt; 16.37\n  \\end{align}\\) \n\nQuestion: How many boxes should be sampled in order to correctly discover that mean is less than 16.37 with the power of 0.99 if in fact the true mean weight is 16.27 or less?\n\n\\(\\alpha = 0.05\\). \\(\\sigma = 0.225\\). We have \\(\\beta = 1 - \\text{power} = 0.01\\), \\(\\Delta = |\\mu_0 - \\mu_1| = 16.37-16.27 = 0.1\\).\nThe formula is \\(n = \\sigma^2 \\frac{(\\color{blue}{z_{\\alpha}} + z_{\\beta})^2}{\\Delta^2}\\)\n\\(z_{\\alpha} = z_{0.05} = 1.645\\), \\(z_{\\beta} = z_{0.01} = 2.33\\).\nThus, \\[ n \\ge\n\\left(0.225^2\\right) \\frac{(1.645 + 2.33)^2}{\\left(0.1^2\\right)} = 79.99 \\]\nThey need at least 80 samples to conduct the test under the specified conditions (\\(\\alpha = 0.05\\), \\(\\beta=0.01\\)).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-ht.html#exercises",
    "href": "infer-ht.html#exercises",
    "title": "16¬† Hypothesis Testing",
    "section": "\n16.7 Exercises",
    "text": "16.7 Exercises\n\n\nHere are summary statistics for randomly selected weights of newborn boys: \\(n =207\\), \\(\\bar{x} = 30.2\\)hg (1hg = 100 grams), \\(s = 7.3\\)hg.\n\nWith significance level 0.01, use the critical value method to test the claim that the population mean of birth weights of females is greater than 30hg.\nDo the test in (c) by using the p-value method.\n\n\nYou are given the following hypotheses: \\[\\begin{align*}\nH_0&: \\mu = 45 \\\\\nH_A&: \\mu \\neq 45\n\\end{align*}\\] We know that the sample standard deviation is 5 and the sample size is 24. For what sample mean would the p-value be equal to 0.05? Assume that all conditions necessary for inference are satisfied.\n\nOur one sample \\(z\\) test is \\(H_0: \\mu = \\mu_0 \\quad H_1: \\mu &lt; \\mu_0\\) with a significance level \\(\\alpha\\).\n\nDescribe how we reject \\(H_0\\) using the critical-value method and the \\(p\\)-value method.\nWhy do the two methods lead to the same conclusion?",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-ht.html#footnotes",
    "href": "infer-ht.html#footnotes",
    "title": "16¬† Hypothesis Testing",
    "section": "",
    "text": "Hypothesis testing is also called Null Hypothesis Statistical Testing (NHST), statistical testing or test of significance.‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>16</span>¬† <span class='chapter-title'>Hypothesis Testing</span>"
    ]
  },
  {
    "objectID": "infer-twomean.html",
    "href": "infer-twomean.html",
    "title": "17¬† Comparing Two Population Means",
    "section": "",
    "text": "17.1 Introduction\nWhy Compare Two Populations?\nWe‚Äôve discussed estimation (Chapter 14) and hypothesis testing (Chapter 16) for one single population mean \\(\\mu.\\) The methods we learned can only be used for one sample or population. However, quite often we are faced with a comparison of parameters from different populations. For example,\nIf these two samples are drawn from two target populations with means, \\(\\mu_1\\) and \\(\\mu_2\\) respectively, the testing problem can be formulated as  \\[\\begin{align}\n  &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 &gt; \\mu_2\n  \\end{align}\\]\nwhere \\(\\mu_1\\) for example is the male mean annual income and \\(\\mu_2\\) is the female mean annual income Or \\(\\mu_1\\) is the mean weight loss from the New Diet group, and \\(\\mu_2\\) is the mean weight loss from the Placebo group.\nTo compare two means, we need two samples, one for one mean. But the two samples may be dependent or independent, and the methods for comparing two means depend on whether the two samples are dependent or not. So let‚Äôs see what dependent and independent samples are.\nDependent and Independent Samples\nThe two samples used to compare two population means can be independent or dependent.\nDependent Samples (Matched Pairs)\nFrom the two examples, we learn that subject 1 may refer to\nIf we have data with only one variable, in R the data is usually saved as a vector. When we have two samples, the two samples can be saved as a vector separately, or saved as a data matrix with the two samples combined by columns like the table below. Each row is for one matched pair, or the same subject. One column is for one sample data. Note that since every subject in dependent samples is paired, the two samples are of the same size \\(n\\).\nIndependent Samples\nWe may want to compare the mean salary level of male and female. What we can do is to collect two samples independently, one for each group, from their own population. Any subject in the male group has nothing to do with any subject in the female group, and any subject in the male group cannot be paired with any subject in the female group in any way.\nThe independent samples can be summarized as the table below. Notice that the two samples can have different sample sizes, \\(n_1\\) and \\(n_2\\) for example. Because the subjects in the two samples are not paired, we can collect salary data from 50 males and 65 females. In the data table, \\(x_{14}\\) means the 4th subject measurement in the first group, and \\(x_{23}\\) means the 3rd subject measurement in the second group. In general, \\(x_{ij}\\) is the \\(j\\)-th measurement in the \\(i\\)-th group.\nInference from Two Samples\nThe statistical methods are different for these two types of samples. The good news is the concepts of confidence intervals and hypothesis testing for one population can be applied to two-population cases.\nLet‚Äôs quickly review the confidence interval and test statistic in the one-sample case.\n\\(\\text{CI = point estimate} \\pm \\text{margin of error (E)}\\)\nMargin of error = critical value \\(\\times\\) standard error of the point estimator\nThe 6 testing steps are the same, and both critical value and \\(p\\)-value method can be applied too.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Comparing Two Population Means</span>"
    ]
  },
  {
    "objectID": "infer-twomean.html#introduction",
    "href": "infer-twomean.html#introduction",
    "title": "17¬† Comparing Two Population Means",
    "section": "",
    "text": "Comparing the mean annual income for Male and Female groups. \n Testing if a diet used for losing weight is effective from Placebo samples and New Diet samples. \n\n\n\n\n\n\n\n\n\nTwo samples are dependent or matched pairs if the sample values are matched, where the matching is based on some inherent relationship. For example,\n\n Height data of fathers and daughters, where the height of each dad is matched with the height of his daughter.  Clearly, father and daughter share the same life style, genes, and other factors that affect both father and daughter‚Äôs height. So the taller the father is, the taller the daughter tends to be. Their heights are positively correlated. In the two sample data sets, the first father height in the father‚Äôs sample is paired with the first daughter height in the daughter‚Äôs sample. Same for the second pair, third pair, and so on.\n Weights of subjects measure before and after some diet treatment, where the subjects are the same both before and after treatments.  In this example, we again have two samples. The two samples are dependent because the subjects in the two samples are identical. Your weight today is of course related to your weight last week, right? In the two sample data sets, the first weight in the sample before diet treatment is paired with the first weight in the sample after treatment. The two sample values belong to the same person. Same for the second pair, third pair, and so on.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nthe first matched pair (dad-daughter)\nthe same person with two measurements (before and after treatment)\n\n\n\n\nSubject\n(Dad) Before\n(Daughter) After\n\n\n\n1\n\\(x_{b1}\\)\n\\(x_{a1}\\)\n\n\n2\n\\(x_{b2}\\)\n\\(x_{a2}\\)\n\n\n3\n\\(x_{b3}\\)\n\\(x_{a3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{bn}\\)\n\\(x_{an}\\)\n\n\n\n\n\n\nTwo samples are independent if the sample values from one population are not related to the sample values from the other. For example,\n\n Salary samples of men and women, where the two samples are drawn independently from the male and female groups. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject of Group 1 (Male)\nMeasurement of Group 1\nSubject of Group 2 (Female)\nMeasurement of Group 2\n\n\n\n1\n\\(x_{11}\\)\n1\n\\(x_{21}\\)\n\n\n2\n\\(x_{12}\\)\n2\n\\(x_{22}\\)\n\n\n3\n\\(x_{13}\\)\n3\n\\(x_{23}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n_1\\)\n\\(x_{1n_1}\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\\(n_2\\)\n\\(x_{2n_2}\\)\n\n\n\n\n\n\n\n\n\ne.g., \\(\\overline{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\n\n\ne.g., \\(t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}}\\)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Comparing Two Population Means</span>"
    ]
  },
  {
    "objectID": "infer-twomean.html#inferences-about-two-means-dependent-samples-matched-pairs",
    "href": "infer-twomean.html#inferences-about-two-means-dependent-samples-matched-pairs",
    "title": "17¬† Comparing Two Population Means",
    "section": "\n17.2 Inferences About Two Means: Dependent Samples (Matched Pairs)",
    "text": "17.2 Inferences About Two Means: Dependent Samples (Matched Pairs)\nIn this section we talk about the inference methods for comparing two population means when the samples are dependent.\n Hypothesis Testing for Dependent Samples \n\n\n\n\n\n\nTo analyze a paired data set, we can simply analyze the differences!\n\n\n\n\n\n\nSuppose we would like to learn if the population means \\(\\mu_1\\) and \\(\\mu_2\\) are different. We can conduct a test like  \\[\\begin{align}\n  &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 \\ne \\mu_2\n  \\end{align}\\] \nThe null hypothesis can also be written as \\(H_0: \\mu_1 - \\mu_2 = 0\\). We don‚Äôt really want to know the value of \\(\\mu_1\\) and/or \\(\\mu_2\\), and we just care about if they are equal, or their difference is zero. Therefore, if we let \\(\\mu_d\\) be the difference \\(\\mu_1 - \\mu_2\\), we can write our hypothesis as \\[\\begin{align}\n  &H_0: \\mu_d = 0 \\\\ &H_1: \\mu_d \\ne 0\n  \\end{align}\\] \nor more generally for any types of test,\n \\[\\begin{align} & H_0: \\mu_1 - \\mu_2 = 0 \\iff \\mu_d = 0 \\\\ & H_1: \\mu_1 - \\mu_2 &gt; 0 \\iff \\mu_d &gt; 0 \\\\ & H_1: \\mu_1 - \\mu_2 &lt; 0 \\iff \\mu_d &lt; 0  \\\\ & H_1: \\mu_1 - \\mu_2 \\ne 0 \\iff \\mu_d \\ne 0 \\end{align}\\] \nFor dependent samples, we just transform the two samples into one difference sample by taking the difference between paired measurements. We use the difference sample to do the inference about the mean difference. The data table below illustrate the idea. We create a new difference sample data \\((d_1, d_2, \\dots, d_n)\\) where the \\(i\\)-th sample difference is \\(x_{1i} - x_{2i}\\), the difference between the \\(i\\)-th measurement in the first sample and the \\(i\\)-th measurement in the second sample.\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(x_1\\)\n\\(x_2\\)\nDifference \\(d = x_1 - x_2\\)\n\n\n\n1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(\\color{red}{d_1}\\)\n\n\n2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(\\color{red}{d_2}\\)\n\n\n3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(\\color{red}{d_3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\color{red}{\\vdots}\\)\n\n\n\\(n\\)\n\\(x_{1n}\\)\n\\(x_{2n}\\)\n\\(\\color{red}{d_n}\\)\n\n\n\nThe sample \\((d_1, d_2, \\dots, d_n)\\) is used to estimate the mean difference \\(\\mu_d = \\mu_1 - \\mu_2\\). So what is our point estimate of \\(\\mu_d\\)? The point estimate is the sample average of \\((d_1, d_2, \\dots, d_n)\\) which is \\(\\overline{d}\\). Actually, the point estimate is equal to \\(\\overline{x}_1 - \\overline{x}_2\\), the estimate for \\(\\mu_1 - \\mu_2\\).\n\n\n\n\n Inference for Paired Data \nHere are the requirements for the inference for paired data. The sample differences \\(\\color{blue}{d_i}\\)s form a random sample, and they are from a normal distribution and/or the sample size \\(n &gt; 30\\). Remember that when analyzing paired data, we focus on the difference of measurements, and this \\(d_i\\) sample becomes our new one single sample for inference about one single population parameter, the mean difference \\(\\mu_d\\).\n\nWith this, the inference for paired data follows the same procedure as the one-sample \\(t\\)-test! Therefore, the test statistic is \\[\\color{blue}{t_{test} = \\frac{\\overline{d}-\\mu_d}{s_d/\\sqrt{n}}} = \\frac{\\overline{d}-0}{s_d/\\sqrt{n}} \\sim T_{n-1}\\] under \\(H_0\\) where \\(\\overline{d}\\) and \\(s_d\\) are the mean and standard deviation of the difference samples \\((d_1, d_2, \\dots, d_n)\\).\n\nThe critical value is either \\(t_{\\alpha, n-1}\\) or \\(t_{\\alpha/2, n-1}\\) depending on if it is a one-tailed or two-tailed test. Below is a table summarizing information necessary to make inferences about paired data.\n\n\n\n\n\n\n\nPaired \\(t\\)-test\nTest Statistic\nConfidence Interval for \\(\\mu_d = \\mu_1 - \\mu_2\\)\n\n\n\n\n\\(\\sigma_d\\) is unknown\n\\(\\large t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}}\\)\n\\(\\large \\overline{d} \\pm t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\)\n\n\nThe test for matched pairs is called a paired \\(t\\)-test.\n\n Example \nConsider a capsule used to reduce blood pressure (BP) for individuals with hypertension. A sample of 10 individuals with hypertension takes the medicine for 4 weeks. The BP measurements before and after taking the medicine are shown in the table below. Does the data provide sufficient evidence that the treatment is effective in reducing BP?\n\n\n\n\n\n\n\n\nSubject\nBefore \\((x_b)\\)\n\nAfter \\((x_a)\\)\n\nDifference \\(d = x_b - x_a\\)\n\n\n\n1\n143\n124\n19\n\n\n2\n153\n129\n24\n\n\n3\n142\n131\n11\n\n\n4\n139\n145\n-6\n\n\n5\n172\n152\n20\n\n\n6\n176\n150\n26\n\n\n7\n155\n125\n30\n\n\n8\n149\n142\n7\n\n\n9\n140\n145\n-5\n\n\n10\n169\n160\n9\n\n\n\nGiven the data \\(x_b\\) and \\(x_a\\), we first take the difference and create the new data \\(d = x_b - x_a\\) for each subject.\n Step 1 \n\nIf we let \\(\\mu_1 =\\) Mean Before, \\(\\mu_2 =\\) Mean After, and \\(\\mu_d = \\mu_1 - \\mu_2\\), then the hypothesis is  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\iff \\mu_d = 0\\\\ &H_1: \\mu_1 &gt; \\mu_2 \\iff \\mu_d &gt; 0 \\end{align}\\)  The key is that ‚Äúthe treatment is effective in reducing BP‚Äù means the mean BP after taking the medicine is lower than that before, so \\(\\mu_1 &gt; \\mu_2\\).\n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \nFor the \\(d\\) sample, \\(\\overline{d} = 13.5\\), \\(s_d= 12.48\\).  \\(t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}} = \\frac{13.5}{12.48/\\sqrt{10}} = 3.42.\\) \n Step 4-c \n\nThis is a right-tailed test.  \\(t_{\\alpha, n-1} = t_{0.05, 9} = 1.833\\).\n\n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} &gt; t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = 3.42 &gt; 1.833  = t_{\\alpha, n-1}\\), we reject \\(H_0\\).\n\n Step 6 \n\n There is sufficient evidence to support the claim that the drug is effective in reducing blood pressure. \n\n\n\n\n\n\n\n\nFigure¬†17.1: Illustration of right-tailed test for blood pressure example\n\n\n\n\nThe 95% CI for \\(\\mu_d = \\mu_1 - \\mu_2\\) is \\[\\begin{align}\\overline{d} \\pm t_{\\alpha/2, df} \\frac{s_d}{\\sqrt{n}} &= 13.5 \\pm t_{0.025, 9}\\frac{12.48}{\\sqrt{10}}\\\\ &= 13.5 \\pm 8.927 \\\\ &= (4.573, 22.427).\\end{align}\\]\nWe are 95% confident that the mean difference in blood pressure is between 4.57 and 22.43. Since the interval does NOT include 0, it leads to the same conclusion as rejection of \\(H_0\\).\n\n\n\nR\nPython\n\n\n\nBelow is the same data as in the previous hypertension example. We load the R data pair_data.RDS into the R session using the load() function. \n\n\n\n# Load the data set\nload(\"../introstatsbook/data/pair_data.RDS\")\npair_data\n\n   before after\n1     143   124\n2     153   129\n3     142   131\n4     139   145\n5     172   152\n6     176   150\n7     155   125\n8     149   142\n9     140   145\n10    169   160\n\n## Create the difference data d\n(d &lt;- pair_data$before - pair_data$after)\n\n [1] 19 24 11 -6 20 26 30  7 -5  9\n\n## sample mean of d\n(d_bar &lt;- mean(d))\n\n[1] 13.5\n\n\n\n\n\n\n\n## sample standard deviation of d\n(s_d &lt;- sd(d))\n\n[1] 12.48332\n\n## t test statistic\n(t_test &lt;- d_bar/(s_d/sqrt(length(d))))\n\n[1] 3.419823\n\n## t critical value\n\nqt(p = 0.95, df = length(d) - 1)\n\n[1] 1.833113\n\n## p value\npt(q = t_test, df = length(d) - 1, \n   lower.tail = FALSE)\n\n[1] 0.003815036\n\n\n\n\nBelow is an example of how to calculate the confidence interval for the change in blood pressure.\n\n## 95% confidence interval for the mean difference of the paired data\nd_bar + c(-1, 1) * qt(p = 0.975, df = length(d) - 1) * (s_d / sqrt(length(d)))\n\n[1]  4.569969 22.430031\n\n\nWe can see that performing these calculations in R leads us to the same conclusions we previously made. In fact, the R function t.test() does Student‚Äôs t-test for us, either one sample or two samples. To do the two sample paired \\(t\\) test, we provides the two samples in the argument x and y. The alternative argument is either ‚Äútwo.sided‚Äù, ‚Äúless‚Äù or ‚Äúgreater‚Äù. We have \\(H_1: \\mu_1 &gt; \\mu_2\\) or \\(\\mu_d &gt; 0\\), so we use ‚Äúgreater‚Äù. The argument \\mu is the difference in means in \\(H_0\\), which is zero in our case. Finally, the paired argument should be set as TRUE in order to do the paired \\(t\\) test.\n\n## t.test() function\nt.test(x = pair_data$before, y = pair_data$after,\n       alternative = \"greater\", mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pair_data$before and pair_data$after\nt = 3.4198, df = 9, p-value = 0.003815\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 6.263653      Inf\nsample estimates:\nmean difference \n           13.5 \n\n\n\nNote that we get the same test statistic and p-value, as well as the test conclusion. However, the one-sided 95% confidence interval shown in the output is not what we want! We may have a one-sided or two-sided test, but we should always use the two-sided confidence interval. When you use the t.test() function to do a one-sided test as we do here, please do not use the confidence interval in the output.\n\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import ttest_rel, t, norm\nimport pandas as pd\n\nBelow is the same data as in the previous hypertension example. We load the csv file pair_data.csv into the Python session using the pd.read_csv() function.\n\npair_data = pd.read_csv('./data/pair_data.csv')\npair_data\n\n   before  after\n0     143    124\n1     153    129\n2     142    131\n3     139    145\n4     172    152\n5     176    150\n6     155    125\n7     149    142\n8     140    145\n9     169    160\n\n\n\n# Create the difference data 'd'\nd = pair_data['before'] - pair_data['after']\nd\n\n0    19\n1    24\n2    11\n3    -6\n4    20\n5    26\n6    30\n7     7\n8    -5\n9     9\ndtype: int64\n\n# Sample mean of 'd'\nd_bar = np.mean(d)\nd_bar\n\n13.5\n\n# Sample standard deviation of 'd'\ns_d = np.std(d, ddof=1)\ns_d\n\n12.48332220738267\n\n# T-test statistic\nt_test = d_bar / (s_d / np.sqrt(len(d)))\nt_test\n\n3.4198226804580676\n\n# T critical value (one-tailed, 95% confidence level)\nt.ppf(0.95, df=len(d)-1)\n\n1.8331129326536335\n\n# P-value\nt.sf(t_test, df=len(d)-1)\n\n0.0038150362846879134\n\n\nBelow is an example of how to calculate the confidence interval for the change in blood pressure.\n\n# 95% confidence interval for the mean difference\nd_bar + pd.Series([-1, 1]) * t.ppf(0.975, df=len(d)-1) * (s_d / np.sqrt(len(d)))\n\n0     4.569969\n1    22.430031\ndtype: float64\n\n\nWe can see that performing these calculations in Python leads us to the same conclusions we previously made.\nIn fact, the Python function ttest_rel() from the scipy.stats module does two sample paired t-test for us. The word rel means ‚Äúrelated‚Äù because it calculates the t-test on TWO RELATED samples. To do the two sample paired \\(t\\) test, we provides the two samples in the first two arguments a and b. The alternative argument is either ‚Äútwo.sided‚Äù, ‚Äúless‚Äù or ‚Äúgreater‚Äù. We have \\(H_1: \\mu_1 &gt; \\mu_2\\) or \\(\\mu_d &gt; 0\\), so we use ‚Äúgreater‚Äù. The function only tests whether or not the mean difference is zero.\n\n# T-test using scipy's built-in function\nttest_rel_res = ttest_rel(a=pair_data['before'], b=pair_data['after'], \n                          alternative='greater')\n\nttest_rel_res.statistic\n\n3.419822680458067\n\nttest_rel_res.df\n\n9\n\nttest_rel_res.pvalue\n\n0.0038150362846879134\n\nttest_rel_res.confidence_interval\n\n&lt;bound method TtestResult.confidence_interval of TtestResult(statistic=3.419822680458067, pvalue=0.0038150362846879134, df=9)&gt;\n\n\nNote that we get the same test statistic and p-value, as well as the test conclusion. The function does not print the confidence interval out though.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Comparing Two Population Means</span>"
    ]
  },
  {
    "objectID": "infer-twomean.html#inference-about-two-means-independent-samples",
    "href": "infer-twomean.html#inference-about-two-means-independent-samples",
    "title": "17¬† Comparing Two Population Means",
    "section": "\n17.3 Inference About Two Means: Independent Samples",
    "text": "17.3 Inference About Two Means: Independent Samples\n Compare Population Means: Independent Samples \nFrequently we would like to compare two different groups. For example,\n\nWhether stem cells can improve heart function. Here the two samples are patients with the stem cell treatment and the ones without the stem cell treatment.\nThe relationship between pregnant women‚Äôs smoking habits and newborns‚Äô weights. Here the two samples are women who smoke and women who don‚Äôt.\nWhether one variation of an exam is harder than another variation. In this case, the two samples are students having exam A and students taking exam B.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn those examples, the two samples are independent. In this section, we are going to learn how to compare their population means. For example, whether the mean score of exam A is higher than the mean score of exam B.\n\n Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \nWhen we deal with two independent samples, we assume they are drawn from two independent populations which are assumed to be normally distributed in this chapter. We are interested in whether the two population means are equal or which is greater. But to do the inference, we need to take care of their standard deviation, \\(\\sigma_1\\) and \\(\\sigma_2\\). First we discuss the case when \\(\\sigma_1 \\ne \\sigma_2\\).\nThe requirements of testing for independent samples with \\(\\sigma_1 \\ne \\sigma_2\\) are\n\nThe two samples are independent.\nBoth samples are random samples.\n\n\\(n_1 &gt; 30\\), \\(n_2 &gt; 30\\) and/or both samples are from a normally distributed population. Large sample sizes are for the application of the central limit theorem. Note that both sample sizes should be large or at least greater than 30. If either one sample size is small, the inference methods discussed here are not valid.\n\nWe are interested in whether the two population means, \\(\\mu_1\\) and \\(\\mu_2\\), are equal or if one is larger than the other. Therefore, we have \\[H_0: \\mu_1 = \\mu_2\\]\nThis is equivalent to testing if their difference is zero, or \\(H_0: \\mu_1 - \\mu_2 = 0\\).\n\n\n\n\n\n\nWe start by finding a point estimator for \\(\\mu_1 - \\mu_2\\). What is the best point estimator for \\(\\mu_1 - \\mu_2\\)?\n\n\n\n\n\n\\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\)!\n\n\n\n Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) \nAgain, to do the inference we start with the associated sampling distribution. If the two samples are from independent normally distributed populations or \\(n_1 &gt; 30\\) and \\(n_2 &gt; 30\\), then at least approximately\n\\[\\small \\overline{X}_1 \\sim N\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1} \\right), \\quad \\overline{X}_2 \\sim N\\left(\\mu_2,\n\\frac{\\sigma_2^2}{n_2} \\right)\\]\nBecause we use \\(\\overline{X}_1 - \\overline{X}_2\\) to estimate \\(\\mu_1 - \\mu_2\\), we need the sampling distribution of \\(\\overline{X}_1 - \\overline{X}_2\\). We won‚Äôt talk about the details, but it can be shown that \\(\\overline{X}_1 - \\overline{X}_2\\) has the sampling distribution \\[\\small \\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} \\color{red}{+} \\color{black}{\\frac{\\sigma_2^2}{n_2}} \\right). \\] Before careful that the variance is the sum of the variance of \\(\\overline{X}_1\\) and \\(\\overline{X}_2\\), even the random variable is their difference. Please take MATH 4700 Introduction to Probability to learn more about the properties of normal distribution.\nTherefore by standardization, we have a standard normal variable \\[\\small Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)\\]\n Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \nWith \\(D_0\\) being a hypothesized value, our testing problem could be one of the followings\n\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\le D_0\\\\ &H_1: \\mu_1 - \\mu_2 &gt; D_0 \\end{align}\\)  (right-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\ge D_0\\\\ &H_1: \\mu_1 - \\mu_2 &lt; D_0 \\end{align}\\)  (left-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 = D_0\\\\ &H_1: \\mu_1 - \\mu_2 \\ne D_0 \\end{align}\\)  (two-tailed)\n\nOften, we care about of the two means are equal, so \\(D_0\\) is zero. But \\(D_0\\) could be any number that fits your research question. For example, you may wonder if mean weight of male is greater than the mean weight of female by 20 pounds. Then your \\(H_1\\) would be \\(H_1: \\mu_1 - \\mu_2 &gt; 20\\) where \\(\\mu_1\\) is the mean weight of male and \\(\\mu_2\\) is the mean weight of female, and \\(D_0\\) is 20.\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, with the \\(\\overline{x}_1\\), \\(\\overline{x}_2\\) calculated by our sample, and under the null hypothesis, the test statistic is the z-score from the sampling distribution which is \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}.\\]\nThen we are pretty much done. We find \\(z_{\\alpha}\\) or \\(z_{\\alpha/2}\\) and follow our testing steps!\nWhat if \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown? You should be able to sense the answer. The test statistic becomes \\(t_{test}\\):\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}. \\]\nWe simply replace the unknown \\(\\sigma_1\\) and \\(\\sigma_2\\) with their corresponding sample estimate \\(s_1\\) and \\(s_2\\)! The critical value is either \\(t_{\\alpha, df}\\) (one-tailed) or \\(t_{\\alpha/2, df}\\) (two-tailed) with the degrees of freedom\n\\[df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\] where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\).\nThe degrees of freedom looks intimidating, but no worries you don‚Äôt need to memorize the formula, and we let the statistical software take care of it. To be conservative (tend to reject \\(H_0\\) less) if the \\(df\\) is not an integer, we round it down to the nearest integer, it does not matter much though.\n Inference About Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \nBelow is a table that summarizes ways to make inferences about independent samples when \\((\\sigma_1 \\ne \\sigma_2)\\).\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 \\ne \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}\\)\n\n\n\nFor unknown \\(\\sigma_1\\) and \\(\\sigma_2\\), we use \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\) to get the \\(p\\)-value, critical value and confidence interval. The unequal-variance t-test is called Welch‚Äôs t-test in the literature.\n Example: Two-Sample t-Test \n\n\nDoes an over-sized tennis racket exert less stress/force on the elbow? The relevant sample statistics are shown below.\n\n\nOver-sized: \\(n_1 = 33\\), \\(\\overline{x}_1 = 25.2\\), \\(s_1 = 8.6\\)\n\n\nConventional: \\(n_2 = 12\\), \\(\\overline{x}_2 = 33.9\\), \\(s_2 = 17.4\\)\n\n\nThe two populations are known to be nearly normal, and because of the large difference in the sample standard deviation suggests \\(\\sigma_1 \\ne \\sigma_2\\). Please form a hypothesis test with \\(\\alpha = 0.05\\), and construct a 95% CI for the mean difference of force on the elbow.\n\n\n\n\n\nSource: unsplash-Jeffery Erhunse\n\n\n\n\n\n Step 1 \n\nLet \\(\\mu_1\\) be the mean force on the elbow for the over-size rackets, and \\(\\mu_2\\) be the mean force for the conventional rackets. The we have a test  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 &lt; \\mu_2 \\end{align}\\) \n\n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(25.2 - 33.9) - 0}{\\sqrt{\\frac{\\color{red}{8.6^2}}{33} + \\frac{\\color{red}{17.4^2}}{12}}} = -1.66\\)\n\n\\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\)\n\n\n\\(\\small A = \\dfrac{8.6^2}{33}\\), \\(\\small B = \\dfrac{17.4^2}{12}\\), \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the computed value of \\(df\\) is not an integer, always round down to the nearest integer.\n\n\n Step 4-c \n\nBecause it is a left-tailed test, the critical value is  \\(-t_{0.05, 13} = -1.771\\). \n\n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} &lt; -t_{\\alpha, df}\\). \\(\\small t_{test} = -1.66 &gt; -1.771 = -t_{\\alpha, df}\\), we fail to reject \\(H_0\\). \n\n Step 6 \n\n There is insufficient evidence to support the claim that the the oversized racket delivers less stress to the elbow. \n\nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is\n\\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}} &= (25.2 - 33.9) \\pm t_{0.025,13}\\sqrt{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}}\\\\&= -8.7 \\pm 11.32 = (-20.02, 2.62).\\end{align}\\]\nWe are 95% confident that the difference in the mean forces is between -20.02 and 2.62. Since the interval includes 0, it leads to the same conclusion as failing to reject \\(H_0\\).\n Two-Sample t-Test in R \n\n\nR\nPython\n\n\n\n\n## Prepare needed variables \nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA &lt;- s1^2 / n1; B &lt;- s2^2 / n2\ndf &lt;- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n\n## Use floor() function to round down to the nearest integer.\n(df &lt;- floor(df))\n\n[1] 13\n\n## t_test\n(t_test &lt;- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n\n[1] -1.659894\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.770933\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 0.06042575\n\n\n\n\n\n# Prepare needed variables\nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA = s1**2 / n1; B = s2**2 / n2\ndf = (A + B)**2 / (A**2/(n1-1) + B**2/(n2-1))\n\n# Round down to the nearest integer\ndf = np.floor(df)\ndf\n\n13.0\n\n# T-test statistic\nt_test = (x1_bar - x2_bar) / np.sqrt(s1**2/n1 + s2**2/n2)\n\n# T critical value\nt.ppf(0.05, df=df)\n\n-1.7709333959867992\n\n# P-value\nt.cdf(t_test, df=df)\n\n0.060425745501011804\n\n\n\n\n\n\n Testing for Independent Samples (\\(\\sigma_1 = \\sigma_2 = \\sigma\\)) \nWe‚Äôve done the case that \\(\\sigma_1 \\ne \\sigma_2\\). Now we are talking about the case when \\(\\sigma_1 = \\sigma_2\\). Because they are equal, the two \\(\\sigma\\)s can be treated as one common \\(\\sigma\\).\n Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) \nAgain, we start with the sampling distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) \\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} \\right).\\] If \\(\\sigma_1 = \\sigma_2 = \\sigma\\), we can write \\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\right).\\] Therefore, \\[ Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1).\\]\n Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\) \nSimilar to the case that \\(\\sigma_1 = \\sigma_2\\), if \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\]\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, we use \\(t_{test}\\) just like we would for the one-sample case. Now here is the key point. We don‚Äôt know the common \\(\\sigma\\). How do we use the two independent samples to construct one point estimate of \\(\\sigma\\), so that the estimate can replace \\(\\sigma\\) in the z-score to get the \\(t\\) test statistic?\n\nThe idea is to use the so-called pooled sample variance to estimate the common population variance, \\(\\sigma^2\\):\n\\[ s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} \\]\nAs \\(\\sigma_1 = \\sigma_2 = \\sigma\\), we just need one sample standard deviation to replace the population standard deviation, \\(\\sigma\\). The two samples are from the populations with the same variance, and \\(s_1^2\\) and \\(s_2^2\\) are estimating the same population variance. But we just need one estimate. The idea is to combine the two sample variances \\(s_1^2\\) and \\(s_2^2\\) together, so that we can use all the information from the two samples to obtain one single point estimate \\(s_p^2\\) for \\(\\sigma^2\\). The pooled estimate \\(s_p^2\\) is in fact the weighted average of \\(s_1^2\\) and \\(s_2^2\\) weighted by their corresponding degrees of freedom \\(n_1-1\\) and \\(n_2-1\\). The idea is that when the sample size is large, we tend to get a more precise estimate. So \\(s_p^2\\) would be closer to \\(s_1^2\\) or \\(s_2^2\\) whichever has large sample size.\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, we replace \\(\\sigma\\) with \\(s_p\\) and get the \\(t\\) test statistic\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\] Here, the critical value is either \\(t_{\\alpha, df}\\) (for one-tailed tests) or \\(t_{\\alpha/2, df}\\) (for two-tailed tests), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[df = n_1 + n_2 - 2\\] Yes, that simple.\n Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\) \nBelow is a table that summarizes ways to make inferences about independent samples when \\(\\sigma_1 = \\sigma_2\\).\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 = \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\n\n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\nUse \\(df = n_1+n_2-2\\) to get the \\(p\\)-value, critical value and confidence interval.\nThe test from two independent samples with \\(\\sigma_1 = \\sigma_2  = \\sigma\\) is usually called two-sample pooled \\(z\\)-test or two-sample pooled \\(t\\)-test.\n\n Example: Weight Loss \n\n\nA study was conducted to see the effectiveness of a weight loss program. Two groups (Control and Experimental) of 10 subjects were selected. The two populations are normally distributed and have the same standard deviation.\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months, and the revelant sample statistics\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\nIs there a sufficient evidence at \\(\\alpha = 0.05\\) to conclude that the program is effective? If yes, construct a 95% CI for \\(\\mu_1 - \\mu_2\\) to show how much effective it is.\n Step 1 \n\nLet \\(\\mu_1\\) be the mean weight loss in the control group, and \\(\\mu_2\\) be the mean weight loss in the experimental group. Then we have  \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 &lt; \\mu_2 \\end{align}\\) \n\n\n\\(\\mu_1 &lt; \\mu_2\\) means the weight loss program, the program the experimental group are in, is effective because on average the participants in the the experimental group loss weights more than those in the control group.\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\nFrom the question we know \\(\\sigma_1 = \\sigma_2\\). So the test statistic is  \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\). \n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(10-1)0.5^2 + (10-1)0.7^2}{10+10-2}}=0.6083\\)\n \\(t_{test} = \\frac{(2.1 - 4.2) - 0}{0.6083\\sqrt{\\frac{1}{10} + \\frac{1}{10}}} = -7.72\\)\n\n\n\n Step 4-c \n\n \\(df = n_1 + n_2 - 2 = 10 + 10 - 2 = 18\\). So \\(-t_{0.05, df = 18} = -1.734\\). \n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} &lt; -t_{\\alpha, df}\\). Since \\(\\small t_{test} = -7.72 &lt; -1.734  = -t_{\\alpha, df}\\), we reject \\(H_0\\).\n\n Step 4-p \n\n The \\(p\\)-value is \\(P(T_{df=18} &lt; t_{test}) \\approx 0\\) \n\n Step 5-p \n\n We reject \\(H_0\\) if \\(p\\)-value &lt; \\(\\alpha\\). Since \\(p\\)-value \\(\\approx 0 &lt; 0.05  = \\alpha\\), we reject \\(H_0\\).\n\n Step 6 \n\n There is sufficient evidence to support the claim that the weight loss program is effective. \n\nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is \\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} &= (2.1 - 4.2) \\pm t_{0.025, 18} (0.6083)\\sqrt{\\frac{1}{10} + \\frac{1}{10}}\\\\ &= -2.1 \\pm 0.572 = (-2.672, -1.528) \\end{align}\\]\nWe are 95% confident that the difference in the mean weight loss is between -2.672 and -1.528. Since the interval does not include 0, it leads to the same conclusion as rejection of \\(H_0\\).\n Two-Sample Pooled t-Test \n\n\nR\nPython\n\n\n\n\n## Prepare values\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\n\n## pooled sample standard deviation\nsp &lt;- sqrt(((n1 - 1) * s1 ^ 2 + (n2 - 1) * s2 ^ 2) / (n1 + n2 - 2))\n\n## degrees of freedom\ndf &lt;- n1 + n2 - 2\n\n## t_test\n(t_test &lt;- (x1_bar - x2_bar) / (sp * sqrt(1 / n1 + 1 / n2)))\n\n[1] -7.719754\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.734064\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 2.028505e-07\n\n\n\n\n\n# Prepare values\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\n\n# Pooled sample standard deviation\nsp = np.sqrt(((n1 - 1) * s1**2 + (n2 - 1) * s2**2) / (n1 + n2 - 2))\n\n# Degrees of freedom\ndf = n1 + n2 - 2\n\n# T-test statistic\nt_test = (x1_bar - x2_bar) / (sp * np.sqrt(1/n1 + 1/n2))\nt_test\n\n-7.719753531984983\n\n# T critical value\nt.ppf(0.05, df=df)\n\n-1.734063606617536\n\n# P-value\nt.cdf(t_test, df=df)\n\n2.0285052120014635e-07\n\n\n\n\n\n\n17.3.1 Sample size formula\nHow large both sample sizes \\(n_1\\) and \\(n_2\\) needed in order to correctly discover a difference in means (reject \\(H_0\\) when \\(H_1\\) is true) with a specified level of power \\(1-\\beta\\) when the difference in means \\(|\\mu_1-\\mu_2| \\ge D\\)?\nSuppose we like to do a two-sample test (independent samples and equal variance \\(\\sigma_1^2 = \\sigma_2^2 = \\sigma^2\\)) and both sample sizes need at least \\(n\\) measurements. It can be shown that \\(n\\) should be at least\n\nOne-tailed test (either left-tailed or right-tailed): \\[ n_1 = n_2 = n = 2\\sigma^2 \\frac{(\\color{blue}{z_{\\alpha}} + z_{\\beta})^2}{D^2} \\]\nTwo-tailed test: \\[ n_1 = n_2 = n = 2\\sigma^2 \\frac{( \\color{blue}{z_{\\alpha/2}} + z_{\\beta})^2}{D^2} \\]",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Comparing Two Population Means</span>"
    ]
  },
  {
    "objectID": "infer-twomean.html#exercises",
    "href": "infer-twomean.html#exercises",
    "title": "17¬† Comparing Two Population Means",
    "section": "\n17.4 Exercises",
    "text": "17.4 Exercises\n\nA study was conducted to assess the effects that occur when children are expected to cocaine before birth. Children were tested at age 4 for object assembly skill, which was described as ‚Äúa task requiring visual-spatial skills related to mathematical competence.‚Äù The 187 children born to cocaine users had a mean of 7.1 and a standard deviation of 2.5. The 183 children not exposed to cocaine had a mean score of 8.4 and a standard deviation of 2.5.\n\nWith \\(\\alpha = 0.05\\), use the critical-value method and p-value method to perform a 2-sample t-test on the claim that prenatal cocaine exposure is associated with lower scores of 4-year-old children on the test of object assembly.\nTest the claim in part (a) by using a confidence interval.\n\n\nListed below are heights (in.) of mothers and their first daughters.\n\nUse \\(\\alpha = 0.05\\) to test the claim that there is no difference in heights between mothers and their first daughters.\nTest the claim in part (a) by using a confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeight of Mother\n66\n62\n62\n63.5\n67\n64\n69\n65\n62.5\n67\n\n\nHeight of Daughter\n67.5\n60\n63.5\n66.5\n68\n65.5\n69\n68\n65.5\n64",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>17</span>¬† <span class='chapter-title'>Comparing Two Population Means</span>"
    ]
  },
  {
    "objectID": "infer-var.html",
    "href": "infer-var.html",
    "title": "18¬† Inference About Variances",
    "section": "",
    "text": "18.1 Inference for One Population Variance\nInference for Population Variances\nLet‚Äôs start with inference for one population variance. For point estimation, the most intuitive and straightforward point estimator for \\(\\sigma^2\\) is the sample variance \\(S^2\\) defined as\n\\[S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}.\\]\nNote that the the denominator is \\(n-1\\), not the sample size \\(n\\). Note that \\(S^2\\) is a random variable because each data point \\(X_i\\) and \\(\\overline{X}\\) are assumed random variables. Dividing by \\(n-1\\) instead of \\(n\\) actually means something. \\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\), i.e., \\(E(S^2) = \\sigma^2\\), which is a good property of a point estimator. If we were able to repeatedly collect data sets of the same size \\(n\\) lots of times, and for each data set we obtain its sample variance, then the average sample variance will be, if not exactly, very close to the true population variance.\nThe inference methods for \\(\\sigma^2\\) introduced in this chapter require the population to be normally distributed.\nChi-Square \\(\\chi^2\\) Distribution\nRemember that when we do the inference for population means, we use either normal distribution or Student‚Äôs \\(t\\) distribution. For the inference about the population variances, we use another distribution called chi-square \\(\\chi^2\\) distribution because the distribution is related to the sampling distribution of some variable involving \\(S^2\\) and \\(\\sigma^2\\). We‚Äôll talk about that later. Let‚Äôs first learn a little about the chi-square \\(\\chi^2\\) distribution.\nAs Student‚Äôs \\(t\\) distribution, the chi-square \\(\\chi^2\\) distribution has one parameter, degrees of freedom \\(df\\). To denote a specific chi-square distribution, we write \\(\\chi^2_{df}\\) like \\(\\chi^2_{2}\\) for a chi-square distribution or variable with degrees of freedom two. Figure¬†18.1 shows \\(\\chi^2\\) distributions with varying degrees of freedom. It is in general a right-skewed distribution, but it gets more and more symmetric as \\(df\\) gets larger. Also, the \\(\\chi^2\\) distribution is defined over positive numbers, one hint why we use it for inferring variances because variance is non-negative, but normal or \\(t\\) distribution is defined on the whole real line.\nFigure¬†18.1: Illustration of \\(\\chi^2\\) distributions with varying degrees of freedom\nUpper Tail and Lower Tail of Chi-Square\nLike standard normal and \\(t\\) distribution, to do the inference for variances, we get to find or define critical values of a chi-square distribution. With some probability \\(\\alpha\\), we define\nFigure¬†18.2 illustrates the \\(\\chi^2\\) critical values. Notice that in \\(N(0, 1)\\), \\(z_{1-\\frac{\\alpha}{2}} = -z_{\\frac{\\alpha}{2}}\\). Because of the symmetry of the distribution, the \\(z\\)-value having area \\(\\alpha/2\\) on the right is the \\(z\\)-value having area \\(\\alpha/2\\) on the left with a negative sign. However, the chi-square distribution is not symmetric, so \\(\\chi^2_{1-\\frac{\\alpha}{2},\\,df} \\ne -\\chi^2_{\\frac{\\alpha}{2},\\,df}\\).\nFigure¬†18.2: Illustration of \\(\\alpha/2\\) significance levels for \\(\\chi^2_{df}\\) distribution\nSampling Distribution\nWhen a random sample of size \\(n\\) is from \\(\\color{red}{N(\\mu, \\sigma^2)}\\), the following sample statistic has the \\(\\chi^2\\) sampling distribution with degrees of freedom \\(n-1\\): \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1}. \\]\nI would like to stress again that the inference method for \\(\\sigma^2\\) introduced here can work poorly if the normality assumption is violated, even for large samples.\nThe sample statistic involves \\(S^2\\) and \\(\\sigma^2\\), so we have manipulate its sampling distribution to obtain the confidence interval for \\(\\sigma^2\\), and use it as a test statistic in the hypothesis testing for \\(\\sigma^2\\).\n\\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\)\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} \\right)}}\\]\nHow do we get this interval? We start with the sampling distribution of \\(\\frac{(n-1)S^2}{\\sigma^2}\\).\nBe careful that the interval for \\(\\sigma^2\\) cannot be expressed as \\((S^2 - m, S^2 + m)\\) for some margin of error \\(m\\) anymore!\nExample: Supermodel Heights\nWe just need to get what we need for constructing the interval from the sample data, sample size \\(n\\), sample variance \\(s^2\\), \\(\\alpha\\), critical values \\(\\chi^2_{\\alpha/2, n-1}\\) and \\(\\chi^2_{1-\\alpha/2, n-1}\\).\nNote that we want the interval for \\(\\sigma\\), not \\(\\sigma^2\\), we take a square root of the lower and upper bound of the interval for \\(\\sigma^2\\). The \\(95\\%\\) CI for \\(\\sigma\\) is \\(\\small \\left( \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}}} \\right) = \\left( \\sqrt{\\frac{(16-1)(3.4)}{27.49}}, \\sqrt{\\frac{(16-1)(3.4)}{6.26}}\\right) = (1.36, 2.85)\\)\nWe are 95% confident that the height of supermodels has standard deviation between 1.36 and 2.85.\nTesting\nBack to the example. Use \\(\\alpha = 0.05\\) to test the claim that ‚Äúsupermodels have heights with a standard deviation that is less than the standard deviation, \\(\\sigma = 7.5\\) cm, for the population of women‚Äù.\nStep 1\nStep 2\nStep 3\nStep 4-c\nStep 5-c\nStep 6\nWe conclude that the heights of supermodels vary less than heights of women in the general women population.\nBack to Pooled t-Test\nIn a two sample pooled t-test, we assume\nWe can use a QQ-plot (and normality tests, Anderson, Shapiro, etc.) to check the assumption of a normal distribution. We now learn how to check the assumption \\(\\sigma_1 = \\sigma_2\\).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Inference About Variances</span>"
    ]
  },
  {
    "objectID": "infer-var.html#inference-for-one-population-variance",
    "href": "infer-var.html#inference-for-one-population-variance",
    "title": "18¬† Inference About Variances",
    "section": "",
    "text": "Important\n\n\n\n\nThe inference methods can work poorly if normality is violated, even if the sample is large.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\chi^2_{\\frac{\\alpha}{2},\\, df}\\) is a \\(\\chi^2\\) value of a \\(\\chi^2\\) distribution with degrees of freedom \\(df\\) such that it has area to the right of \\(\\alpha/2\\).\n\\(\\chi^2_{1-\\frac{\\alpha}{2},\\, df}\\) is a \\(\\chi^2\\) value of a \\(\\chi^2\\) distribution with degrees of freedom \\(df\\) such that it has area to the left of \\(\\alpha/2\\). In other words, it has area to the right of \\(1 - \\alpha/2\\), and that‚Äôs why it has a subscript \\(1-\\frac{\\alpha}{2}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P\\left(\\chi^2_{1-\\frac{\\alpha}{2}, \\,n-1} &lt; \\frac{(n-1)S^2}{\\sigma^2} &lt; \\chi^2_{\\frac{\\alpha}{2},\\, n-1} \\right) = 1 - \\alpha\\) (Goal: isolate \\(\\sigma^2\\))\n\n\\(P\\left(\\frac{\\chi^2_{1-\\frac{\\alpha}{2}, \\,n-1}}{(n-1)S^2} &lt; \\frac{1}{\\sigma^2} &lt; \\frac{\\chi^2_{\\frac{\\alpha}{2},\\, n-1}}{(n-1)S^2} \\right) = 1 - \\alpha\\) (divided by \\((n-1)S^2\\))\n\n\\(P\\left(\\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} &gt; \\sigma^2 &gt; \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\,n-1}} \\right) = 1 - \\alpha\\) (take reciprocal)\n\n\\(P\\left(\\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}} &lt; \\sigma^2 &lt; \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\,n-1}} \\right) = 1 - \\alpha\\) (smaller value on the left)\n\n\n\n\n\n\n\n\n\nListed below are heights (cm) for the simple random sample of 16 female supermodels.\n\n\nR\nPython\n\n\n\n\nheights &lt;- c(178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176)\n\n\n\n\nimport numpy as np\nheights = np.array([178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176])\n\n\n\n\nThe supermodels‚Äô heights are normally distributed. Please construct a \\(95\\%\\) confidence interval for population standard deviation \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 16\\), \\(s^2 = 3.4\\), \\(\\alpha = 0.05\\).\n\\(\\chi^2_{\\alpha/2, n-1} = \\chi^2_{0.025, 15} = 27.49\\)\n\\(\\chi^2_{1-\\alpha/2, n-1} = \\chi^2_{0.975, 15} = 6.26\\)\n\n\n\n\n\nR\nPython\n\n\n\nWe use qchisq() to get the \\(\\chi^2\\) critical values. The probability \\(p\\) and the degrees of freedom df need to be specified. You should be able to understand the rest part of the code. Enjoy it.\n\n## set values\nn &lt;- 16\ns2 &lt;- var(heights)\nalpha &lt;- 0.05\n\n## two chi-square critical values\nchi2_right &lt;- qchisq(p = alpha/2, df = n - 1, lower.tail = FALSE)\nchi2_left &lt;- qchisq(p = alpha/2, df = n - 1, lower.tail = TRUE)\n\n## two bounds of CI for sigma2\nci_sig2_lower &lt;- (n - 1) * s2 / chi2_right\nci_sig2_upper &lt;- (n - 1) * s2 / chi2_left\n\n## two bounds of CI for sigma\n(ci_sig_lower &lt;- sqrt(ci_sig2_lower))\n\n[1] 1.362104\n\n(ci_sig_upper &lt;- sqrt(ci_sig2_upper))\n\n[1] 2.853802\n\n\n\n\n\nimport numpy as np\nfrom scipy.stats import chi2\n\nWe use chi2 family in scipy.stats to get the \\(\\chi^2\\) critical values. The probability \\(p\\) and the degrees of freedom df need to be specified. You should be able to understand the rest part of the code. Enjoy it.\n\n# Set values\nn = 16\ns2 = np.var(heights, ddof=1)  # sample variance\nalpha = 0.05\n\n# Chi-square critical values\nchi2_right = chi2.isf(alpha/2, df=n-1)\nchi2_left = chi2.ppf(alpha/2, df=n-1)\n\n# Confidence interval for variance (sigma^2)\nci_sig2_lower = (n - 1) * s2 / chi2_right\nci_sig2_upper = (n - 1) * s2 / chi2_left\n\n# Confidence interval for standard deviation (sigma)\nci_sig_lower = np.sqrt(ci_sig2_lower)\nci_sig_lower\n\n1.3621044553698662\n\nci_sig_upper = np.sqrt(ci_sig2_upper)\nci_sig_upper\n\n2.8538016067984957\n\n\n\n\n\n\n\n\n\nWe are comparing the \\(\\sigma\\) of heights of supermodels with the \\(\\sigma\\) of heights of women in general which is 7.5. So the hypothesize value \\(\\sigma_0\\) is 7.5. We wonder if supermodel height standard deviation is smaller than 7.5. Therefore, we have test \\(H_0: \\sigma = \\sigma_0\\) vs.¬†\\(H_1: \\sigma &lt; \\sigma_0\\), where \\(\\sigma_0 = 7.5\\) cm.\n\n\n\n\\(\\alpha = 0.05\\)\n\n\n\nThe test statistic comes from the variable \\(\\chi_{test}^2 = \\frac{(n-1)S^2}{\\sigma^2}\\) that follows \\(\\chi^2_{n-1}\\) distribution. Under \\(H_0\\), we have \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), drawn from \\(\\chi^2_{n-1}\\).\n\n\n\nThis is a left-tailed test.\nThe critical value is \\(\\chi_{1-\\alpha, df}^2 = \\chi_{0.95, 15}^2 = 7.26\\)\n\n\n\n\nReject \\(H_0\\) in favor of \\(H_1\\) if \\(\\chi_{test}^2 &lt; \\chi_{1-\\alpha, df}^2\\).\nSince \\(0.91 &lt; 7.26\\), we reject \\(H_0\\). \n\n\n\n\n\n\n\nThere is sufficient evidence to support the claim that supermodels have heights with a standard deviation that is less than the standard deviation for the population of all women.\n\n\n\n\n\n\n \\(n_1 \\ge 30\\) and \\(n_2 \\ge 30\\) or that both samples are drawn from normal populations. \n \\(\\sigma_1 = \\sigma_2\\)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Inference About Variances</span>"
    ]
  },
  {
    "objectID": "infer-var.html#inference-for-comparing-two-population-variances",
    "href": "infer-var.html#inference-for-comparing-two-population-variances",
    "title": "18¬† Inference About Variances",
    "section": "\n18.2 Inference for Comparing Two Population Variances",
    "text": "18.2 Inference for Comparing Two Population Variances\n F Distribution \nFor comparing two population variances, we need another distribution called \\(F\\) distribution. The \\(F\\) distribution has two parameters \\(df_1\\) and \\(df_2\\), a hint why it is used for comparing two variances. We write a specific \\(F\\) distribution \\(F_{df_1, df_2}\\). Second, the \\(F\\) distribution is also right-skewed. Like \\(\\chi^2\\) distribution, the \\(F\\) distribution is defined over positive numbers. Figure¬†18.3 illustrates \\(F\\) distribution with different parameters. You can see that when \\(df_1\\) and \\(df_2\\) are both large, the \\(F\\) distribution looks bell-shaped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†18.3: F distributions with different parameters.\n\n\n\n\n\n\n Upper and Lower Tail of F Distribution \nWe denote \\(F_{\\alpha, \\, df_1, \\, df_2}\\) as the \\(F\\) quantile such that \\(P(F_{df_1, df_2} &gt; F_{\\alpha, \\, df_1, \\, df_2}) = \\alpha\\). With it, we can find the critical values \\(F_{\\frac{\\alpha}{2}, \\, df_1, \\, df_2}\\) and \\(F_{1-\\frac{\\alpha}{2}, \\, df_1, \\, df_2}\\) used in constructing the confidence interval for the ratio \\(\\sigma^2_1/\\sigma^2_2\\) discussed next.\n\n\n\n\n\n\n\nFigure¬†18.4: Illustration of \\(\\alpha/2\\) significance levels for \\(F_{df_1, df_2}\\) distribution\n\n\n\n\nSampling Distribution \nWhen random samples of sizes \\(n_1\\) and \\(n_2\\) have been independently drawn from two normally distributed populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\) respectively, the ratio \\(\\frac{S_1^2/S_2^2}{\\sigma_1^2/\\sigma_2^2}\\) has the \\(F\\) sampling distribution \\[\\frac{S_1^2/S_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}.\\]\n\n\n\n\n\n\nImportant\n\n\n\n\nThe order of degrees of freedom matters! \\(F_{n_1-1, \\, n_2-1} \\ne F_{n_2-1, \\, n_1-1}\\). Please don‚Äôt mess around.\n\n\n\n \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\) \nFrom the sampling distribution of the ratio, the \\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, \\, n_1 - 1, \\, n_2 - 1}} \\right)}}\\]\nHow do we get the interval? Not surprising. We start with the sampling distribution of \\(\\frac{S_1^2/S_2^2}{\\sigma_1^2/\\sigma_2^2}\\).\n\n\\(P\\left(F_{1-\\alpha/2, \\, n_1 - 1, \\, n_2 - 1} &lt; \\frac{S_1^2/S_2^2}{\\sigma_1^2/\\sigma_2^2} &lt; F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1} \\right) = 1 - \\alpha\\) (Goal: isolate \\(\\sigma_1^2/\\sigma_2^2\\))\n\\(P\\left(\\frac{1}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}} &lt; \\frac{\\sigma_1^2/\\sigma_2^2}{S_1^2/S_2^2} &lt; \\frac{1}{F_{1-\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}} \\right) = 1 - \\alpha\\) (take reciprocal)\n\\(P\\left(\\frac{S_1^2/S_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}} &lt; \\frac{\\sigma_1^2}{\\sigma_2^2} &lt; \\frac{S_1^2/S_2^2}{F_{1-\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}} \\right) = 1 - \\alpha\\) (times \\(S_1^2/S_2^2\\))\n\n\n\n\n\n\n\nNote\n\n\n\nThe confidence interval for \\(\\sigma_1^2 / \\sigma_2^2\\) cannot be expressed as \\(\\left(\\frac{s_1^2}{s_2^2}-m, \\frac{s_1^2}{s_2^2} + m\\right)\\) anymore!\n\n\n\n F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) \n Step 1 \nFor comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\), the test can be either right-tailed or two-tailed. Left-tailed testing is not necessary because we can always define the population whose variance is hypothetically larger than the variance of another population as the first population.\n\nRight-tailed:  \\(\\small \\begin{align} &H_0: \\sigma_1 \\le \\sigma_2 \\\\ &H_1: \\sigma_1 &gt; \\sigma_2 \\end{align}\\) \n\nTwo-tailed:  \\(\\small \\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\) \n\n\n Step 2 \n\n\\(\\alpha = 0.05\\)\n\n Step 3 \n\nUnder \\(H_0\\), \\(\\sigma_1 = \\sigma_2\\), and the test statistic is \\[\\small F_{test} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} = \\frac{s_1^2}{s_2^2} \\sim F_{n_1-1, \\, n_2-1}\\] The denominator is gone because the ratio is one.\n\n Step 4-c \n\nRight-tailed:  \\(F_{\\alpha, \\, n_1-1, \\, n_2-1}\\) .\nTwo-tailed:  \\(F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\) \n\n\n Step 5-c \n\nRight-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha, \\, n_1-1, \\, n_2-1}\\).\nTwo-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{test} \\le F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\)\n\n\n\n Example: Weight Loss \nThis is our previous example.\n\n\nA study was conducted to see the effectiveness of a weight loss program. Two groups (Control and Experimental) of 10 subjects were selected. The two populations are normally distributed and have the same standard deviation.\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months.\n\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\n\n\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\n\n Check if \\(\\sigma_1 = \\sigma_2\\) \n\n\n\\(n_1 = 10\\), \\(s_1 = 0.5 \\, lb\\)\n\n\n\\(n_2 = 10\\), \\(s_2 = 0.7 \\, lb\\)\n\n\n Step 1 \n\n\\(\\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\)\n\n Step 2 \n\n\\(\\alpha = 0.05\\)\n\n Step 3 \n\nThe test statistic is \\(F_{test} = \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\).\n\n Step 4-c \n\nThis is a two-tailed test.\nThe critical value is \\(F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\) or \\(F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\).\n\n\n\n\n\n\n\n\n\n Step 5-c \n\nIs \\(F_{test} &gt; 4.03\\) or \\(F_{test} &lt; 0.25\\)?\n\nNo.¬†\n\n\n\n Step 6 \n\nThe evidence is not sufficient to reject the claim that \\(\\sigma_1 = \\sigma_2\\).\n\n 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) \n\n\n\n\\(\\small F_{\\alpha/2, \\, df_1, \\, df_2} = F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\)\n\\(\\small F_{1-\\alpha/2, \\, df_1, \\, df_2} = F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\)\n\\(\\small \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\)\nThe 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\small \\begin{align} &\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, df_1, \\, df_2}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, df_1, \\, df_2}} \\right) \\\\ &= \\left( \\frac{0.51}{4.03}, \\frac{0.51}{0.25} \\right) = \\left(0.127, 2.05\\right)\\end{align}\\] \n\n\n\n\n\n\n\n\n\n\n\n\nWe are 95% confident that the ratio \\(\\sigma_1^2 / \\sigma_2^2\\) is between 0.127 and 2.04. Because one is included in this interval, meaning that \\(\\sigma^2_1 = \\sigma^2_2\\), it leads to the same conclusion as the F test.\n\n\n\nR\npython\n\n\n\nWe use qf() to find the \\(F\\) critical values. \n\n\n\n## set values\nn1 &lt;- 10; n2 &lt;- 10\ns1 &lt;- 0.5; s2 &lt;- 0.7\nalpha &lt;- 0.05\n\n## 95% CI for sigma_1^2 / sigma_2^2\nf_small &lt;- qf(p = alpha / 2, df1 = n1 - 1, df2 = n2 - 1,lower.tail = TRUE)\nf_big &lt;- qf(p = alpha / 2, df1 = n1 - 1, df2 = n2 - 1, lower.tail = FALSE)\n\n## lower bound\n(s1 ^ 2 / s2 ^ 2) / f_big\n\n[1] 0.1267275\n\n## upper bound\n(s1 ^ 2 / s2 ^ 2) / f_small\n\n[1] 2.054079\n\n\n\n\n\n\n\n## Testing sigma_1 = sigma_2\n(test_stats &lt;- s1 ^ 2 / s2 ^ 2)\n\n[1] 0.5102041\n\n\n\n\n\n\nWe use f.ppf() and f.isf() in scipy.stats to find the \\(F\\) critical values.\n\nfrom scipy.stats import f\n\n# Set values\nn1 = 10; n2 = 10\ns1 = 0.5; s2 = 0.7\nalpha = 0.05\n\n# F-distribution critical values\nf_small = f.ppf(alpha/2, dfn=n1-1, dfd=n2-1)\nf_big = f.isf(alpha/2, dfn=n1-1, dfd=n2-1)\n\n# Confidence interval for the ratio of variances (sigma_1^2 / sigma_2^2)\n# lower bound\n(s1**2/s2**2)/f_big\n\n0.126727476884926\n\n# upper bound\n(s1**2/s2**2)/f_small\n\n2.054078652185193\n\n\n\n# F-test statistic\ntest_stats = (s1**2)/(s2**2)\ntest_stats\n\n0.5102040816326532\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nIf we have the entire two samples data, we can use the R built-in function var.test(x, y, alternative = \"two.sided\") to perform a \\(F\\) test to compare the variances of two samples from normal populations. The arguments x and y are numeric vectors of data values. Argument alternative must be one of \"two.sided\" (default), \"greater\" or \"less\".",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Inference About Variances</span>"
    ]
  },
  {
    "objectID": "infer-var.html#comparing-more-than-two-population-variances",
    "href": "infer-var.html#comparing-more-than-two-population-variances",
    "title": "18¬† Inference About Variances",
    "section": "\n18.3 Comparing More Than Two Population Variances*",
    "text": "18.3 Comparing More Than Two Population Variances*\nSo far we use \\(\\chi^2\\) distribution to do inference for one population variance, and \\(F\\) distribution to compare two variances. What if we want to compare more than two population variances? For example, what do we do if we have this kind of test:\n \\(\\begin{align}\n  &H_0: \\sigma_1 = \\sigma_2 = \\dots = \\sigma_t\\\\\n  &H_1: \\text{Population variances are not all equal}\n  \\end{align}\\) \nwhere we have \\(t &gt; 2\\) different groups. In this scenario, we can use the so-called Brown-Forsythe-Levene (BFL) test. The BFL test assumes that we have \\(t &gt; 2\\) independent samples. Then BFL replaces \\(y_{ij}\\), the \\(j\\)-th observation in group \\(i\\), with \\(z_{ij}\\). \\(z_{ij} = |y_{ij} - \\tilde{y}_i|\\), where \\(\\tilde{y}_i\\) is the sample median or mean of group \\(i\\).\n\n\n\n\n\n\nNote\n\n\n\nBrown-Forsythe-Levene test actually represents two tests: Brown-Forsythe test and Levene‚Äôs test, where Brown-Forsythe test uses the sample medians in \\(z_{ij}\\) and Levene‚Äôs test uses the sample means. This is the only difference between the two tests, and therefore some will refer them as one Brown-Forsythe-Levene test, or some even refer one to the other. For example, the R function car::leveneTest() used in the example actually perform the Brown-Forsythe test by default with its argument center=median.\nUsing means has larger statistical power for symmetric, moderate-tailed, distributions. But when data are non-normal, either skewed or heavy tailed, using medians is recommended because of good robustness against many types of non-normal data and while retaining good statistical power.\n\n\nThe test statistic is \\(\\color{blue}{L_{test} = \\frac{\\sum_{i=1}^tn_i(\\overline{z}_{i\\cdot} - \\overline{z}_{\\cdot\\cdot})^2 / (t - 1)}{\\sum_{i=1}^t \\sum_{j = 1}^{n_i}(z_{ij} - \\overline{z}_{i\\cdot})^2 / (N-t)}}\\), where \\(\\overline{z}_{i\\cdot} = \\sum_{j = 1}^{n_i}z_{ij}/n_{i}\\) is the mean of the \\(z_{ij}\\)s from the group \\(i\\), \\(\\overline{z}_{\\cdot\\cdot} = \\sum_{i=1}^t\\sum_{j = 1}^{n_i}z_{ij}/N\\) is the mean of all \\(z_{ij}\\)s, and \\(N = \\sum_{i=1}^tn_i\\).\nThe decision rule is that we reject \\(H_0\\) if \\(\\color{blue}{L_{test} &gt; F_{\\alpha, \\, t-1, \\, N-t}}\\).\n\n\n\n\nThere are other tests for testing the homogeneity of variances, but the BFL test performs better when data are from a non-normal distribution. It corrects for the skewness of distribution by using deviations from group medians. The BFL test is more robust. It is less likely to incorrectly declare that the assumption of equal variances has been violated.\n\n Example: Automobile Additives (Example 7.8 in SMD) \nThree different additives that are marketed for increasing the miles per gallon (mpg) for automobiles. The percentage increase in mpg was recorded for a 250-mile test drive for each additive for 10 randomly assigned cars. Is there a difference between the three additives with respect to their variability?\n\n\nR\nPython\n\n\n\nThe data are saved in R data frame data_additive.\n\ndata_additive\n\n   mpg_increase additive\n1           4.2        1\n2           2.9        1\n3           0.2        1\n4          25.7        1\n5           6.3        1\n6           7.2        1\n7           2.3        1\n8           9.9        1\n9           5.3        1\n10          6.5        1\n11          0.2        2\n12         11.3        2\n13          0.3        2\n14         17.1        2\n15         51.0        2\n16         10.1        2\n17          0.3        2\n18          0.6        2\n19          7.9        2\n20          7.2        2\n21          7.2        3\n22          6.4        3\n23          9.9        3\n24          3.5        3\n25         10.6        3\n26         10.8        3\n27         10.6        3\n28          8.4        3\n29          6.0        3\n30         11.9        3\n\n\n\n\n\n\n\n\n\n\n\n\nThe data are saved in Python pd.DataFrame data_additive.\n\n\n    mpg_increase additive\n0            4.2        1\n1            2.9        1\n2            0.2        1\n3           25.7        1\n4            6.3        1\n5            7.2        1\n6            2.3        1\n7            9.9        1\n8            5.3        1\n9            6.5        1\n10           0.2        2\n11          11.3        2\n12           0.3        2\n13          17.1        2\n14          51.0        2\n15          10.1        2\n16           0.3        2\n17           0.6        2\n18           7.9        2\n19           7.2        2\n20           7.2        3\n21           6.4        3\n22           9.9        3\n23           3.5        3\n24          10.6        3\n25          10.8        3\n26          10.6        3\n27           8.4        3\n28           6.0        3\n29          11.9        3\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe boxplot suggests that Additive 2 has larger variation than the other two in terms of the percentage increase in mpg. We will use the BFL test to decide whether or not their variation difference is just a random sampling consequences or it is statistically significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe procedure of BLT test is shown step by step as follows.\n\nStep 1:  \\(\\begin{align}\n&H_0: \\sigma_1 = \\sigma_2 = \\sigma_3\\\\\n&H_1: \\text{Population variances are not all equal}\n\\end{align}\\) \nStep 2: \\(\\alpha = 0.05\\)\nStep 3: Test statistic \\(\\color{blue}{L_{test} = \\frac{\\sum_{i=1}^tn_i(\\overline{z}_{i\\cdot} - \\overline{z}_{\\cdot\\cdot})^2 / (t - 1)}{\\sum_{i=1}^t \\sum_{j = 1}^{n_i}(z_{ij} - \\overline{z}_{i\\cdot})^2 / (N-t)}} = \\frac{235.8/(3-1)}{1742.6/(30-3)} = 1.872\\).\nStep4-c: Critical value is \\(F_{\\alpha, \\, t-1, \\, N-t} = F_{0.05, \\, 3-1, \\, 30-3} = 3.35\\).\nStep5-c: Since \\(L_{test}  = 1.872 &lt; F_{\\alpha, \\, t-1, \\, N-t} = 3.35\\), we do not reject \\(H_0\\).\nStep6: There is insufficient evidence to support the claim that there is a difference in the population variances of the percentage increase in mpg for the three additives.\n\n\n\nR\nPython\n\n\n\nOnce you understand the idea and the process of doing the test, in practice, we use R function leveneTest() in the car (Companion to Applied Regression) package to help us perform the test.\n\n\n\nlibrary(car) # Load the package car\nleveneTest(y = data_additive$mpg_increase, \n           group = data_additive$additive)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  2  1.8268 0.1803\n      27               \n\n\nIn the function, we put the response variable data in y, and the factor defining groups in group. By default, it will use sample medians to perform the test. The p-value is greater than 0.05 and it leads to the same conclusion that there is insufficient evidence to say there is a difference in the population variances of the percentage increase in mpg for the three additives.\n\n\n\n\nOnce you understand the idea and the process of doing the test, in practice, we use Python function levene() in scipy.stats to help us perform the test.\n\nfrom scipy.stats import levene\n# Levene's test\nlevene(\n    data_additive[data_additive['additive'] == 1]['mpg_increase'],\n    data_additive[data_additive['additive'] == 2]['mpg_increase'],\n    data_additive[data_additive['additive'] == 3]['mpg_increase'])\n\nLeveneResult(statistic=1.82684271595955, pvalue=0.18025794484918528)\n\n\nIn the function, each sample data can only be one-dimensional. By default, it will use sample medians to perform the test. The p-value is greater than 0.05 and it leads to the same conclusion that there is insufficient evidence to say there is a difference in the population variances of the percentage increase in mpg for the three additives.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Inference About Variances</span>"
    ]
  },
  {
    "objectID": "infer-var.html#exercises",
    "href": "infer-var.html#exercises",
    "title": "18¬† Inference About Variances",
    "section": "\n18.4 Exercises",
    "text": "18.4 Exercises\n\nThe data about male and female pulse rates are summarized below.\n\nConstruct a 95% CI for \\(\\sigma_{male}\\) of pulse rates for males.\nConstruct a 95% CI for \\(\\sigma_{male}/\\sigma_{female}\\).\nDoes it appear that the population standard deviations for males and females are different? Why or why not?\n\n\n\n\n\n\n\nMale\nFemale\n\n\n\n\\(\\overline{x}\\)\n71\n75\n\n\n\\(s\\)\n9\n12\n\n\n\\(n\\)\n14\n12",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>18</span>¬† <span class='chapter-title'>Inference About Variances</span>"
    ]
  },
  {
    "objectID": "infer-prop.html",
    "href": "infer-prop.html",
    "title": "19¬† Inference About Proportions",
    "section": "",
    "text": "19.1 Categorical Data\nOne Categorical Variable with Two Categories\nWe start with one categorical variable with two categories. Let \\(X\\) be the categorical variable Gender with 2 categories, Male and Female. Our data table may look like the following. The first subject is Male, so we mark it in the Male column. The second subject is Female, so we check it in the Female column, and so on.\nWhen we collect the categorical data, usually we are interested in their count. We wonder which category has more counts than the other. So we can make a one-way frequency/count table as follows. It is one-way because the count table is for one categorical variable only. Here in the data there are \\(y\\) males and \\(n-y\\) females, where \\(n\\) is the sample size.\nNow the number of males or the proportion of males can be viewed as a random variable because the count, \\(Y\\), or the proportion \\(Y/n\\) varies from sample to sample. Suppose you want to learn the proportion of male students at Marquette, what would you do. You would probably randomly sample some Marquette students, and count how many of them are males, right? If you do the sampling again, the count will be different from the count you got previously. Therefore, to learn the male proportion, we could use the sample proportion \\(Y/n\\) as an estimator that follows some (sampling) distribution.\nOne question for you. What probability distribution might be appropriate for the count, \\(Y\\)?\nProbability Distribution for Count Data: Two Levels\nIn our example, each subject is either Male or Female, and with the fixed sample size, we wonder how many subjects are Males. Any probability distribution comes into your mind? Well, \\(binomial(n, \\pi)\\) could be a good option for count data with 2 categories.\nThe count \\(Y\\) of male students, or the number of success, has the binomial probability \\[P(Y = y \\mid n, \\pi) = \\frac{n!}{y!(n-y)!}\\pi^{y}(1-\\pi)^{n-y}\\]\nDo we know \\(\\pi\\)? Absolutely not! The parameter \\(\\pi\\) is the proportion (or relative frequency) of male students, which is what we want to estimate and learn from data. Therefore, our goal is to estimate or test the population proportion, \\(\\pi\\), of the category Male given the assumption that the count of Male \\(Y\\) is binomially distributed \\(Y \\sim binomial(n, \\pi)\\).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Inference About Proportions</span>"
    ]
  },
  {
    "objectID": "infer-prop.html#categorical-data",
    "href": "infer-prop.html#categorical-data",
    "title": "19¬† Inference About Proportions",
    "section": "",
    "text": "Subject\nMale\nFemale\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne-way frequency/count table\n\n\n\n\\(X\\)\nCount\n\n\n\nMale\n\\(y\\)\n\n\nFemale\n\\(n-y\\)\n\n\n\n\n\n\n\n\n\nFixed number of trials:  We can view each sampled subject as one trial in the experiment, and we have fixed \\(n\\) subjects. \n\nEach trial results in one of two outcomes.  Clearly, in our survey, there are only two possible answers. Each subject is either Male or Females. \n\nTrials are independent.  If the subjects are randomly sampled, the students in the sample are independent. \n\nThe probability of success \\(\\pi\\) is constant:  The proportion of being in category Male is \\(\\pi\\), which is constant at the given point of time.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Inference About Proportions</span>"
    ]
  },
  {
    "objectID": "infer-prop.html#inference-for-a-single-proportion",
    "href": "infer-prop.html#inference-for-a-single-proportion",
    "title": "19¬† Inference About Proportions",
    "section": "\n19.2 Inference for a Single Proportion",
    "text": "19.2 Inference for a Single Proportion\n Hypothesis Testing for \\(\\pi\\) \nThe point estimator for the population proportion \\(\\pi\\) is the sample proportion \\(Y/n\\). The true probability is approximated by the relative frequency in the sample data. Let‚Äôs first learn how to do testing for \\(\\pi\\). This is the one-sample proportion \\(z\\) test.\n\n Step 0: Method Assumptions \n\nThe method requires that  \\(n\\pi_0 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\)  where \\(\\pi_0\\) is the hypothesized value or the value to be tested. The larger \\(n\\pi_0\\) and \\(n(1-\\pi_0)\\) are, the better. In fact, the method relies on the central limit theorem, and uses standard normal distribution to do the test. Large \\(n\\pi_0\\) and \\(n(1-\\pi_0)\\) leads to better normal approximation.\n\n Step 1: Set the Null and Alternative Hypothesis \n\nWe are interested in the proportion of some category being equal to, greater than or less than some value.  \\(\\begin{align} &H_0: \\pi = \\pi_0 \\\\ &H_1: \\pi &gt; \\pi_0 \\text{ or } \\pi &lt; \\pi_0 \\text{ or } \\pi \\ne \\pi_0 \\end{align}\\) \n\n\n Step 2: Set the Significance Level, \\(\\alpha\\) \n Step 3: Calculate the Test Statistic \n\nWith the central limit theorem, it can be shown that the sampling distribution of the statistic \\(Y/n\\) is approximately normal with mean \\(\\pi\\) and standard error, \\(\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\). Therefore, \\[Z = \\frac{Y/n - \\pi}{\\sqrt{\\frac{\\pi(1-\\pi)}{n}}} \\dot\\sim N(0, 1).\\] Then under the null \\(H_0\\), the test statistic is  \\(z_{test} = \\dfrac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\) where \\(\\hat{\\pi} = \\frac{y}{n}\\) is the realized sample proportion. \n\n\n Step 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed) \n\nSame as before.\n\n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n \\(H_1: \\pi &gt; \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &gt; z_{\\alpha}\\) \n \\(H_1: \\pi &lt; \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &lt; -z_{\\alpha}\\) \n \\(H_1: \\pi \\ne \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| &gt; z_{\\alpha/2}\\) \n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n Confidence Interval for \\(\\pi\\) \nTo construct the confidence interval for \\(\\pi\\), we rely on some assumption too. Unlike testing that has a hypothesized value \\(\\pi_0\\), there is no hypothesized value in confidence interval, and we only use information from the data to get an interval. To ensure that the normal approximation is fairly good, the interval formula requires \\(n\\hat{\\pi} \\ge 5\\) and \\(n(1-\\hat{\\pi}) \\ge 5\\).  Can you see the difference between this requirement and the testing requirement?\n\n\nThe \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi\\) is \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] where \\(\\hat{\\pi} = y/n\\). Since \\(\\pi\\) is unknown, we use its estimate \\(\\hat{\\pi}\\) instead: \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}\\]\nSuch normal approximation interval is called the Wald interval.   \n\n Example: Exit Poll \nSuppose we collect data on 1,000 voters in an election with only two candidates, R and D, and every voter must vote for either one of them.\n\n\n\n\nVoter\nR\nD\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the data, we want to predict who won the election. Let \\(Y\\) be the number of voters that voted for R. Assume the count, \\(Y\\), is sampled from \\(binomial(n = 1000, \\pi)\\), and \\(\\pi = P(\\text{a voter voted for R}) =\\) (population) proportion of all voters that for R. Parameter \\(\\pi\\) is the unknown parameter to be estimated or tested. In an exit poll of 1,000 voters, 520 voted for R. At \\(\\alpha = 0.05\\), predict whether or not R won the election.\n\n\n\n\n Hypothesis Testing \n Step 0 \n\n\n\\(\\pi_0 = 1/2\\).  \\(n\\pi_0 = 1000(1/2) = 500 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) \n\n\n Step 1 \n\n\nR won the election means that the proportion of all voters voting for R is greater than 50%.  \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi &gt; 1/2 \\text{ (more than half voted for R)} \\end{align}\\) \n\n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(z_{test} = \\frac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} =  \\frac{\\frac{520}{1000} - 0.5}{\\sqrt{\\frac{0.5(1-0.5)}{1000}}} = 1.26\\) \n\n Step 4-c \n\n \\(z_{\\alpha} = z_{0.05} = 1.645\\) \n\n Step 5-c \n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} &gt; z_{\\alpha}\\). \n Since \\(z_{test} &lt; z_{\\alpha}\\), we do not reject \\(H_0\\). \n\n Step 6 \n\n We do not have sufficient evidence to conclude that R won. \nWe make the same conclusion using the \\(p\\)-value method.\n\n\\[ p\\text{-value} = P(Z &gt; 1.26) = 0.1 &gt; 0.05\\]\n Confidence Interval \nFirst we check the assumption: + \\(n\\hat{\\pi} = 1000(0.52) = 520 \\ge 5\\) and \\(n(1-\\hat{\\pi}) = 480 \\ge 5\\).\n\nEstimate the proportion of all voters that voted for R using a 95% confidence interval.\n\n\\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}} = 0.52 \\pm z_{0.025}\\sqrt{\\frac{0.52(1-0.52)}{1000}} = (0.49, 0.55).\\]\n\n\nR\nPython\n\n\n\nTo perform the one-sample proportion \\(z\\) test in R, we use the function prop.test(). The argument correct tells us whether or not we want to do continuity correction. Our method here does not do continuity correction. It does not matter much when our method assumption is satisfied.\nTo obtain the confidence interval, we should set alternative = \"two.sided\" because the interval is two sided. Be careful. The interval we learn is the Wald interval. However, the interval from the prop.test() output is not the Wald interval, but the so-called Wilson interval. There are lots of variants of confidence intervals for binomial proportions, and one can use the BinomCI() function in the DescTools package to obtain them.\n\n## one proportion test using normal approximation\n(prop_test_res &lt;- prop.test(x = 520, n = 1000, p = 0.5, alternative = \"greater\", \n          correct = FALSE))\n\n\n    1-sample proportions test without continuity correction\n\ndata:  520 out of 1000, null probability 0.5\nX-squared = 1.6, df = 1, p-value = 0.103\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.4939945 1.0000000\nsample estimates:\n   p \n0.52 \n\n# Use alternative = \"two.sided\" to get CI\nprop_ci &lt;- prop.test(x = 520, n = 1000, p = 0.5, \n                     alternative = \"two.sided\", correct = FALSE)\nprop_ci$conf.int\n\n[1] 0.4890177 0.5508292\nattr(,\"conf.level\")\n[1] 0.95\n\n## The Wilson interval and Wald interval\nDescTools::BinomCI(x = 520, n = 1000, method = \"wilson\")\n\n      est    lwr.ci    upr.ci\n[1,] 0.52 0.4890177 0.5508292\n\nDescTools::BinomCI(x = 520, n = 1000, method = \"wald\")\n\n      est    lwr.ci    upr.ci\n[1,] 0.52 0.4890351 0.5509649\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDoing inference with normal approximation to binomial variables is more complicated than what we discuss here. The main reason is that we use a continuous normal distribution to approximate a discrete binomial distribution. Check the Wiki page if you don‚Äôt believe it. No worries at this moment unless you want to be a statistician doing research in this field!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you look at the proportion test output carefully, you‚Äôll find that the test statistic is a chi-squared test statistic, not the \\(z\\) test statistic. In fact, the square of the \\(z\\) statistic is equal to the chi-squared statistic with degrees of freedom one, i.e., \\(z_{test}^2 = \\chi^2_{1, test}\\), and the \\(z\\) test here is equivalent to the chi-squared test discussed in the next chapter (Chapter 20) for two-sided tests.\n\nz_test &lt;- (0.52 - 0.5) / sqrt((0.52)*0.48 / 1000)\nz_test ^ 2\n\n[1] 1.602564\n\nprop_test_res$statistic\n\nX-squared \n      1.6 \n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe method we learn here uses normal approximation to binomial variables. One can also perform the exact binomial test that directly uses binomial probabilities to calculate the \\(p\\)-value and do the testing. As its name suggests, it is what we should use for binomial inference because it is exact. If the requirement of the normal approximation method we learn is not met, the exact binomial test should be used. When the sample size is large, and the expected proportion is not extreme being close to 0 or 1, the normal approximation method and the exact binomial test have pretty similar inference results.\nWe can use binom.test() function to perform the exact binomial test. The page discusses the relationship between the exact binomial test and other methods.\n\n## exact binom test\nbinom.test(x = 520, n = 1000, p = 0.5, alternative = \"greater\")\n\n\n    Exact binomial test\n\ndata:  520 and 1000\nnumber of successes = 520, number of trials = 1000, p-value = 0.1087\nalternative hypothesis: true probability of success is greater than 0.5\n95 percent confidence interval:\n 0.4934948 1.0000000\nsample estimates:\nprobability of success \n                  0.52 \n\n## confidence interval\nbi &lt;- binom.test(x = 520, n = 1000, p = 0.5, alternative = \"two.sided\")\nbi$conf.int\n\n[1] 0.4885149 0.5513671\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\n\n\n\nIt needs a bit extra work to perform the one-sample proportion \\(z\\) test in Python because unfortunately scipy.stats does not provide any function for the test. Unless you want to calculate and do the test step by step, we can use the function proportions_ztest() provided in the package statsmodels. In RStudio, with the R reticulate package, we can install the Python package in our R session using the command py_install(\"statsmodels\").\n\nimport numpy as np\nimport statsmodels.api as sm\nfrom statsmodels.stats.proportion import proportions_ztest, proportion_confint\n\nThe argument value in proportions_ztest() is the value of the null hypothesis equal to the proportion.\n\n# One proportion z-test using normal approximation\nz_prop_stat, pval = proportions_ztest(count=520, nobs=1000, value=0.5, alternative='larger')\nz_prop_stat\n\n1.2659242088545843\n\npval\n\n0.1027701091113093\n\n\nThe first returned value is the test statistic for the z-test, and the second is the p-value. Note that when we specify the alternative, we use the word ‚Äòlarger‚Äô, not ‚Äògreater‚Äô that is used in the functions provided by scipy.stats, ttest_rel() for example.\nTo obtain the confidence interval, we use proportion_confint(). The method is set to be ‚Äònormal‚Äô because we are doing normal approximation. The interval we learn is the Wald interval.\n\nprop_ci = proportion_confint(count=520, nobs=1000, alpha=0.05, method='normal')\nprop_ci\n\n(0.48903505011072596, 0.5509649498892741)\n\n\nThere are lots of variants of confidence intervals for binomial proportions that can be specified through the method argument\n\n# normal : asymptotic normal approximation\n# \n# agresti_coull : Agresti-Coull interval\n# \n# beta : Clopper-Pearson interval based on Beta distribution\n# \n# wilson : Wilson Score interval\n# \n# jeffreys : Jeffreys Bayesian Interval\n# \n# binom_test : Numerical inversion of binom_test\n\nFor example,\n\nci_wilson = proportion_confint(count=520, nobs=1000, alpha=0.05, method='wilson')\nci_wilson\n\n(0.4890177246575191, 0.5508292050030589)\n\n\n\n\n\n\n\n\nWarning\n\n\n\nDoing inference with normal approximation to binomial variables is more complicated than what we discuss here. The main reason is that we use a continuous normal distribution to approximate a discrete binomial distribution. Check the Wiki page if you don‚Äôt believe it. No worries at this moment unless you want to be a statistician doing research in this field!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe method we learn here uses normal approximation to binomial variables. One can also perform the exact binomial test that directly uses binomial probabilities to calculate the \\(p\\)-value and do the testing. As its name suggests, it is what we should use for binomial inference because it is exact. If the requirement of the normal approximation method we learn is not met, the exact binomial test should be used. When the sample size is large, and the expected proportion is not extreme being close to 0 or 1, the normal approximation method and the exact binomial test have pretty similar inference results.\nWe can use binomtest() function in scipy.stats to perform the exact binomial test. The page discusses the relationship between the exact binomial test and other methods. Note that when confidence interval is calculated, the alternative shoud be two-sided.\n\nfrom scipy.stats import binomtest\n\nbinom_test_res = binomtest(k=520, n=1000, p=0.5, alternative='greater')\nbinom_test_res.statistic\n\n0.52\n\nbinom_test_res.pvalue\n\n0.10872414660207055\n\nbinom_test_ci = binomtest(k=520, n=1000, p=0.5, alternative='two-sided')\nbinom_test_ci.proportion_ci(confidence_level=0.95, method='exact')\n\nConfidenceInterval(low=0.4885148824521465, high=0.551367057305466)\n\n\nAlso be careful that the proportion_confint() from statsmodels can generate the confidence interval of a binomial test with method='binom_test'. However, its result is not the same as the one shown before.\n\n# from statsmodels\nfrom statsmodels.stats.proportion import binom_test\nbinom_test(count=520, nobs=1000, prop=0.5, alternative='larger')\n\n0.10872414660207055\n\nci_binom = proportion_confint(count=520, nobs=1000, alpha=0.05, method='binom_test')\nci_binom\n\n(0.48899690054625045, 0.5510150259185691)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Inference About Proportions</span>"
    ]
  },
  {
    "objectID": "infer-prop.html#inference-for-two-proportions",
    "href": "infer-prop.html#inference-for-two-proportions",
    "title": "19¬† Inference About Proportions",
    "section": "\n19.3 Inference for Two Proportions",
    "text": "19.3 Inference for Two Proportions\nMany times we want to compare two population proportions, say \\(\\pi_1\\) and \\(\\pi_2\\). We could assume there are two independent binomial experiments with the same possible outcomes. For example, we could have male and female voters, and every voter in each group has an opinion about president‚Äôs performance whose outcome is either Approve or Not approve. Let the number of male voters approving the performance is \\(Y_1\\) and the number of female voters approving the performance is \\(Y_2\\). We can assume \\(Y_1\\) and \\(Y_2\\) both follow binomial distribution but with their own parameters, \\(n_1\\) and \\(\\pi_1\\) for \\(Y_1\\), and \\(n_2\\) and \\(\\pi_2\\) for \\(Y_2\\). Political analysts may want to know whether or not the male presidential approval rate \\(\\pi_1\\) is higher than the female approval rate \\(\\pi_2\\).\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\n\\(n_1\\) trials\n\n\\(n_2\\) trials\n\n\n\n\\(Y_1\\) number of successes\n\n\\(Y_2\\) number of successes\n\n\n\\(Y_1 \\sim binomial(n_1, \\pi_1)\\)\n\\(Y_2 \\sim binomial(n_2, \\pi_2)\\)\n\n\n\n\n\n\\(\\pi_1\\): Population proportion of success of Group 1\n\n\\(\\pi_2\\): Population proportion of success of Group 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\) \nThe method introduced here is based on the central limit theorem and normal approximation to binomial distribution. The idea is similar to the one sample proportion \\(z\\) test, and we perform two sample proportion \\(z\\) test.\n Step 0: Check Method Assumptions \n\nIn order to perform the \\(z\\) test, the following requirements must be met:  \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\) \n\n\n Step 1: Set the Null and Alternative Hypothesis \n\n \\(\\begin{align}  &H_0: \\pi_1 = \\pi_2 \\\\ &H_1: \\pi_1 &gt; \\pi_2 \\text{ or } \\pi_1 &lt; \\pi_2 \\text{ or } \\pi_1 \\ne \\pi_2 \\end{align}\\) \n\n Step 2: Set the Significance Level, \\(\\alpha\\) \n Step 3: Calculate the Test Statistic \n\nIt can be shown that \\[\\frac{\\frac{Y_1}{n_1} - \\frac{Y_2}{n_2} - (\\pi_1 - \\pi_2)}{\\sqrt{\\frac{\\pi_1(1-\\pi_1)}{n_1} + \\frac{\\pi_2(1-\\pi_2)}{n_2}}} \\dot\\sim N(0, 1)\\] Under the null hypothesis, \\(\\pi_1 = \\pi_2 = \\pi\\), with the sample data, the statistic is reduced to \\[\\frac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\pi(1-\\pi)(\\frac{1}{n_1} + \\frac{1}{n_2})}},\\] where \\(\\hat{\\pi}_1 = y_1/n_1\\) and \\(\\hat{\\pi}_2 = y_2/n_2\\). We still cannot use this quantity as a test statistic because it involves the unknown parameter \\(\\pi\\). Like two sample pooled \\(t\\) test, since \\(\\pi_1 = \\pi_2 = \\pi\\) under \\(H_0\\), we combine the two samples using all the trials to get a better pooled sample proportion \\(\\bar{\\pi} = \\frac{y_1+y_2}{n_1+n_2}\\) to estimate the common proportion \\(\\pi\\). Therefore, the test statistic is\n\n \\[z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2})}}.\\] \n Step 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed) \nSame before.\n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \n\n \\(H_1: \\pi_1 &gt; \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &gt; z_{\\alpha}\\) \n \\(H_1: \\pi_1 &lt; \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z &lt; -z_{\\alpha}\\) \n \\(H_1: \\pi_1 \\ne \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| &gt; z_{\\alpha/2}\\) \n\n\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n Confidence Interval for \\(\\pi_1 - \\pi_2\\)\nTo get the Wald confidence interval for \\(\\pi_1 - \\pi_2\\), it requires\n\\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\). Again, no hypothesized values, and sample proportions are used.\nThe \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi_1 - \\pi_2\\) is \\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\nThere is no pooled estimate \\(\\bar{\\pi}\\) in the interval because the interval is not constructed under the hypothesis that \\(\\pi_1 = \\pi_2 = \\pi\\). The parameters \\(\\pi_1\\) and \\(\\pi_2\\) are estimated separately by the individual sample proportions \\(\\hat{\\pi}_1\\) and \\(\\hat{\\pi}_2\\).\n\n Example: Effectiveness of Learning \n\n\nSuppose we do a study on 300 students to compare the effectiveness of learning statistics in online vs.¬†in-person programs. We randomly assign 125 students to the online program, and the remaining 175 to the in-person program. The exam results are shown in the table below.\n\n\n\n\n\nSource: Unsplash-JESHOOTS.COM\n\n\n\n\n\n\n\nExam Results\nOnline Instruction\nIn-Person Instruction\n\n\n\nPass\n94\n113\n\n\nFail\n31\n62\n\n\nTotal\n125\n175\n\n\n\nIs there sufficient evidence to conclude that the online program is more effective than the traditional in-person program at \\(\\alpha=0.05\\)? \n Hypothesis Testing \n Step 0 \n\n \\(\\hat{\\pi}_1 = 94/125 = 0.75\\) and \\(\\hat{\\pi}_2 = 113/175 = 0.65\\). \n \\(n_1\\hat{\\pi}_1 = 94 &gt; 5\\), \\(n_1(1-\\hat{\\pi}_1) = 31 &gt; 5\\), and \\(n_2\\hat{\\pi}_2 = 113 &gt; 5\\), \\(n_2(1-\\hat{\\pi}_2) = 62 &gt; 5\\) \n\nThe assumptions are satisfied.\n Step 1 \n\nLet \\(\\pi_1\\) \\((\\pi_2)\\) be the population proportion of students passing the exam in the online (in-person) program.  \\(H_0: \\pi_1 = \\pi_2\\) vs.¬†\\(H_1: \\pi_1 &gt; \\pi_2\\) \n\n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(\\bar{\\pi} = \\frac{94+113}{125+175} = 0.69\\) \n \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2})}} = \\frac{0.75 - 0.65}{\\sqrt{0.69(1-0.69)(\\frac{1}{125} + \\frac{1}{175})}} = 1.96\\) \n\n Step 4-c \n\n \\(z_{\\alpha} = z_{0.05} = 1.645\\) \n\n Step 5-c \n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} &gt; z_{\\alpha}\\). \n Since \\(z_{test} &gt; z_{\\alpha}\\), we reject \\(H_0\\). \n\n Step 6 \n\n We have sufficient evidence to conclude that the online program is more effective. \n\n Confidence Interval \nWe want to know how effective the online program is, so we estimate \\(\\pi_1 - \\pi_2\\) using a \\(95\\%\\) confidence interval\n\\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{0.05/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]  The 95% confidence interval is \\[0.75 - 0.65 \\pm 1.96\\sqrt{\\frac{(0.75)(1-0.75)}{125} + \\frac{(0.65)(1-0.65)}{175}}\\\\\n= (0.002, 0.210)\\]\nBecause 0 is not included in this interval, we reach the same conclusion as the hypothesis testing.\n\n\nR\nPython\n\n\n\nBelow is a demonstration of how to make inferences about two proportions in R. We still use prop.test() function, but here we provide the number of successes and the number of trials in the arguments x and n respectively as a vector whose first element is for the first group and second element for the second group. Please be consistent with the order and don‚Äôt mess up.\n\n(prop_test2 &lt;- prop.test(x = c(94, 113), n = c(125, 175), \n          alternative = \"greater\", correct = FALSE))\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(94, 113) out of c(125, 175)\nX-squared = 3.8509, df = 1, p-value = 0.02486\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01926052 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.7520000 0.6457143 \n\nprop_ci2 &lt;- prop.test(x = c(94, 113), n = c(125, 175), \n                     alternative = \"two.sided\", correct = FALSE)\nprop_ci2$conf.int\n\n[1] 0.002588801 0.209982628\nattr(,\"conf.level\")\n[1] 0.95\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe two sample proportion \\(z\\) test is equivalent to the chi-squared test introduced in the next chapter (Chapter 20) that considers more than two possible outcomes.\n\npi_bar &lt;- (94 + 113)/(125 + 175)\npi_1_hat &lt;- 94/125\npi_2_hat &lt;- 113/175\nz_test2 &lt;- (pi_1_hat - pi_2_hat) / (sqrt(pi_bar*(1-pi_bar)*(1/125+1/175)))\nz_test2 ^ 2\n\n[1] 3.850932\n\nprop_test2$statistic\n\nX-squared \n 3.850932 \n\n\n\n\n\n\nBelow is a demonstration of how to make inferences about two proportions in Python. We still use proportions_ztest() function, but here we provide the number of successes and the number of trials in the arguments count and nobs respectively as an array whose first element is for the first group and second element for the second group. Please be consistent with the order and don‚Äôt mess up.\n\n# Two proportion Z-test\n# One-tailed test\nz_test_stat, p_val = proportions_ztest(count=np.array([94, 113]), \n                                       nobs=np.array([125, 175]), \n                                       alternative='larger')\n\nz_test_stat\n\n1.9623790859613837\n\np_val\n\n0.024859182908509945\n\n\nHowever, if we use proportion_confint() for calculating the confidence interval for the mean difference of two proportions, the results do not match what the formula show. proportion_confint() actually calculate two CIs, one for each proportion.\n\nci_low, ci_upp = proportion_confint(count=np.array([94, 113]), \n                                    nobs=np.array([125, 175]), \n                                    alpha=0.05, \n                                    method='normal')\nci_low\n\narray([0.67629443, 0.57485022])\n\nci_upp\n\narray([0.82770557, 0.71657835])\n\n\nWe are actually comparing proportions of two independent binomial samples. In Python, test_proportions_2indep() and confint_proportions_2indep() help us test whether the two proportions are equal or one is greater than the other, and provide CI for the proportion difference.\n\nfrom statsmodels.stats.proportion import test_proportions_2indep\ntest_prop_2ind_res = test_proportions_2indep(\n    count1=94, nobs1=125, count2=113, nobs2=175,\n    compare='diff', alternative='larger', correction=False,\n    method='score')\ntest_prop_2ind_res.statistic\n\n1.9623790859613837\n\ntest_prop_2ind_res.pvalue\n\n0.024859182908509945\n\n\nWe see that there are several arguments that controls how the test should be performed. The option correction=False and method='score' matches the results obtained from the formula. We don‚Äôt need to worry too much which method we should use at this moment. Although different options give us different test statistics and p-value, the difference is small, and the conclusion usually remain the same.\nOur confidence interval is a Wald interval, and method='wald'. It does not matter whether correction is true or false. Again, there are several difference methods for calculating confidence intervals. Their difference is not large, and the conclusion stays the same most of the time.\n\nfrom statsmodels.stats.proportion import confint_proportions_2indep\nconfint_proportions_2indep(count1=94, nobs1=125, count2=113, nobs2=175,\n                           compare='diff', method='wald', correction=True)\n\n(0.002588800743036121, 0.2099826278283925)\n\nconfint_proportions_2indep(count1=94, nobs1=125, count2=113, nobs2=175,\n                           compare='diff', method='wald', correction=False)\n\n(0.002588800743036121, 0.2099826278283925)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Inference About Proportions</span>"
    ]
  },
  {
    "objectID": "infer-prop.html#exercises",
    "href": "infer-prop.html#exercises",
    "title": "19¬† Inference About Proportions",
    "section": "\n19.4 Exercises",
    "text": "19.4 Exercises\n\nLipitor (atorvastatin) is a drug used to control cholesterol. In clinical trials of Lipitor, 98 subjects were treated with Lipitor and 245 subjects were given a placebo. Among those treated with Lipitor, 6 developed infections. Among those given a placebo, 24 developed infections. Use a 0.05 significance level to test the claim that the rate of inflections was the same for those treated with Lipitor and those given a placebo.\n\nTest the claim using the critical-value and p-value methods.\nTest the claim by constructing a confidence interval.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>19</span>¬† <span class='chapter-title'>Inference About Proportions</span>"
    ]
  },
  {
    "objectID": "infer-cat.html",
    "href": "infer-cat.html",
    "title": "20¬† Inference about Categorical Data",
    "section": "",
    "text": "20.1 Test of Goodness of Fit\nA citizen in Wauwatosa, WI is curious about the question:\nSuppose from the government we learn the distribution of registered voters based on races. We then collect 275 jurors racial information, and see if the racial distribution of the sample is kind of consistent with the registered voter racial distribution. The count information is summarized in the table below.\nThe first thing we can do is convert the count into proportion or relative frequency, so that the sample proportion and the target proportion can be easily paired and compared.\nWhile the proportions in the juries do not precisely represent the population proportions, it is unclear whether these data provide convincing evidence that the sample is not representative. If the jurors really were randomly sampled from the registered voters, we might expect small differences due to chance. However, unusually large differences may provide convincing evidence that the juries were not representative. Specifically, we want a test to answer the question\nThe test we need is the goodness-of-fit test.\nGoodness-of-Fit Test\nA goodness-of-fit test tests the hypothesis that the observed frequency distribution fits or conforms to some claim distribution. In the jury example, our observed (relative) frequency distribution is \\((0.745, 0.095, 0.091, 0.069)\\) and the claim distribution is the racial distribution of registered voters \\((0.72, 0.07, 0.12, 0.09)\\). The jury representatives work as our sample, and the registered voters is the population. We wonder if the sample is representative of the general population. If yes, the sample should looks like the population, and sample distribution should conform to the population distribution.\nHere is the question: ‚ÄúIf the individuals are randomly selected to serve on a jury, about how many of the 275 people would we expect to be white? How about black?‚Äù We ask this question because we want to know how the jury distribution looks like if the distribution does follow the distribution of the registered voters. If what we expect to see is far from what we observe, then the jury is probably not randomly sampled from the registered voters. This matches our testing rationale. We do the testing under the null hypothesis, the scenario that the observed frequency distribution fits or conforms to some claim distribution.\nAccording to the claimed distribution, \\(72\\%\\) of the population is white, so we would expect about \\(72\\%\\) of the jurors to be white. In other words, we expect to see \\(0.72 \\times 275 = 198\\) white jurors in the sample of 275 jury members. We expect about \\(7\\%\\) of the jurors to be black. This corresponds to about \\(0.07 \\times 275 = 19.25\\) black jurors in the sample. The table below shows the observed count and the expected count if members are randomly selected for each ethnic group. In general, the expected count of a category is the total count times the proportion or probability of the category.\nWhile some sampling variation is expected, the observed count and expected count should be similar if there was no bias in selecting the members of the jury. But how similar is similar enough? We want to test whether the differences are strong enough to provide convincing evidence that the jurors were not selected from a random sample of all registered voters.\nGoodness-of-Fit Test Example\nBefore we introduce the test procedure, to have better performance the goodness-of-fit test requires each expected count is as least five. The higher the better.\nIn words, our hypotheses are  \\[\\begin{align} &H_0: \\text{No racial bias in who serves on a jury, and } \\\\ &H_1: \\text{There is racial bias in juror selection} \\end{align}\\]\nIf the true racial distribution of juries is \\((\\pi_1, \\pi_2, \\pi_3, \\pi_4)\\), where\nwe want to know if the distribution conforms to the claim racial distribution of register voters \\((\\pi_1^0, \\pi_2^0, \\pi_3^0, \\pi_4^0) = (0.72, 0.07, 0.12, 0.09)\\) where\nIn general, we can rewrite our hypotheses in mathematical notations:  \\[\\begin{align} &H_0: \\pi_1 = \\pi_1^0,  \\pi_2 = \\pi_2^0, \\dots, \\pi_k = \\pi_k^0\\\\ &H_1: \\pi_i \\ne \\pi_i^0 \\text{ for some } i \\end{align}\\]\nNote that \\(H_1\\) is not \\(\\pi_i \\ne \\pi_i^0 \\text{ for all } i\\). As long as one \\(\\pi_i \\ne \\pi_i^0\\) holds, the \\(H_0\\) statement is not true.\nThe test statistic is a chi-squared statistic from the chi-squared distribution with degrees of freedom \\(k-1\\). Under \\(H_0\\), the test statistic is\n\\[\\chi^2_{test} = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} + \\cdots + \\frac{(O_k - E_k)^2}{E_k},\\] where \\(O_i\\) is the observed count of the \\(i\\)-th category, and \\(E_i = n\\pi_i^0\\) is the expected count of the \\(i\\)-th category, \\(i = 1, \\dots, k.\\)\nThe goodness-of-fit test is a chi-squared test that is always right-tailed. So we reject \\(H_0\\) if  \\(\\chi^2_{test} &gt; \\chi^2_{\\alpha, k-1}\\).  Look at the test statistic formula carefully. When will the test statistic be large or the evidence be strong? The numerator term \\((O_i-E_i)^2\\) is the squared distance between the observed and expected counts. When the two are farther away from each other, the test statistic or evidence will be stronger. As a result, when the observed count distribution is more inconsistent with the distribution we expect to see, it is more likely to conclude that the sample is not randomly drawn from the claim or assumed population distribution.\nBack to our example.\nUnder \\(H_0\\), \\(\\chi^2_{test} = \\frac{(205 - 198)^2}{198} + \\frac{(26 - 19.25)^2}{19.25} + \\frac{(25 - 33)^2}{33} + \\frac{(19 - 24.75)^2}{24.75} = 5.89\\).\nWith \\(\\alpha = 0.05\\), \\(\\chi^2_{0.05, 3} = 7.81\\). Because \\(5.89 &lt; 7.81\\), we fail to reject \\(H_0\\) in favor of \\(H_1\\). Therefore, we do not have convincing evidence of racial bias in the juror selection process.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Inference about Categorical Data</span>"
    ]
  },
  {
    "objectID": "infer-cat.html#test-of-goodness-of-fit",
    "href": "infer-cat.html#test-of-goodness-of-fit",
    "title": "20¬† Inference about Categorical Data",
    "section": "",
    "text": "Are the selected jurors racially representative of the population?\n\n\n\n\nWell the idea is that if the jury is representative of the population, once we collect our sample of juries, the sample proportions should reflect the proportions of the population of eligible jurors (i.e.¬†registered voters).\n\n\n\n\n\nSource: https://stock.adobe.com/images/diverse-group-of-business-people/116680830\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n205\n26\n25\n19\n275\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n1.00\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\nRepresentation in juries\n0.745\n0.095\n0.091\n0.069\n1.00\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n1.00\n\n\n\n\n\nAre the proportions of juries close enough to the proportions of registered voters, so that we are confident saying that the jurors really were randomly sampled from the registered voters?\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs the binomial case, the multi-class counts \\(O_1, O_2 \\dots O_k\\) and proportions \\(O_1/n, O_2/n \\dots O_k/n\\) are random variables before actual data are collected. Their value varies from sample to sample.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\nTotal\n\n\n\n\nObserved Count \\(O_i\\)\n\n205\n26\n25\n19\n275\n\n\n\nExpected Count \\(E_i\\)\n\n198\n19.25\n33\n24.75\n275\n\n\nPopulation Proportion \\((H_0)\\)\n\n0.72\n0.07\n0.12\n0.09\n1.00\n\n\n\n\n\n\n\n\n\n\n\\(\\pi_1\\) is the proportion of White in jury\n\n\\(\\pi_2\\) is the proportion of Black in jury\n\n\\(\\pi_3\\) is the proportion of Hispanic in jury\n\n\\(\\pi_4\\) is the proportion of Asian in jury\n\n\n\n\n\\(\\pi_1^0\\) is the proportion of White in register voters\n\n\\(\\pi_2^0\\) is the proportion of Black in register voters\n\n\\(\\pi_3^0\\) is the proportion of Hispanic in register voters\n\n\\(\\pi_4^0\\) is the proportion of Asian in register voters\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nBelow is an example of how to perform a goodness-of-fit test in R. Because the goodness-of-fit test is a chi-squared test, we use the function chisq.test(). The argument x is the observed count vector of length \\(k\\), and the argument p is the hypothesized proportion distribution which is a vector of probabilities of the same length as x. The function does not provide the critical value, but it gives us the \\(p\\)-value. Since the \\(p\\)-value is greater than \\(0.05\\), we do not reject \\(H_0\\).\n\nobs &lt;- c(205, 26, 25, 19)\npi_0 &lt;- c(0.72, 0.07, 0.12, 0.09)\n\n## Use chisq.test() function\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 5.8896, df = 3, p-value = 0.1171\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe chi-squared test can be used in two-sided one-sample proportion test. Suppose we are doing the following test\n\\[\\begin{align} &H_0: \\pi_1 = 0.5,  \\pi_2 = 0.5 \\\\ &H_1: \\pi_i \\ne 0.5 \\text{ for some } i \\end{align}\\]\nBecause there are only two categories, we automatically know the value of \\(\\pi_2\\) when we know the value of \\(\\pi_1\\) as \\(\\pi_2 = 1 - \\pi_1\\). So the hypothesis can be reduced to\n\\[\\begin{align} &H_0: \\pi_1 = 0.5 \\\\ &H_1: \\pi_1 \\ne 0.5 \\end{align}\\] or even\n\\[\\begin{align} &H_0: \\pi = 0.5 \\\\ &H_1: \\pi \\ne 0.5 \\end{align}\\] when we use \\(\\pi\\) for the proportion of the first category and \\(1-\\pi\\) for the second. We just transform a chi-squared test setting into a two-sided one-sample proportion test setting!\nSuppose \\(n = 1000\\), \\(O_1 = 520\\), \\(O_2 = 480\\), \\(\\pi_1^0 = \\pi_2^0 = 0.5\\). Our chi-squared test is\n\nchisq.test(x = c(520, 480), p = c(0.5, 0.5))\n\n\n    Chi-squared test for given probabilities\n\ndata:  c(520, 480)\nX-squared = 1.6, df = 1, p-value = 0.2059\n\n\nIn the language of the one-sample proportion test, we have \\(n = 1000\\), \\(y = 520\\), \\(\\pi^0 = 0.5\\). \\((y = O_1 = 520, n-y = O_2 = 480)\\)\n\nprop.test(x = 520, n = 1000, p = 0.5, alternative = \"two.sided\", \n          correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  520 out of 1000, null probability 0.5\nX-squared = 1.6, df = 1, p-value = 0.2059\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4890177 0.5508292\nsample estimates:\n   p \n0.52 \n\n\nDo you see the two tests are equivalent? They have the same test statistic, degrees of freedom as well as \\(p\\)-value. Both lead to the same conclusion.\n\n\n\n\nBelow is an example of how to perform a goodness-of-fit test in Python. Because the goodness-of-fit test is a chi-squared test, we use the function chisquare() in scipy.stats. The argument f_obs is the observed count vector of length \\(k\\), and the argument f_exp is the hypothesized or expected number of observations if the proportion distribution follows the hypothesized or expected proportions. The function does not provide the critical value, but it gives us the \\(p\\)-value. Since the \\(p\\)-value is greater than \\(0.05\\), we do not reject \\(H_0\\).\n\nfrom scipy.stats import chisquare\n\n\n# Observed frequencies\nobs = [205, 26, 25, 19]\n\n# Expected proportions\npi_0 = [0.72, 0.07, 0.12, 0.09]\n\n# Convert expected proportions to expected frequencies\nexpected = [p * sum(obs) for p in pi_0]\n\n# Chi-square goodness-of-fit test\nchisquare(f_obs=obs, f_exp=expected)\n\nPower_divergenceResult(statistic=5.889610389610387, pvalue=0.11710619130850633)\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe chi-squared test can be used in two-sided one-sample proportion test. Suppose we are doing the following test\n\\[\\begin{align} &H_0: \\pi_1 = 0.5,  \\pi_2 = 0.5 \\\\ &H_1: \\pi_i \\ne 0.5 \\text{ for some } i \\end{align}\\]\nBecause there are only two categories, we automatically know the value of \\(\\pi_2\\) when we know the value of \\(\\pi_1\\) as \\(\\pi_2 = 1 - \\pi_1\\). So the hypothesis can be reduced to\n\\[\\begin{align} &H_0: \\pi_1 = 0.5 \\\\ &H_1: \\pi_1 \\ne 0.5 \\end{align}\\] or even\n\\[\\begin{align} &H_0: \\pi = 0.5 \\\\ &H_1: \\pi \\ne 0.5 \\end{align}\\] when we use \\(\\pi\\) for the proportion of the first category and \\(1-\\pi\\) for the second. We just transform a chi-squared test setting into a two-sided one-sample proportion test setting!\nSuppose \\(n = 1000\\), \\(O_1 = 520\\), \\(O_2 = 480\\), \\(\\pi_1^0 = \\pi_2^0 = 0.5\\). Our chi-squared test is\n\n# Observed frequencies\nobs = [520, 480]\n\n# Expected proportions\nexpected = [0.5, 0.5]\n\n# Convert expected proportions to expected frequencies\nexpected_freq = [p * sum(obs) for p in expected]\n\n# Chi-square test\nchisq_test_res = chisquare(f_obs=obs, f_exp=expected_freq)\nchisq_test_res\n\nPower_divergenceResult(statistic=1.6, pvalue=0.20590321073206466)\n\n\nIn the language of the one-sample proportion test, we have \\(n = 1000\\), \\(y = 520\\), \\(\\pi^0 = 0.5\\). \\((y = O_1 = 520, n-y = O_2 = 480)\\)\n\nfrom statsmodels.stats.proportion import proportions_ztest, proportion_confint\n\nz_test_stat, pval = proportions_ztest(count=520, nobs=1000, value=0.5, \n                  alternative='two-sided')\npval                 \n\n0.2055402182226186\n\nproportion_confint(count=520, nobs=1000, alpha=0.05, method='normal')\n\n(0.48903505011072596, 0.5509649498892741)\n\n\nDo you see the two tests are equivalent? If you look at the proportion test output carefully, you‚Äôll find that the square of the \\(z\\) statistic is equal to the chi-squared statistic with degrees of freedom one, i.e., \\(z_{test}^2 = \\chi^2_{1, test}\\), and the \\(z\\) test here is equivalent to the chi-squared test for two-sided tests.\n\nz_test_stat\n\n1.2659242088545843\n\nz_test_stat ** 2\n\n1.6025641025641053\n\nchisq_test_res.statistic\n\n1.6\n\n\nBoth lead to the same conclusion.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Inference about Categorical Data</span>"
    ]
  },
  {
    "objectID": "infer-cat.html#test-of-independence",
    "href": "infer-cat.html#test-of-independence",
    "title": "20¬† Inference about Categorical Data",
    "section": "\n20.2 Test of Independence",
    "text": "20.2 Test of Independence\nSo far we consider one categorical variable with general \\(k\\) categories. In some situations, we have two categorical variables being considered, and we wonder if one affects the other distribution. Formally, we want to test whether or not the two variables are independent each other. The test doing this is the independence test.\n Contingency Table and Expected Count\n Contingency Table \nLet‚Äôs start with an example. A popular question in politics is ‚ÄúDoes the opinion of the President‚Äôs job performance depend on gender?‚Äù This question may involve two categorical variables such as\n\nJob performance: approve, disapprove, no opinion\nGender: male, female\n\nTo answer such question, as every other test, we collect our data and see if there is any sufficient evidence to say the two variables are dependent. When two categorical variables are involved, we usually summarize the data in a contingency table or two-way frequency/count table as shown below.\n\n\n\n\n\nApprove\nDisapprove\nNo Opinion\nTotal\n\n\n\n\nMale\n18\n22\n10\n50\n\n\n\nFemale\n23\n20\n12\n55\n\n\n\nTotal\n41\n42\n22\n105\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhen we consider a two-way table, we often would like to know, are these variables related in any way? That is, are they dependent (versus independent)? If President‚Äôs approval rate has nothing to do with gender, the Job performance distributions in the male and female groups should be similar or consistent. In other words, whether or not the person is male or female does not affect how President‚Äôs job performance is viewed.\n Expected Count \nThe idea is similar to goodness-of-fit test. We first calculate the expected count in each cell (Total row and column are excluded) in the contingency table, the count we expect to see under the condition that the two variables are independent. We always do our analysis in the world of null hypothesis that two variables have no relationship.\n\n\n\n\nApprove\nDisapprove\nNo Opinion\nTotal\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n50\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n55\n\n\nTotal\n41\n42\n22\n105\n\n\n\nThe expected count for the \\(i\\)-th row and \\(j\\)-th column, which is listed in parentheses in the table above, is calculated by: \\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\] For example, the expected count of Disapprove in the Female group is \\[\\text{Expected Count}_{\\text{row 2; col 2}} = \\frac{\\text{(row 2 total}) \\times (\\text{col 2 total})}{\\text{table total}} = \\frac{42 \\times 55}{105} = 22.\\]\nWe are ready for doing test of independence once all expected counts are obtained.\n\n Test of Independence Procedure \nThe test of independence requires that every expected count \\(E_{ij} \\ge 5\\) in the contingency table. The higher the better.\nAs we discussed before, we believe the variables are independent unless strong evidence says they are not. So our hypotheses are  \\[\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\]\nThe test statistic formula is pretty similar to the test of goodness-of-fit. The test of independence is also a chi-squared test. The chi-squared test statistic is\n\\[\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}},\\] where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the contingency table. The idea is the same as the test of goodness-of-fit. Under the assumption of independence, if the observed counts are far away from the counts we should expect to see, we get a larger test statistic, and tend to conclude that the independence assumption is not reasonable.\nThe chi-squared test is right-tailed, so we reject \\(H_0\\) if \\(\\chi^2_{test} &gt; \\chi^2_{\\alpha, \\, df}\\), where \\(df = (r-1)(c-1)\\).\n Example \n\n\n\nApprove\nDisapprove\nNo Opinion\nTotal\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n50\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n55\n\n\nTotal\n41\n42\n22\n105\n\n\n\n \\[\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\]\nThe test statistic is \\[\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\]\nThe critical value is \\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\). Since \\(\\chi_{test}^2 &lt; \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\). Therefore we fail to conclude that the opinion of the President‚Äôs job performance depends on gender.\nCalculating all the expected counts is tedious, especially when the variables have many categories. In practice we never calculate them by hand, and use computing software to do so.\n\n\nR\nPython\n\n\n\n\nBelow is an example of how to perform the test of independence using R. Since the test of independence is a chi-squared test, we still use the chisq.test() function. This time we need to prepare the contingency table as a matrix, and put the matrix in the x argument in the function. That‚Äôs it! R does everything for us. If we save the result as an object like ind_test which is a R list, we can get access to information related to the test, such as the expected counts ind_test$expected.\n\n(contingency_table &lt;- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(ind_test &lt;- chisq.test(x = contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.65019, df = 2, p-value = 0.7225\n\n## extract expected counts\nind_test$expected\n\n         [,1] [,2]     [,3]\n[1,] 19.52381   20 10.47619\n[2,] 21.47619   22 11.52381\n\n## critical value\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  \n\n[1] 5.991465\n\n\n\n\nBelow is an example of how to perform the test of independence using Python. The function we use is chi2_contingency() in scipy.stats since the test of independence relies on a contingency table. We first prepare the contingency table as a matrix, and put the matrix in the observed argument in the function. That‚Äôs it! Python does everything for us. If we save the result as an object like ind_test, we can get access to information related to the test, such as the expected counts ind_test.expected_freq.\n\nimport numpy as np\nfrom scipy.stats import chi2_contingency, chi2\n\ncontingency_table = np.array([[18, 22, 10], [23, 20, 12]])\ncontingency_table\n\narray([[18, 22, 10],\n       [23, 20, 12]])\n\n# Chi-square test of independence\nind_test = chi2_contingency(observed=contingency_table)\nind_test.statistic\n\n0.6501914936504742\n\nind_test.pvalue\n\n0.7224581772535765\n\nind_test.dof\n\n2\n\nind_test.expected_freq\n\narray([[19.52380952, 20.        , 10.47619048],\n       [21.47619048, 22.        , 11.52380952]])\n\n# Critical value for chi-square distribution\nchi2.ppf(0.95, df=(2-1)*(3-1))\n\n5.991464547107979",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Inference about Categorical Data</span>"
    ]
  },
  {
    "objectID": "infer-cat.html#test-of-homogeneity",
    "href": "infer-cat.html#test-of-homogeneity",
    "title": "20¬† Inference about Categorical Data",
    "section": "\n20.3 Test of Homogeneity",
    "text": "20.3 Test of Homogeneity\nTest of homogeneity is a generalization of the two-sample proportion test. This test determines if two or more populations (or subgroups of a population) have the same distribution of a single categorical variable having two or more categories.\nMore to be added.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Inference about Categorical Data</span>"
    ]
  },
  {
    "objectID": "infer-cat.html#exercises",
    "href": "infer-cat.html#exercises",
    "title": "20¬† Inference about Categorical Data",
    "section": "\n20.4 Exercises",
    "text": "20.4 Exercises\n\nA researcher has developed a model for predicting eye color. After examining a random sample of parents, she predicts the eye color of the first child. The table below lists the eye colors of offspring. On the basis of her theory, she predicted that 87% of the offspring would have brown eyes, 8% would have blue eyes, and 5% would have green eyes. Use 0.05 significance level to test the claim that the actual frequencies correspond to her predicted distribution.\n\n\n\nEye Color\nBrown\nBlue\nGreen\n\n\nFrequency\n127\n21\n5\n\n\n\nIn a study of high school students at least 16 years of age, researchers obtained survey results summarized in the accompanying table. Use a 0.05 significance level to test the claim of independence between texting while driving and driving when drinking alcohol. Are these two risky behaviors independent of one another?\n\n\n\n\nDrove after drinking alcohol?\n\n\n\n\n\nYes\nNo\n\n\nTexted while driving\n720\n3027\n\n\nDid not text while driving\n145\n4472",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>20</span>¬† <span class='chapter-title'>Inference about Categorical Data</span>"
    ]
  },
  {
    "objectID": "infer-p.html",
    "href": "infer-p.html",
    "title": "21¬† The Issues of \\(p\\)-value and Statistical Significance",
    "section": "",
    "text": "Our brain has this annoying habit to think that [‚Ä¶], if under some hypothesis, results are unlikely, then the hypothesis is unlikely. This is false. ‚Äì Christophe Michel (1974 - )\n\n\nThere are three types of lies: lies, damn lies, and statistics. ‚Äì Benjamin Disraeli (1804 - 1881)\n\nWe have been using the classical or frequentist approach to do a variety of statistical inferences. But Dr.¬†Yu once said he never uses p-value in his own research, and there are many issues with what is taught in Intro Stats (MATH 1700/4720/4740). In fact, the null hypothesis significance testing (NHST) paradigm and the p-value usage have been much criticized and shown to be problematic, misused, and resulting in reproducibility and replication crisis in scientific research.\nSome references are\n\nWikipedia: Misuse of p-values\nA. Reinhart (2015), ‚ÄúStatistics Done Wrong‚Äù, No Starch Press, San Francisco.\nR. L. Wasserstein and Nicole A. Lazar (2016), ‚ÄúThe ASA Statement on p-Values: Context, Process, and Purpose‚Äù, The American Statistician, 70:2, 129-133.\nV. Amrhein, S. Greenland and B. McShane (2019), ‚ÄúRetire statistical significance‚Äù, Nature, 567, 305-307.\nB. McShane, D. Gal, A. Gelman, C. Robert and Jennifer L. Tackett (2019), ‚ÄúAbandon Statistical Significance‚Äù, The American Statistician, 73:sup1, 235-245.\nA. Gelamn and E. Loken (2014), ‚ÄúThe Statistical Crisis in Science‚Äù, American Scientist, 102, 460-465.\nR. L. Wasserstein, A. L. Schirm and N. A. Lazar (2019), ‚ÄúMoving to a World Beyond p &lt; 0.05‚Äù, The American Statistician, 73:sup1, 1-19.\nStatistically Discernible by Editorial of Journal of Statistics Education",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>21</span>¬† <span class='chapter-title'>The Issues of $p$-value and Statistical Significance</span>"
    ]
  },
  {
    "objectID": "infer-bayes.html",
    "href": "infer-bayes.html",
    "title": "22¬† Bayesian Thinking and Inference",
    "section": "",
    "text": "22.1 Bayesian Thinking\nBefore we dive into Bayesian statistics, let‚Äôs take a quiz to determine whether you currently lean more towards a Bayesian or frequentist mindset. This quiz is adapted from Bayes Rules, a great new book on Bayesian statistics. I encourage you to spend some time on this if you would like to delve deeper.\nTime tally up your quiz score. Are you frequentist or Bayesian? Totals from 4‚Äì5 indicate that your current thinking is fairly frequentist, whereas totals from 9‚Äì12 indicate alignment with the Bayesian philosophy. In between these extremes, totals from 6‚Äì8 indicate that you see strengths in both philosophies.\nYou are a frequentist or Bayesian? Are those confidence interval or hypothesis testing thing all Greek to you? Maybe it is because",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Bayesian Thinking and Inference</span>"
    ]
  },
  {
    "objectID": "infer-bayes.html#bayesian-thinking",
    "href": "infer-bayes.html#bayesian-thinking",
    "title": "22¬† Bayesian Thinking and Inference",
    "section": "",
    "text": "All knowledge degenerates into probability; and this probability is greater or less, according to our expereience of veracity or deceitfulness of our understanding, and according to the simplicity or intricacy of the question. ‚Äì David Hume (1711 - 1776)\n\n\n\n\nWhen flipping a fair coin, we say that ‚Äúthe probability of flipping Heads is 0.5.‚Äù How do you interpret this probability? (a = 1 pt, b = 3 pts, c = 2 pts)\n\nIf I flip this coin over and over, roughly 50% will be Heads.\nHeads and Tails are equally plausible.\nBoth a. and b. make sense.\n\n\n\nAn election is coming up and a pollster claims that candidate Yu has a 0.9 probability of winning. How do you interpret this probability? (a = 1 pt, b = 3 pts, c = 1 pt)\n\nIf we observe the election over and over, candidate Yu will win roughly 90% of the time.\nCandidate Yu is much more likely to win than to lose.\nThe pollster‚Äôs calculation is wrong. Candidate Yu will either win or lose, thus their probability of winning can only be 0 or 1.\n\n\n\nTwo claims. (a = 3 pts, b = 1 pt)\n(1) Ben claims he can predict the coin flip outcome. To test his claim, you flip a fair coin 8 times and he correctly predicts all.\n(2) Emma claims she can distinguish natural and artificial sweeteners. To test her claim, you give her 8 samples and she correctly identifies each.\nIn light of these experiments, what do you conclude?\n\nYou‚Äôre more confident in Emma‚Äôs claim than Ben‚Äôs claim.\nThe evidence supporting Ben‚Äôs claim is just as strong as the evidence supporting Emma‚Äôs claim.\n\n\n\nSuppose that during a doctor‚Äôs visit, you tested positive for COVID. If you only get to ask the doctor one question, which would it be? (a = 3 pts, b = 1 pt)\n\nWhat‚Äôs the chance that I actually have COVID?\nIf in fact I don‚Äôt have COVID, what‚Äôs the chance that I would‚Äôve gotten this positive test result?\n\n\n\n\n\n\n\nTotals 4-5: your thinking is frequentist\n\n\n\n\n\n\nYoung Fisher from Wiki\n\n\n\n\n\nTotals 9-12: your thinking is Bayesian\n\n\n\n\n\n\nThomas Bayes from Wiki\n\n\n\n\n\n\nTotals 6-8: you see strengths in both philosophies",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Bayesian Thinking and Inference</span>"
    ]
  },
  {
    "objectID": "infer-bayes.html#the-meaning-of-probability-relative-frequency-vs.-relative-plausibility",
    "href": "infer-bayes.html#the-meaning-of-probability-relative-frequency-vs.-relative-plausibility",
    "title": "22¬† Bayesian Thinking and Inference",
    "section": "\n22.2 The Meaning of Probability: Relative Frequency vs.¬†Relative Plausibility\n",
    "text": "22.2 The Meaning of Probability: Relative Frequency vs.¬†Relative Plausibility\n\nThe frequentist interprets probability as the long-run relative frequency of a repeatable experiment. We‚Äôve seen this interpretation before in Chapter 7. We know that using relative frequencies as probability has several issues:\n\nüòï How large of a number is large enough?\nüòï Meaning of ‚Äúunder similar conditions‚Äù\nüòï The relative frequency is reliable under identical conditions?\nüëâ We only obtain an approximation instead of exact value.\nüòÇ How do you compute the probability that Chicago Cubs wins the World Series next year?\n\n\n\n\n\n\n\n\n\nIn statistical inference, our research questions or events of interest are often not repeatable or very difficult to replicate. For example, what is the probability that Donald Trump will win the 2024 presidential election? What is the probability that the Milwaukee Bucks will win the 2025 NBA championship? What would the mean income level for males be if every man was required to serve in the military for two years? Still many political scientists, sport analysts, or economists are answering those questions, right?! To rationalize their arguments, and to better answer these types of research questions, we must interpret probability in a different way.\n\nIn the Bayesian philosophy, a probability measures the relative plausibility of an event. In the Bayesian paradigm, we can still compute the probability of some event whose frequency is 0! Here, probability is a quantitative measure that quantifies the relative plausibility of the event according to some mathematical/statistical model. This probability is subjective, and it logically describes our epistemic uncertainty about some thing. Because ‚ÄúMilwaukee Bucks wins the 2025 NBA championship‚Äù cannot be repeatable, we are not able to observe its ‚Äúobjective‚Äù frequencies. However, anybody, from a 5th grade girl to an ESPN NBA analyst, can express their opinion about how likely this event occurs, based on information they currently have and their own judgmental rules. The Bayesian probability is a unified way of expressing everybody‚Äôs judgement and uncertainty about something happening, and such relative plausibility is computed through a statistical model including the information or beliefs one carries. In Bayesian eyes, no probability is purely objective. Probability is model-dependent. Think about it. When you, or even an artificial intelligence (AI) machine choose which method to use to calculate the desired probability, whether or not the event is repeatable and relative frequencies can be collected, you (AI) implicitly express your preference and judgement on how the probability, or more generally the knowledge you make inference to, is computed.\nLet‚Äôs conclude the interpretation of probability by some examples. We usually see the chance of winning presidency when we watch TV news waiting for the election result. Apparently, the chance of winning is a (subjective) relative plausibility measure calculated by some political scientists using some statistical models.\n\n\nFor the statement ‚Äúcandidate A has a 0.9 probability of winning‚Äù, a frequentist might say\n\nthe conclusion is wrong or\nweirdly say in long-run hypothetical repetitions of the election, candidate A would win roughly 90% of the time.\n\nA Bayesian would say based on analysis the candidate A is 9 times more likely to win than to lose.\nBack to our Quiz 1. For the statement ‚Äúthe probability of flipping Heads is 0.5‚Äù, a frequentist would conclude that if we flip the coin over and over, roughly 1/2 of these flips will be Heads. A Bayesian would conclude that Heads and Tails are equally likely.\n\n\n\n\n\n\nSource: https://x.com/nytgraphics/status/796195155158171648",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Bayesian Thinking and Inference</span>"
    ]
  },
  {
    "objectID": "infer-bayes.html#prior-information-and-empirical-evidence",
    "href": "infer-bayes.html#prior-information-and-empirical-evidence",
    "title": "22¬† Bayesian Thinking and Inference",
    "section": "\n22.3 Prior Information and Empirical Evidence",
    "text": "22.3 Prior Information and Empirical Evidence\n\n\n\n\n\n\n\n\n\n\n\nHow can we live if we don‚Äôt change? ‚Äì Beyonc√©. Lyric from ‚ÄúSatellites.‚Äù\n\n\nUncertainty is a personal matter; it is not the uncertainty but your uncertainty. ‚Äì Dennis Lindley, Understanding Uncertainty (2006)\n\n\nA people without the knowledge of their past history, origin and culture is like a tree without roots. ‚Äì Marcus Garvey (1887 - 1940)\n\n\nOpinion is the medium between knowledge and ignorance. ‚Äì Plato (427 ‚Äì 348 BC)\n\nEverybody changes their mind. You likely even changed your mind in the last minute. For example, suppose there‚Äôs a new Italian restaurant in your town. It has a 5-star online rating and you love Italian food! Thus, prior to ever stepping foot in the restaurant, you anticipate that it will be quite delicious. On your first visit, you collect some edible data: your pasta dish arrives a soggy mess. Weighing the stellar online rating against your own terrible meal (which might have just been a fluke), you update your knowledge: this is a 3-star not 5-star restaurant. Willing to give the restaurant another chance, you make a second trip. On this visit, you‚Äôre pleased with your Alfredo and increase the restaurant‚Äôs rating to 4 stars. You continue to visit the restaurant, collecting edible data and updating your knowledge each time.\n\n\n\n\n\n\n\nFigure¬†22.1: Fig 1.1 of Bayes Rules. The figures not being sourced come from this book too.\n\n\n\n\n\nWe use data and prior beliefs to update our knowledge (posterior), and repeating. So today‚Äôs prior is yesterday‚Äôs posterior!\nWe continuously update our knowledge about the world as we accumulate lived experiences, or collect data.\n\nFigure¬†22.2 shows Bayesian Knowledge-building Process. If you‚Äôre an environmental scientist, yours might be an analysis of the human role in climate change. You don‚Äôt walk into such an inquiry without context ‚Äì you carry a degree of incoming or prior information based on previous research and experience. Naturally, it‚Äôs in light of this information that you interpret new data, weighing both in developing your updated or posterior information.\n\n\n\n\n\n\n\nFigure¬†22.2\n\n\n\n\nFrequentist relies on (limited) data only. In Question 3, in a frequentist analysis, ‚Äú8 out of 8‚Äù is ‚Äú8 out of 8‚Äù no matter if it‚Äôs in the context of Ben‚Äôs coins or Emma‚Äôs sweeteners. Thus frequentists have equally confident conclusions that Ben can predict coin flips and Emma can distinguish between natural and artificial sweeteners.\n\n\n\n\n\n\n\n\nHowever, do you really believe Ben‚Äôs claim 100%? ü§î üòï. Let me guess. In fact, we judge their claim before evidence are collected, don‚Äôt we? ü§î You probably think Ben overstates his ability but Emma‚Äôs claim sounds relatively reasonable, right?\nFrequentist throws out all prior knowledge in favor of a mere 8 data points. Bayesian analyses balance and weight our prior experience/knowledge/belief and new data/evidence to judge a claim or make a conclusion.\n\n\n\n\n\n\n\n\n\n\nHowever, we are not stubborn! If Ben had correctly predicted the outcome of 1 million coin flips, the strength of this data would far surpass that of our prior judgement, leading to a posterior conclusion that perhaps Ben is psychic!\n\n\n\n\n\n\n\n\n\n\n\nThe concept, whether referred to as prior information or prior belief, essentially represents our current state of knowledge about something before we gather additional evidence, data, or observations. This prior knowledge serves as the foundation upon which we build our understanding or enhance our knowledge of the subject of interest.\nPersonally, I prefer referring to the current state of knowledge as our opinion ‚Äî or, to be more modest, our two cents! As Bayesians, we are individuals with opinions. We learn and grow from our life experiences, gradually aligning our opinions closer to the truth as we become more knowledgeable. I believe (and this is my opinion) that this process mirrors how we learn anything throughout our lives, from birth until our last day. A person without opinions is akin to someone without a brain‚Äîif you ask them a question, their response might always be, ‚ÄúI have no idea,‚Äù ‚ÄúI have no judgment,‚Äù or ‚Äúwe should gather evidence to answer the question.‚Äù\nHowever, as we grow up, we watch TV, read papers, attend classes, and continuously build our knowledge and judgments about the world around us. Our opinions, values, and the behavior shaped by these values and judgments define who we are. In essence, a Bayesian holds an opinion on everything!\nOpinions lie at the core of the debate between Bayesians and frequentists. The reliance on opinions is a key reason why Bayes‚Äô rule faced rejection over the past two centuries. Scientists have long sought to ensure that their work remains objective, but opinions are often viewed as inherently subjective. Consequently, frequentists and the broader scientific community have considered the subjectivity of opinions to be a fundamental flaw of Bayesianism.\nHowever, while opinions are indeed subjective, they are by no means arbitrary, particularly when they are derived using Bayes‚Äô rule and adhere to the laws of probability. In fact, Bayesians view opinions as fundamental strengths of Bayesian reasoning‚Äîprovided these opinions are shaped by Bayes‚Äô rule. Opinions are considered a cornerstone of rationality. This is the most contentious assertion of Bayesianism.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Bayesian Thinking and Inference</span>"
    ]
  },
  {
    "objectID": "infer-bayes.html#asking-different-questions",
    "href": "infer-bayes.html#asking-different-questions",
    "title": "22¬† Bayesian Thinking and Inference",
    "section": "\n22.4 Asking Different Questions",
    "text": "22.4 Asking Different Questions\n\nOur brain has this annoying habit to think that [‚Ä¶], if under some hypothesis, results are unlikely, then the hypothesis is unlikely. This is false. ‚Äì Christophe Michel (1974 - )\n\n\nAn approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem. ‚Äì John Tukey (1915 - 2000)\n\n\n\nInterestingly, Bayesians usually answer the question we care about.\nIn Question 4,\n\nBayesians answer (a) what‚Äôs the chance that I actually have COVID?\n\nFrequentists answer (b) if in fact I do not have COVID, what‚Äôs the chance that I would‚Äôve gotten this positive test result?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Positive\nTest Negative\nTotal\n\n\n\nCOVID\n3\n1\n4\n\n\nNo COVID\n9\n87\n96\n\n\nTotal\n12\n88\n100\n\n\n\n\n\\(H_0\\): Do not have COVID vs.¬†\\(H_1\\): Have COVID\nA frequestist assesses the uncertainty of the observed data in light of an assumed hypothesis \\(P(Data \\mid H_0) = 9/96\\)\nA Bayesian assesses the uncertainty of the hypothesis in light of the observed data \\(P(H_0 \\mid Data) = 9/12\\)\n\nA Bayesian analysis would ask: Given my positive test result, what‚Äôs the chance that I actually have the disease? Since only 3 of the 12 people that tested positive have the disease, there‚Äôs only a 25% chance that you have the disease. Thus, when we take into account the disease‚Äôs rarity and the relatively high false positive rate, it‚Äôs relatively unlikely that you actually have the disease. What a relief.\nSince disease status isn‚Äôt repeatable, the probability you have the disease is either 1 or 0 ‚Äì you have it or you don‚Äôt. To the contrary, medical testing (and data collection in general) is repeatable. You can get tested for the disease over and over and over. Thus, a frequentist analysis would ask: If I don‚Äôt actually have the disease, what‚Äôs the chance that I would‚Äôve tested positive? Since only 9 of the 96 people without the disease tested positive, there‚Äôs a roughly 10% (9/96) chance that you would‚Äôve tested positive even if you didn‚Äôt have the disease.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Bayesian Thinking and Inference</span>"
    ]
  },
  {
    "objectID": "infer-bayes.html#bayesian-inference",
    "href": "infer-bayes.html#bayesian-inference",
    "title": "22¬† Bayesian Thinking and Inference",
    "section": "\n22.5 Bayesian Inference",
    "text": "22.5 Bayesian Inference\nHere we use the Fake News example in the Bayes Rules! book to illustrate a Bayesian model.\n\n\n\nTell if an incoming article is fake. The usage of an ! might seem odd for a real article. The exclamation point data is more consistent with fake news.\n\n\n\nPrior info: 40% of the articles are fake\n\n\n#   type   n percent\n#   fake  60     0.4\n#   real  90     0.6\n#  Total 150     1.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nData come in: Check several fake and real articles, and found ! is more consistent with fake news.\n\n\n#  title_has_excl fake real\n#           FALSE   44   88\n#            TRUE   16    2\n#           Total   60   90\n\n\n\n22.5.1 Bayesian Updating Rule\n\n\n\n\n\n\n\n\n\n\\(F\\): an article is fake.\nThe prior probability model\n\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\nProbability \\(P(\\cdot)\\)\n\n0.4\n0.6\n1\n\n\n\n#  title_has_excl fake real\n#           FALSE   44   88\n#            TRUE   16    2\n#           Total   60   90\n\n\n\\(D\\): an article title has exclamation mark.\nConditional probability: \\(P(D \\mid F) = 16/60 = 0.27\\); \\(P(D \\mid F^c) = 2/90 = 0.02\\).\n\nOpposite position:\n\nKnow the incoming article used ! (observed data)\nDon‚Äôt know whether or not the article is fake (what we want to decide).\n\n\nCompare \\(P(D \\mid F)\\) and \\(P(D \\mid F^c)\\) to ascertain the relative likelihoods of observed data \\(D\\) under different scenarios of the uncertain article status.\n\nSince exclamation point usage is so much more likely among fake news than real news, this data provides some evidence that the article is fake To help distinguish this application of conditional probability calculations from that when \\(D\\) is uncertain and \\(F\\) is known, we‚Äôll utilize the following likelihood function notation.\n\n22.5.2 Likelihood Function\n\n\n\nLikelihood function \\(L(\\cdot\\mid D)\\):\n\n\\[L(F \\mid D) = P(D \\mid F) \\text{ and } L(F^c \\mid D) = P(D \\mid F^c)\\]\n\n\n\nWhen \\(F\\) is known, the conditional probability function \\(P(\\cdot \\mid F)\\) compares the probabilities of an unknown event \\(D\\), \\(D^c\\), occurring with \\(F\\): \\[P(D \\mid F) \\text{  vs. } P(D^c \\mid F)\\]\n\n\n\n\n\nWhen \\(D\\) is known, the likelihood function \\(L(\\cdot \\mid D) = P(D \\mid \\cdot)\\) evaluates the relative compatibility of data \\(D\\) with \\(F\\) or \\(F^c\\): \\[L(F \\mid D) \\text{  vs. } L(F^c \\mid D)\\]\n\n\n\n\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\n\nProbability \\(P(\\cdot)\\)\n\n0.4\n0.6\n1\n\n\nLikelihood \\(L(\\cdot \\mid D)\\)\n\n0.27\n0.02\n0.29\n\n\n\n\n\nThe likelihood function is not a probability function!\n\nThe likelihood function is essential to Bayesianism. When we want to update our belief or knowledge about the chance that a book is fake or not, given the information collected from the data, we need the likelihood function. Sometimes it may require a little imagination. It is like a thought experiment. If we were in the world that all books/articles are fake, what is the chance that we get the data like the one at hand? Similarly, what is that chance if we now lived in the world without any fake articles? Here the two hypotheses (fake vs.¬†not fake), alternative theories, or proposed mechanisms postulate how data are generated and what data will be like under such mechanisms. And the likelihood function tells us which mechanism the current data fit in better.\nThe likelihood function is not a probability function! Although the word probability is avoided, the word likelihood is still misleading or hard to understand sometimes. The likelihood of the data is not the credence of the theory/mechanism which is \\(P(F \\mid D)\\) obtained by the Bayes‚Äô rule.\n\n22.5.3 Bayes‚Äô Rule for Posterior\nTo be Bayesian is to rest all knowledge upon the language of conditional probabilities. We obtain the credence of an article being fake given the fact that it has an exclamation mark in its title \\(P(F \\mid D)\\) using the conditional probability formula, and the law of total probabilities.\n\\[\\begin{align*} P(F \\mid D) &= \\frac{P(F \\cap D)}{P(D)}\\\\ &= \\frac{P(D \\mid F)P(F)}{P(D)} \\\\ &= \\frac{P(D \\mid F)P(F)}{P(D \\mid F)P(F) + P(D \\mid F^c)P(F^c)}\\\\ &= \\frac{L(F \\mid D)P(F)}{L(F \\mid D)P(F) + L(F^c \\mid D)P(F^c)}\\end{align*}\\]\nIn general, the update rule has the form\n\\[\\text{posterior = } \\frac{\\text{likelihood} \\cdot \\text{prior }}{ \\text{normalizing constant}} \\]\nThe normalizing constant \\(P(D)\\) is known as marginal likelihood or evidence. Here, the thing we care about, whether it is fake or not, only has two possible values, \\(F\\) or \\(F^c\\). The marginal likelihood is then a sum of two terms. It combines different reasonings in mutually incompatible versions of reality. Even two terms summed up together could make Bayes‚Äô rule hard to apply and understand, not to mention that there could be several even infinitely many possible values of what we care about. The computation of this normalizing constant can be extremely hard, and that‚Äôs why some people do not want to use Bayesian methods, even Bayesianism makes more sense to them. Fortunately, many statistical software packages for implementing Bayesian methods have been developed, doing all the necessary computation for us. If you love the Bayesian philosophy, don‚Äôt hesitate to use Bayes‚Äô rule!\n\n\nStarted with a prior understanding that there‚Äôs a 40% chance that the incoming article would be fake. Yet upon observing the use of an exclamation point in the title\n\n\n‚ÄúThe president has a funny secret!‚Äù\n\n\na feature that‚Äôs more common to fake news. Our posterior understanding evolved quite a bit ‚Äì the chance that the article is fake jumped to 89%.\n\n\n\n\n\n\n\n\nEvent\n\\(F\\)\n\\(F^c\\)\nTotal\n\n\n\nPrior prob \\(P(\\cdot)\\)\n\n0.4\n0.6\n1\n\n\nPosterior prob \\(P(\\cdot \\mid D)\\)\n\n0.89\n0.11\n1",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Bayesian Thinking and Inference</span>"
    ]
  },
  {
    "objectID": "infer-bayes.html#bayesian-inference-for-random-variables",
    "href": "infer-bayes.html#bayesian-inference-for-random-variables",
    "title": "22¬† Bayesian Thinking and Inference",
    "section": "\n22.6 Bayesian Inference for Random Variables",
    "text": "22.6 Bayesian Inference for Random Variables\n\nIf people do not believe that mathematics is simple, it is only because they do not realize how complicated life is. ‚Äì John von Neumann (1903 - 1957)\n\n\nTruth is much too complicated to allow anything but approximations. ‚Äì John von Neumann (1903 - 1957)\n\nIn this section, we use the example in Chapther 3 of Bayes Rules! to illustrate how to do the inference about the unknown parameter, for example the population mean \\(\\mu\\), using Bayes‚Äô rule. Instead of either simply providing a point estimate or calculating the confidence interval for the parameter, the Bayesian inference estimates the whole probability distribution of the parameter given the data. The distribution is the posterior distribution of the parameter that shows our credence of the parameter value.\nIn frequentist approaches, the parameter is considered unknown but fixed and constant. In contrast, Bayesian philosophy treats the parameter as variable, often assuming it to be a random variable following some probability distribution. This perspective acknowledges that the parameter may frequently change over time. For instance, do you believe the mean GPA of Marquette students remains unchanged? Or that the mean height or weight of Marquette students stays constant over time? These examples illustrate that many parameters are not static but evolve, reflecting the Bayesian view of a dynamic and probabilistic world.\nEven if the parameter remains fixed at an unknown value, Bayesians still consider all the values they believe the true value could possibly be when making inferences. When Bayesians say the parameter value varies, they are referring to the idea that the true value has many possible values, and their belief about the true value is not quite certain or changes. The posterior distribution, or posterior probability, represents the plausibility of each possible value of the parameter, reflecting how their understanding of the true value changes with new evidence.\nIn the Bayesian framework, one never places all their confidence in a single model or parameter because, as the saying goes, ‚ÄúAll models are wrong.‚Äù In parametric statistical inference, each parameter value typically corresponds to a specific model. Even if there were a true model defined by a true parameter value, Bayesians avoid putting all their eggs in one basket. Why is that? Consider this analogy: if you believe one of five company stocks will rise in value tomorrow, would you invest all your money in just one stock, or would you diversify your investments across all five stocks, allocating different shares based on your belief in the likelihood of each stock‚Äôs price increase? Moreover, what is considered ‚Äútruth‚Äù can change over time. For instance, people once believed the world was flat‚Äîa ‚Äútruth‚Äù that later proved to be incorrect. Therefore, Bayesians spread their credence across a range of possibilities, acknowledging that even widely accepted truths can evolve.\nYes, I know you want to pursue the truth. But unfortunately, ‚Äútruth is much too complicated to allow anything but approximations.‚Äù Also ‚Äúall models are wrong.‚Äù However, some models are more believable and useful than others. The universe is too complicated to be fully described by a model, whether it is mathematical, statistical, or physical. Since we never be able to capture the truth, it would benefit us if we switch our focus to the useful models that better approximates the truth. In estimation and prediction, almost all questions have no simple, unambiguous answer because we are always uncertain about something we don‚Äôt know. Therefore, the uncertainty quantification for the unknowns is important because good uncertainty quantification better describes what the truth could possibly be. As a result, what we need to pursue or develop is a less wrong but quite useful model that well qualifies the uncertainty and our credence of the unknown parameter. In other words, the model gives us a pretty well approximated posterior distribution.\nYes, the pursuit of truth is a noble endeavor, but as the saying goes, ‚Äútruth is much too complicated to allow anything but approximations.‚Äù Furthermore, ‚Äúall models are wrong, but some models are more believable and useful than others.‚Äù The complexity of the universe is such that no model‚Äîwhether mathematical, statistical, or physical‚Äîcan fully capture it. Since we can never truly grasp the truth, it is more beneficial to focus on developing models that better approximate the truth and are useful in practice.\nIn the realms of estimation and prediction, most questions lack simple, unambiguous answers because there is always some uncertainty surrounding the unknowns. This is why uncertainty quantification is crucial; it provides a better description of what the truth could potentially be. Consequently, our goal should be to develop models that are less wrong but highly useful, effectively quantifying uncertainty and our confidence in the unknown parameter. In other words, we should strive to create models that provide a well-approximated posterior distribution, offering a more accurate representation of the possible truths.\nDone with the concepts. Let‚Äôs see the mathematical formulation of Bayesian inference. Suppose the unknown parameter to be estimated is \\(\\theta\\) and data are denoted as \\({\\bf Y} = (Y_1, \\dots, Y_n)\\). Both are assumed random variables before the data are collected. Our job is to do inference about \\(\\theta\\), or obtain the posterior distribution of \\(\\theta\\). Unlike frequentists, who develop a variety of tools for different estimation problems, Bayesians rely on just one fundamental principle: Bayes‚Äô rule.\nLet \\(\\pi(\\theta)\\) be the prior pmf/pdf of \\(\\theta\\). Let \\(L(\\theta \\mid y_1,\\dots, y_n)\\) be the likelihood of \\(\\theta\\) given observed data \\(\\by = \\{y_i \\}_{i = 1}^n\\). Then with the Bayes‚Äô rule, the posterior distribution of \\(\\theta\\) given \\(\\by\\) is\n\\[\\pi(\\theta \\mid \\by) = \\frac{L(\\theta \\mid \\by)\\pi(\\theta)}{p(\\by)}\\] where \\[p(\\by) = \\begin{cases} \\int_{\\Theta} L(\\theta \\mid \\by)\\pi(\\theta) ~ d\\theta & \\text{if } \\theta \\text{ is continuous }\\\\\n\\sum_{\\theta \\in \\Theta} L(\\theta \\mid \\by)\\pi(\\theta) & \\text{if } \\theta \\text{ is discrete }\n\\end{cases}\\]\nHere, \\(\\Theta\\) is the collection of all possible values of \\(\\theta\\). To obtain the posterior \\(\\pi(\\theta \\mid \\by)\\), we need to compute the marginal likelihood \\(p(\\by)\\) that is the core of Bayesian computation. As you can see, when \\(\\Theta\\) contains plenty of or infinitely many possible values of \\(\\theta\\), or when several, say \\(K\\), unknown parameters are estimated simultaneously in one single model, i.e., \\(\\theta = (\\theta_1, \\dots, \\theta_K)\\), this normalizing constant is getting hard to compute. One of the Bayesian mainstream research is to develop well-approximated and computationally efficient algorithms to compute the marginal likelihood, and hence the posterior distribution.\nWe don‚Äôt need to worry about the computation issue at this moment. We focus on the ideas and concepts of applying Bayes‚Äô formula. Notice that the marginal likelihood \\(p(\\by)\\) has nothing to do with \\(\\theta\\) because \\(\\theta\\) has been integrated out. Therefore, the posterior is in fact proportional to the numerator of the formula:\n\\[\\pi(\\theta \\mid \\by ) = \\frac{L(\\theta \\mid \\by)\\pi(\\theta)}{p(\\by)} \\propto_{\\theta} L(\\theta \\mid \\by)\\pi(\\theta)\\]\n\\[\\text{posterior } \\propto \\text{ likelihood } \\cdot \\text{ prior } \\]\nIf, from our model, we determine that our posterior distribution is proportional to a known probability distribution, then our work is essentially complete. We have captured the entire shape of the posterior distribution, and the missing constant simply serves to scale it so that it becomes a valid probability distribution, with the integral under its density curve equal to one.\n\n\n22.6.1 Motivation Example\n\n\n\nMichelle has decided to run for governor of Wisconsin.\n\n\nAccording to previous 30 polls,\n\nMichelle‚Äôs support is centered round 45%\nshe polled at around 35% in the dreariest days and around 55% in the best days\n\n\nWith this prior information, we‚Äôd like to estimate/update Michelle‚Äôs support by conducting a new poll.\n\n\n\n\n\n\n\n\n\n\n\n\nKey: Describe prior and data information using probabilistic models.\n\nThe parameter to be estimated is \\(\\theta\\), the Michelle‚Äôs support, which is between 0 and 1.\n\n\n22.6.1.1 Prior Distribution\n\nA popular probability distribution for probability is beta distribution, \\(\\text{beta}(\\alpha, \\beta)\\), where \\(\\alpha &gt; 0\\) and \\(\\beta &gt; 0\\) are shape parameters.\n\n\\[\\pi(\\theta \\mid \\alpha, \\beta) = \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}\\theta^{\\alpha - 1}(1-\\theta)^{\\beta-1}\\]\n\n\n\n\n\n\n\n\n\nIn a Bayesian model, we write \\(\\theta \\sim \\text{beta}(\\alpha, \\beta)\\) or \\(\\pi(\\theta) = \\text{beta}(\\alpha, \\beta)\\).\nIn the prior model, \\(\\alpha\\) and \\(\\beta\\) are hyperparameters to be chosen to reflect our prior information.\n\nHere is what we know before we conduct a new survey.\n\nMichelle‚Äôs support is centered round 45%, and she polled at around 35% in the dreariest days and around 55% in the best days.\n\n\n\nWe can choose \\(\\alpha\\) and \\(\\beta\\) so that the prior mean is about 0.45 and the range is from 0.35 to 0.55 using the fact that\n\n\\(\\E(\\theta) = \\frac{\\alpha}{\\alpha + \\beta}\\)\n\\(\\Var(\\theta) = \\frac{\\alpha\\beta}{(\\alpha+\\beta)^2(\\alpha+\\beta+1)}\\)\n\n\n\nWe decide to use \\(\\text{beta}(45, 55)\\) as our prior distribution illustrating our prior belief and uncertainty about the Michelle‚Äôs support rate. The distribution is shown below.\n\n\n\n\n\n\n\n\nThe process of choosing the optimal or reasonable hyperparameter values is called tuning or hyperparameter optimization.\n\n22.6.1.2 Likelihood\n\nYou plan to conduct a new poll of \\(n = 50\\) Cheeseheads and record \\(Y\\), the number that support Michelle.\n\n\nWhat distribution can be used for modeling likelihood connecting the data \\(y\\) and the parameter we are interested, \\(\\theta\\)?\n\n\nIf voters answer the poll independently, and the probability that any polled voter supports Michelle is \\(\\theta\\), we could consider\n\n\\[Y \\mid \\theta \\sim \\text{binomial}(n=50, \\theta)\\]\n\nThe poll result is \\(y = 30\\), the likelihood is\n\n\\[L(\\theta \\mid y = 30) = {50 \\choose 30}\\theta^{30}(1-\\theta)^{20}, \\quad \\theta \\in (0, 1)\\]\nThe likelihood function is shown below.\n\n\n\n\n\n\n\n\nThe likelihood tells us that if \\(\\text{binomial}(n=50, \\theta)\\) is the data generating mechanism, the data \\(y = 30\\) is most compatible with the binomial mechanism when \\(\\theta\\) is 0.6. In other words, \\(\\text{binomial}(n=50, 0.6)\\) is the model that most likely generate the data \\(y = 30\\).\nAt this point, you might be wondering why we use the beta distribution to describe our prior belief and why we consider the thought experiment as a binomial experiment. These are indeed valid questions. Typically, we choose well-known and popular distributions or functions for our prior and likelihood because they allow us to derive the posterior distribution with minimal computational effort. You could argue that more sophisticated distributions or functions might better capture our belief and the specifics of the thought experiment. However, it‚Äôs important to remember that ‚Äútruth is much too complicated to allow anything but approximations,‚Äù and ‚Äúall models are wrong, but some are useful.‚Äù In this context, using the beta distribution and binomial likelihood proves to be particularly useful. It simplifies the tuning of hyperparameters in the prior distribution, and the posterior can be easily obtained. Moreover, as we will see, the posterior distribution remains within the beta distribution family, which greatly aids in interpreting how we update our knowledge.\n\n22.6.1.3 Bayesian model and posterior distribution\nOnce we decide the prior distribution and likelihood, a Bayesian model is specified:\n\\[\\begin{align}Y \\mid \\theta &\\sim \\text{binomial}(n=50, \\theta)\\\\ \\theta &\\sim \\text{beta}(45, 55)\n\\end{align}\\]\nThen our goal is to obtain the posterior distribution \\(\\pi(\\theta \\mid y)\\). Here we don‚Äôt need any statistical algorithm, and we can derive the posterior distribution exactly through mathematical calculation.\n\\[\n\\begin{align} \\pi(\\theta \\mid y) &\\propto_{\\theta} L(\\theta \\mid y)\\pi(\\theta) \\\\\n&= {50 \\choose 30}\\theta^{30}(1-\\theta)^{20} \\times \\frac{\\Gamma(100)}{\\Gamma(45)\\Gamma(55)}\\theta^{44}(1-\\theta)^{54}\\\\\n&\\propto_{\\theta} \\theta^{74}(1-\\theta)^{74}\\\\\n&= \\frac{\\Gamma(150)}{\\Gamma(75)\\Gamma(75)} \\theta^{74}(1-\\theta)^{74} \\\\\n&= \\text{beta}(75, 75)\\end{align}\n\\]\nusing the fact that \\(\\int_{\\mathcal{X}} f(x) dx = 1\\) for any pdf \\(f(x)\\).\nIn the calculation, we first write down the specific expression of the prior distribution and likelihood. Then we drop all constant terms with respect to \\(\\theta\\), for example, \\({50 \\choose 30}\\) and \\(\\frac{\\Gamma(100)}{\\Gamma(45)\\Gamma(55)}\\). These terms have nothing to do with the shape of the posterior distribution.\nThe interesting part is that the terms related to \\(\\theta\\) from the prior and likelihood can be combined together, ending up with a single term \\(\\theta^{74}(1-\\theta)^{74}\\). Take a closer look at this term. First, it can be written as \\(\\theta^{75 - 1}(1-\\theta)^{75-1}\\). Then this term is actually the term related to \\(\\theta\\) in the beta distribution when \\(\\alpha = 75\\) and \\(\\beta = 75\\), which is called the kernel of the probability distribution. Therefore, through the calculation, we actually capture the kernel of the distribution \\(\\text{beta}(75, 75)\\). We are done. When we get the kernel of some probability distribution in the calculation of the posterior distribution, the posterior distribution will be that probability distribution. Therefore, the posterior distribution \\(\\pi(\\theta \\mid y)\\) is \\(\\frac{\\Gamma(150)}{\\Gamma(75)\\Gamma(75)} \\theta^{74}(1-\\theta)^{74}\\), the \\(\\text{beta}(75, 75)\\) distribution. We don‚Äôt need to worry about how to calculate the normalizing constant \\(\\frac{\\Gamma(150)}{\\Gamma(75)\\Gamma(75)}\\). Once we get \\(\\theta^{74}(1-\\theta)^{74}\\), the only way we make the kernel become a valid probability distribution is to multiply by the constant (w.r.t \\(\\theta\\)) \\(\\frac{\\Gamma(150)}{\\Gamma(75)\\Gamma(75)}\\) that has been known from the beta distribution.\n\nDid you find that the prior and the posterior belong to the same family of probability distribution? They are both beta distributions. The prior is \\(\\pi(\\theta) = \\text{beta}(45, 55)\\) and the posterior is \\(\\pi(\\theta \\mid y) = \\text{beta}(75, 75)\\). When the prior and the posterior belong to the same family with respect to some likelihood, such prior is called a conjugate prior.\n\n22.6.1.4 Relationship between prior, likelihood and posterior\nFigure¬†22.3 illustrates the prior, likelihood, and posterior distributions in Michelle‚Äôs example. First, it‚Äôs important to note that the likelihood is not a probability distribution, so it has been scaled to resemble a density function for comparison. Let‚Äôs examine how our belief about Michelle‚Äôs support is updated. Initially, we believed her support was likely around 45%. However, after incorporating the new poll results, the model suggests that support around 60% offers the highest likelihood. This new evidence leads us to reconsider, suggesting that Michelle‚Äôs support may have increased.\nBayesians are not rigid in their thinking; we continuously update our knowledge and beliefs as new evidence emerges. The updated belief, reflected in the posterior distribution, effectively combines both prior information and new evidence. As a result, the posterior distribution typically falls between the prior and the likelihood. In this case, our belief about Michelle‚Äôs support is now highest within the range of 45% to 60%.\n\n\n\n\n\n\n\nFigure¬†22.3: Prior, likelihood and posterior of the motivation beta-binomial example.\n\n\n\n\nAdditionally, the variation in the posterior distribution is always smaller than that in the prior distribution. This occurs because the posterior distribution incorporates more information about Michelle‚Äôs support than the prior distribution does. The posterior distribution not only reflects the original prior information but also integrates the new data provided by the recent evidence. As a result, with more information about the parameter, we gain greater confidence in its likely value, leading to reduced uncertainty. Essentially, our belief becomes more refined and precise as we gather more evidence.\nThis process of belief updating can be understood more deeply through the mathematical relationship between the prior and posterior distributions. The parameters \\(\\alpha\\) and \\(\\beta\\) in the beta distribution are not just arbitrary numbers; they carry significant meaning. Specifically, the variance of the beta distribution decreases as\\(\\alpha\\) and \\(\\beta\\) increase. This explains why the posterior distribution \\(\\pi(\\theta \\mid y) = \\text{beta}(75, 75)\\) has a smaller variance than the prior distribution \\(\\pi(\\theta \\mid y) = \\text{beta}(45, 55)\\).\nIn this beta-binomial example, \\(\\alpha + \\beta = 45 + 55 = 100\\) an be interpreted as the ‚Äúprior sample size.‚Äù Out of these 100 hypothetical prior samples, 45 support Michelle. The new data consists of a sample size of 50, with 30 supporting Michelle. Now, observe what happens with the posterior. When we combine the prior sample with the new data sample, we have a total of 150 samples in the posterior analysis. Out of these 150 data points, \\(45 + 30 = 75\\) support Michelle, leading to the posterior distribution \\(\\pi(\\theta \\mid y) = \\text{beta}(75, 75)\\). The posterior sample size is now \\(75 + 75 = 150\\), with exactly 75 supporting Michelle.\nThrough this beta distribution, we can see that the parameters \\(\\alpha\\) and \\(\\beta\\) reflect the amount of information we have about the parameter \\(\\theta\\). This is how we systematically update our knowledge, with the posterior distribution representing a refined belief that incorporates both the prior information and the new data.\nIf each data point represents one piece of information, it‚Äôs clear that the posterior distribution now holds 150 pieces of information - 50 more than the prior distribution. Where do these additional 50 pieces come from? They come from the newly collected data. As we gather more information, the variability and uncertainty about the parameter decrease, leading to greater knowledge and confidence in the potential values of the parameter. The process of updating the posterior reflects this accumulation of information, resulting in a more precise and reliable estimate.\nour posterior belief can be viewed as a weighted average of the prior and the likelihood, with the weights determined by the amount of information each contributes. In this example, the prior carries 100 pieces of information, while the new data contributes 50 pieces. Consequently, when we update our belief, the posterior belief is influenced two-thirds by the prior and one-third by the new data, since \\(100/(100+50) = 2/3\\) and \\(50/(100+50) =1/3\\). This explains why the posterior mean support rate is \\(0.45 \\times (2/3) + 0.6 \\times 1/3 = 0.5\\). The posterior reflects a balanced combination of our prior knowledge and the new evidence, weighted by the information each provides. The Bayesians are logically consistent.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Bayesian Thinking and Inference</span>"
    ]
  },
  {
    "objectID": "infer-bayes.html#further-reading-and-references",
    "href": "infer-bayes.html#further-reading-and-references",
    "title": "22¬† Bayesian Thinking and Inference",
    "section": "\n22.7 Further Reading and References",
    "text": "22.7 Further Reading and References\n\nBayes Rules!\nAll About that Bayes: Probability, Statistics, and the Quest to Quantify Uncertainty by Dr.¬†Kristin Lennox\nThe Equation of Knowledge by Dr.¬†Le Nguyen Hoang",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>22</span>¬† <span class='chapter-title'>Bayesian Thinking and Inference</span>"
    ]
  },
  {
    "objectID": "infer-nonpar.html",
    "href": "infer-nonpar.html",
    "title": "23¬† Nonparametric Tests",
    "section": "",
    "text": "23.1 Wilcoxon Signed-Rank Test for Matched Pairs\nThe Wilcoxon sign-rank test is a nonparametric test that uses ranks for the following tests:\nTherefore, the Wilcoxon sign-rank test can be used for one or paired samples. When comparing matched pairs, the Wilcoxon sign-rank test is the nonparametric version of the paired \\(t\\)-test.\nHere we talk about the procedure for matched pairs, The same procedure is used for the one population case by creating matched pairs by pairing each sample value with the claimed median.\nThe requirements of the Wilcoxon sign-rank test are\nThere is no requirement that the data have a normal distribution.\nKeep in mind that instead of testing the mean of differences, the Wilcoxon signed-rank test tests the median of the differences (\\(med_d\\)).",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "infer-nonpar.html#wilcoxon-signed-rank-test-for-matched-pairs",
    "href": "infer-nonpar.html#wilcoxon-signed-rank-test-for-matched-pairs",
    "title": "23¬† Nonparametric Tests",
    "section": "",
    "text": "Testing the claim that a population of matched pairs has the property that the matched pairs have differences with a median equal to zero.\nTesting the claim that a single population of individual values has a median equal to some claimed value.\n\n\n\n\n\nThe data are (simple) random samples.\nThe population distribution of differences is approximately symmetric about the unknown median.\n\n\n\n\n\n\n23.1.1 Idea of Wilcoxon Signed-Rank Test\nThe hypothesis is\n\n\n\\(H_0: med_d = 0\\) (median of the difference = 0)\n\n\\(H_1: med_d &gt; 0\\) or \\(med_d &lt; 0\\) or \\(med_d \\ne 0\\)\n\n\nThe first thought is that If the median of the differences is 0, we expect to see about half of the differences \\(d_1, \\dots, d_n\\) is negative (\\(-\\)), and the other half of differences is positive (\\(+\\)). But this sign test uses only the information about direction (sign) of the data, and ignores the magnitude of the differences.\n\nThe following example demonstrates that relying solely on the signs of differences to determine whether the median is zero can be misleading. Consider the paired data presented on the left. As with the two-sample paired \\(t\\) test, we first calculate the differences between the paired observations. Next, we count the number of positive and negative differences. In this case, we have 3 positive differences \\((+1, +2, +3)\\) and 4 negative differences \\((-20, -18, -10, -14)\\). Since the number of positive signs is roughly equal to the number of negative signs, one might hastily conclude that the median of the differences is 0. However, this conclusion ignores the magnitude of the differences, which could lead to an incorrect inference.\nHowever, if we examine the difference data more closely, we notice that the positive differences have relatively small magnitudes, while the negative differences have much larger magnitudes. This observation suggests that the median of the differences is likely to be negative, contrary to what the simple count of positive and negative signs might indicate.\n\n\n\n\n\n\n\n\n\n\nAs a result, the core idea behind the Wilcoxon Signed-Rank Test is that if the median of the differences is indeed 0, not only should the number of positive differences be roughly equal to the number of negative differences, but also the rank sum of the positive differences should be close to the rank sum of the negative differences. This approach takes into account both the number and the magnitude of the differences, providing a more accurate assessment of whether the median is truly zero.\nThe figure below illustrates the concept behind the Wilcoxon Signed-Rank test. After calculating the difference data, we take the absolute values of these differences to determine their magnitudes. Next, we rank these magnitudes from smallest to largest, with the smallest magnitude receiving the first rank, the next smallest receiving the second rank, and so on.\nOnce the ranks are assigned, we calculate the rank sum. For the positive differences, the ranks are 1, 2, and 3, resulting in a rank sum of 6. For the negative differences, the ranks are 4, 5, 6, and 7, leading to a rank sum of 22. If the median of the differences were zero, these two rank sums should be fairly close to each other, ideally around half of the total rank sum, which is \\((1 + 2 + 3 + \\cdots + 7)/2 = 14\\).\nHowever, in this case, the larger rank sum for the negative differences suggests that the median is likely negative. Whether this difference in rank sums is significant enough to reject the null hypothesis depends on the test statistic, critical value, or p-value obtained through a formal testing procedure, which is typically handled by statistical software nowadays. The details are skipped here, and you can explore more from other introductory statistics textbooks.\n\n\n\n\n\n\n\n\n\n\n\nThe entire procedure of Wilcoxon signed-rank test when \\(n\\) is small (\\(&lt; 50\\)) is summarized below. \n\n\nThe hypothesis is\n\n\n\\(H_0: med_d = 0\\) (median of the difference = 0)\n\n\\(H_1: med_d &gt; 0\\) or \\(med_d &lt; 0\\) or \\(med_d \\ne 0\\)\n\n\n\nStep 1: Calculate the differences (\\(d_i\\)s) in the \\(n\\) pairs of observations.\nStep 2: Delete all zero values of \\(d_i\\).\nStep 3: Rank the absolute values (magnitude) of the differences.\nStep 4: Compute the rank sums for positive and negative differences, \\(T_+\\) and \\(T_{-}\\).\nStep 5: Let the test statistic \\(T_{test}\\) be the smaller of \\(T_+\\) and \\(T_{-}\\).\nStep 6: Given \\(\\alpha\\) and \\(n\\), find the critical value \\(T_{crit}\\). Or find the p-value.\nStep 7: Reject \\(H_0\\) if \\(T_{test} &lt; T_{crit}\\) or p-value &lt; \\(\\alpha\\).\n\nWhen \\(n\\) is relatively large, say larger than 50, we can transform \\(T_{test}\\) into a \\(z\\) test statistic, and follow the ordinary testing procedure using the standard normal distribution. The entire procedure is shown below.  \n\n\n\n\nStep 1: Calculate the differences (\\(d_i\\)s) in the \\(n\\) pairs of observations.\nStep 2: Delete all zero values of \\(d_i\\).\nStep 3: Rank the absolute values (magnitude) of the differences.\nStep 4: Compute the rank sums for positive and negative differences, \\(T_+\\) and \\(T_{-}\\).\nStep 5: Let \\(T\\) be the smaller of \\(T_+\\) and \\(T_{-}\\).\nStep 6: Compute the test statistic \\(z_{test} = \\frac{T - \\frac{n(n+1)}{4}}{\\sqrt{\\frac{n(n+1)(2n+1)}{24}}}\\).\nStep 7: Given \\(\\alpha\\), find the critical value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed).\n\nStep 8: Decision Rule\n\n\\(H_1: med_d &gt; 0\\): Reject \\(H_0\\) if  \\(z_{test} &lt; -z_{\\alpha}\\) .\n\\(H_1: med_d &lt; 0\\): Reject \\(H_0\\) if  \\(z_{test} &lt; -z_{\\alpha}\\) .\n\\(H_1: med_d \\ne 0\\): Reject \\(H_0\\) if  \\(z_{test} &lt; -z_{\\alpha/2}\\) .",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "infer-nonpar.html#example-of-wilcoxon-signed-rank-test-example-6.9-of-smd",
    "href": "infer-nonpar.html#example-of-wilcoxon-signed-rank-test-example-6.9-of-smd",
    "title": "23¬† Nonparametric Tests",
    "section": "\n23.2 Example of Wilcoxon Signed-Rank Test (Example 6.9 of SMD)",
    "text": "23.2 Example of Wilcoxon Signed-Rank Test (Example 6.9 of SMD)\nA city park department compared a new formulation of a fertilizer, brand A, to the previously used fertilizer, brand B, on each of 20 different softball fields. Each field was divided in half, with brand A randomly assigned to one half of the field and brand B to the other. Sixty pounds of fertilizer per acre were then applied to the fields. The effect of the fertilizer on the grass grown at each field was measured by the weight (in pounds) of grass clippings produced by mowing the grass at the fields over a 1-month period. Evaluate whether brand A tends to produce more grass than brand B.\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nWe will perform the Wilcoxon signed-rank test using R. First we prepare the data set paired_data in R like this:\n\n\n   Field     a     b difference\n1      1 211.4 186.3       25.1\n2      2 204.4 205.7       -1.3\n3      3 202.0 184.4       17.6\n4      4 201.9 203.6       -1.7\n5      5 202.4 180.4       22.0\n6      6 202.0 202.0        0.0\n7      7 202.4 181.5       20.9\n8      8 207.1 186.7       20.4\n9      9 203.6 205.7       -2.1\n10    10 216.0 189.1       26.9\n11    11 208.9 183.6       25.3\n12    12 208.7 188.7       20.0\n13    13 213.8 188.6       25.2\n14    14 201.6 204.2       -2.6\n15    15 201.8 181.6       20.2\n16    16 200.3 208.7       -8.4\n17    17 201.8 181.5       20.3\n18    18 201.5 208.7       -7.2\n19    19 212.1 186.8       25.3\n20    20 203.4 182.9       20.5\n\n\n\n\nThe command wilcox.test() can perform the Wilcoxon signed-rank test. The arguments x and y are the paired data from Brand A and Brand B, respectively. By default, paired = FALSE. So we have to set paired = TRUE because we have matched pairs data. We set alternative = \"greater\" because we would like to know whether brand A (in x) tends to produce more grass than brand B (in y). Here mu is actually referred to median, not the mean.\nBy default, when \\(n &lt; 50\\) and the difference data have no zero values, the exact p-value and confidence interval are constructed, i.e., the first procedure before is used. When \\(n &gt; 50\\), wilcox.test() will compute the p-value and confidence interval based on normal approximation, like the second procedure shown before. When normal approximation is used, correct argument is be further specified to decide whether continuity correction is used.\n\n\n\n\n\n\nWarning\n\n\n\nThe x and y data should be consistent with alternative. If we put paired_data$b in the x argument, and paired_data$a in the y argument, then alternative = \"less\" should be set. The order matters.\n\n\nOur dataset consists of 20 observations, but it‚Äôs important to note that we have a tie in the sixth observation where brand A and brand B have the same measurements. If you do not specify exact=FALSE in the function, R will issue a warning and automatically use the normal approximation with continuity correction. The following two code snippets will produce the same result.\n\nwilcox.test(x = paired_data$a, y = paired_data$b, mu = 0, \n            alternative = \"greater\", paired = TRUE, conf.int = TRUE)\n\nWarning in wilcox.test.default(x = paired_data$a, y = paired_data$b, mu = 0, :\ncannot compute exact p-value with zeroes\n\n\nWarning in wilcox.test.default(x = paired_data$a, y = paired_data$b, mu = 0, :\ncannot compute exact confidence interval with zeroes\n\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  paired_data$a and paired_data$b\nV = 169, p-value = 0.001549\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 9.049985      Inf\nsample estimates:\n(pseudo)median \n      12.12435 \n\n\n\nwilcox.test(x = paired_data$a, y = paired_data$b, mu = 0, \n            alternative = \"greater\", paired = TRUE, conf.int = TRUE,\n            exact = FALSE, correct = TRUE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  paired_data$a and paired_data$b\nV = 169, p-value = 0.001549\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 9.049985      Inf\nsample estimates:\n(pseudo)median \n      12.12435 \n\n\nIf we remove the sixth observation, the exact p-value and confidence interval can be constructed. In the following examples, the first two code chunks will yield the same result, both utilizing the exact p-value. However, the third code chunk, with exact = FALSE, will use the normal approximation to calculate the p-value instead.\n\npaired_data &lt;- paired_data[-6, ]\nwilcox.test(x = paired_data$a, y = paired_data$b, mu = 0, \n            alternative = \"greater\", paired = TRUE, conf.int = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  paired_data$a and paired_data$b\nV = 169, p-value = 0.0008469\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 9.05  Inf\nsample estimates:\n(pseudo)median \n        12.075 \n\n\n\nwilcox.test(x = paired_data$a, y = paired_data$b, mu = 0, \n            alternative = \"greater\", paired = TRUE, conf.int = TRUE,\n            exact = TRUE)\n\n\n    Wilcoxon signed rank exact test\n\ndata:  paired_data$a and paired_data$b\nV = 169, p-value = 0.0008469\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 9.05  Inf\nsample estimates:\n(pseudo)median \n        12.075 \n\n\n\nwilcox.test(x = paired_data$a, y = paired_data$b, mu = 0, \n            alternative = \"greater\", paired = TRUE, conf.int = TRUE,\n            exact = FALSE)\n\n\n    Wilcoxon signed rank test with continuity correction\n\ndata:  paired_data$a and paired_data$b\nV = 169, p-value = 0.001549\nalternative hypothesis: true location shift is greater than 0\n95 percent confidence interval:\n 9.049985      Inf\nsample estimates:\n(pseudo)median \n      12.12435 \n\n\nAs the sample size increases, the exact p-value and confidence interval will become increasingly similar to those obtained using the normal approximation.\n\n\nWe will perform the Wilcoxon signed-rank test using Python. First we prepare the data set paired_data in Python like this:\n\nimport pandas as pd\npaired_data = pd.read_csv(\"./data/paired_data_nonpar.csv\")\npaired_data['difference'] = paired_data['a'] - paired_data['b']\npaired_data\n\n    Field      a      b  difference\n0       1  211.4  186.3        25.1\n1       2  204.4  205.7        -1.3\n2       3  202.0  184.4        17.6\n3       4  201.9  203.6        -1.7\n4       5  202.4  180.4        22.0\n5       6  202.0  202.0         0.0\n6       7  202.4  181.5        20.9\n7       8  207.1  186.7        20.4\n8       9  203.6  205.7        -2.1\n9      10  216.0  189.1        26.9\n10     11  208.9  183.6        25.3\n11     12  208.7  188.7        20.0\n12     13  213.8  188.6        25.2\n13     14  201.6  204.2        -2.6\n14     15  201.8  181.6        20.2\n15     16  200.3  208.7        -8.4\n16     17  201.8  181.5        20.3\n17     18  201.5  208.7        -7.2\n18     19  212.1  186.8        25.3\n19     20  203.4  182.9        20.5\n\n\nThe command wilcoxon() in scipy.stats can perform the Wilcoxon signed-rank test. The arguments x and y are the paired data from Brand A and Brand B, respectively. We set alternative = \"greater\" because we would like to know whether brand A (in x) tends to produce more grass than brand B (in y).\n\nfrom scipy.stats import wilcoxon\n\n\n\n\n\n\n\nWarning\n\n\n\nThe x and y data should be consistent with alternative. If we put paired_data$b in the x argument, and paired_data$a in the y argument, then alternative = \"less\" should be set. The order matters.\n\n\nWhen \\(n\\) is sufficiently large, the null distribution of the normalized test statistic is approximately normal, and method = 'approx' can be used to compute the p-value. When \\(n\\) is small, the normal approximation may not be accurate, and method='exact' is preferred. The default method='auto', selects between the two: when \\(n &lt;= 50\\) and there are no zeros, the exact method is used; otherwise, the approximate method is used.\nOur dataset consists of 20 observations, but it‚Äôs important to note that we have a tie in the sixth observation where brand A and brand B have the same measurements. If you do not set method='approx' in the function, Python will issue a warning and automatically use the normal approximation with or without continuity correction based on the argument correction. By default, correction=False.\n\n# by default method='auto' which is 'exact' in our case\nwilcoxon(x=paired_data['a'], y=paired_data['b'], \n         alternative='greater', method='auto', correction=False)\n\n/Users/chenghanyu/.virtualenvs/r-reticulate/lib/python3.12/site-packages/scipy/stats/_axis_nan_policy.py:531: UserWarning: Exact p-value calculation does not work if there are zeros. Switching to normal approximation.\n  res = hypotest_fun_out(*samples, **kwds)\nWilcoxonResult(statistic=169.0, pvalue=0.0014510517244235824)\n\n\n\n# method='auto' which is 'exact'. correction=True\nwilcoxon(x=paired_data['a'], y=paired_data['b'], \n         alternative='greater', method='auto', correction=True)\n\nWilcoxonResult(statistic=169.0, pvalue=0.0015492135924460434)\n\n\n\nwilcoxon(x=paired_data['a'], y=paired_data['b'], \n         alternative='greater', correction=True, method='approx')\n\nWilcoxonResult(statistic=169.0, pvalue=0.0015492135924460434)\n\n\nIf we remove the sixth observation, the exact p-value and confidence interval can be constructed. In the following examples, the first two code chunks will yield the same result, both utilizing the exact p-value. However, the third code chunk, with method='approx', will use the normal approximation to calculate the p-value instead.\n\n# Removing the 6th observation for paired test\npaired_data = paired_data.drop(5)\n\n\n## correction does not mattter \nwilcoxon(paired_data['a'], paired_data['b'], \n         alternative='greater', method='exact')\n\nWilcoxonResult(statistic=169.0, pvalue=0.00084686279296875)\n\n\n\n## correction does not mattter\nwilcoxon(paired_data['a'], paired_data['b'],\n         alternative='greater',method='auto')\n\nWilcoxonResult(statistic=169.0, pvalue=0.00084686279296875)\n\n\n\nwilcoxon(paired_data['a'], paired_data['b'], \n         alternative='greater', correction=True, method='approx')\n\nWilcoxonResult(statistic=169.0, pvalue=0.0015492135924460434)\n\n\n\nwilcoxon(paired_data['a'], paired_data['b'], \n         alternative='greater', correction=False, method='approx')\n\nWilcoxonResult(statistic=169.0, pvalue=0.0014510517244235824)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "infer-nonpar.html#wilcoxon-rank-sum-test-mann-whitney-u-test1-for-two-independent-samples",
    "href": "infer-nonpar.html#wilcoxon-rank-sum-test-mann-whitney-u-test1-for-two-independent-samples",
    "title": "23¬† Nonparametric Tests",
    "section": "\n23.3 Wilcoxon Rank-Sum Test (Mann-Whitney U Test)1 for Two Independent Samples",
    "text": "23.3 Wilcoxon Rank-Sum Test (Mann-Whitney U Test)1 for Two Independent Samples\nWilcoxon rank-sum test is the nonparametric version of the independent two-sample \\(t\\)-test. The only requirementof this method is that the two independent samples are random samples.2 The two populations can be non-normal and follow any distribution.\nLike Wilcoxon signed-rank test, instead of comparing two means, the Wilcoxon rank-sum test compares two medians (\\(med\\)). The figure below shows two non-normal distributions with different median values.\n\n\n\n\n\n\n\n\n\n\n23.3.1 Idea of Wilcoxon Rank-Sum Test\nThe Wilcoxon rank-sum test uses ranks of sample data from two independent populations to test the null hypothesis that two independent samples come from populations with equal medians. Symbolically we write our \\(H_0\\) and \\(H_1\\) as follows.\n\n\\(H_0: med_1 = med_2\\)\n\n\\(H_1: med_1 &gt; med_2\\) or \\(med_1 &lt; med_2\\) or \\(med_1 \\ne med_2\\)\n\n\nThe idea is that if the two samples are from the distributions with the same median value, then if we combine the two samples into one big sample, and all sample values are ranked, the high and low ranks should fall evenly between the two samples.\nCheck my hand-written example below. If the true medians are such that \\(med_1 \\ne med_2\\), the two distributions (green and yellow) will generally not be closely located.. Suppose we have 5 data values from each population, \\((x_1, \\dots, x_5)\\) for the green distribution, and \\((y_1, \\dots, y_5)\\) for the yellow distribution. We can combine the \\(x\\) and \\(y\\) samples, and rank all 10 value from smallest to largest. This ranking helps us compare the positions of the green and yellow sample values within the combined dataset, providing insight into the relationship between the two distributions.\nIn this example, all \\(x\\) values are smaller than the \\(y\\) values, so so the ranks 1st through 5th belong to the \\(x\\) sample, while the \\(y\\) values are ranked 6th through 10th. When we calculate the rank sum for each group, the \\(x\\) sample has rank sum of 15 and the \\(y\\) sample has rank sum of 40. These two rank sums are significantly different from each other.\nWhen the two distributions are far apart or their medians differ greatly, the sample values from one population will generally be smaller (or larger) than those from the other, leading to a large difference in rank sums. This difference in rank sums forms the basis for inferring whether the two medians are equal or not. By examining the gap between the two rank sums, we can draw conclusions about the relationship between the medians of the two populations.\n\n\n\n\n\n\n\n\n\nWhat if \\(med_1 = med_2\\)? Now consider the example below, where again we have two populations‚Äîgreen and yellow. Although their shapes differ, they share the same median value, and the two distributions are more or less aligned in the same position. As a result, the \\(x\\) and \\(y\\) samples generated from these two populations will have fairly similar values.\nWhen we combine the two samples and rank the data values from smallest to largest, the ranks are interspersed between the two samples. Specifically, the \\(x\\) sample has ranks 1, 3, 6, 8, and 10, while the \\(y\\) sample has ranks 2, 4, 5, 7, and 9. In this scenario, a data value from one population is just as likely to be greater than or smaller than a data value from the other population.\nWhen we calculate the rank sum for each group, the two rank sums will not differ significantly. The Wilcoxon rank-sum test uses a threshold to determine how large the difference between the two rank sums needs to be in order to conclude that there is sufficient evidence to claim the two medians are different. If the rank sums are close, as in this example, it suggests that the medians are likely equal. Conversely, a large gap in rank sums would indicate a difference in medians.\n\n\n\n\n\n\n\n\n\nThe procedures for conducting the Wilcoxon rank-sum test differ slightly depending on whether the sample size is small or large. For large samples, the normal approximation is used. While the specific details of the method can be complex, understanding the underlying concept is more important. In practice, the Wilcoxon rank-sum test is typically performed using statistical software, which handles the computations for both small and large sample scenarios.\n\n\nProcedure of Wilcoxon Rank-Sum Test When \\(\\small n_1 \\le 10\\) and \\(\\small n_2 \\le 10\\)\n\nStep 1: Combine the two samples into one big sample.\nStep 2: Find the test statistic, \\(T_{test}\\), the sum of the ranks for the observations from population 1.\nStep 3: Find the critical values, \\(T_U\\) and \\(T_L\\).\n\nStep 4: Decision Rule\n\n\\(H_1: med_1 &gt; med_2\\): Reject \\(H_0\\) if \\(T_{test} &gt; T_U\\)\n\\(H_1: med_1 &lt; med_2\\): Reject \\(H_0\\) if \\(T_{test} &lt; T_L\\)\n\\(H_1: med_1 \\ne med_2\\): Reject \\(H_0\\) if \\(T_{test} &lt; T_L\\) or \\(T_{test} &gt; T_U\\)\n\n\n\n\n\n\n\nProcedure of Wilcoxon rank-sum test When \\(\\small n_1 &gt; 10\\) and \\(\\small n_2 &gt; 10\\)\n\nStep 1: Combine the two samples into one big sample.\nStep 2: Find the test statistic, \\(z_{test} = \\frac{T - \\mu_T}{\\sigma_T}\\), where \\(R\\) is the sum of the ranks for the observations from population 1, \\(\\mu_T = \\frac{n_1(n_1+n_2+1)}{2}\\), \\(\\sigma_T = \\sqrt{\\frac{n_1n_2(n_1+n_2+1)}{12}}\\).\nStep 3: Find the critical value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed).\n\nStep 4: Decision Rule\n\n\\(H_1: med_1 &gt; med_2\\): Reject \\(H_0\\) if \\(z_{test} \\ge z_{\\alpha}\\)\n\\(H_1: med_1 &lt; med_2\\): Reject \\(H_0\\) if \\(z_{test} \\le -z_{\\alpha}\\)\n\\(H_1: med_1 \\ne med_2\\): Reject \\(H_0\\) if \\(|z_{test}| \\ge z_{\\alpha/2}\\)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "infer-nonpar.html#example-of-wilcoxon-rank-sum-test-example-6.5-of-smd",
    "href": "infer-nonpar.html#example-of-wilcoxon-rank-sum-test-example-6.5-of-smd",
    "title": "23¬† Nonparametric Tests",
    "section": "\n23.4 Example of Wilcoxon Rank-Sum Test (Example 6.5 of SMD)",
    "text": "23.4 Example of Wilcoxon Rank-Sum Test (Example 6.5 of SMD)\nMany states are considering lowering the blood-alcohol level at which a driver is designated as driving under the influence (DUI) of alcohol. An investigator for a legislative committee designed the following test to study the effect of alcohol on reaction time. Ten participants consumed a specified amount of alcohol. Another group of 10 participants consumed the same amount of a nonalcoholic drink, a placebo. The two groups did not know whether they were receiving alcohol or the placebo.\n\n\nR\nPython\n\n\n\nThe 20 participants‚Äô average reaction times (in seconds) to a series of simulated driving situations are saved in two_sample_data in R shown below. Does it appear that alcohol consumption increases reaction time?\n\n\n\n\n   Placebo Alcohol\n1     0.90    1.46\n2     0.37    1.45\n3     1.63    1.76\n4     0.83    1.44\n5     0.95    1.11\n6     0.78    3.07\n7     0.86    0.98\n8     0.61    1.27\n9     0.38    2.56\n10    1.97    1.32\n\n\n\n\nTo perform the Wilcoxon rank-sum test, we again use the function wilcox.test(). This time paired = FALSE should be used because we are dealing with two independent samples. In this example, the sample size is 10, so we can consider using the small-size procedure. In R, we set exact = TRUE.\n\nwilcox.test(x = two_sample_data$Placebo, y = two_sample_data$Alcohol, mu = 0, \n            alternative = \"less\", paired = FALSE, conf.int = TRUE, \n            exact = TRUE)\n\n\n    Wilcoxon rank sum exact test\n\ndata:  two_sample_data$Placebo and two_sample_data$Alcohol\nW = 15, p-value = 0.003421\nalternative hypothesis: true location shift is less than 0\n95 percent confidence interval:\n  -Inf -0.37\nsample estimates:\ndifference in location \n                 -0.61 \n\n\nWe could still use the normal approximation that is mainly for studies with a large sample size, like the the following code chunk is doing. The two results are different, and since the sample size is small, the first approach is preferred.\n\nwilcox.test(x = two_sample_data$Placebo, y = two_sample_data$Alcohol, mu = 0, \n            alternative = \"less\", paired = FALSE, conf.int = TRUE, \n            exact = FALSE)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  two_sample_data$Placebo and two_sample_data$Alcohol\nW = 15, p-value = 0.004554\nalternative hypothesis: true location shift is less than 0\n95 percent confidence interval:\n       -Inf -0.3699702\nsample estimates:\ndifference in location \n            -0.6100186 \n\n\n\n\nThe 20 participants‚Äô average reaction times (in seconds) to a series of simulated driving situations are saved in two_sample_data in Python shown below. Does it appear that alcohol consumption increases reaction time?\n\ntwo_sample_data = pd.read_csv(\"./data/two_sample_data_nonpar.csv\")\ntwo_sample_data\n\n   Placebo  Alcohol\n0     0.90     1.46\n1     0.37     1.45\n2     1.63     1.76\n3     0.83     1.44\n4     0.95     1.11\n5     0.78     3.07\n6     0.86     0.98\n7     0.61     1.27\n8     0.38     2.56\n9     1.97     1.32\n\n\nTo perform the Wilcoxon rank-sum test, we use the function mannwhitneyu() in scipy.stats. In this example, the sample size is 10, so we can consider using the small-size procedure. In Python, we set method='exact'. By default, the function use method='auto' which chooses 'exact' when the size of one of the samples is less than or equal to 8 and there are no ties, and chooses the method 'asymptotic' otherwise.\n\n# Mann-Whitney U test (two-sample)\nfrom scipy.stats import mannwhitneyu\nmannwhitneyu(x=two_sample_data['Placebo'], \n             y=two_sample_data['Alcohol'],\n             alternative='less',\n             method='exact')\n\nMannwhitneyuResult(statistic=15.0, pvalue=0.0034207278789322136)\n\n\nWe could still use the normal approximation that is mainly for studies with a large sample size, like the the following code chunk is doing. The two results are different, and since the sample size is small, the exact approach is preferred.\n\nmannwhitneyu(x=two_sample_data['Placebo'], \n             y=two_sample_data['Alcohol'],\n             alternative='less',\n             method='asymptotic',\n             use_continuity=True)\n\nMannwhitneyuResult(statistic=15.0, pvalue=0.004554248199015482)\n\n\n\nmannwhitneyu(x=two_sample_data['Placebo'], \n             y=two_sample_data['Alcohol'],\n             alternative='less',\n             method='asymptotic',\n             use_continuity=False)\n\nMannwhitneyuResult(statistic=15.0, pvalue=0.004075485796751346)",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "infer-nonpar.html#kruskal-wallis-test-for-three-or-more-samples",
    "href": "infer-nonpar.html#kruskal-wallis-test-for-three-or-more-samples",
    "title": "23¬† Nonparametric Tests",
    "section": "\n23.5 Kruskal-Wallis Test for Three or More Samples",
    "text": "23.5 Kruskal-Wallis Test for Three or More Samples\nKruskal-Wallis test is for comparing three or more samples. We discuss it in Chapter 24.",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "infer-nonpar.html#footnotes",
    "href": "infer-nonpar.html#footnotes",
    "title": "23¬† Nonparametric Tests",
    "section": "",
    "text": "Wilcoxon rank-sum test and Mann-Whitney U test are equivalent. Although the two methods construct a different test statistic, they both apply to the same situations and always lead to the same conclusions.‚Ü©Ô∏é\nWhen either one or both samples have 10 or fewer values, some extra steps are needed. We don‚Äôt need to worry about that at the moment.‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Inference",
      "<span class='chapter-number'>23</span>¬† <span class='chapter-title'>Nonparametric Tests</span>"
    ]
  },
  {
    "objectID": "model.html",
    "href": "model.html",
    "title": "Statistical Models",
    "section": "",
    "text": "The Statistical Inference Part discusses fundamental statistical inference methods, such as interval estimation and hypothesis testing. The methods can be classical/frequentist, Bayesian, parametric, nonparametric or resampling-based such as bootstrapping.\nIn this Part: Statistical Models, we focus on more sophisticated statistical methods including Analysis of Variance, Linear Regression, Bayesian Linear Regression, and Survival Analysis. Of course there are tons of other statistical models out there, but the models discussed in this book are basic but important ones because many models are based on those models, and improve their performance.\nA statistical model is a mathematical model that describes the data generating process which is the core of inferential statistics. The methods discussed the Inference Part can be viewed as a very basic statistical model. A sophisticated statistical model not only does the inference about unknown population parameters but also make prediction about future or unseen observations or data, the core of modern statistical machine learning.",
    "crumbs": [
      "Statistical Models"
    ]
  },
  {
    "objectID": "model-anova.html",
    "href": "model-anova.html",
    "title": "24¬† Analysis of Variance",
    "section": "",
    "text": "24.1 ANOVA Rationale\nOne-Way Analysis of Variance\nA factor (treatment) is a property or characteristic (categorical variable) that allows us to distinguish the different populations from one another. In fact, when we compare two population means, the two samples are from the populations separated by the categories of a factor. For example, we have gender as a factor that separates the human population into male and female populations, then we compare their mean salary. In our previous examples, type of device and treatment of trees are factors.\nOne-way ANOVA examines the effect of single categorical variable on the mean of a numerical variable. The ANOVA model is one-way because there is only one factor being considered. If two factors are examined to see how they affect the mean of a numerical variable, the model is called two-way ANOVA. The numerical variable in ANOVA or regression model (Chapter 27) is called the response variable because we want to see how the variable responses to the changes of factors or other variables.\nInterestingly, we analyze  variances  to test the equality of three or more population  means ü§î. It sounds counterintuitive, but later you will see why this makes sense.\nRequirements\nThe one-way ANONA model has the following requirements or assumptions.\nRationale\nThe section is quite important because we discuss in detail why we use variance size to test whether or not population means are equal.\nSuppose we have two data sets Data 1 and Data 2, and both have three groups to be compared. Interestingly, Data 1 and Data 2 have the same group sample means \\(\\bar{y}_1\\), \\(\\bar{y}_2\\) and \\(\\bar{y}_3\\) denoted as red dots in Figure¬†24.1. However, they differ with regards to the variance within each group. Data 1 has small variance within samples, while Data 2 has large variance within samples. Within groups, the data points in Data 1 are quite tight and close each other, and therefore they are all close to their sample mean. On the contrary, the data points in Data 2 are quite distant each other, even they are in the same group. Such distribution pattern tells us that the populations from which the samples of Data 1 are drawn have small variance \\(\\sigma^2\\).\nBefore we go into more details, let‚Äôs see if you have a great intuition.\nFigure¬†24.1: Boxplots illustrating the variance within samples.\nIf your answer is Data 1, congratulations you are correct!\nThe difference in sample means in Data 1 is more likely due to the true difference in population means (\\(\\mu_1, \\mu_2, \\mu_3\\) not all the same). Because of the small variation within groups, in Data 1 a value drawn from group 1 is very unlikely to be drawn from another group because with the normality assumption, the chance to be drawn from another group is very tiny. Figure¬†24.2 clearly illustrates this idea. The samples of Data 1 are so well-separated that we are more confident to say they are drawn from three well-separated populations that are not overlapped each other, and have distinct population means \\(\\mu_1, \\mu_2\\), and \\(\\mu_3\\).\nIf the variance within groups is large, as shown at the bottom of Figure¬†24.2, all three samples are mixed up together, as if they are all from the common population, even though they are in fact are from three distinct populations \\(N(\\mu_1, \\sigma^2), N(\\mu_2, \\sigma^2), N(\\mu_3, \\sigma^2)\\) and \\(\\mu_1 \\ne \\mu_2 \\ne \\mu_3\\). Note that the three populations still have their own distinct population mean which is actually identical to the mean with small variance, but it is hard for us to learn this fact from their mixed random samples. Because the three samples are indistinguishable, we don‚Äôt have strong evidence to conclude that their corresponding population has its own population mean, and tend to conclude that the three samples are from a common population \\(N(\\mu, \\sigma^2)\\).\nFigure¬†24.2: Populations with small and large variance within samples.\nThe three samples in Figure¬†24.3 are from a common population \\(N(\\mu, \\sigma^2)\\). The samples look pretty similar to the samples with large variance in Figure¬†24.2. In other words, either one common population or three populations with large variance can produce quite similar samples. With the samples only, we cannot tell which data generating mechanism is the true one generating such data.\nFigure¬†24.3: Three samples drawn from a common normal population.\nVariation Between Samples & Variation Within Samples\nThere are two types of variation we need to consider in order to determine whether population means are identical. They are variation between samples and variation within samples.\nWe have discussed the variation within samples or variance within groups, which measures the variability of data points in each group. This is actually the sample point estimate of the population variance \\(\\sigma^2\\) because the data points in the \\(i\\)-th group are assumed from the \\(i\\)-th population \\(N(\\mu_i, \\sigma^2).\\) There are \\(k\\) sample variance, one for each group. Later, we will learn how to combine them to get one single variance within samples as an estimate of \\(\\sigma^2\\).\nVariation between samples, on the other hand, measures variability of sample means. The farther away from each other the sample means are, the larger variation between samples. In our Data 1 and Data 2 example, their variation between samples are very close because the relative location of their sample means are basically the same. Figure¬†24.4 illustrating the variance between samples. Data 3 and 4 have the same variance within samples, but Data 3 have small variation between samples and Data 4 have large variation between samples. Clearly, the sample means in Data 4 are farther away from each other, comparing to Data 3.\nFigure¬†24.4: Boxplots illustrating the variance between samples.\nLet me ask you the same question for Data 3 and 4.\nYour answer should be Data 4. When the sample means \\(\\bar{y}_1\\), \\(\\bar{y}_2\\) and \\(\\bar{y}_3\\) are far away from each other, and they serve as the unbiased point estimate of \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\), respectively, we tend to claim that \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not identical.\nWe have an important finding. Whether or not there is a difference in population means depends on the relative size of variation between samples and variation within samples. When variability between samples is large in comparison to the variation within samples, like Data 1 and Data 4, we tend to conclude that the population means are not all the same. When variation between samples is small relatively to the variation within samples, like Data 2 and Data 3, it‚Äôs hard to exclude the possibility that all samples comes from the same population.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "model-anova.html#anova-rationale",
    "href": "model-anova.html#anova-rationale",
    "title": "24¬† Analysis of Variance",
    "section": "",
    "text": "Each sub-population formed by the category of the factor is normally distributed. In the two-sample \\(z\\) or \\(t\\) test, we also have such assumption that the two samples are drawn independently from two normally distributed populations or at least both sample sizes are sufficiently large.\nThe populations have the same variance \\(\\sigma^2\\). ANOVA does not ridiculously assume all the population variances are known. Instead, the model admits the variances are unknown, but assumes they are all equal. Also sounds ridiculous? Anyway, it is what it is, and this is one of the limitations of ANOVA, although equality of variances is a pretty loose requirement. It is also possible to transform our data, so that the transformed samples have similar magnitude of variance. Does this assumption remind you of something? The two-sample pooled \\(t\\)-test has the equality of variance assumption too. In this point of view, one-way ANOVA is a generalization of the two-sample pooled \\(t\\)-test. The next two requirements will be not surprising at all because they are also requirements of the two-sample pooled \\(t\\)-test.\nThe samples are random samples. Without mentioned explicitly, the inference methods and statistical models introduced in the book are based on random samples.\nThe samples are independent of each other. They are not matched or paired in any way.\n\n\n\n\n\n\n\nData and Math Notation for ANOVA\n\n\n\n\n\n\n\nSource: Table 8.5 of SMD.\n\n\n\n\n\\(y_{ij}\\): \\(j\\)-th observation from population \\(i\\). \\(\\color{blue}{(y_{24}:\\text{ 4-th observation from population 2})}\\)\n\\(n_i\\): number of values in the \\(i\\)-th group/sample. (\\(i = 1, \\dots, k = 5\\)) \\(\\color{blue}{\\small (n_1 = n_2 = \\dots = n_5 = 4)}\\)\n\\(\\bar{y}_{i\\cdot}\\): the average of values in the \\(i\\)-th sample, \\(\\bar{y}_{i\\cdot} = \\frac{\\sum_{j=1}^{n_i}y_{ij}}{n_i}.\\)\n\\(N\\): total number of values in all samples combined, \\(\\small N = n_1 + \\dots + n_k = \\sum_{i=1}^kn_i\\). \\(\\color{blue}{\\small (N = 4 + \\dots + 4 = 20)}\\)\n\\(\\bar{y}_{\\cdot\\cdot}\\): the grand sample mean that is the average of all sample values combined, \\(\\bar{y}_{\\cdot\\cdot} = \\frac{\\sum_{i=1}^{k}\\sum_{j=1}^{n_i}y_{ij}}{N} = \\frac{\\sum_{i=1}^{k}n_i\\bar{y}_{i\\cdot}}{n_1 + \\dots + n_t}\\)\n\nMathematically, suppose there are \\(k\\) populations/samples, and for group \\(i =1, \\dots, k\\), \\(n\\) observations are drawn. Then\n\\[y_{ij} \\stackrel{iid}{\\sim} N(\\mu_i, \\sigma^2), ~~ j = 1, 2, \\dots, n.\\]\nOur goal is to test whether or not \\(\\mu_1 = \\mu_2 = \\cdots = \\mu_k.\\) If we do not reject \\(H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k,\\) then all samples are viewed as samples from a common normal distribution, i.e., \\(N(\\mu_i, \\sigma^2)\\) and \\(\\mu = \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\).\n\\[y_{ij} \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2), ~~ j = 1, 2, \\dots, n.\\]\n\n\n\n\n\n\n\n\n\n\n\n\nFor which data do you feel more confident in saying the population means \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not all the same?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlways keep in mind that all we have are samples, and we never know the true means \\(\\mu_1\\), \\(\\mu_2\\), and \\(\\mu_3\\). In the figures, we assume we know the true populations, and see what the samples look like. Statistical inference is trickier. We want to have a decision rule, so that we know how much the samples are well separated is enough to say they are from three different populations.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor which data do you feel more confident in saying the population means \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not all the same?",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "model-anova.html#anova-procedures",
    "href": "model-anova.html#anova-procedures",
    "title": "24¬† Analysis of Variance",
    "section": "\n24.2 ANOVA Procedures",
    "text": "24.2 ANOVA Procedures\nANOVA is usually done by providing the ANOVA table below.\n\n\n\n\n\n\n\n\nIn this section, we are going to learn the meaning of every cell in the table, and how to use the table to do the testing of equality of population means.\n\nThe hypotheses is  \\[\\begin{align} &H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\\\  &H_1: \\text{Population means are not all equal} \\end{align}\\] \nNote that the alternative hypothesis is not \\(H_1: \\mu_1 \\ne \\mu_2 \\ne \\cdots \\ne \\mu_k\\). This is just one scenario where \\(H_0\\) is not satisfied. Any \\(\\mu_i \\ne \\mu_j , i \\ne j\\) violates \\(H_0\\), and should be a possibility of \\(H_1\\).\n\nWe learned that whether or not there is a difference in population means depends on the relative size of variation between samples and variation within samples. Statistician Ronald Fisher found a way to define a variable which is the ratio of variance between samples to variance within samples, and the variable follows the \\(F\\) distribution: \\[\\frac{\\text{variance between samples}}{\\text{variance within samples}} \\sim F_{df_B,\\, df_W}\\] The degrees of freedom \\(df_B\\) is paired with variance between samples and \\(df_W\\) is for variance within samples. Be careful the order matters.\nANONA uses F test. If the variance between samples is much larger than the variance within samples, then the \\(F\\) test statistic \\(F_{test}\\) will be much greater than 1, which may be over the \\(F\\) critical value, and \\(H_0\\) is rejected.\nThe key question is how variance between samples and variance within samples are defined so that the ratio is \\(F\\) distributed.\n\n\n\n\n\n Variance Within Samples \nOne-way ANOVA is a generalization of the two-sample pooled \\(t\\)-test. In the two-sample pooled \\(t\\)-test with equal variance \\(\\sigma^2\\), we have the pooled sample variance \\[s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\\]\n\n\n\nHow about the pooled sample variance for \\(k\\) samples? ANOVA assumes the populations have the same variance such that \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2 = \\sigma^2\\). With the same logic, we can have the pooled sample variance from \\(k\\) samples \\[\\boxed{s_W^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2}{n_1 + n_2 + \\cdots + n_k - k}}\\] where \\(s_i^2\\), \\(i = 1, \\dots ,k\\), is the sample variance of \\(i\\)-th group. \\(s_W^2\\) represents a combined estimate of the common variance \\(\\sigma^2\\). It measures variability of the observations within the \\(k\\) populations. Note that each \\(s_i^2\\) measures the variability within the \\(i\\)-th sample. So this pooled estimate is the variance within samples.\n\n\n\n\n\n\n\n\n\n Variance Between Samples \nThe variance between samples measures variability among sample means for the \\(k\\) groups, which is defined as \\[\\boxed{s^2_{B} = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^2}{k-1}}\\] where\n\n\n\\(\\bar{y}_{i\\cdot}\\) is the \\(i\\)-th sample mean.\n\n\\(\\bar{y}_{\\cdot\\cdot}\\) is the grand sample mean with all data points in all groups combined.\n\n\\(n_i\\) is the sample size of the \\(i\\)-th sample.\n\nThe variance between samples measures the magnitude of how large is the deviation from group means to the overall grand mean. When \\(\\bar{y}_{i\\cdot}\\)s are away from each other, \\((\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^2\\) will be large, so is \\(s^2_{B}\\).\n\n\n\n\n\n\n\n\nIn fact, not only \\(s^2_{W}\\) estimates \\(\\sigma^2\\), \\(s^2_{B}\\) is also an estimate of \\(\\sigma^2\\). If \\(H_0\\) is true \\((\\mu_1 = \\cdots = \\mu_k = \\mu)\\), any variation in the sample means is due to chance and randomness, so it shouldn‚Äôt be too large. \\(\\bar{y}_{1\\cdot}, \\cdots, \\bar{y}_{k\\cdot}\\) should be close each other and should be close to \\(\\bar{y}_{\\cdot \\cdot}\\). This leads to a small \\(s^2_{B}\\) and small ratio \\(s^2_{B}/s^2_{W}\\) that is our \\(F\\) test statistic. That‚Äôs why when \\(\\mu_1 = \\cdots = \\mu_k = \\mu\\), we have small \\(F\\) test statistic and tend to not reject \\(H_0\\).\n\n ANOVA Table: Sum of Squares \nIn the ANOVA table, there is a column Sum of Squares. There are three types of sum of squares. Let‚Äôs learn what they are.\nTotal Sum of Squares (SST) measures the total variation around \\(\\bar{y}_{\\cdot\\cdot}\\) in all of the sample data combined (ignoring the groups), which is defined as: \\[\\color{blue}{SST = \\sum_{j=1}^{n_i}\\sum_{i=1}^{k} \\left(y_{ij} - \\bar{y}_{\\cdot\\cdot}\\right)^2}\\] where \\(y_{ij}\\) is the \\(j\\)-th data point in the \\(i\\)-th group.\nSum of Squares Between Samples (SSB) measures the variation between sample means: \\[\\color{blue}{SSB = \\sum_{i=1}^{k}n_i \\left(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}\\right)^2}\\]\nSum of Squares Within Samples (SSW) measures the variation of any value, \\(y_{ij}\\), about its sample mean, \\(\\bar{y}_{i\\cdot}\\): \\[\\color{blue}{SSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\left(y_{ij} - \\bar{y}_{i\\cdot}\\right)^2 = \\sum_{i=1}^{k} (n_i - 1)s_i^2}\\]\n Sum of Squares Identity \nThe three sum of squares are related by the identity \\[SST = SSB + SSW.\\]\nIntuitively, for data points \\(y_{ij}\\), their squared distance from the grand sample mean \\(\\bar{y}_{\\cdot\\cdot}\\) can be decomposed into two parts: (1) the squared distance between their own group sample mean and the grand sample mean, and (2) the their squared distance from their own group sample mean.\nNote that the sum of squares statistics have associated degrees of freedom. More interestingly, the three degrees of freedom also form an identity. So\n\\[df_{T} = df_{B} + df_{W}\\] where\n\n\n\\(df_{T} = N-1\\) is the degrees of freedom of \\(SST\\)\n\n\n\\(df_{B} = k - 1\\) is the degrees of freedom of \\(SSB\\)\n\n\n\\(df_{W} = N - k\\) is the degrees of freedom of \\(SSW\\).\n\nWhen a sum of squares divided by its degrees of freedom, we get its mean square (MS), i.e., \\[\\text{mean square} = \\dfrac{\\text{sum of squares}}{\\text{degrees of freedom}}.\\] We are particularly interested in the mean square between (MSB) and mean square within (MSW):\n\n\\(MSB = \\frac{SSB}{k-1} = s^2_{B}\\)\n\\(MSW = \\frac{SSW}{N-k} = s^2_{W}\\)\n\nPlease check the formula of \\(SSB\\) and \\(SSW\\). You will find that \\(MSB\\) is our variance between samples and \\(MSW\\) is our variance within samples! So our \\(F\\) test statistic is \\[F_{test} = \\frac{MSB}{MSW}.\\]\nUnder \\(H_0\\), \\(s^2_{B}/s_W^2\\) is a statistic from \\(F_{k-1, \\, N-k}\\) distribution. The first degrees of freedom is \\(df_{B} = k - 1\\), and the second is \\(df_{W} = N - k\\). They cannot be switched.\nWe reject \\(H_0\\) in favor of \\(H_1\\) if \\(F_{test} &gt; F_{\\alpha, \\, k - 1,\\, N-k}\\), or \\(p\\)-value \\(P(F_{k - 1,\\, N-k} &gt; F_{test}) &lt; \\alpha\\) for some significance level \\(\\alpha\\).\n ANOVA Table \nWe are done! We‚Äôve talked about every cell in the ANOVA table, and we can use the table to do the test and make a conclusion about the equality of population means.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "model-anova.html#anova-example",
    "href": "model-anova.html#anova-example",
    "title": "24¬† Analysis of Variance",
    "section": "\n24.3 ANOVA Example",
    "text": "24.3 ANOVA Example\nWe hypothesize that a nutrient called ‚Äúisoflavones‚Äù varies among three types of food: (1) cereals and snacks, (2) energy bars and (3) veggie burgers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sample of 5 is taken from each type of food and the amount of isoflavones is measured. Is there a sufficient evidence to conclude that the mean isoflavone levels vary among these food items at \\(\\alpha = 0.05\\)?\n\n\n\nR\nPython\n\n\n\nWe are going to learn to generate a ANOVA table using R.\n Data \nIn order to use the R built-in function for ANOVA, we get to make sure the data matrix is in the right format. The original data set we get may be something like object data below, where each column represents the five isoflavones measurements of a food type. It is not a ‚Äúwrong‚Äù data format, but just not what we need for doing ANOVA. The data frame data_anova is the data format needed for ANOVA. There are two columns, our response variable isoflavones measurement labelled y, and the factor or categorical variable that may affect the response value, which is food type labelled food. With this format, there will be totally \\(N=15\\) observations and rows. Please transform any data into this kind of data format before you do ANOVA.\n\n\n\n\ndata\n\n   1  2  3\n1  3 19 25\n2 17 10 15\n3 12  9 12\n4 10  7  9\n5  4  5  8\n\n\n\n\n\n\n\n\n\ndata_anova\n\n    y    food\n1   3 cereals\n2  17 cereals\n3  12 cereals\n4  10 cereals\n5   4 cereals\n6  19  energy\n7  10  energy\n8   9  energy\n9   7  energy\n10  5  energy\n11 25  veggie\n12 15  veggie\n13 12  veggie\n14  9  veggie\n15  8  veggie\n\n\n\n\nThe boxplot kind of gives us the isoflavones distribution by food type. It is hard to say whether the food type affects isoflavone level or not, and we need ANOVA to help us make the conclusion.\n\n\n\n\n\n\n\nFigure¬†24.5: Boxplot of the Isoflavone Content in 3 Types of Food\n\n\n\n\n\n\nWe are going to learn to generate a ANOVA table using Python.\n Data \n\n\n\ndata\n\narray([[ 3, 19, 25],\n       [17, 10, 15],\n       [12,  9, 12],\n       [10,  7,  9],\n       [ 4,  5,  8]])\n\n\n\n\n\n\n\n\n\ndata_anova\n\n     y     food\n0    3  cereals\n1   17  cereals\n2   12  cereals\n3   10  cereals\n4    4  cereals\n5   19   energy\n6   10   energy\n7    9   energy\n8    7   energy\n9    5   energy\n10  25   veggie\n11  15   veggie\n12  12   veggie\n13   9   veggie\n14   8   veggie\n\n\n\n\nThe boxplot kind of gives us the isoflavones distribution by food type. It is hard to say whether the food type affects isoflavone level or not, and we need ANOVA to help us make the conclusion.\n\n\n\n\n\n\n\n\n\n\n\n\n Test Assumptions \nBefore implementing any statistical method, always check its method assumptions.\nANOVA requires\n\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3\\)\nData are generated from a normal distribution for each type of food.\n\nWell we did not learn to test the equality of more than two population variances, but believe me I did the test, and the three variances are not significantly different from each other. 1 Even the the variances are not all equal, the ANOVA performance will not be worse much. Equality of variances is not a strict requirement. George Box of UW-Madison showed that as long as the sample sizes are (nearly) equal, the largest variance can be up to 9 times the smallest one and the result of ANOVA will continue to be reliable. A general rule of thumb for equal variances is to compare the smallest and largest sample standard deviations. This is much like the rule of thumb for equal variances for the test for independent means. If the ratio of these two sample standard deviations falls within 0.5 to 2, then it may be that the assumption is not violated.\nTo check the normality, we can check their QQ plots. 2 Figure¬†24.6 shows that there is no obvious non-normal pattern although two data points are outside the blue 95% confidence region. The normality is not very restrictive as well. As long as the distribution is not very skewed, ANOVA works pretty well.\nWe say the ANOVA \\(F\\) test is robust to the violation of the two assumptions.\n\n\n\n\n\n\n\n\n\n\nFigure¬†24.6: QQ plots for each type of food\n\n\n\n\n ANOVA Testing \nWe are interested in the following test:\n \\(\\begin{align}&H_0: \\mu_1 = \\mu_2 = \\mu_3\\\\&H_1: \\mu_is \\text{ not all equal} \\end{align}\\) \nwhere \\(\\mu_1\\), \\(\\mu_2\\), \\(\\mu_3\\) stand for the population mean level of isoflavones of food cereals and snacks, energy bars, and veggie burgers respectively.\nWe could follow the regular six-step testing procedure, but generating the ANOVA table is more straightforward.\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nIn R, we can do all the calculations and generate an ANOVA table using just one line of code. We first use the popular function lm() to implement ANOVA. The first argument is formula that has the form response ~ factor where response is the numeric response vector, and factor is the categorical factor vector. In our data data_anova, y is our response, and food is our factor. Note that if we just write y and food in the formula, R will render an error because R does not recognize y and food because they are not an R object but a column name of data data_anova. Therefore, we need to tell R where y and food are from by specifying the data set they are referred. If you want to specify the data name, you can get access to the response and factor vector by extracting them using data_anova$y and data_anova$food.\nThe word lm stands for linear model, and ANOVA is a linear model. If you just run lm(formula = data_anova$y ~ data_anova$food), it will show the linear model output related to ANOVA. We don‚Äôt need it at this moment, and we will discuss more about linear model in Regression Chapter 27.\nTo obtain the ANOVA table, we apply the function anova() to the lm object. In the output, food is the source of variation between samples. Residuals is for the source of variation within samples. F value is the \\(F\\) test statistic value, not the critical value. Pr(&gt;F) is the \\(p\\)-value. Since \\(p\\)-value &gt; 0.05, we do not reject \\(H_0\\). The evidence is not sufficient to reject the equality of population means.\n\nanova(lm(formula = y ~ food, data = data_anova))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nfood       2   60.4  30.200  0.8282 0.4603\nResiduals 12  437.6  36.467               \n\nanova(lm(formula = data_anova$y ~ data_anova$food))\n\nAnalysis of Variance Table\n\nResponse: data_anova$y\n                Df Sum Sq Mean Sq F value Pr(&gt;F)\ndata_anova$food  2   60.4  30.200  0.8282 0.4603\nResiduals       12  437.6  36.467               \n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is another way to generate the ANOVA table in R. We use summary() and aov() functions.\n\nsummary(aov(y ~ food, data = data_anova))\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)\nfood         2   60.4   30.20   0.828   0.46\nResiduals   12  437.6   36.47               \n\n# oneway.test(y ~ food, data = data_anova, var.equal = TRUE)\n\n\n\n\n\nIn Python, we can do all the calculations and generate an ANOVA table using simple code. We first use the function formula.ols() in statsmodels.api to create a linear model from a formula and dataframe. ols means ordinary least squares which is a method for a linear method. We will discuss it in Chapter 27. The first argument is formula that has the form response ~ factor where response is the numeric response vector, and factor is the categorical factor vector. In our data data_anova, y is our response, and food is our factor. Note that the formula has to be a string.\nThen, we need to tell Python where y and food are from by specifying the data set they are referred in the argument data.\nThe code sm.formula.ols(formula='y ~ food', data=data_anova) only creates a model, but it is not actually fit by our data. We need to add .fit() to complete the modeling fitting using the ols approach.\n\nimport statsmodels.api as sm\nfrom statsmodels import stats\nmodel = sm.formula.ols(formula='y ~ food', data=data_anova).fit()\n\nTo obtain the ANOVA table, we apply the function anova_lm() to the fitted model. In the output, food is the source of variation between samples. Residual is for the source of variation within samples. F is the \\(F\\) test statistic value, not the critical value. PR(&gt;F) is the \\(p\\)-value. Since \\(p\\)-value &gt; 0.05, we do not reject \\(H_0\\). The evidence is not sufficient to reject the equality of population means.\n\nsm.stats.anova_lm(model)\n\n            df  sum_sq    mean_sq         F   PR(&gt;F)\nfood       2.0    60.4  30.200000  0.828154  0.46035\nResidual  12.0   437.6  36.466667       NaN      NaN\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe function f_oneway() in scipy.stats provides a fast way to do the ANOVA without outputting the ANOVA table. It just provides the F test statistic and p-value.\n\nfrom scipy import stats\nstats.f_oneway(data[:, 0], data[:, 1], data[:, 2])\n\nF_onewayResult(statistic=0.8281535648994516, pvalue=0.46034965646255643)",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "model-anova.html#unequal-variances",
    "href": "model-anova.html#unequal-variances",
    "title": "24¬† Analysis of Variance",
    "section": "\n24.4 Unequal Variances",
    "text": "24.4 Unequal Variances\nThe assumption of normality and equality of variances are both loose requirements. George Box of UW-Madison showed that as long as the sample sizes are (nearly) equal, the largest variance can be up to 9 times the smallest one and the result of ANOVA will continue to be reliable.\nIf population variances differ by large amount, we can transform the data. If the original variable is \\(y\\), and the variances associated with \\(y\\) across the treatments are not equal (heterogeneous), it may be necessary to work with a new variable such \\(\\sqrt{y}\\), \\(\\log y\\), or some other transformed variable.\n\n If \\(\\sigma^2 \\propto \\mu\\), use \\(Y_T = \\sqrt{Y}\\) or \\(\\sqrt{Y+0.375}\\). \n If \\(\\sigma^2 \\propto \\mu^2\\), use \\(Y_T = \\log{(Y)}\\) or \\(\\log(Y+1)\\). \n\nThe following example demonstrates how data transformation can significantly equalize variances across groups. In the original data \\(y\\), variance increases with the treatment type, which violates the assumption of homogeneity of variance necessary for performing ANOVA. Directly applying ANOVA to \\(y\\) may result in unconvincing outcomes. However, by transforming the data from \\(y\\) to \\(\\sqrt{y}\\), the variances across the three groups become much more similar, thereby satisfying the homogeneity of variance assumption and making the ANOVA results more reliable.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n24.4.1 Example of Unequal Variances (Example 8.4 SMD)\nBiologists believe that Mississippi river causes the oxygen level to be depleted near the Gulf of Mexico. To test this hypothesis water samples are taken at different distances from the mouth of Mississippi river, and the amounts of dissolve oxygen (in ppm) are recorded.\n\n\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nThe origin data set data_oxygen is saved as\n\n\n   1KM 5KM 10KM 20KM\n1    1   4   20   37\n2    5   8   26   30\n3    2   2   24   26\n4    1   3   11   24\n5    2   8   28   41\n6    2   5   20   25\n7    4   6   19   36\n8    3   4   19   31\n9    0   3   21   31\n10   2   3   24   33\n\n\n\nFirst we learn that the homogeneity of variance assumption is violated by checking the boxplot.\n\n\n\n\n\n\n\n\nThis is also verified by the Levene‚Äôs test using the function car::leveneTest(). The Levene‚Äôs test \\(p\\)-value is 0.02, so we reject the \\(H_0\\): Equality of variance.\n\nlibrary(car)\nleveneTest(oxygen ~ km, data = data_oxygen_tidy)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value  Pr(&gt;F)  \ngroup  3  3.7003 0.02029 *\n      36                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNote that here we use the data data_oxygen_tidy. As mentioned before, when doing analysis or fitting a statistical model, we prefer this tidy data that each column represents a variable, and each row stands for one observed value.\n\n\n   oxygen km\n1       1  1\n2       5  1\n3       2  1\n4       1  1\n5       2  1\n6       2  1\n7       4  1\n8       3  1\n9       0  1\n10      2  1\n11      4  5\n12      8  5\n13      2  5\n14      3  5\n15      8  5\n16      5  5\n17      6  5\n18      4  5\n19      3  5\n20      3  5\n21     20 10\n22     26 10\n23     24 10\n24     11 10\n25     28 10\n26     20 10\n27     19 10\n28     19 10\n29     21 10\n30     24 10\n31     37 20\n32     30 20\n33     26 20\n34     24 20\n35     41 20\n36     25 20\n37     36 20\n38     31 20\n39     31 20\n40     33 20\n\n\n\n\nThe origin data set data_oxygen is saved as\n\n\n   1KM  5KM  10KM  20KM\n0    1    4    20    37\n1    5    8    26    30\n2    2    2    24    26\n3    1    3    11    24\n4    2    8    28    41\n5    2    5    20    25\n6    4    6    19    36\n7    3    4    19    31\n8    0    3    21    31\n9    2    3    24    33\n\n\nFirst we learn that the homogeneity of variance assumption is violated by checking the boxplot.\n\n\n\n\n\n\n\n\nThis is also verified by the Levene‚Äôs test using the function levene() in scipy .stats. The Levene‚Äôs test \\(p\\)-value is 0.02, so we reject the \\(H_0\\): Equality of variance.\n\n# Levene's test for homogeneity of variances\nstats.levene(data_oxygen['1KM'], \n             data_oxygen['5KM'], \n             data_oxygen['10KM'],\n             data_oxygen['20KM'])\n\nLeveneResult(statistic=3.700319780721791, pvalue=0.020291806138675962)\n\n\n\n\n\nTo get some understanding of how we should transform our data to make the variances equal, let‚Äôs calculate \\(\\frac{s_i^2}{\\bar{y}_i}\\) for \\(i = 1, 2, 3, 4\\).\n\n\n\\(\\frac{s_1^2}{\\bar{y}_1} = 0.99\\), \\(\\frac{s_2^2}{\\bar{y}_2} = 0.97\\), \\(\\frac{s_3^2}{\\bar{y}_3} = 1.06\\), \\(\\frac{s_4^2}{\\bar{y}_4} = 0.97\\)\n\n\nThis tells us, nearly, the sample variance is proportional to the sample mean of the same group. So we can use the transformation \\(Y_T = \\sqrt{Y+0.375}\\).\n\n\n\n\n\n\n\n\n\n\nNow, the variances for each group of the transformed data are much reasonably close.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "model-anova.html#anova-mathematical-model",
    "href": "model-anova.html#anova-mathematical-model",
    "title": "24¬† Analysis of Variance",
    "section": "\n24.5 ANOVA Mathematical Model*",
    "text": "24.5 ANOVA Mathematical Model*\nTo generalize the ANOVA, it is easier to think of one-factor ANOVA in the following way: \\[y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}, \\quad j = 1, 2, \\dots, n_i, \\quad i = 1, 2, \\dots, t\\]\n\n\n\\(\\mu\\) is the overall mean across all \\(t\\) populations.\n\n\\(\\tau_i\\) is the effect due to \\(i\\)-th treatment.\n\n\\(\\epsilon_{ij}\\) is the random deviation of \\(y_{ij}\\) about the \\(i\\)-th population mean \\(\\mu_i = \\mu+ \\tau_i\\).\n\n\\(\\mu\\) and \\(\\tau_i\\) are unknown constants.\nANOVA is a linear model.\n\nThe assumptions of ANOVA are\n\n\n\\(\\epsilon_{ij}\\)s are independent and normally distributed with mean 0.\n\n\\(Var(\\epsilon_{ij}) = \\sigma^2\\) ( a constant value )\n\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\n\nFigure¬†24.7 illustrate the ANOVA model structure. There are 4 treatments, and the overall mean \\(\\mu = 75.5\\). The 4 treatments are \\(\\tau_1 = -5.5\\), \\(\\tau_2 = -17.5\\), \\(\\tau_3= -14.5\\), and \\(\\tau_4 = 8.5\\).\n\n\n\n\n\n\n\nFigure¬†24.7: Illustration of ANOVA model structure. Figure 16.7 of Applied Linear Statistical Models, 5th ed.\n\n\n\n\n\nLet \\(y_{13}\\) is the third measurement drawn from population 1, the its value can be decomposed into three parts: the overall mean, the adjustment due to being in the first treatment, and the random variation of the first population: \\[y_{13} = \\mu + \\tau_1 + \\epsilon_{13}.\\]\nIf \\(y_{13} = 72\\), then \\[y_{13} = \\mu + \\tau_1 + \\epsilon_{13} \\iff 72 = 75.5 + (-5.5) + 2.\\]\n\nAgain, our model is \\(y_{ij} = \\mu + \\tau_i + \\epsilon_{ij} = \\mu_i + \\epsilon_{ij}, \\quad j = 1, 2, \\dots, n_i, \\quad i = 1, 2, \\dots, t.\\)\nTo test whether all samples come from the same population, we test whether all group means are equal:\n \\(\\begin{align}\n  &H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_t\\\\\n  &H_1: \\text{Population means are not all equal}\n  \\end{align}\\) \nThis is equivalent to testing whether all treatment effects are zero, or there is no treatment/factor/group effect:\n \\(\\begin{align}\n  &H_0: \\tau_i = 0 \\text{ for all } i = 1, 2, \\dots, t\\\\\n  &H_1: \\tau_i \\ne 0 \\text{ for some } i\n  \\end{align}\\) \nTest statistics and decision rule are same as before. \\(s^2_{B} = MSB = \\frac{SSB}{df_B}\\) and \\(s^2_{W} = MSW = \\frac{SSW}{df_W}\\). Then the test statistic is \\(F_{test} = \\frac{s^2_{B}}{s_W^2}\\). We reject \\(H_0\\) if \\(F_{test} &gt; F_{\\alpha, \\, df_{B},\\, df_{W}}\\) or the \\(p\\)-value \\(P(F &gt; F_{test}) &lt; \\alpha\\).\n\n24.5.1 Check Assumption that \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\n\nWe learn that \\(y_{ij} = \\mu + \\tau_i + \\epsilon_{ij} =  \\mu_i + \\epsilon_{ij}\\) where \\(\\mu_i = \\mu + \\tau_i\\). As before, we need to check model assumptions. One assumption is \\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\). Notice that it implies normality and homogeneity of variance assumptions.\nFor the model, we have \\(\\epsilon_{ij} = y_{ij} - \\mu_i\\). However, \\(\\mu_i\\) is unknown. We are not able to get the values of \\(\\epsilon_{ij}\\) and check whether they are normally distributed. One solution is to find its estimate. We first estimate the errors \\(\\epsilon_{ij}\\) by \\(e_{ij}\\) called residuals: \\[e_{ij} = y_{ij} - \\hat{\\mu}_i =  y_{ij} - \\bar{y}_{i\\cdot},\\] where \\(\\hat{\\mu}_i = \\bar{y}_{i\\cdot}\\). We use a hat symbol \\(\\hat{}\\) to denote the estimated value for an unknown parameter, or the predicted value of the response. Here we don‚Äôt know the group population means \\(\\mu_i\\), but they can be estimated by their corresponding group sample means \\(\\bar{y}_{i\\cdot}\\).\n\n\nTo test normal distribution of errors \\(\\epsilon_{ij}\\), we look at the normal probability plot of \\(e_{ij}\\).\n\nTo test that the \\(Var(\\epsilon_{ij})\\) is constant of \\(\\sigma^2\\), we look at the scatter plot of the residuals \\(e_{ij}\\) and the predicted or fitted values \\(\\hat{y}_{ij}\\), where \\[\\hat{y}_{ij} = \\hat{\\mu}_i = \\bar{y}_{i\\cdot}.\\] \\(\\hat{y}_{ij}\\) is the fitted value of the \\(j\\)th observation in the \\(i\\)th group, which is an estimate of the observed data point \\(y_{ij}\\). The fitted value \\(\\hat{y}_{ij}\\) is the one produced from the model, and of course for the most of the time will not be the same as the observed value \\(y_{ij}\\). From ANOVA, \\(\\hat{y}_{ij} = \\bar{y}_{i\\cdot}\\). That is, if we want to estimate any value drawn from the \\(i\\)th population, ANOVA gives us a point estimate, the \\(i\\)th group sample mean, to estimate any \\(j\\)th observation from the \\(i\\)th population.\nThe samples in the \\(i\\)th population are not the same due to the variation of the normal distribution. However, when estimating each individual observation, ANOVA would not provide different values for different observations, but one single common value for all individual observations. The sampling variation cannot be captured by ANOVA, and using the mean value \\(\\bar{y}_{i\\cdot}\\) as an estimate is the best ANOVA can do.\n\n\n\nLet‚Äôs go back to the oxygen level example. The original data look like\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo get the sample mean for each group, we can grab each column data, and calculate its average. For example, \\(\\bar{y}_{1\\cdot} = 2.2\\).\n\n\nR\nPython\n\n\n\n\ncolMeans(data_oxygen)\n\n 1KM  5KM 10KM 20KM \n 2.2  4.6 21.2 31.4 \n\n\nThe fitted value can also be obtained from the fitted result by the fitted linear model from lm(). oxygen_fit$fitted.values gives us fitted values for all observations. Note that the first 10 observations all have the same fitted value 2.2 because they are all from the first group (1 km). Similarly for other groups.\n\n# data_oxygen_tidy\n## fit a linear model to get fitted values and residuals\noxygen_fit &lt;- lm(oxygen ~ km, data = data_oxygen_tidy)\noxygen_fit$fitted.values\n\n   1    2    3    4    5    6    7    8    9   10   11   12   13   14   15   16 \n 2.2  2.2  2.2  2.2  2.2  2.2  2.2  2.2  2.2  2.2  4.6  4.6  4.6  4.6  4.6  4.6 \n  17   18   19   20   21   22   23   24   25   26   27   28   29   30   31   32 \n 4.6  4.6  4.6  4.6 21.2 21.2 21.2 21.2 21.2 21.2 21.2 21.2 21.2 21.2 31.4 31.4 \n  33   34   35   36   37   38   39   40 \n31.4 31.4 31.4 31.4 31.4 31.4 31.4 31.4 \n\n\nWe can organize the fitted values, saving them as a data frame like the original data data_oxygen.\n\nfitted_val &lt;- matrix(oxygen_fit$fitted.values, 10, 4)\n\n## use the same column name as the original data\ncolnames(fitted_val) &lt;- paste0(c(1, 5, 10, 20), \"KM\")\n\n## fitted value data\nas.data.frame(fitted_val)\n\n   1KM 5KM 10KM 20KM\n1  2.2 4.6 21.2 31.4\n2  2.2 4.6 21.2 31.4\n3  2.2 4.6 21.2 31.4\n4  2.2 4.6 21.2 31.4\n5  2.2 4.6 21.2 31.4\n6  2.2 4.6 21.2 31.4\n7  2.2 4.6 21.2 31.4\n8  2.2 4.6 21.2 31.4\n9  2.2 4.6 21.2 31.4\n10 2.2 4.6 21.2 31.4\n\n\n\nNow let‚Äôs check the residuals \\(e_{ij} = y_{ij} - \\bar{y}_{i\\cdot}\\). We can obtain the residuals using its definition\n\ndata_oxygen_tidy[, 1] - oxygen_fit$fitted.values\n\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n -1.2   2.8  -0.2  -1.2  -0.2  -0.2   1.8   0.8  -2.2  -0.2  -0.6   3.4  -2.6 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n -1.6   3.4   0.4   1.4  -0.6  -1.6  -1.6  -1.2   4.8   2.8 -10.2   6.8  -1.2 \n   27    28    29    30    31    32    33    34    35    36    37    38    39 \n -2.2  -2.2  -0.2   2.8   5.6  -1.4  -5.4  -7.4   9.6  -6.4   4.6  -0.4  -0.4 \n   40 \n  1.6 \n\n\nor we can simply grab them from the fitted result:\n\noxygen_fit$residuals\n\n    1     2     3     4     5     6     7     8     9    10    11    12    13 \n -1.2   2.8  -0.2  -1.2  -0.2  -0.2   1.8   0.8  -2.2  -0.2  -0.6   3.4  -2.6 \n   14    15    16    17    18    19    20    21    22    23    24    25    26 \n -1.6   3.4   0.4   1.4  -0.6  -1.6  -1.6  -1.2   4.8   2.8 -10.2   6.8  -1.2 \n   27    28    29    30    31    32    33    34    35    36    37    38    39 \n -2.2  -2.2  -0.2   2.8   5.6  -1.4  -5.4  -7.4   9.6  -6.4   4.6  -0.4  -0.4 \n   40 \n  1.6 \n\n\n\n\nThe residuals must look like a normal distribution. The normal probability plot is somewhat close to a straight line.\n\n\n\n\n\n\n\n\nThe Shapiro test and Anderson-Darling test give us a different conclusion. We could either transform the data to make it look more normal, or collect more data points to gain more information about how the data are distributed.\n\nshapiro.test(oxygen_fit$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  oxygen_fit$residuals\nW = 0.95469, p-value = 0.1101\n\nad.test(oxygen_fit$residuals)\n\n\n    Anderson-Darling normality test\n\ndata:  oxygen_fit$residuals\nA = 0.87454, p-value = 0.02271\n\n\n\nResidual plots is another good way to check the model assumptions, especially whether or not the homogeneity of variance is hold. A residual plot is a scatterplot of \\(e_{ij}\\) (y-axis) versus \\(\\hat{y}_{ij}\\) (x-axis).\n\n\nplot(x = oxygen_fit$fitted.values, y = oxygen_fit$residuals)\n\n\n\n\n\n\n\n\n\nIf the homogeneity of variance holds, the residual points should be scattered around value 0 with the same degree of spreadness. Due to the cone shape, we see that the residual variance is increasing with the fitted values or kilometers, a sign of non-constant variances. We can conclude that \\(Var(\\epsilon_{ij})\\) is not constant. We can further say that this variance is a function of \\(\\bar{y}_{i\\cdot}\\).\n\nSince the model assumptions are violated, we transform our data to correct unequal variances. As we learned in the previous section, we use the transformation \\(Y_T = \\sqrt{Y + 0.375}\\). The transformed data set is called data_oxygen_tidy_trans.\n\ndata_oxygen_tidy_trans &lt;- data_oxygen_tidy\ndata_oxygen_tidy_trans[, 1] &lt;- sqrt(data_oxygen_tidy[, 1] + 0.375)\n\nWe then fit ANOVA model on the transformed data.\n\noxygen_fit_trans &lt;- lm(oxygen ~ km, data = data_oxygen_tidy_trans)\nanova(oxygen_fit_trans)\n\nAnalysis of Variance Table\n\nResponse: oxygen\n          Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nkm         3 113.095  37.698   153.3 &lt; 2.2e-16 ***\nResiduals 36   8.853   0.246                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWe can check the model assumptions on the transformed data too to make sure that fitting ANOVA to the data makes sense.\n\n\n\n\n\n\n\n\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  oxygen_fit_trans$residuals\nW = 0.97324, p-value = 0.4529\n\n\n\n    Anderson-Darling normality test\n\ndata:  oxygen_fit_trans$residuals\nA = 0.33815, p-value = 0.4855\n\n\nFrom the qqplot and the testing results we can see that the transformed data are more like a normal distribution. There is no significant pattern in the residual plot, showing that unequal variances have been corrected.\nThis result is more convincing than the one using the original data that violates the ANOVA model assumptions. Although the two models both reject the null hypothesis, the numerical results obtained from the transformed data interpret the relationship between the response and the factors more accurately.\n\nanova(oxygen_fit)\n\nAnalysis of Variance Table\n\nResponse: oxygen\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nkm         3 5793.1 1931.03   129.7 &lt; 2.2e-16 ***\nResiduals 36  536.0   14.89                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n# Calculate the mean for each group\ngroup_means = data_oxygen.mean()\ngroup_means\n\n1KM      2.2\n5KM      4.6\n10KM    21.2\n20KM    31.4\ndtype: float64\n\n\n\ndata_oxygen_tidy = pd.read_csv(\"./data/data_oxygen_tidy.csv\")\n\nThe fitted value can also be obtained from the fitted result by the fitted linear model from formula.ols(). oxygen_fit.fittedvalues gives us fitted values for all observations. Note that the first 10 observations all have the same fitted value 2.2 because they are all from the first group (1 km). Similarly for other groups.\nIn the formula, since the variable km is in fact categorical, we need to add C() to turn its type from integer to string.\n\noxygen_fit = sm.formula.ols(formula='oxygen ~ C(km)', data=data_oxygen_tidy).fit()\noxygen_fit.fittedvalues\n\n0      2.2\n1      2.2\n2      2.2\n3      2.2\n4      2.2\n5      2.2\n6      2.2\n7      2.2\n8      2.2\n9      2.2\n10     4.6\n11     4.6\n12     4.6\n13     4.6\n14     4.6\n15     4.6\n16     4.6\n17     4.6\n18     4.6\n19     4.6\n20    21.2\n21    21.2\n22    21.2\n23    21.2\n24    21.2\n25    21.2\n26    21.2\n27    21.2\n28    21.2\n29    21.2\n30    31.4\n31    31.4\n32    31.4\n33    31.4\n34    31.4\n35    31.4\n36    31.4\n37    31.4\n38    31.4\n39    31.4\ndtype: float64\n\n\nWe can organize the fitted values, saving them as a data frame like the original data data_oxygen.\n\nfitted_val = np.reshape(oxygen_fit.fittedvalues.values, (10, 4), order='F')\npd.DataFrame(fitted_val, columns=['1KM', '5KM', '10KM', '20KM'])\n\n   1KM  5KM  10KM  20KM\n0  2.2  4.6  21.2  31.4\n1  2.2  4.6  21.2  31.4\n2  2.2  4.6  21.2  31.4\n3  2.2  4.6  21.2  31.4\n4  2.2  4.6  21.2  31.4\n5  2.2  4.6  21.2  31.4\n6  2.2  4.6  21.2  31.4\n7  2.2  4.6  21.2  31.4\n8  2.2  4.6  21.2  31.4\n9  2.2  4.6  21.2  31.4\n\n\n\nNow let‚Äôs check the residuals \\(e_{ij} = y_{ij} - \\bar{y}_{i\\cdot}\\). We can obtain the residuals using its definition\n\ndata_oxygen_tidy['oxygen'].values - oxygen_fit.fittedvalues.values\n\narray([ -1.2,   2.8,  -0.2,  -1.2,  -0.2,  -0.2,   1.8,   0.8,  -2.2,\n        -0.2,  -0.6,   3.4,  -2.6,  -1.6,   3.4,   0.4,   1.4,  -0.6,\n        -1.6,  -1.6,  -1.2,   4.8,   2.8, -10.2,   6.8,  -1.2,  -2.2,\n        -2.2,  -0.2,   2.8,   5.6,  -1.4,  -5.4,  -7.4,   9.6,  -6.4,\n         4.6,  -0.4,  -0.4,   1.6])\n\n\nor we can simply grab them from the fitted result:\n\noxygen_fit.resid\n\n0     -1.2\n1      2.8\n2     -0.2\n3     -1.2\n4     -0.2\n5     -0.2\n6      1.8\n7      0.8\n8     -2.2\n9     -0.2\n10    -0.6\n11     3.4\n12    -2.6\n13    -1.6\n14     3.4\n15     0.4\n16     1.4\n17    -0.6\n18    -1.6\n19    -1.6\n20    -1.2\n21     4.8\n22     2.8\n23   -10.2\n24     6.8\n25    -1.2\n26    -2.2\n27    -2.2\n28    -0.2\n29     2.8\n30     5.6\n31    -1.4\n32    -5.4\n33    -7.4\n34     9.6\n35    -6.4\n36     4.6\n37    -0.4\n38    -0.4\n39     1.6\ndtype: float64\n\n\n\n\n\n\n\n\n\n\nThe Shapiro test and Anderson-Darling test give us a different conclusion. We could either transform the data to make it look more normal, or collect more data points to gain more information about how the data are distributed.\n\n# Shapiro-Wilk test for normality\nstats.shapiro(oxygen_fit.resid)\n\nShapiroResult(statistic=0.9546933369577799, pvalue=0.11005075717451401)\n\n\n\n# Anderson-Darling test for normality\nstats.anderson(oxygen_fit.resid)\n\nAndersonResult(statistic=0.8745376678064076, critical_values=array([0.531, 0.605, 0.726, 0.847, 1.007]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]), fit_result=  params: FitParams(loc=6.661338147750939e-15, scale=3.7072347839851933)\n success: True\n message: '`anderson` successfully fit the distribution to the data.')\n\n\nResidual plots is another good way to check the model assumptions, especially whether or not the homogeneity of variance is hold. A residual plot is a scatterplot of \\(e_{ij}\\) (y-axis) versus \\(\\hat{y}_{ij}\\) (x-axis).\n\nplt.scatter(oxygen_fit.fittedvalues, oxygen_fit.resid)\n\n\n\n\n\n\n\n\n\nIf the homogeneity of variance holds, the residual points should be scattered around value 0 with the same degree of spreadness. Due to the cone shape, we see that the residual variance is increasing with the fitted values or kilometers, a sign of non-constant variances. We can conclude that \\(Var(\\epsilon_{ij})\\) is not constant. We can further say that this variance is a function of \\(\\bar{y}_{i\\cdot}\\).\n\nSince the model assumptions are violated, we transform our data to correct unequal variances. As we learned in the previous section, we use the transformation \\(Y_T = \\sqrt{Y + 0.375}\\). The transformed data set is called data_oxygen_tidy_trans.\n\ndata_oxygen_tidy_trans = data_oxygen_tidy.copy()\n# Apply the transformation to the first column (oxygen)\ndata_oxygen_tidy_trans['oxygen'] = np.sqrt(data_oxygen_tidy['oxygen'] + 0.375)\n\nWe then fit ANOVA model on the transformed data.\n\noxygen_fit_trans = sm.formula.ols(formula='oxygen ~ C(km)',\n                                  data=data_oxygen_tidy_trans).fit()\nsm.stats.anova_lm(oxygen_fit_trans)\n\n            df      sum_sq    mean_sq           F        PR(&gt;F)\nC(km)      3.0  113.095273  37.698424  153.303333  1.478071e-20\nResidual  36.0    8.852666   0.245907         NaN           NaN\n\n\nWe can check the model assumptions on the transformed data too to make sure that fitting ANOVA to the data makes sense.\n\n\n\n\n\n\n\n\n\n\nShapiroResult(statistic=0.9732395195196447, pvalue=0.4529490122256537)\n\n\nAndersonResult(statistic=0.3381544911538725, critical_values=array([0.531, 0.605, 0.726, 0.847, 1.007]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]), fit_result=  params: FitParams(loc=9.769962616701378e-16, scale=0.4764361953451715)\n success: True\n message: '`anderson` successfully fit the distribution to the data.')\n\n\nFrom the qqplot and the testing results we can see that the transformed data are more like a normal distribution. There is no significant pattern in the residual plot, showing that unequal variances have been corrected.\nThis result is more convincing than the one using the original data that violates the ANOVA model assumptions. Although the two models both reject the null hypothesis, the numerical results obtained from the transformed data interpret the relationship between the response and the factors more accurately.\n\noxygen_fit = sm.formula.ols('oxygen ~ C(km)', data=data_oxygen_tidy).fit()\nsm.stats.anova_lm(oxygen_fit)\n\n            df  sum_sq      mean_sq           F        PR(&gt;F)\nC(km)      3.0  5793.1  1931.033333  129.696269  2.353012e-19\nResidual  36.0   536.0    14.888889         NaN           NaN",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "model-anova.html#nonparametric-approach-kruskal-wallis-test",
    "href": "model-anova.html#nonparametric-approach-kruskal-wallis-test",
    "title": "24¬† Analysis of Variance",
    "section": "\n24.6 Nonparametric Approach: Kruskal-Wallis Test",
    "text": "24.6 Nonparametric Approach: Kruskal-Wallis Test\nIn ANOVA, the sample from each factor level is from a normal population. What if the distribution is non-normal? One solution is to use the Kruskal-Wallis test, which can be viewed as a generalization of the Wilcoxon rank sum test (Mann-Whitney U test). Suppose we have \\(k\\) samples from \\(k\\) populations. We like to test the hypothesis that the \\(k\\) samples were drawn from populations with the same median:\n\n \\(\\begin{align}\n&H_0: \\text{All $k$ populations have the same median}\\\\\n&H_1: \\text{Not all the medians are the same}\n\\end{align}\\) \n\nThe requirements of the Kruskal-Wallis test are\n\nAt least 3 independent samples\nEach sample has at least 5 observations for approximating a \\(\\chi^2\\) distribution. If samples have fewer than 5 observations, the \\(\\chi^2\\) approximation doe snot work well, and another way to find critical values is needed.\n\nThere is no requirement that the populations have a normal distribution or any other particular distribution.\nThe testing details are skipped here, but let‚Äôs see how we can perform this test to answer our question when the normality assumption of ANOVA is not satisfied.\n\n\n\n\nR\nPython\n\n\n\nWe use the R built-in data set airquality for illustration. The data collected daily air quality measurements in New York, May to September 1973.\n\nstr(airquality)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\nhead(airquality)\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\n## remove observations with missing values using complete.cases()\nair_data &lt;- airquality[complete.cases(airquality), ] \n\nOur goal is to test at .05 significance level if the monthly ozone densities in New York have the same median from May to September 1973.\nFirst let‚Äôs check the normality assumption. We can use qqnorm(), but here the function qqPlot() from the car package is used. The blue shaded area shows the 95% confidence interval for straight line that indicates normality. Since there are not many observations in each group May to September, it may be hard to see whether normality is satisfied for every group.\n\nlibrary(car)\npar(mgp = c(2.5, 1, 0))\npar(mar = c(4, 4, 2, 1))\ncar::qqPlot(Ozone ~ Month, data = air_data, layout=c(1, 5))\n\n\n\n\n\n\n\n\n\n\nWe use the R built-in data set airquality for illustration. The data collected daily air quality measurements in New York, May to September 1973. We use dropna() to remove observations with missing values.\n\n# Load air quality data\nairquality = pd.read_csv(\"./data/airquality.csv\")\nair_data = airquality.dropna()\nair_data\n\n     Ozone  Solar.R  Wind  Temp  Month  Day\n0     41.0    190.0   7.4    67      5    1\n1     36.0    118.0   8.0    72      5    2\n2     12.0    149.0  12.6    74      5    3\n3     18.0    313.0  11.5    62      5    4\n6     23.0    299.0   8.6    65      5    7\n..     ...      ...   ...   ...    ...  ...\n147   14.0     20.0  16.6    63      9   25\n148   30.0    193.0   6.9    70      9   26\n150   14.0    191.0  14.3    75      9   28\n151   18.0    131.0   8.0    76      9   29\n152   20.0    223.0  11.5    68      9   30\n\n[111 rows x 6 columns]\n\n\n\n\n\nWhat we can do is to use a more formal testing procedure to decide whether are not the normality is satisfied. There are several methods for normality test out there such as Kolmogorov-Smirnov (K-S) test, Shapiro-Wilk‚Äôs test, and Anderson-Darling test.\nThe K-S test can be used to compare a sample against any reference distribution, not just the normal distribution. However, the K-S test is more focused on the central part of the distribution and might miss deviations in the tails. It is generally less powerful for detecting deviations from normality compared to other tests like the Shapiro-Wilk or Anderson-Darling tests.\nThe Shapiro-Wilk test is specifically designed to assess the normality of a distribution. It is generally more powerful than the K-S and Anderson-Darling tests for detecting departures from normality, especially in small to moderately sized samples. The Anderson-Darling test is a modification of the K-S test and gives more weight to the tails of the distribution.\n\nWhen we do the normality test, the null hypothesis of these tests is that ‚Äúsample distribution IS normal‚Äù. If the test is significant, the distribution is non-normal.\n\n\nR\nPython\n\n\n\nTo perform Shapiro-Wilk test, we use shapiro.test() function in R. Note that May and Semptember are not normal.\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  air_data[air_data$Month == 5, 1]\nW = 0.71273, p-value = 1.491e-05\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  air_data[air_data$Month == 9, 1]\nW = 0.78373, p-value = 4.325e-05\n\n\nTo perform Anderson-Darling test, we use ad.test() function in the R package nortest.\n\nlibrary(nortest)\nad.test(air_data[air_data$Month == 5, 1])\n\n\n    Anderson-Darling normality test\n\ndata:  air_data[air_data$Month == 5, 1]\nA = 1.7452, p-value = 0.0001286\n\nad.test(air_data[air_data$Month == 9, 1])\n\n\n    Anderson-Darling normality test\n\ndata:  air_data[air_data$Month == 9, 1]\nA = 2.4088, p-value = 2.943e-06\n\n\nFor the K-S test, we use ks.test() function. We need to add ‚Äúpnorm‚Äù in the function, so that it knows you are comparing your data with the normal distribution.\n\n\nWarning in ks.test.default(air_data[air_data$Month == 5, 1], \"pnorm\"): ties\nshould not be present for the one-sample Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  air_data[air_data$Month == 5, 1]\nD = 0.9583, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nWarning in ks.test.default(air_data[air_data$Month == 9, 1], \"pnorm\"): ties\nshould not be present for the one-sample Kolmogorov-Smirnov test\n\n\n\n    Asymptotic one-sample Kolmogorov-Smirnov test\n\ndata:  air_data[air_data$Month == 9, 1]\nD = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two-sided\n\n\nWe just conform that not all sub-samples are normally distributed, and using the Kruskal-Wallis test could be making more sense. It is super easy to perform the Kruskal-Wallis test. Just one line of code with the formula as lm() in the kruskal.test() function. \n\nkruskal.test(formula = Ozone ~ Month, data = air_data) \n\n\n    Kruskal-Wallis rank sum test\n\ndata:  Ozone by Month\nKruskal-Wallis chi-squared = 26.309, df = 4, p-value = 2.742e-05\n\n\nIt‚Äôs an (approximate) \\(\\chi^2\\) test with degrees of freedom \\(k-1\\). Here, we have \\(k=5\\) months. Since \\(p\\)-value is approaching to 0, we reject \\(H_0\\). We conclude that there is sufficient evidence to reject the claim that the 5 monthly ozone densities in New York have the same median.\n\n\nTo perform Shapiro-Wilk test, we use shapiro() function in scipy.stats. Note that May and Semptember are not normal.\n\nstats.shapiro(x=air_data[air_data['Month'] == 5]['Ozone'])\n\nShapiroResult(statistic=0.7127260508018644, pvalue=1.4905276180225703e-05)\n\nstats.shapiro(x=air_data[air_data['Month'] == 9]['Ozone'])\n\nShapiroResult(statistic=0.7837314830546598, pvalue=4.325436491991434e-05)\n\n\nTo perform Anderson-Darling test, we use anderson() function in scipy.stats.\n\nstats.anderson(x=air_data[air_data['Month'] == 5]['Ozone'])\n\nAndersonResult(statistic=1.7451940221773548, critical_values=array([0.513, 0.584, 0.701, 0.817, 0.972]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]), fit_result=  params: FitParams(loc=24.125, scale=22.88594119427528)\n success: True\n message: '`anderson` successfully fit the distribution to the data.')\n\nstats.anderson(x=air_data[air_data['Month'] == 9]['Ozone'])\n\nAndersonResult(statistic=2.4087626260322885, critical_values=array([0.52 , 0.592, 0.71 , 0.828, 0.985]), significance_level=array([15. , 10. ,  5. ,  2.5,  1. ]), fit_result=  params: FitParams(loc=31.448275862068964, scale=24.141822346436413)\n success: True\n message: '`anderson` successfully fit the distribution to the data.')\n\n\nFor the K-S test, we use kstest() function scipy.stats. We need to add ‚Äúnorm‚Äù in the function, so that it knows you are comparing your data with the normal distribution.\n\nstats.kstest(rvs=air_data[air_data['Month'] == 5]['Ozone'], cdf='norm')\n\nKstestResult(statistic=0.9583016620915003, pvalue=1.5271435453275413e-33, statistic_location=4.0, statistic_sign=-1)\n\nstats.kstest(rvs=air_data[air_data['Month'] == 9]['Ozone'], cdf='norm')\n\nKstestResult(statistic=0.9999999999987201, pvalue=0.0, statistic_location=7.0, statistic_sign=-1)\n\n\nWe just conform that not all sub-samples are normally distributed, and using the Kruskal-Wallis test could be making more sense. We use kruskal() in scipy.stats to perform such test. \n\n# Kruskal-Wallis test for Ozone levels across different months\nstats.kruskal(air_data[air_data['Month'] == 5]['Ozone'],\n              air_data[air_data['Month'] == 6]['Ozone'],\n              air_data[air_data['Month'] == 7]['Ozone'],\n              air_data[air_data['Month'] == 8]['Ozone'],\n              air_data[air_data['Month'] == 9]['Ozone'])\n\nKruskalResult(statistic=26.308626792507006, pvalue=2.741846175019204e-05)\n\n\n\n\n\n\n\n\nNote\n\n\n\nA more concise way to write the code is\n\n# Kruskal-Wallis test for Ozone levels across different months\nstats.kruskal(*[air_data[air_data['Month']==m]['Ozone'] for m in sorted(air_data['Month'].unique())])\n\n\n\nIt‚Äôs an (approximate) \\(\\chi^2\\) test with degrees of freedom \\(k-1\\). Here, we have \\(k=5\\) months. Since \\(p\\)-value is approaching to 0, we reject \\(H_0\\). We conclude that there is sufficient evidence to reject the claim that the 5 monthly ozone densities in New York have the same median.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "model-anova.html#footnotes",
    "href": "model-anova.html#footnotes",
    "title": "24¬† Analysis of Variance",
    "section": "",
    "text": "The test I use is Brown-Forsythe test. It can be performed using the function onewaytests::bf.test().‚Ü©Ô∏é\nThere are several tests for normality, such as Shapiro‚ÄìWilk test (stats::shapiro.test()), Anderson‚ÄìDarling test (nortest::ad.test()), and Kolmogorov‚ÄìSmirnov test (stats::ks.test()).‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>24</span>¬† <span class='chapter-title'>Analysis of Variance</span>"
    ]
  },
  {
    "objectID": "model-multicomp.html",
    "href": "model-multicomp.html",
    "title": "25¬† Multiple Comparison*",
    "section": "",
    "text": "25.1 Multiplicity of Hypotheses\nWhat do we do after ANOVA? In ANOVA we test\nIf we reject \\(H_0\\) in favor of \\(H_1\\), we conclude that at least one of the \\(t\\) population means differ from the others, or \\(\\mu_i \\ne \\mu_j\\) for some \\(i\\) and \\(j\\). But the question is, we only conclude that there is some difference, but we don‚Äôt know which means differ from each other, or what pairs \\((\\mu_i, \\mu_j)\\) are different. ANONA itself does not answer such questions. Results of an ANOVA do NOT tell us which group(s) is(are) different from the others.\nSo after ANOVA, we may want to test\nWe will be testing many hypotheses. As we discussed before, if there are \\(t\\) populations under the study, there are \\(t\\) chooses 2 \\(t \\choose{2}\\) pairs to be tested. For example, if there are 5 treatments, there will be 10 hypotheses. The test statistic will be like \\(t_{ij} = \\frac{\\bar{y}_i - \\bar{y}_j}{s_p^{ij} \\sqrt{\\left( \\frac{1}{n_i} + \\frac{1}{n_j} \\right)}}\\) for all pairs of \\((i, j)\\), assuming all have the same population variance.\nHowever, performing too many pairewise tests simultaneously is problematic. If there are 10 hypotheses and each is tested at \\(\\alpha = 0.05\\), there will be a high chance that one hypothesis will be falsely rejected. \\[\\small{\\begin{align} P(\\text{At least 1 hypothesis is falsely rejected}) &= 1 - P(\\text{No hypothesis is falsely rejected}) \\\\ &= 1 - (1 - \\alpha)^{t} \\\\ & = 1 - (1 - 0.05)^{10} = 0.4 \\end{align}}\\] provided that all tests or hypotheses are independent. This probability is called the family-wise error rate (FWER). We‚Äôll take about it in more detail later.\nThis high probability problem always arises due to multiplicity of hypotheses, meaning that multiple testings are conducted simultaneously in one single research study. This problem of multiplicity is more serious when you are testing more hypotheses.\n[Example: Gene Expression Analysis] A data set is collected on gene expressions on 10,000 genes. This kind of research is done if you are trying to find genes responsible for cancer. If those genes are correctly detected, you can study their structure and come up with a cure.\nIf we test each of the hypotheses at \\(\\alpha = 0.05\\), then \\(P(\\text{At least 1 hypothesis is falsely rejected}) \\approx 1\\). In fact, on average 5% of them (500 hypotheses) will be falsely discovered even if all null hypotheses are true. This is a big problem in scientific studies.\nIn this chapter we will learn how to do multiple pairwise comparisons or testings simultaneously while controlling the familywise error rate at a low level, reducing the number of false positives, or mitigating the inflated Type-I error rate due to multiple comparisons. The main methods introduced here include Bonferroni correction, Fisher‚Äôs LSD (Least Significant Difference), Tukey‚Äôs HSD (honestly significant difference), and Dunnett‚Äôs Method. Before we jump into these methods, let‚Äôs first look at an examples, refreshing our knowledge of ANOVA.\nExample of Multiple Comparison: ANOVA (Example 9.4 in SMD)\nA study was done to test 5 different agents used to control weeds. Each of these agents were applied to sample of 6 one-acre plots. The hay was harvested and the total yield was recorded.\nThe questions are\nThe first question can be answered by ANOVA. Let‚Äôs check the assumptions, and perform ANOVA.\nFirst we check the equality of variances \\(H_0: \\sigma_1^2 = \\dots = \\sigma_5^2\\).\nNext, we perform ANOVA to test if there is a difference among the agents.\nWe reject \\(H_0\\), and conclude that there is a difference among the agents.\nIn the ANOVA, we only conclude that mean yields are different under different agents. How do we say which agent is the best? Are chemical agents (Agent 4, 5) better than the biological agents (Agent 2, 3)? All of these questions can be answered by all pairwise comparisons.\nHere shows the boxplot for each weed agent. The ANOVA has told us there are some mean yield differences among these agents, and we are going to find out which 2 agents or which pair produce statistically discernible different mean yields.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Multiple Comparison*</span>"
    ]
  },
  {
    "objectID": "model-multicomp.html#multiplicity-of-hypotheses",
    "href": "model-multicomp.html#multiplicity-of-hypotheses",
    "title": "25¬† Multiple Comparison*",
    "section": "",
    "text": "\\(\\begin{align}\n  &H_0: \\mu_1 = \\mu_2 = \\dots = \\mu_t\\\\\n  &H_1: \\mu_i \\ne \\mu_j \\text{ for some } i, j\n  \\end{align}\\) \n\n\n\n\n\n \\(\\begin{align}\n  &H_0^{ij}: \\mu_i = \\mu_j\\\\\n  &H_1^{ij}: \\mu_i \\ne \\mu_j\n  \\end{align}\\)  for all pairs of \\((i, j)\\) using, say, two-sample \\(t\\)-test.\n\n\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nThe data set is saved in the R data frame data_weed.\n\n\n   yield agent type\n1   1.05     1 None\n2   1.09     1 None\n3   1.32     1 None\n4   1.13     1 None\n5   1.13     1 None\n6   1.33     1 None\n7   1.37     2 Bio1\n8   1.12     2 Bio1\n9   1.20     2 Bio1\n10  1.24     2 Bio1\n11  1.47     2 Bio1\n12  1.35     2 Bio1\n13  1.36     3 Bio2\n14  1.33     3 Bio2\n15  1.27     3 Bio2\n16  1.50     3 Bio2\n17  1.37     3 Bio2\n18  1.14     3 Bio2\n19  1.64     4 Chm1\n20  1.41     4 Chm1\n21  1.30     4 Chm1\n22  1.46     4 Chm1\n23  1.31     4 Chm1\n24  1.37     4 Chm1\n25  1.45     5 Chm2\n26  1.34     5 Chm2\n27  1.61     5 Chm2\n28  1.54     5 Chm2\n29  1.40     5 Chm2\n30  1.66     5 Chm2\n\n\nSummary statistics are shown below.\n\n\n agent sample_mean sample_sd sample_size type\n     1        1.17     0.120           6 None\n     2        1.29     0.127           6 Bio1\n     3        1.33     0.120           6 Bio2\n     4        1.42     0.125           6 Chm1\n     5        1.50     0.127           6 Chm2\n\n\n\n\nThe data set is saved in the Python data frame data_weed.\n\nimport pandas as pd\n# Load the data from a CSV file\ndata_weed = pd.read_csv(\"./data/data_weed.csv\")\ndata_weed\n\n     yield  agent  type\n0   1.0480      1   NaN\n1   1.0896      1   NaN\n2   1.3151      1   NaN\n3   1.1275      1   NaN\n4   1.1349      1   NaN\n5   1.3348      1   NaN\n6   1.3659      2  Bio1\n7   1.1238      2  Bio1\n8   1.2049      2  Bio1\n9   1.2387      2  Bio1\n10  1.4730      2  Bio1\n11  1.3517      2  Bio1\n12  1.3621      3  Bio2\n13  1.3342      3  Bio2\n14  1.2703      3  Bio2\n15  1.4950      3  Bio2\n16  1.3714      3  Bio2\n17  1.1350      3  Bio2\n18  1.6369      4  Chm1\n19  1.4142      4  Chm1\n20  1.3014      4  Chm1\n21  1.4625      4  Chm1\n22  1.3093      4  Chm1\n23  1.3657      4  Chm1\n24  1.4532      5  Chm2\n25  1.3362      5  Chm2\n26  1.6145      5  Chm2\n27  1.5390      5  Chm2\n28  1.3967      5  Chm2\n29  1.6603      5  Chm2\n\n\n\n\n agent sample_mean sample_sd sample_size type\n     1        1.17     0.120           6 None\n     2        1.29     0.127           6 Bio1\n     3        1.33     0.120           6 Bio2\n     4        1.42     0.125           6 Chm1\n     5        1.50     0.127           6 Chm2\n\n\n\n\n\n\n\n\n\n\n\nif there is a difference between the agents\nwhich agent provides the best yield\n\n\n\n\n\nR\nPython\n\n\n\n\nlibrary(car)\n(levene_test &lt;- leveneTest(yield ~ agent, data = data_weed))\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  4    0.12   0.97\n      25               \n\n# Bartlett test is adapted for normally distributed data.\n(bartlett_test &lt;- bartlett.test(yield ~ agent, data = data_weed))\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  yield by agent\nBartlett's K-squared = 0.03, df = 4, p-value = 1\n\n# The Fligner-Killeen test is most robust against departures from normality. Use it when there are outliers.\n(fligner_test &lt;- fligner.test(yield ~ agent, data = data_weed))\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  yield by agent\nFligner-Killeen:med chi-squared = 0.4, df = 4, p-value = 1\n\n\nThen we check the normality assumption \\(H_0:\\) Data are generated from a normal distribution for each type of weed agents. The qqplot looks good.\n\nlibrary(car)\nqqPlot(yield ~ type, data = data_weed, layout=c(1, 5))\n\n\n\n\n\n\n\n\n\n\nimport scipy.stats as stats\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Levene's Test\n# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.levene.html\nstats.levene(data_weed['yield'][data_weed['agent'] == 1],\n             data_weed['yield'][data_weed['agent'] == 2],\n             data_weed['yield'][data_weed['agent'] == 3],\n             data_weed['yield'][data_weed['agent'] == 4],\n             data_weed['yield'][data_weed['agent'] == 5])\n\nLeveneResult(statistic=0.11984660886763815, pvalue=0.9741358089178117)\n\n# Bartlett's Test \n# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.bartlett.html\nstats.bartlett(data_weed['yield'][data_weed['agent'] == 1],\n               data_weed['yield'][data_weed['agent'] == 2],\n               data_weed['yield'][data_weed['agent'] == 3],\n               data_weed['yield'][data_weed['agent'] == 4],\n               data_weed['yield'][data_weed['agent'] == 5])\n\nBartlettResult(statistic=0.02859647293854971, pvalue=0.99989874938744)\n\n# Fligner-Killeen Test\n# https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.fligner.html\nstats.fligner(data_weed['yield'][data_weed['agent'] == 1],\n              data_weed['yield'][data_weed['agent'] == 2],\n              data_weed['yield'][data_weed['agent'] == 3],\n              data_weed['yield'][data_weed['agent'] == 4],\n              data_weed['yield'][data_weed['agent'] == 5])\n\nFlignerResult(statistic=0.4334833327653211, pvalue=0.9796448746915967)\n\n\nThen we check the normality assumption \\(H_0:\\) Data are generated from a normal distribution for each type of weed agents. The qqplot looks good.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\begin{align}\n&H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_5\\\\\n&H_1: \\mu_i \\ne \\mu_j \\text{ for some pairs } (i, j)\n\\end{align}\\) \n\n\n\nR\nPython\n\n\n\n\nanova(lm(yield ~ agent, data = data_weed))\n\nAnalysis of Variance Table\n\nResponse: yield\n          Df Sum Sq Mean Sq F value Pr(&gt;F)   \nagent      4  0.365  0.0912    5.96 0.0016 **\nResiduals 25  0.383  0.0153                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n## need to rename yield. need to figure out why\ndata_weed.rename(columns={'yield': 'yield_value'}, inplace=True)\nsm.stats.anova_lm(ols('yield_value ~ C(agent)', data=data_weed).fit())\n\n            df    sum_sq   mean_sq         F    PR(&gt;F)\nC(agent)   4.0  0.364675  0.091169  5.958351  0.001647\nResidual  25.0  0.382525  0.015301       NaN       NaN\n\n\n\n\n\n\n\n\n\n \\(\\begin{align}\n&H_0^{ij}: \\mu_i = \\mu_j\\\\\n&H_1^{ij}: \\mu_i \\ne \\mu_j\n\\end{align}\\) \n\n\n\n\nR\nPython",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Multiple Comparison*</span>"
    ]
  },
  {
    "objectID": "model-multicomp.html#familywise-error-rate-fwer-and-bonferroni-correction",
    "href": "model-multicomp.html#familywise-error-rate-fwer-and-bonferroni-correction",
    "title": "25¬† Multiple Comparison*",
    "section": "\n25.2 Familywise Error Rate (FWER) and Bonferroni Correction",
    "text": "25.2 Familywise Error Rate (FWER) and Bonferroni Correction\n\n25.2.1 Familywise Error Rate (FWER)\nInstead of using Type I error rate \\(\\alpha\\) for each single 2-sample test, we should use a different type of error rate. We need to control the so-called familywise error rate (FWER) denoted as \\(\\alpha_F=P(\\text{Falsely reject at least one hypotheses}) \\le \\alpha_0\\), \\(F\\) for Familywise. This is a probability of falsely rejecting at least one hypothesis of all the hypotheses considered. We hope \\(\\alpha_F\\) is less than or equal to some small value say \\(\\alpha_0\\), just like we hope the Type I error rate of each individual test \\(\\alpha &lt; 0.05\\).\nThe idea is that when we are doing multiple comparisons, we view all the comparisons, here in the weed yield example, all 10 2-sample tests, as a whole single problem like one single family, and we are controlling the falsely rejection rate for that one family, not the falsely rejection rate for every single family member, a 2 sample test in the family.\nIf there are \\(m\\) (independent) hypotheses each tested at significance level \\(\\alpha_I\\), then \\(\\alpha_F = 1 - (1 - \\alpha_I)^m\\). So the larger \\(m\\) is, the higher \\(\\alpha_F\\) is. To achieve a desired level of \\(\\alpha_F\\), which is typically as small as individual \\(\\alpha_I\\), we might lose power of true discovery because to make \\(\\alpha_F\\) small, that is, we decrease the probability of falsely rejecting a \\(H_0\\), we tend to be more conservative to reject \\(H_0\\), and we make it harder to reject \\(H_0\\). But when doing so, we may miss some false \\(H_0\\) that should have been rejected.\n\n\n\n\n\n\nAs shown in the figure below, if \\(\\alpha_I = 0.1\\), then \\(\\alpha_F \\approx 1\\) when \\(m\\) is more than 40, meaning that there will definitely be a false discovery in the study. If \\(\\alpha_I = 0.05\\), then the probability of committing a false discovery is higher than 80%! In practice, \\(m\\) can be 10 thousands or even millions large. Then \\(\\alpha_F\\) is basically one, and there are lots of false rejections of \\(H_0\\) in the study, resulting in misleading scientific conclusions. We need to find a way to control or adjust \\(\\alpha_F\\) to a much smaller level.\n\n\n\n\n\n\n\n\n\n\n25.2.2 Bonferroni Correction\nWe will discuss four methods: Bonferroni, Fisher‚Äôs LSD (Least Significant Difference), Tukey‚Äôs HSD (Honestly Significant Difference), and Dunnett‚Äôs method. With \\(H_0^{ij}: \\mu_i = \\mu_j\\) and \\(H_1^{ij}: \\mu_i \\ne \\mu_j\\), in all of the methods we reject \\(H_0^{ij}\\) if \\(|\\overline{y}_i - \\overline{y}_j| &gt; C\\) for some value \\(C\\).\nThe idea is that when \\(\\mu_i \\ne \\mu_j\\), it‚Äôs more likely to have large \\(|\\overline{y}_i - \\overline{y}_j|\\). When \\(|\\overline{y}_i - \\overline{y}_j|\\) is so large that is over some determined threshold \\(C\\), we conclude that \\(\\mu_i \\ne \\mu_j\\). Think about the 2-sample pooled \\(t\\)-test. The test statistic is \\(\\small t_{test} = \\frac{\\overline{y}_1 - \\overline{y}_2}{\\sqrt{s_p^2 \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)}}\\), and we reject \\(H_0\\) if \\(|t_{test}| &gt; t_{\\alpha/2, \\, n_1+n_2-2}\\). Therefore, we reject \\(H_0\\) if\n\\[|\\overline{y}_1 - \\overline{y}_2| &gt; t_{\\alpha/2, \\, n_1+n_2-2} \\sqrt{s_p^2 \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)}.\\]\nThe problem is the threshold \\(t_{\\alpha/2, \\, n_1+n_2-2} \\sqrt{s_p^2 \\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)}\\) is not that good when lots of tests being considered simultaneously. The multiple comparison methods try to provide better critical value and estimate of variance, so that \\(\\alpha_F\\) is properly controlled. We will look into the pros and cons for these methods later.\n\nThe first method is Bonferroni correction. The Bonferroni inequality provides a method for selecting \\(\\alpha_I\\) so that \\(\\alpha_F\\) is bounded below a specific value. The Bonferroni inequality provides a rough upper bound for \\(\\alpha_F\\) that \\[ \\alpha_F = 1 - (1 - \\alpha_I)^m \\le (\\alpha_I^0 + \\dots + \\alpha_I^m).\\] When each of the \\(m\\) tests has the same rate \\(\\alpha_I\\), \\[ \\alpha_F \\le m\\alpha_I.\\]\nIf there are \\(m\\) hypotheses, and we want \\(\\alpha_F \\le \\alpha_0 = 0.05\\), then we can test individual hypothesis at \\(\\alpha_I = \\frac{\\alpha_0}{m} = \\frac{0.05}{m}\\), and then use a standard method such as \\(t\\)-test. This will guarantee that \\[\\alpha_F \\le m\\alpha_I=m\\left(\\frac{\\alpha_0}{m}\\right) = m\\left(\\frac{0.05}{m}\\right) = 0.05.\\]\nHowever, the problem with this approach is that if \\(m\\) is large, \\(\\alpha_I = \\frac{0.05}{m}\\) will be very small, and the chance of rejecting \\(H_0\\) will be small. This correction procedure is very conservative with respect to \\(\\alpha_F\\) and the power of Bonferroni correction method is very poor.\n\nIn the Bonferroni method, we use test statistic \\(t_{ij} = \\dfrac{|\\overline{y}_i - \\overline{y}_j|}{\\sqrt{\\hat{\\sigma}^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}\\) with Type I error rate of \\(\\alpha_I = \\frac{\\alpha}{m}\\), \\(m\\) is the number of hypotheses.\nThe value \\(\\hat{\\sigma}^2 = s_W^2 = MSW = \\frac{SSW}{df_{W}}\\) from ANOVA is the pooled variance (pooled by total \\(t\\) samples) that estimates the common variance \\(\\sigma^2\\), where \\(df_{W} = n_1+\\dots +n_t - t\\). We then reject \\(H_0\\) or say that pair \\((\\mu_i, \\mu_j)\\) are statistically discernibly different if \\[\\color{blue}{\\boxed{|\\overline{y}_i - \\overline{y}_j| &gt; t_{\\frac{\\alpha}{2m}, df_{W}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}}\\]\nLook at the formula carefully. The differences between Bonferroni and the unadjusted 2-sample pooled \\(t\\)-test are\n\nChange the critical value from \\(t_{\\frac{\\alpha}{2}, \\, n_i+n_j-2}\\) to \\(t_{\\frac{\\alpha/m}{2}, \\, n_1+\\dots +n_t - t}\\)\n\nChange the pooled variance from \\(s_p^2\\) pooled by the \\(i\\)-th and \\(j\\)-th samples only to \\(\\hat{\\sigma}^2 = s_W^2\\) pooled by all \\(t\\) samples\n\nIn Bonferroni method, we make the individual error rate smaller, and we consider all groups when estimating \\(\\sigma^2\\). The estimate \\(\\hat{\\sigma}^2\\) is generally better than \\(s_p^2\\) because it averages over more sample variances together, and it makes the estimate less varied and closer to the true value of \\(\\sigma^2\\) if the truth is that all populations have the common \\(\\sigma^2\\). Think about it. \\(\\hat{\\sigma}^2\\) uses more information from all samples, but \\(s_p^2\\) only considers the \\(i\\)th and \\(j\\)th groups.\n\n Example of Multiple Comparison: Bonferroni \n\n\n agent sample_mean sample_sd sample_size type\n     1        1.17     0.120           6 None\n     2        1.29     0.127           6 Bio1\n     3        1.33     0.120           6 Bio2\n     4        1.42     0.125           6 Chm1\n     5        1.50     0.127           6 Chm2\n\n\nBack to the weed yield example, and see the pair comparison result using the Bonferroni method. We have 5 different weed agents. One is control group, no agent (Agent 1), 2 biological agents (Agent 2 and 3), and 2 chemical agents (Agent 4 and 5).\nWe know \\(n_i\\) and \\(s_i\\), so we can get the followings:\n\n\\(SSW = \\sum_{i}(n_i-1)s_i^2 = 0.3825\\), \\(df_{W} = 25\\).\n\\(s_W^2 = MSW = \\hat{\\sigma}^2 = \\frac{SSW}{df_{W}} = \\frac{0.3825}{25} = 0.0153\\).\n\nAlso, set \\(\\alpha = 0.05\\), and \\(m = 10\\), so\n\n\n\\(\\frac{\\alpha}{2m} = \\frac{0.05}{2\\cdot 10} = 0.0025\\), and \\(t_{\\frac{\\alpha}{2m}=0.0025, \\, df_W=25} = 3.078\\)\n\n\nTherefore, using the Bonferroni method, we conclude \\(\\mu_i - \\mu_j \\ne 0\\) if \\(|\\overline{y}_i - \\overline{y}_j| &gt; 3.078\\sqrt{0.0153\\left(\\frac{1}{6}+ \\frac{1}{6}\\right)} = 0.2198.\\)\nHere lists comparisons between Agent 1 (control) with Agent 2 to 5.\n\nAgent 1 v.s. 5: \\(|1.500 ‚Äì1.175| = 0.325 &gt; 0.2198\\)\nAgent 1 v.s. 4: \\(|1.415 ‚Äì1.175| = 0.240 &gt; 0.2198\\)\nAgent 1 v.s. 3: \\(|1.328 ‚Äì1.175| = 0.153 &lt; 0.2198\\)\n\nAgent 1 v.s. 2: \\(|1.293 ‚Äì1.175| = 0.118 &lt; 0.2198\\)\n\n\nWe can see that Chm2 (Agent 5) and Chm1 (Agent 4) are different from Control (Agent 1), but Bio1 and Bio2 are not significantly different from the control group.\n\n\\(\\color{blue}{|\\overline{y}_i - \\overline{y}_j| &gt; t_{\\frac{\\alpha}{2m}, df_{W}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}\\)\n\n\nR\nPython\n\n\n\nLet me first show how I do the Bonferroni correction using R step by step.\n\ny_bar &lt;- c(1.175, 1.293, 1.328, 1.415, 1.500)\n## all pairs\n(y_bar_pair &lt;- combn(y_bar, 2))  \n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,] 1.18 1.18 1.18 1.18 1.29 1.29 1.29 1.33 1.33  1.42\n[2,] 1.29 1.33 1.42 1.50 1.33 1.42 1.50 1.42 1.50  1.50\n\n## all pair distances |y_bar_i - y_bar_j|\ny_bar_dist &lt;- apply(y_bar_pair, 2, dist)\ndd &lt;- t(combn(1:5, 2))\nnames(y_bar_dist) &lt;- paste(dd[, 1], dd[, 2], sep = \" vs \")\ny_bar_dist\n\n1 vs 2 1 vs 3 1 vs 4 1 vs 5 2 vs 3 2 vs 4 2 vs 5 3 vs 4 3 vs 5 4 vs 5 \n 0.118  0.153  0.240  0.325  0.035  0.122  0.207  0.087  0.172  0.085 \n\n\n\n## set values\nalpha &lt;- 0.05; m &lt;- choose(5, 2); n &lt;- 6\n\n## perform anova\naov_res &lt;- aov(yield ~ agent, data = data_weed) \n\n## extract df_W\n(df &lt;- df.residual(aov_res)) \n\n[1] 25\n\n## extract s_W\n(sw &lt;- sigma(aov_res) )\n\n[1] 0.124\n\n## critical value of bonferroni\n(t_bon &lt;- qt(p = alpha / (2 * m), df = df, lower.tail = FALSE))\n\n[1] 3.08\n\n## margin of error\n(E_bon &lt;- t_bon * sw * sqrt((1/n + 1/n)))\n\n[1] 0.22\n\n## decision\n(y_bar_dist &gt; E_bon) \n\n1 vs 2 1 vs 3 1 vs 4 1 vs 5 2 vs 3 2 vs 4 2 vs 5 3 vs 4 3 vs 5 4 vs 5 \n FALSE  FALSE   TRUE   TRUE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a shortcut to do Bonferroni correction for pairwise comparisons. We can use the function pairwise.t.test(). If we want to do 2-sample pooled \\(t\\) test, argument pool.sd should be FALSE because this argument is for the pooled variance pooled by all groups combined, not group \\(i\\) and \\(j\\). The p.adjust.method is none because we do not control or adjust FWER. The var.equal should be TRUE, so that the pooled \\(t\\) test is used. Otherwise, it will perform 2 sample \\(t\\) test with non-equal variances.\nThe output is a matrix including p-values of all pair comparison testings. We can tell which pairs are significant by comparing their p-value with the significance level \\(\\alpha\\).\nIf we want to do Bonferroni correction, use p.adjust.method = \"bonferroni\". Again, in Bonferroni, (1, 4) and (1, 5) are statistically significantly different. In traditional 2 sample \\(t\\) test, significant results include (1, 4), (1, 5), (2, 5), and (3, 5).\n\n## individual 2-sample pooled t-test but not pooled by all groups\npairwise.t.test(data_weed$yield, data_weed$agent, pool.sd = FALSE,\n                p.adjust.method = \"none\", var.equal = TRUE) \n\n\n    Pairwise comparisons using t tests with non-pooled SD \n\ndata:  data_weed$yield and data_weed$agent \n\n  1     2     3     4    \n2 0.129 -     -     -    \n3 0.052 0.634 -     -    \n4 0.007 0.124 0.246 -    \n5 0.001 0.018 0.036 0.269\n\nP value adjustment method: none \n\n## Bonferroni correction\npairwise.t.test(data_weed$yield, data_weed$agent, \n                p.adjust.method = \"bonferroni\") \n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  data_weed$yield and data_weed$agent \n\n  1     2     3     4    \n2 1.000 -     -     -    \n3 0.421 1.000 -     -    \n4 0.025 1.000 1.000 -    \n5 0.001 0.077 0.237 1.000\n\nP value adjustment method: bonferroni \n\n\n\n\nLet me first show how I do the Bonferroni correction using Python step by step.\n\nimport itertools\n# Given data\ny_bar = np.array([1.175, 1.293, 1.328, 1.415, 1.500])\n\n# All pairs\ny_bar_pair = np.array(list(itertools.combinations(y_bar, 2)))\ny_bar_pair\n\narray([[1.175, 1.293],\n       [1.175, 1.328],\n       [1.175, 1.415],\n       [1.175, 1.5  ],\n       [1.293, 1.328],\n       [1.293, 1.415],\n       [1.293, 1.5  ],\n       [1.328, 1.415],\n       [1.328, 1.5  ],\n       [1.415, 1.5  ]])\n\n# All pair distances |y_bar_i - y_bar_j|\ny_bar_dist = np.abs(y_bar_pair[:, 0] - y_bar_pair[:, 1])\ny_bar_dist\n\narray([0.118, 0.153, 0.24 , 0.325, 0.035, 0.122, 0.207, 0.087, 0.172,\n       0.085])\n\n# Combination indices (for naming)\ndd = np.array(list(itertools.combinations(range(1, 6), 2)))\n\n# Create names for the distances\ny_bar_dist_names = [f\"{i} vs {j}\" for i, j in dd]\n\n# Display the distances with names\ny_bar_dist_dict = dict(zip(y_bar_dist_names, y_bar_dist))\ny_bar_dist_dict\n\n{'1 vs 2': 0.11799999999999988, '1 vs 3': 0.15300000000000002, '1 vs 4': 0.24, '1 vs 5': 0.32499999999999996, '2 vs 3': 0.03500000000000014, '2 vs 4': 0.12200000000000011, '2 vs 5': 0.20700000000000007, '3 vs 4': 0.08699999999999997, '3 vs 5': 0.17199999999999993, '4 vs 5': 0.08499999999999996}\n\n\n\nfrom scipy.stats import t\nimport math\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\n# Set values\nalpha = 0.05\nm = math.comb(5, 2)  # Equivalent to choose(5, 2)\nn = 6\n\n# Assuming 'data_weed' is your DataFrame and 'yield' and 'agent' are column names\n# Perform ANOVA\ndata_weed['agent'] = data_weed['agent'].astype(str)\nmodel_fit = ols('yield_value ~ agent', data=data_weed).fit()\nanova_table = sm.stats.anova_lm(model_fit)\n\n# Extract df_W (degrees of freedom) and s_W (residual standard deviation)\ndf = model_fit.df_resid\ndf\n\n25.0\n\nsw = np.sqrt(anova_table['mean_sq'].iloc[1])\nsw\n\n0.12369717161951066\n\n# Critical value for Bonferroni correction\nt_bon = t.ppf(1 - alpha / (2 * m), df)\nt_bon\n\n3.078199460536119\n\n# Margin of error\nE_bon = t_bon * sw * np.sqrt((1/n + 1/n))\nE_bon\n\n0.2198345252258888\n\n# Assuming y_bar_dist was already calculated (from previous code)\n# Decision\ny_bar_dist &gt; E_bon\n\narray([False, False,  True,  True, False, False, False, False, False,\n       False])\n\n\nIf we don‚Äôt do any correction, we can do pairwise t test for all pairs directly using ttest_ind() that conducts t-test for the means of two independent samples. It is brutal force, and there should be a better way to conduct all pairwise two-sample pooled t test separately by default.\n\nfrom scipy import stats\ntest12 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '1', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '2', 'yield_value'])\ntest12.pvalue\n\n0.12937022290977485\n\ntest13 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '1', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '3', 'yield_value'])\ntest13.pvalue\n\n0.051684324302770854\n\ntest14 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '1', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '4', 'yield_value'])\ntest14.pvalue                  \n\n0.0068949544337039625\n\ntest15 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '1', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '5', 'yield_value'])\ntest15.pvalue                       \n\n0.0010440332236669495\n\ntest23 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '2', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '3', 'yield_value']) \ntest23.pvalue\n\n0.6335961134332914\n\ntest24 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '2', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '4', 'yield_value'])\ntest24.pvalue\n\n0.12419123394098548\n\ntest25 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '2', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '5', 'yield_value'])\ntest25.pvalue\n\n0.01786227373198035\n\ntest34 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '3', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '4', 'yield_value'])\ntest34.pvalue\n\n0.24606446614093755\n\ntest35 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '3', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '5', 'yield_value'])\ntest35.pvalue\n\n0.0360878773096397\n\ntest45 = stats.ttest_ind(data_weed.loc[data_weed['agent'] == '4', 'yield_value'],\n                         data_weed.loc[data_weed['agent'] == '5', 'yield_value'])\ntest45.pvalue\n\n0.2687698246654925\n\n\nThere is a shortcut to do Bonferroni correction for pairwise comparisons. We can use the method t_test_pairwise(). The output saved in result_frame is a matrix including p-values of all pair comparison testings. We can tell which pairs are significant by comparing their p-value with the significance level \\(\\alpha\\).\nIf we want to do Bonferroni correction, use method='bonferroni'. Again, in Bonferroni, (1, 4) and (1, 5) are statistically significantly different. In traditional 2 sample \\(t\\) test, significant results include (1, 4), (1, 5), (2, 5), and (3, 5).\n\npairwise_t_results = model_fit.t_test_pairwise(term_name='agent', method='bonferroni', alpha=0.05)\npairwise_t_results.result_frame\n\n         coef   std err  ...  pvalue-bonferroni  reject-bonferroni\n2-1  0.118017  0.071417  ...           1.000000              False\n3-1  0.153017  0.071417  ...           0.420707              False\n4-1  0.240017  0.071417  ...           0.024990               True\n5-1  0.325000  0.071417  ...           0.001194               True\n3-2  0.035000  0.071417  ...           1.000000              False\n4-2  0.122000  0.071417  ...           0.999726              False\n5-2  0.206983  0.071417  ...           0.076997              False\n4-3  0.087000  0.071417  ...           1.000000              False\n5-3  0.171983  0.071417  ...           0.237341              False\n5-4  0.084983  0.071417  ...           1.000000              False\n\n[10 rows x 8 columns]\n\n\n\n\n\nFor each pair, we can draw its confidence interval for \\(\\mu_i - \\mu_j\\). The CI formula is \\(\\color{blue}{(\\overline{y}_i - \\overline{y}_j) \\pm t_{\\frac{\\alpha}{2m}, df_{W}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}\\). The yellow dot represents \\(\\overline{y}_i - \\overline{y}_j\\). Centered at \\(\\overline{y}_i - \\overline{y}_j\\), we plus and minus the margin of error \\(t_{\\frac{\\alpha}{2m}, df_{W}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}\\) to get the upper and lower bound of the CI. If the CI does not include zero, we conclude that \\(\\mu_i\\) and \\(\\mu_j\\) are not equal. Here again \\(\\mu_1\\) and \\(\\mu_4\\) are different. \\(\\mu_1\\) and \\(\\mu_5\\) are different too.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Multiple Comparison*</span>"
    ]
  },
  {
    "objectID": "model-multicomp.html#fishers-lsd-least-significant-difference",
    "href": "model-multicomp.html#fishers-lsd-least-significant-difference",
    "title": "25¬† Multiple Comparison*",
    "section": "\n25.3 Fisher‚Äôs LSD (Least Significant Difference)",
    "text": "25.3 Fisher‚Äôs LSD (Least Significant Difference)\nThe second method introduced here is Fisher‚Äôs LSD, short for Least Significant Difference. We reject \\(H_0\\) or say that pair \\((\\mu_i, \\mu_j)\\) are significantly different if \\[\\color{blue}{\\boxed{|\\overline{y}_i - \\overline{y}_j| &gt; t_{\\frac{\\alpha}{2}, df_{W}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}}\\] Unlike the Bonferroni method, Fisher‚Äôs LSD does NOT correct for multiple comparisons or FWER. The diffrence between Bonferroni and Fisher‚Äôs LSD is that Bonferroni uses \\(\\alpha/m\\) but Fisher‚Äôs LSD uses \\(\\alpha\\) itself. The Fisher‚Äôs LSD test is basically a set of individual \\(t\\)-tests. Two-sample pooled \\(t\\)-tests compute the pooled variance from only the two groups being compared, while the Fisher‚Äôs LSD test computes the pooled variance from all the groups (which gains power). Also, the degrees fo freedom in the \\(t\\) critical value is \\(df_W\\), not \\(n_i+n_j-2\\) in the 2 sample pooled \\(t\\) test.\n\n\n\n\n Example of Multiple Comparison: Fisher‚Äôs LSD \n\n\n agent sample_mean sample_sd sample_size type\n     1        1.17     0.120           6 None\n     2        1.29     0.127           6 Bio1\n     3        1.33     0.120           6 Bio2\n     4        1.42     0.125           6 Chm1\n     5        1.50     0.127           6 Chm2\n\n\nThe process of doing Fisher‚Äôs LSD is pretty similar to doing Bonferroni.\n\n\\(\\frac{\\alpha}{2}  = 0.025\\), and \\(t_{0.025, 25} = 2.0595\\)\nWe conclude \\(\\mu_i - \\mu_j \\ne 0\\) if \\(|\\overline{y}_i - \\overline{y}_j| &gt; 2.0595\\sqrt{0.0153\\left(\\frac{1}{6}+ \\frac{1}{6}\\right)} = 0.1471\\)\nAgent 1 v.s. 5: \\(|1.500 ‚Äì1.175| = 0.325 &gt; 0.1471\\)\nAgent 1 v.s. 4: \\(|1.415 ‚Äì1.175| = 0.240 &gt; 0.1471\\)\nAgent 1 v.s. 3: \\(|1.328 ‚Äì1.175| = 0.153 &gt; 0.1471\\)\nAgent 1 v.s. 2: \\(|1.293 ‚Äì1.175| = 0.118 &lt; 0.1471\\)\nAgent 2 v.s. 5: \\(|1.500 ‚Äì1.293| = 0.207 &gt; 0.1471\\)\nAgent 2 v.s. 4: \\(|1.415 ‚Äì1.293| = 0.112 &lt; 0.1471\\)\nAgent 2 v.s. 3: \\(|1.328 ‚Äì1.293| = 0.035 &lt; 0.1471\\)\nAgent 3, 4 and 5 are different from control. Agent 2 and 5 are different.\n\n\n\\(\\color{blue}{|\\overline{y}_i - \\overline{y}_j| &gt; t_{\\frac{\\alpha}{2}, df_{W}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}\\)\n\n\n\nR\nPython\n\n\n\n\ndf\n\n[1] 25\n\nsw ^ 2\n\n[1] 0.0153\n\n(t_lsd &lt;- qt(p = alpha/2, df = df, lower.tail = FALSE))\n\n[1] 2.06\n\n(E_lsd &lt;- t_lsd * sw * sqrt(1/n + 1/n))\n\n[1] 0.147\n\n(y_bar_dist &gt; E_lsd)\n\n1 vs 2 1 vs 3 1 vs 4 1 vs 5 2 vs 3 2 vs 4 2 vs 5 3 vs 4 3 vs 5 4 vs 5 \n FALSE   TRUE   TRUE   TRUE  FALSE  FALSE   TRUE  FALSE   TRUE  FALSE \n\n\n\n\n\nIf we use pairwise.t.test(), pool.sd = TRUE because we do use the pooled variance \\(s_W^2\\) that is pooled by all 5 samples. p.adjust.method = \"none\" because Fisher‚Äôs LSD does not adjust the Type I error rate. It still uses \\(\\alpha\\) in every individual testing. The p-value result for every pair comparison shows that if \\(\\alpha = 0.05\\) is the threshold or cutoff value, (1, 3), (1, 4), (1, 5), (2, 5), and (3, 5) pair comparisons reject their \\(H_0\\) and conclude that there is a mean yield difference between their groups.\n\npairwise.t.test(data_weed$yield, data_weed$agent, pool.sd = TRUE, \n                p.adjust.method = \"none\") ## Fisher‚Äôs LSD\n\n\n    Pairwise comparisons using t tests with pooled SD \n\ndata:  data_weed$yield and data_weed$agent \n\n  1     2     3     4    \n2 0.111 -     -     -    \n3 0.042 0.628 -     -    \n4 0.002 0.100 0.235 -    \n5 1e-04 0.008 0.024 0.245\n\nP value adjustment method: none \n\n\n\n\n\ndf\n\n25.0\n\nsw ** 2\n\n0.015300990266666674\n\nt_lsd = t.ppf(1 - alpha/2, df)\nt_lsd\n\n2.059538552753294\n\nE_lsd = t_lsd * sw * np.sqrt(1/n + 1/n)\nE_lsd\n\n0.14708523139370552\n\ny_bar_dist &gt; E_lsd\n\narray([False,  True,  True,  True, False, False,  True, False,  True,\n       False])\n\n\nThe following should be replaced with LSD method which is still not found in any Python package. Need to update this.\n\npairwise_t_results = model_fit.t_test_pairwise(term_name='agent', method='bonferroni', alpha=0.05)\npairwise_t_results.result_frame\n\n\n\n\n\nThe Fisher‚Äôs LSD confidence interval for \\(\\mu_i - \\mu_j\\) is \\(\\color{blue}{(\\overline{y}_i - \\overline{y}_j) \\pm t_{\\frac{\\alpha}{2}, df_{W}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}\\). The CI for (1, 3), (1, 4), (1, 5), (2, 5), and (3, 5) do not include zero, leading to the same conclusion as the previous p-value method.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Multiple Comparison*</span>"
    ]
  },
  {
    "objectID": "model-multicomp.html#tukeys-hsd-honestly-significant-difference",
    "href": "model-multicomp.html#tukeys-hsd-honestly-significant-difference",
    "title": "25¬† Multiple Comparison*",
    "section": "\n25.4 Tukey‚Äôs HSD (honestly significant difference)",
    "text": "25.4 Tukey‚Äôs HSD (honestly significant difference)\n\n\n\nThe next method is the Tukey‚Äôs HSD method. We reject \\(H_0\\) or say that pair \\((\\mu_i, \\mu_j)\\) are significantly different if \\[\\color{blue}{\\boxed{|\\overline{y}_i - \\overline{y}_j| &gt; \\frac{q_{\\alpha}(t, df_{W})}{\\sqrt{2}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}}\\]\nHere, the square root term or the standard error keeps the same as before, but we don‚Äôt use a \\(t\\) critical value anymore. Instead, we use a different critical value \\(\\frac{q_{\\alpha}(t, df_{W})}{\\sqrt{2}}\\), where \\(q_{\\alpha}(k, v)\\) is the upper-tail critical value of the studentized range distribution for comparing \\(k\\) different populations with degrees of freedom \\(v\\). Because we assume there are \\(t\\) populations and df is \\(df_W\\), I put them in the formula.\nStudentized range distribution is similar to \\(t\\) distribution when comparing 2 population means, but they are different things, and studentized range distribution actually controls FWER in some way but \\(t\\) distribution does not. No need to worry about the details of this distribution. We just use it to apply the Tukey‚Äôs HSD method.\nIn R, we can use qtukey(p = alpha, nmeans = k, df = v, lower.tail = FALSE) to obtain \\(q_{\\alpha}(k, v)\\) with specified \\(\\alpha\\), number of means to be compared \\(k\\), and the degrees of freedom \\(v\\). Notice that the \\(\\alpha\\) here is the familywise error rate \\(\\alpha_F\\). We directly specify and control \\(\\alpha_F\\) level, and the probability of falsely rejecting at least one pairwise tests is \\(\\alpha_F\\).\n\n\n\n\n\n\n Example of Multiple Comparison: Tukey‚Äôs HSD \n\n\n agent sample_mean sample_sd sample_size type\n     1        1.17     0.120           6 None\n     2        1.29     0.127           6 Bio1\n     3        1.33     0.120           6 Bio2\n     4        1.42     0.125           6 Chm1\n     5        1.50     0.127           6 Chm2\n\n\n\n\\(k = 5\\), \\(df_{W} = 25\\), \\(n = 6\\). Therefore, \\(q_{\\alpha}(k, v) = q_{0.05}(5, 25) = 4.153\\).\nWe conclude \\(\\mu_i - \\mu_j \\ne 0\\) if \\(|\\overline{y}_i - \\overline{y}_j| &gt; 4.153\\sqrt{\\frac{0.0153}{6}} = 0.2097\\)\nAgent 1 v.s. 5: \\(|1.500 ‚Äì1.175| = 0.325 &gt; 0.2097\\)\nAgent 1 v.s. 4: \\(|1.415 ‚Äì1.175| = 0.240 &gt; 0.2097\\)\nAgent 1 v.s. 3: \\(|1.328 ‚Äì1.175| = 0.153 &lt; 0.2097\\)\nAgent 1 v.s. 2: \\(|1.293 ‚Äì1.175| = 0.118 &lt; 0.2097\\)\nAgent 2 v.s. 5: \\(|1.500 ‚Äì1.293| = 0.207 &lt; 0.2097\\)\nAgent 2 v.s. 4: \\(|1.415 ‚Äì1.293| = 0.112 &lt; 0.2097\\)\nAgent 2 v.s. 3: \\(|1.328 ‚Äì1.293| = 0.035 &lt; 0.2097\\)\nAgents 4 and 5 are different from control. No Significant difference between agents 3, 4, 5 and 2.\n\n\n\\(\\color{blue}{|\\overline{y}_i - \\overline{y}_j| &gt; q_{\\alpha}(k, df_{W})\\sqrt{\\frac{s_W^2}{n}}}\\)\n\n\n\nR\nPython\n\n\n\n\ndf\n\n[1] 25\n\nsw^2\n\n[1] 0.0153\n\n(q_tukey &lt;- qtukey(p = alpha, nmeans = 5, df = df, lower.tail = FALSE))\n\n[1] 4.15\n\n(E_tukey &lt;- q_tukey * sw * sqrt(1 / n))\n\n[1] 0.21\n\n(y_bar_dist &gt; E_tukey)\n\n1 vs 2 1 vs 3 1 vs 4 1 vs 5 2 vs 3 2 vs 4 2 vs 5 3 vs 4 3 vs 5 4 vs 5 \n FALSE  FALSE   TRUE   TRUE  FALSE  FALSE  FALSE  FALSE  FALSE  FALSE \n\n\n\n\n\nA faster way to conduct Tukey‚Äôs HSD test is to use R function TukeyHSD(). After doing ANOVA, we can save the result, say res_aov, then to implement Tukey method, just put the ANOVA result in the function TukeyHSD(). It will provide all information for each pair comparison.\n\nres_aov &lt;- aov(yield ~ agent, data = data_weed)\n(tukey_test &lt;- TukeyHSD(res_aov))\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = yield ~ agent, data = data_weed)\n\n$agent\n     diff      lwr   upr p adj\n2-1 0.118 -0.09172 0.328 0.480\n3-1 0.153 -0.05672 0.363 0.234\n4-1 0.240  0.03028 0.450 0.019\n5-1 0.325  0.11526 0.535 0.001\n3-2 0.035 -0.17474 0.245 0.988\n4-2 0.122 -0.08774 0.332 0.447\n5-2 0.207 -0.00276 0.417 0.054\n4-3 0.087 -0.12274 0.297 0.741\n5-3 0.172 -0.03776 0.382 0.146\n5-4 0.085 -0.12476 0.295 0.757\n\n\n\n\n\n\n\n\n\nIn the output, diff is \\(\\overline{y}_i - \\overline{y}_j\\) where \\(i\\) and \\(j\\) are shown in the first column. lwr and upr are the lower bound and upper bound of the CI for \\(\\mu_i - \\mu_j\\), respectively. The adjusted p-values are shown in the p-adj column. With \\(\\alpha_F = 0.05\\), (4, 1) and (5, 1) are statistically discernible. Their corresponding lower bound is greater than zero, meaning that it‚Äôs CI for \\(\\mu_i - \\mu_j\\) does not cover zero, so \\(\\mu_i - \\mu_j\\) is statistically significantly different from zero.\n\n\nThe studentized range distribution in Python is implemented by the function studentized_range, again from scipy.stats.\n\ndf\n\n25.0\n\nsw**2\n\n0.015300990266666674\n\nfrom scipy.stats import studentized_range\nq_tukey = studentized_range.ppf(q=1-alpha, k=5, df=df)\nq_tukey\n\n4.1533633299635335\n\nE_tukey = q_tukey * sw * np.sqrt(1 / n)\nE_tukey\n\n0.2097413545569429\n\ny_bar_dist &gt; E_tukey\n\narray([False, False,  True,  True, False, False, False, False, False,\n       False])\n\n\nA faster way to conduct Tukey‚Äôs HSD test is to use Python function pairwise_tukeyhsd() from statsmodels.stats.multicomp. It will provide all information for each pair comparison.\n\nfrom statsmodels.stats.multicomp import pairwise_tukeyhsd\ntukey = pairwise_tukeyhsd(endog=data_weed['yield_value'], \n                          groups=data_weed['agent'], alpha=0.05)\nprint(tukey)\n\nMultiple Comparison of Means - Tukey HSD, FWER=0.05\n===================================================\ngroup1 group2 meandiff p-adj   lower  upper  reject\n---------------------------------------------------\n     1      2    0.118 0.4797 -0.0917 0.3278  False\n     1      3    0.153 0.2342 -0.0567 0.3628  False\n     1      4     0.24 0.0192  0.0303 0.4498   True\n     1      5    0.325  0.001  0.1153 0.5347   True\n     2      3    0.035 0.9876 -0.1747 0.2447  False\n     2      4    0.122 0.4472 -0.0877 0.3317  False\n     2      5    0.207 0.0543 -0.0028 0.4167  False\n     3      4    0.087 0.7411 -0.1227 0.2967  False\n     3      5    0.172 0.1461 -0.0378 0.3817  False\n     4      5    0.085 0.7569 -0.1248 0.2947  False\n---------------------------------------------------\n\n\nIn the output, meandiff is \\(\\overline{y}_i - \\overline{y}_j\\) where \\(i\\) and \\(j\\) are shown in the first column. lower and upper are the lower bound and upper bound of the CI for \\(\\mu_i - \\mu_j\\), respectively. The last column represents p-values. With \\(\\alpha_F = 0.05\\), (4, 1) and (5, 1) are statistically discernible. Their corresponding lower bound is greater than zero, meaning that it‚Äôs CI for \\(\\mu_i - \\mu_j\\) does not cover zero, so \\(\\mu_i - \\mu_j\\) is statistically significantly different from zero.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Multiple Comparison*</span>"
    ]
  },
  {
    "objectID": "model-multicomp.html#dunnetts-method",
    "href": "model-multicomp.html#dunnetts-method",
    "title": "25¬† Multiple Comparison*",
    "section": "\n25.5 Dunnett‚Äôs Method",
    "text": "25.5 Dunnett‚Äôs Method\nThe last but not least method introduced here is Dunnett‚Äôs method. We reject \\(H_0\\) or say that pair \\((\\mu_i, \\mu_j)\\) are significantly different if\n\\[\\color{blue}{\\boxed{|\\overline{y}_i - \\overline{y}_j| &gt; d_{\\alpha}(t - 1, df_{W})\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}}\\]\nWell we still have the same standard error, the square root term, but we use another critical value \\(d_{\\alpha}(t-1, df_W)\\). In general, for \\(d_{\\alpha}(k, df)\\), the value of Dunnett‚Äôs test distribution, \\(k\\) is the number of non-control groups or factors.\nNote that this method is only used for comparing with a control or reference group. In our example, Agent 1 is the control group. So the parameter \\(k\\) is \\(t-1\\), where \\(t\\) is the number of agents, which is 5. Again, the \\(\\alpha\\) here is the familywise error rate \\(\\alpha_F\\). We directly specify and control \\(\\alpha_F\\) level, and the probability of falsely rejecting at least one pairwise tests is \\(\\alpha_F\\).\nConsider 2 treatment groups and one control group. If you only want to compare the 2 treatment groups with the control group, and do not want to compare the 2 treatment groups to each other, the Dunnett‚Äôs test is preferred.\n\n\n\n\n\n\n\n\n Example of Multiple Comparison: Dunnett‚Äôs Method \n\n\\(t - 1 = 4\\), \\(df_{W} = 25\\), \\(n = 6\\). Therefore, \\(d_{\\alpha}(t - 1,  df_{W}) = d_{0.05}(4, 25) = 2.61\\).\nWe conclude \\(\\mu_i - \\mu_j \\ne 0\\) if \\(|\\overline{y}_i - \\overline{y}_j| &gt; 2.61\\sqrt{\\frac{2(0.0153)}{6}} = 0.1862\\)\nAgent 1 v.s. 5: \\(|1.500 ‚Äì1.175| = 0.325 &gt; 0.1862\\)\nAgent 1 v.s. 4: \\(|1.415 ‚Äì1.175| = 0.240 &gt; 0.1862\\)\nAgent 1 v.s. 3: \\(|1.328 ‚Äì1.175| = 0.153 &lt; 0.1862\\)\nAgent 1 v.s. 2: \\(|1.293 ‚Äì1.175| = 0.118 &lt; 0.1862\\)\nOnly the chemical agents 4 and 5 are different from control. No biological agents are different from control.\n\n\\(\\color{blue}{|\\overline{y}_i - \\overline{y}_j| &gt; d_{\\alpha}(t - 1, df_{W})\\sqrt{\\frac{2s_W^2}{n}}}\\)\n\n\nR\nPython\n\n\n\nThere is no R built-in function that computes \\(d_{\\alpha}\\) critical value. But we can install the package nCDunnett, and use the function qNCDun(). NC stands for Non-Central. The argument p is the probability to the left tail and keep in mind that the subscript \\(\\alpha\\) in the critical value is the probability to the right tail. So we get to put \\(1-\\alpha\\) there. nu is the degrees of freedom. rho here we use 0.5, meaning that all sample sizes are equal. The sample size in each group is 6, and we need to specify it 4 times corresponding to the number of non-control groups. delta is the non-centrality parameter that needs to be specified 4 times too. Here we assume they are all zero, i.e., the test uses the central distribution.\n\nlibrary(nCDunnett)\n(d_dun &lt;- qNCDun(p = 1 - alpha, nu = df, \n                rho = c(0.5, 0.5, 0.5, 0.5), \n                delta = c(0, 0, 0, 0), two.sided = TRUE))\n\n[1] 2.61\n\n(E_dun &lt;- d_dun * sw * sqrt(2 / n))\n\n[1] 0.186\n\nhead(y_bar_dist &gt; E_dun, 4)\n\n1 vs 2 1 vs 3 1 vs 4 1 vs 5 \n FALSE  FALSE   TRUE   TRUE \n\n\nDescTools.DunnettTest() performs Dunnett‚Äôs test that does multiple comparisons of means against a control group.\n\nlibrary(DescTools)\n\n\nAttaching package: 'DescTools'\n\n\nThe following object is masked from 'package:car':\n\n    Recode\n\n\nThe following object is masked from 'package:openintro':\n\n    cards\n\nDunnettTest(x=data_weed$yield, g=data_weed$agent)\n\n\n  Dunnett's test for comparing several treatments with a control :  \n    95% family-wise confidence level\n\n$`1`\n     diff  lwr.ci upr.ci    pval    \n2-1 0.118 -0.0682  0.304 0.30737    \n3-1 0.153 -0.0332  0.339 0.12931    \n4-1 0.240  0.0538  0.426 0.00845 ** \n5-1 0.325  0.1387  0.511 0.00043 ***\n\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nThere is no functions in Python that calculates probabilities and quantiles of the Dunnett‚Äôs distribution.\nscipy.stats.dunnett() performs Dunnett‚Äôs test that does multiple comparisons of means against a control group.\n\nfrom scipy.stats import dunnett\ndunnett_res = dunnett(data_weed[data_weed['agent'] == '2']['yield_value'], \n                      data_weed[data_weed['agent'] == '3']['yield_value'],\n                      data_weed[data_weed['agent'] == '4']['yield_value'],\n                      data_weed[data_weed['agent'] == '5']['yield_value'],\n                      control=data_weed[data_weed['agent'] == '1']['yield_value'],\n                      alternative='two-sided')\nprint(dunnett_res)\n\nDunnett's test (95.0% Confidence Interval)\nComparison               Statistic  p-value  Lower CI  Upper CI\n (Sample 0 - Control)      1.653     0.307    -0.068     0.304\n (Sample 1 - Control)      2.143     0.129    -0.033     0.339\n (Sample 2 - Control)      3.361     0.009     0.054     0.426\n (Sample 3 - Control)      4.551     0.000     0.139     0.511\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe result shows (1, 4) and (1, 5) have different means. The figure below shows the CIs for \\(\\mu_1 - \\mu_j, j = 2, 3, 4, 5\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAll comparisons illustrated are two-sided tests. One-sided tests can be applied too.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Multiple Comparison*</span>"
    ]
  },
  {
    "objectID": "model-multicomp.html#comparison-of-methods",
    "href": "model-multicomp.html#comparison-of-methods",
    "title": "25¬† Multiple Comparison*",
    "section": "\n25.6 Comparison of Methods",
    "text": "25.6 Comparison of Methods\nTo sum up, from the example, we had \\(\\mu_i - \\mu_j \\ne 0\\) if\n\nBonferroni: \\(|\\overline{y}_i - \\overline{y}_j| &gt; 0.2198\\)\nFisher‚Äôs LSD: \\(|\\overline{y}_i - \\overline{y}_j| &gt; 0.1471\\)\nTukey HSD: \\(|\\overline{y}_i - \\overline{y}_j| &gt; 0.2097\\)\nDunnett: \\(|\\overline{y}_i - \\overline{y}_j| &gt; 0.1862\\)\nFisher‚Äôs LSD does not control FWER, but all others do have FWER 0.05.\nBonferroni is the most conservative method and has the poorest discovery rate. You see its threshold value is the largest, making it the most difficult one to reject \\(H_0\\), and hence results in a lower power or lower discovery rate.\nDiscovery rate for the Tukey‚Äôs is better than Bonferroni, but not as good as Dunnett‚Äôs. However, Dunnett‚Äôs used only to compare with the control.\nIf the objective is to compare only with a control, then Dunnett‚Äôs is more powerful among three. Otherwise, Tukey‚Äôs is more powerful than Bonferroni.\nAlthough Bonferroni is not very powerful, it does have advantage that it can be used in any situation (whether it is one factor or multi-factor analyses) whenever there are multiple hypotheses.\nAll methods here are based on the condition that the data are random samples from normal distributions with equal variances. There are nonparametric multiple comparison procedures out there, such as Kruskal‚ÄìWallis.\nHere we focus on balanced data. When there are large differences in the number of samples, care should be taken when selecting multiple comparison procedures.\n\n\n\nThere is another method called Scheffe‚Äôs method. Unlike methods specifically for pairwise comparisons such as Tukey HSD, Scheffe‚Äôs method can generally investigate all possible contrasts of the means. In other words, it considers not only th pairwise comparisons but any possible combinations of the means, for example \\(c_1\\mu_1 + c_2\\mu_2 + \\cdots + c_t\\mu_t\\).\nFor pairwise comparisons, we reject \\(H_0\\) or say that pair \\((\\mu_i, \\mu_j)\\) are significantly different if\n\\[\\color{blue}{\\boxed{|\\overline{y}_i - \\overline{y}_j| &gt; \\sqrt{(t-1) F_{\\alpha, t-1, N-t}}\\sqrt{s_W^2\\left( \\frac{1}{n_i} + \\frac{1}{n_j}\\right)}}}\\] For pairwise comparisons, the Scheff√© test has lower statistical power than other tests, even more conservative than Bonferroni method. To increase power, the Scheff√© method needs larger sample size.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>25</span>¬† <span class='chapter-title'>Multiple Comparison*</span>"
    ]
  },
  {
    "objectID": "model-twoway.html",
    "href": "model-twoway.html",
    "title": "26¬† Two-Way ANOVA*",
    "section": "",
    "text": "26.1 Interaction Effect\nWhen two factors are included in a model, understanding their interaction effect is crucial. Let‚Äôs consider an example. Suppose we have two factors: the Amount of coffee consumed (either one cup or three cups a day) and When the coffee is consumed (either in the morning or in the evening). We want to examine how these two factors affect the length of time it takes to complete a specific task. By analyzing the interaction between these factors, we can determine whether the effect of coffee consumption on task completion time varies depending on the time of day the coffee is consumed. This insight is key to understanding the combined influence of the factors on the response variable.\nFigure¬†26.1 shows some possible interaction plots. The top row shows two interaction plots without an interaction effect. The bottom two, on the other hand, are the plots with some interaction effect.\nFigure¬†26.1: Source:\nThe topleft plots shows, when amount of coffee consumed increases from 1 cup to 3 cups, the time to complete a task is shortened and the amount of time shortened when the coffee is consumed in the morning is the same as the amount of time shortened when the coffee is consumed in the evening. Both shorten 20 minutes. With the morning, the time is shortened from 40 to 20. With the evening, the time is shortened from 30 to 10. Therefore, the effect of amount of coffee on the time to complete a task does not depend on the other factor: when the coffee is consumed, whether it is morning or evening.\nThe top-left plot illustrates a scenario where increasing the amount of coffee consumed from 1 cup to 3 cups results in a shorter time to complete a task. Importantly, the reduction in time is the same regardless of when the coffee is consumed‚Äîwhether in the morning or in the evening. Specifically, the task completion time decreases by 20 minutes in both cases: from 40 to 20 minutes in the morning, and from 30 to 10 minutes in the evening. This indicates that the effect of the amount of coffee consumed on the time to complete a task is independent of the time of consumption‚Äîmorning or evening. There is no interaction effect between the two factors in this scenario.\nWe know that increasing the amount of coffee consumed from 1 cup to 3 cups shortens the time to complete a task. Now, if we compare drinking coffee in the evening to drinking it in the morning, we find that the evening saves 10 minutes. This is illustrated by the red line (representing ‚Äúevening‚Äù) consistently being 10 minutes below the blue line (representing ‚Äúmorning‚Äù) across all levels of coffee consumption.\nTo assess the effect of the factor ‚ÄúAmount of coffee,‚Äù we simply pick a line (either red or blue) and observe how the response value (time to complete the task) changes as we move along the x-axis (number of cups of coffee). To evaluate the effect of the other factor, ‚ÄúDrinking coffee time,‚Äù we examine the vertical distance between the two lines. In this case, the distance between the lines is consistently 10 minutes, regardless of whether we measure it at 1 cup or 3 cups of coffee. This consistency indicates that the time of day when coffee is consumed has a uniform effect on task completion time, independent of the amount of coffee consumed.\nNow, let‚Äôs examine the top-right plot. This plot also indicates no interaction effect. The time required to complete the task is the same whether a person drinks 1 cup of coffee or 3 cups of coffee. In other words, the amount of coffee consumed has no effect on the task completion time. The response variable (task completion time) is independent of the amount of coffee consumed.\nMoreover, this lack of effect holds true regardless of whether the coffee is consumed in the morning or the evening, as shown by the two parallel lines in the plot. The parallel lines indicate that there is no interaction effect between the time of coffee consumption and the amount of coffee consumed. The task completion time remains unchanged across different amounts of coffee, and this relationship is consistent in both the morning and evening scenarios.\nThe time required to complete a task is 20 minutes shorter when drinking coffee in the morning, as indicated by the vertical distance of 20 between the two lines. This 20-minute difference is consistent regardless of the number of coffee cups consumed, reinforcing that there is no interaction between the amount of coffee consumed and the time of consumption. The lack of interaction means that the effect of drinking coffee in the morning versus the evening remains constant, irrespective of the quantity of coffee.\nAt the bottom left, the effect of the amount of coffee on the time needed to complete a task does depend on when the coffee is consumed, indicating a clear interaction effect. Specifically, if coffee is consumed in the evening, drinking more coffee actually worsens working efficiency, requiring 30 minutes longer to complete a task. In contrast, the effect of consuming coffee in the morning is entirely different‚Äîdrinking more coffee (3 cups) reduces the time needed to complete a task, thereby improving working efficiency.\nThis demonstrates that there is an interaction effect between the amount of coffee consumed and the time of consumption. In this scenario, it is not possible to draw a conclusion about the effect of one factor (e.g., the amount of coffee) on the response variable (task completion time) without considering the other factor (e.g., time of consumption). Different times of consumption lead to entirely different conclusions about the impact of the amount of coffee consumed, underscoring the importance of accounting for both factors in the analysis.\nThe bottom right plot also demonstrates an interaction effect. When coffee is consumed in the evening, the amount of coffee has no effect on the time needed to complete a task, as indicated by the horizontal line segment. However, when coffee is consumed in the morning, the situation changes‚Äîdrinking 3 cups of coffee reduces the time required to complete the task.\nThis difference in outcomes based on the time of coffee consumption indicates that there is indeed an interaction between the two factors. The impact of the amount of coffee consumed on task completion time varies depending on whether the coffee is consumed in the morning or evening, highlighting the importance of considering both factors together when analyzing their effects.\nIn summary, in practice, if the line segments are approximately parallel, it suggests that there is little to no interaction effect between the factors. Conversely, if the two lines are not parallel, this indicates the presence of some interaction effect, as illustrated in the bottom of the figure. The degree of interaction is reflected in how much the lines diverge from being parallel.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Two-Way ANOVA*</span>"
    ]
  },
  {
    "objectID": "model-twoway.html#two-way-anova-model",
    "href": "model-twoway.html#two-way-anova-model",
    "title": "26¬† Two-Way ANOVA*",
    "section": "\n26.2 Two-Way ANOVA Model*",
    "text": "26.2 Two-Way ANOVA Model*\nSuppose we have two factors A and B, having a and b levels respectively. The two-way ANOVA model includes not only the two treatment effects but also their interaction term:\n\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij} + \\epsilon_{ijk}, ~ i = 1, \\dots, a, ~ j = 1, \\dots, b, ~k = 1, \\dots, n.\\]\n\n\\(y_{ijk}\\): the response value from the \\(k\\)-th experimental unit receiving the \\(i\\)-th level of factor A and the \\(j\\)-th level of factor B.\n\\(\\mu\\): overall mean, an unknown constant.\n\\(\\alpha_i\\): effect due to the \\(i\\)-th level of factor A, an unknown constant, \\(i = 1, \\dots, a\\).\n\\(\\beta_j\\): effect due to the \\(j\\)-th level of factor B, an unknown constant, \\(j = 1, \\dots, b\\).\n\\(\\gamma_{ij}\\): interaction effect of the \\(i\\)-th level of factor A with the \\(j\\)-th level of factor B, an unknown constant.\n\\(\\epsilon_{ijk}\\): random error\n\\(\\mu_{ij}\\): the \\((i, j)\\) treatment mean, which is equal to \\(\\mu + \\alpha_i + \\beta_j + \\gamma_{ij}\\). Note that the \\((i, j)\\) treatment mean level is affected by which level of the two factors it measures, due to not only their main effect but also their interaction effect.\nAssumptions: \\(\\epsilon_{ijk}\\)s are independent and \\(\\epsilon_{ijk} \\sim N(0, \\sigma^2)\\). Like one-way ANOVA, all errors are from the same normal distribution with mean zero and some common variance \\(\\sigma^2\\), no matter what they come from which group or which data replicate.\n\n\n\n\n\n\n\nNote\n\n\n\nFor simplicity, here we only consider equal sample size case. That is, the number of replications is the same for all ab treatments. The model and its analysis results may not be generalized to the case when the number of replications are different in different sub groups.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Two-Way ANOVA*</span>"
    ]
  },
  {
    "objectID": "model-twoway.html#two-way-anova-model-example",
    "href": "model-twoway.html#two-way-anova-model-example",
    "title": "26¬† Two-Way ANOVA*",
    "section": "\n26.3 Two-Way ANOVA Model Example",
    "text": "26.3 Two-Way ANOVA Model Example\n\n\\(y_{ijk} = \\mu + \\alpha_i + \\beta_j + \\gamma_{ij} + \\epsilon_{ijk}\\), \\(i = 1, \\dots, a\\), \\(j = 1, \\dots, b\\), \\(k = 1, \\dots, n\\).\n Factor A: amount of coffee.  We have 2 levels and \\(a = 2\\). Let 1 = 1 cup and 2 = 3 cups.\n Factor B: drinking time.  We have 2 levels and \\(b = 2\\). Let 1 = morning and 2 = evening.\n Response variable: time required to complete a task. \n\\(y_{124}\\): the \\(4\\)-th response value (\\(k = 4\\)) that 1 cup of coffee is consumed (\\(i = 1\\)) and the drinking time is evening (\\(j = 2\\)).\n\\(\\alpha_1\\): effect of amount of coffee when 1 cup (\\(i = 1\\)) is consumed.\n\\(\\beta_2\\): effect of drinking time when coffee is consumed in the evening (\\(j = 2\\)).\n\\(\\gamma_{12}\\): interaction effect of having 1 cup of coffee in the evening \\((i = 1\\) and \\(j = 2)\\).\n\\(\\mu_{12}\\): the \\((1, 2)\\) treatment mean, which is equal to \\(\\mu + \\alpha_1 + \\beta_2 + \\gamma_{12}\\). The mean time to complete a task when having 1 cup of coffee in the evening.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Two-Way ANOVA*</span>"
    ]
  },
  {
    "objectID": "model-twoway.html#testings-of-two-way-anova",
    "href": "model-twoway.html#testings-of-two-way-anova",
    "title": "26¬† Two-Way ANOVA*",
    "section": "\n26.4 Testings of Two-Way ANOVA",
    "text": "26.4 Testings of Two-Way ANOVA\nIn two-way ANOVA, we need to first test the existence of interaction effect. That is\n\n \\(\\begin{align}\n  &H_0: \\gamma_{ij} = 0 \\text{ for all } i, j\\\\\n  &H_1: \\text{As least one }\\gamma_{ij} \\ne 0\n  \\end{align}\\) \n\nIf the interaction is significant, then the main factor effects of A or B may have no interpretation at all. The main effects must be interpreted very carefully and considered level by level. As we did explaining interaction plots, we should check the effect of factors case by case. There is no really main effect in interaction exists.\nIf the interaction is not significant, we can test the main effects of the two factors.\n\n \\(\\begin{align}\n  &H_0: \\alpha_{i} = 0 \\text{ for all } i\\\\\n  &H_1: \\text{As least one } \\alpha_{i} \\ne 0\n  \\end{align}\\) \n \\(\\begin{align}\n  &H_0: \\beta_{j} = 0 \\text{ for all } j\\\\\n  &H_1: \\text{As least one } \\beta_{j} \\ne 0\n  \\end{align}\\)",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Two-Way ANOVA*</span>"
    ]
  },
  {
    "objectID": "model-twoway.html#two-way-anova-table",
    "href": "model-twoway.html#two-way-anova-table",
    "title": "26¬† Two-Way ANOVA*",
    "section": "\n26.5 Two-Way ANOVA Table",
    "text": "26.5 Two-Way ANOVA Table\n\n\n\n\n\n\n\nFigure¬†26.2: Source: Table 14.15 in SMD\n\n\n\n\n\nFigure¬†26.2 shows the two-way ANOVA table. The source of variation of the response is decomposed into four parts, the main effect of factor A \\((SSA)\\), the main effect of factor B \\((SSB)\\), the interaction effect \\((SSAB)\\), and the unexplained random error variation \\((SSE)\\). Therefore we have\n\\[TSS = SSA + SSB + SSAB + SSE\\] The corresponding degrees of freedom and mean squares are shown in the table.\n\nTo test interaction effect, use \\(F_{test} = \\frac{MSAB}{MSE}\\) with \\(df_1 = (a-1)(b-1)\\) and \\(df_2 = ab(n-1)\\).\nTo test main effect of factor A, use \\(F_{test} = \\frac{MSA}{MSE}\\) with \\(df_1 = a-1\\) and \\(df_2 = ab(n-1)\\).\nTo test main effect of factor B, use \\(F_{test} = \\frac{MSB}{MSE}\\) with \\(df_1 = b-1\\) and \\(df_2 = ab(n-1)\\).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Two-Way ANOVA*</span>"
    ]
  },
  {
    "objectID": "model-twoway.html#example-of-two-way-anova-example-14.6-in-smd",
    "href": "model-twoway.html#example-of-two-way-anova-example-14.6-in-smd",
    "title": "26¬† Two-Way ANOVA*",
    "section": "\n26.6 Example of Two-way ANOVA (Example 14.6 in SMD)",
    "text": "26.6 Example of Two-way ANOVA (Example 14.6 in SMD)\nDetermine the effects of 4 different pesticides on the yield of fruit from 3 different varieties of a citrus tree. Eight trees from each variety were randomly selected from an orchard. The 4 pesticides were then randomly assigned to 2 trees of each variety, and applications were made according to recommended levels.\n\n\nR\nPython\n\n\n\nThe data are saved in the R data frame data_pesticide.\n\ndata_pesticide\n\n   Yield Variety Pesticide\n1     49       1         1\n2     50       1         2\n3     43       1         3\n4     53       1         4\n5     39       1         1\n6     55       1         2\n7     38       1         3\n8     48       1         4\n9     55       2         1\n10    67       2         2\n11    53       2         3\n12    85       2         4\n13    41       2         1\n14    58       2         2\n15    42       2         3\n16    73       2         4\n17    66       3         1\n18    85       3         2\n19    69       3         3\n20    85       3         4\n21    68       3         1\n22    92       3         2\n23    62       3         3\n24    99       3         4\n\nstr(data_pesticide)\n\n'data.frame':   24 obs. of  3 variables:\n $ Yield    : num  49 50 43 53 39 55 38 48 55 67 ...\n $ Variety  : Factor w/ 3 levels \"1\",\"2\",\"3\": 1 1 1 1 1 1 1 1 2 2 ...\n $ Pesticide: Factor w/ 4 levels \"1\",\"2\",\"3\",\"4\": 1 2 3 4 1 2 3 4 1 2 ...\n\n\nNote that the two factors need to be a vector of type character or factor. To fit a two-way ANOVA, we can use aov() function. In the formula, we use Pesticide * Variety to tell aov() to do two-way ANOVA with interaction. This is equivalent to Pesticide + Variety + Pesticide:Variety, where each variable name represents their main effect, and Pesticide:Variety is the specific interaction term. If we put Yield ~ Pesticide + Variety in the formula, a two-way ANOVA without interaction is performed.\nThe ANOVA table is generated by summary(twoway_fit). We first look at the interaction term. The p-value is greater than 0.05 and we have insufficient evidence to indicate an interaction between pesticide levels and variety of trees levels.\n\ntwoway_fit &lt;- aov(formula = Yield ~ Pesticide * Variety, \n                  data = data_pesticide)\nsummary(twoway_fit)\n\n                  Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nPesticide          3   2227   742.5  17.556  0.00011 ***\nVariety            2   3996  1998.0  47.244 2.05e-06 ***\nPesticide:Variety  6    457    76.2   1.801  0.18168    \nResiduals         12    508    42.3                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nBecause interaction is not significant, we then can test the main effects of the 2 factors. The first row tests the main effect of Pesticide, and the second row tests the main effect of Variety. Both effects are significant. We conclude that there is a difference in mean yields among 4 pesticides and there is a difference in mean yields among 3 varieties of trees.\n\n\n\n\n\n\nLastly, we can use command interaction.plot() to generate the interaction plot or the profile plot.\n\ninteraction.plot(x.factor = data_pesticide$Pesticide, \n                 trace.factor = data_pesticide$Variety, \n                 response = data_pesticide$Yield, \n                 trace.label = \"Variety\",\n                 type = \"b\", col = 1:3, leg.bty = \"o\", leg.bg = \"beige\", lwd = 2,\n                 las = 1, pch = c(18, 24, 22), \n                 xlab = \"Pesticide\", ylab = \"Mean response\",\n                 main = \"Interaction Plot (Profile Plot)\")\n\n\n\n\n\n\n\n\n\nThe data are saved in the Python DataFrame data_pesticide.\n\ndata_pesticide\n\n    Yield  Variety  Pesticide\n0      49        1          1\n1      50        1          2\n2      43        1          3\n3      53        1          4\n4      39        1          1\n5      55        1          2\n6      38        1          3\n7      48        1          4\n8      55        2          1\n9      67        2          2\n10     53        2          3\n11     85        2          4\n12     41        2          1\n13     58        2          2\n14     42        2          3\n15     73        2          4\n16     66        3          1\n17     85        3          2\n18     69        3          3\n19     85        3          4\n20     68        3          1\n21     92        3          2\n22     62        3          3\n23     99        3          4\n\ndata_pesticide.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 24 entries, 0 to 23\nData columns (total 3 columns):\n #   Column     Non-Null Count  Dtype\n---  ------     --------------  -----\n 0   Yield      24 non-null     int64\n 1   Variety    24 non-null     int64\n 2   Pesticide  24 non-null     int64\ndtypes: int64(3)\nmemory usage: 708.0 bytes\n\n\n\ndata_pesticide['Pesticide'] = data_pesticide['Pesticide'].astype(str)\ndata_pesticide['Variety'] = data_pesticide['Variety'].astype(str)\n\nNote that the two factors need to be a vector of type string. To fit a two-way ANOVA, we can use ols() and anova_lm(). In the formula, we use Pesticide * Variety to tell ols() to do two-way ANOVA with interaction. This is equivalent to Pesticide + Variety + Pesticide:Variety, where each variable name represents their main effect, and Pesticide:Variety is the specific interaction term. If we put Yield ~ Pesticide + Variety in the formula, a two-way ANOVA without interaction is performed.\nThe ANOVA table is generated by sm.stats.anova_lm(twoway_fit). We first look at the interaction term. The p-value is greater than 0.05 and we have insufficient evidence to indicate an interaction between pesticide levels and variety of trees levels.\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\ntwoway_fit = ols('Yield ~ Pesticide * Variety', data=data_pesticide).fit()\nsm.stats.anova_lm(twoway_fit)\n\n                     df       sum_sq      mean_sq          F    PR(&gt;F)\nPesticide           3.0  2227.458333   742.486111  17.556322  0.000110\nVariety             2.0  3996.083333  1998.041667  47.244335  0.000002\nPesticide:Variety   6.0   456.916667    76.152778   1.800657  0.181684\nResidual           12.0   507.500000    42.291667        NaN       NaN\n\n\nBecause interaction is not significant, we then can test the main effects of the 2 factors. The first row tests the main effect of Pesticide, and the second row tests the main effect of Variety. Both effects are significant. We conclude that there is a difference in mean yields among 4 pesticides and there is a difference in mean yields among 3 varieties of trees.\nLastly, we can use command interaction_plot() from statsmodels.graphics.factorplots to generate the interaction plot or the profile plot.\n\nimport matplotlib.pyplot as plt\nfrom statsmodels.graphics.factorplots import interaction_plot\nfig = interaction_plot(\n    x=data_pesticide['Pesticide'],\n    trace=data_pesticide['Variety'],\n    response=data_pesticide['Yield'],\n)\nplt.title('Interaction Plot (Profile Plot)')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe three lines are more or less parallel with each other, and no significant interaction is out there. Although Variety 3 would have large response changes with different types of pesticide, especially compared to Variety 1, such difference is not significant enough for the model to conclusion that the interaction exists, given the samll sample size in each group.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>26</span>¬† <span class='chapter-title'>Two-Way ANOVA*</span>"
    ]
  },
  {
    "objectID": "model-reg.html",
    "href": "model-reg.html",
    "title": "27¬† Simple Linear Regression",
    "section": "",
    "text": "27.1 Correlation\nRelationship Between 2 Numerical Variables\nWe first define the linear correlation between two numerical variables. Depending on your research question, one may be classified as the explanatory variable and the other the response variable. However, in correlation, the two variables do not necessarily have such explanatory-response relationship, and they can be any numerical variables being interested. We will discuss the meaning of explanatory and response variable in the regression section.\nCan you provide an example that two variables are associated? Here I provide some examples.\nIn each item, the two variables are related each other in some way. Usually the taller a person is, the heavier he is. Note that such relationship is not deterministic, but describe a general trend. We could have a 6‚Äô6‚Äù guy with 198 lbs, and a 5‚Äô9‚Äù with 220 lbs, but this an individual case, not an overall pattern. This concept is important because it is represented in the calculation of correlation coefficient and linear regression model.\nScatterplots\nFigure¬†27.1: Examples of scatterplots\nThe most clear way to show the relationship between two numerical variables is to create a scatterplot. The overall pattern or relationship can be detected several ways in a scatterplot.\nAlthough scatterplots can show form, direction, and strength of the relationship, we often want to have a numerical measure that can quantify these properties. If the relationship is linear, we use linear correlation coefficient to quantify the direction and strength of the linear relationship.\nLinear Correlation Coefficient\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables \\(X\\) and \\(Y\\): \\[r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right) = \\frac{\\sum_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\overline{x})^2\\sum_{i=1}^n(y_i-\\overline{y})^2}}\\]\nThis is the sample coefficient because the statistic is calculated by the sample data. 1\nYou don‚Äôt need to memorize the formula, but it would be great if you can understand how the formula is related to the scatter plot, and why the formula can quantify the direction and strength of the linear relationship.\nFirst, the coefficient is between negative one and positive one, \\(-1 \\le r\\le 1\\). \\(r &gt; 0\\) means positive relationship, so the larger value of \\(X\\) is, the larger value of \\(Y\\) tends to be. If \\(r = 1\\), we have the strongest or perfect positive linear relationship. If we connect all the data points by a line segment, all line segments form a positively sloped straight line as shown in Figure¬†27.5.\nLet‚Äôs look at Figure¬†27.2 and see how the formula and the scatter plot are related. Since \\(s_x\\), \\(s_y\\), and \\(n\\) are positive, the sign of coefficient \\(r\\) is determined by the sum \\(\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\). When the two variables are positively correlated, there are more data points in the first and third quadrants, and less data points in the second and fourth quadrants, where the quadrants are separated by \\((\\overline{x}, \\overline{y})\\). Therefore, \\((x_i - \\overline{x})(y_i - \\overline{y}) &gt;0\\) in the first and third quadrants, and \\((x_i - \\overline{x})(y_i - \\overline{y}) &lt; 0\\) in the second and fourth quadrants. Since we have more positive terms of \\((x_i - \\overline{x})(y_i - \\overline{y})\\) the sum \\(\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y})\\), and the coefficient \\(r\\) is positive.\nFigure¬†27.2: Positive correlation between two variables.\nWhen \\(r &lt; 0\\), the variables have a negative relationship. The larger value of \\(X\\) is, the smaller value of \\(Y\\) tends to be. When \\(r = -1\\), we have the perfect or strongest negative linear relationship. Figure¬†27.3 illustrates the negative correlation between variables.\nFigure¬†27.3: Negative correlation between two variables\nWhen \\(r = 0\\), it means the two variables have no linear relationship. The scatter plot Figure¬†27.4 shows an example of no linear relationship of \\(X\\) and \\(Y\\). When \\(r = 0\\), \\(\\sum_{i=1}^n (x_i - \\overline{x})(y_i - \\overline{y}) = 0\\), and the number of data points in each quadrant would be similar. Be careful, it is possible that \\(r = 0\\) even the number is not the same.\nOne property of \\(r\\) is that it has no units of measurement, so scale changes do not affect \\(r\\). No matter what units of \\(X\\) and \\(Y\\) are, \\(r\\) is always between negative one and positive one. The reason is that \\(x\\) and \\(y\\) are normalized in the coefficient formula.\nLast but not least. It is possible that there is a strong relationship between two variables, but they still have \\(r = 0\\)! Remember that the correlation coefficient \\(r\\) measures the direction and and strength of the linear relationship. Yes, the linear relationship ONLY. If two variables don‚Äôt have any relationship, they are not linearly correlated, and their correlation coefficient is 0. However, if \\(r = 0\\), we can just say the two variables have no linear relationship, but they may be nonlinearly related, or associated in any other way.\nFigure¬†27.4: No correlation between two variables\nThe bottom row in Figure¬†27.5 shows several nonlinear \\(X\\)-\\(Y\\) relationships that do not have linear association at all. Next time you see \\(r = 0\\), don‚Äôt over interpret the \\(X\\)-\\(Y\\) relationship.\nFigure¬†27.5: Examples of relationships between two variables and their correlation coefficients. (https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg)",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#correlation",
    "href": "model-reg.html#correlation",
    "title": "27¬† Simple Linear Regression",
    "section": "",
    "text": "height and weight \n income and age \n SAT/ACT math score and verbal score \n amount of time spent studying for an exam and exam grade \n\n\n\n\n\n\n\n\n\n\nForm: As shown in Figure¬†27.1, the relationship between the two variables may be linear. Heavier cars tend to be less fuel efficient. The subjects in the data may be separated into two groups or clusters based on the value of the two variables. The volcano‚Äôs eruption duration and time waited are either small or large. Scatterplts can show other patterns such as quadratic, cubic or any other nonlinear relationships. In this chapter, we stay with the linear relationship.\nDirection: The variables can be positively associated or negatively associated, or not associated. In the scatterplot, the linear pattern is described by a line summarized by the data points in the plot. If the slope of the line is positive (negative), the two variables are positively (negatively) associated, meaning that one gets large as the other gets small. If the slope of the line is zero, the two variables have no association, and whether one variable‚Äôs value is large or small does not affect the other variable‚Äôs value.\nStrength: Strength is how close the data points lie to the line or nonlinear curve trend in the scatter plot. The variables‚Äô relationship is strong (weak) if the data points are pretty close (far away) to the line.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nIn R, we use the function cor() to calculate the correlation coefficient \\(r\\). We put the two variables‚Äô data in the first two arguments x and y. Here we use the built-in mtcars dataset, and calculate the correlation between the high miles per gallon (mpg) and car weight (wt). The coefficient is \\(r =\\) -0.87. Usually, when \\(|r| &gt; 0.7\\), the linear relationship is considered strong, and \\(|r| &lt; 0.3\\) for weak linear relationship.\n\n\n\nplot(x = mtcars$wt, y = mtcars$mpg, \n     main = \"MPG vs. Weight\", \n     xlab = \"Car Weight\", \n     ylab = \"Miles Per Gallon\", \n     pch = 16, col = 4, las = 1)\n\n\n\n\n\n\n\n\n\n\n\ncor(x = mtcars$wt,\n    y = mtcars$mpg)\n\n[1] -0.8676594\n\n\n\n\n\n\nSource: unsplash-sippakorn yamkasikorn\n\n\n\n\n\n\n\nIn Python, we use the function corr() to calculate the correlation coefficient \\(r\\) for two pd.Series. The syntax is like first_pd.Series.corr(second_pd.Series).\nHere we use the built-in mtcars dataset, and calculate the correlation between the high miles per gallon (mpg) and car weight (wt). The coefficient is \\(r =\\) -0.87. Usually, when \\(|r| &gt; 0.7\\), the linear relationship is considered strong, and \\(|r| &lt; 0.3\\) for weak linear relationship.\n\n\n\nimport matplotlib.pyplot as plt\n# Scatter plot of MPG vs. Weight\nplt.scatter(mtcars['wt'], mtcars['mpg'], \n            color='blue')\nplt.title('MPG vs. Weight')\nplt.xlabel('Car Weight')\nplt.ylabel('Miles Per Gallon')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nmtcars['wt'].corr(mtcars['mpg'])\n\n-0.8676593765172281\n\n\n\n\n\n\nSource: unsplash-sippakorn yamkasikorn",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#meaning-of-regression",
    "href": "model-reg.html#meaning-of-regression",
    "title": "27¬† Simple Linear Regression",
    "section": "\n27.2 Meaning of Regression",
    "text": "27.2 Meaning of Regression\n What is Regression? \nRegression models the relationship between one or more numerical/categorical response variables \\((Y)\\) and one or more numerical/categorical explanatory variables \\((X)\\). We use explanatory variables to explain the behavior of the response variables, or how the response variables response to the changes of the explanatory variables.\nExplanatory variables are also called independent variables, predictors, regressors, covariates, exogenous variables, and inputs. Response variables are also called dependent variables, targets, endogenous variables, outcome variables, outputs, and labels.\nThe followings are some examples.\n\nPredict  College GPA \\((Y)\\) using students‚Äô ACT/SAT score \\((X)\\) \nEstimate how much  Sales \\((Y)\\) increase as Advertising Expenditure \\((X)\\) increases $1000 dollars \nHow the  Crime Rate \\((Y)\\) in some county changes with the Median Household Income Level \\((X)\\) \n\nIn regression, we use a regression function, \\(f(X)\\), to describe how a response variable \\(Y\\), on average, changes as an explanatory variable \\(X\\) changes. In other words, the regression function captures the general pattern or relationship between \\(X\\) and \\(Y\\).\nLet‚Äôs first pretend we know what the true regression function \\(f(X)\\) is.\nFor a general regression model, the function needs not be linear, and could be of any type. Figure¬†27.6 is an example of a regression function \\(f(X)\\) which is log-shaped. The idea is that for each value of the predictor \\(x\\), the response value \\(y\\) is generated based on the value of \\(x\\) and the regression function \\(f(x)\\). However, while the \\(X\\)-\\(f(X)\\) relationship is deterministic, the \\(X\\)-\\(Y\\) relationship is not, even though the \\(y\\) value is pretty close to \\(f(x)\\) given some value of \\(x\\). That‚Äôs why if we collect sample data, we will have the two variable data \\((x_i, y_i)\\) as the blue points scattered around the function \\(f(x)\\). For any value of \\(x\\), the data generating mechanism for \\(y\\) takes the level of \\(x\\) into account through \\(f(x)\\), but also includes some other factors that affect the value of \\(y\\). As a result, \\(y \\ne f(x)\\) in general, and we have \\[y = f(x) + \\text{some value caused by factors other than } x.\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†27.6: Example of a Regression Function.\n\n\n\n\n\n\nFor example, if you want to use a student‚Äôs SAT/ACT Score (\\(X\\)) to predict his College GPA (\\(Y\\)), you can assume \\(Y\\) is affected by \\(X\\) through the regression function \\(f(X)\\). However, do you think SAT/ACT Score is the only factor that affects one‚Äôs college academic performance? Absolutely not. The regression function \\(f(X)\\) does not explain all the possible variation of \\(Y\\). Therefore, even the SAT/ACT Score is fixed at some level, an individual college GPA may be lower than the function value \\(f(x)\\) due to other factors such as less time spent on studying. How do we treat factors other than \\(X\\)? In regression, when \\(X\\) has been taken into account, we think these factors affect the value of \\(Y\\) in a random way with no significant pattern.\nAlthough \\(Y \\ne f(X)\\) in general, in regression we believe or make a assumption that the mean or expected value of \\(Y\\) at \\(X = x\\) is \\(f(x)\\), the regression function value at \\(X = x\\)! That is, mathematically \\(E(Y \\mid X=x) = \\mu_{Y \\mid X = x} = f(x)\\). Therefore, \\[Y_{\\mid X = x} = \\mu_{Y\\mid X = x} + \\text{some value caused by factors other than } x.\\]\nNow we have learned an important concept in regression: the regression function \\(f(x)\\) describes the general or average relationship between \\(X\\) and \\(Y\\), or the relationship between \\(X\\) and the mean of \\(Y\\), \\(\\mu_Y\\), not the relationship between \\(X\\) and individual \\(Y\\). In regression, we care about \\(f(x)\\) because it tells us how the response \\(Y\\) changes on average in response to the changes of \\(X\\). For the GPA example, \\(f(x)\\) enables us to know how SAT/ACT Score affects College GPA in general, regardless of other factors. We don‚Äôt care about individual variation or up and downs because those have nothing to do with \\(X\\), and do not help us capture the pattern or relationship. If for each value of SAT/ACT Score, we were able to repeatedly collect a College GPA, the effect of other factors on College GPA will be washed out when those College GPA values are averaged.\n\n Unknown Regression Function \nUnfortunately the true underlying relationship between \\(X\\) and the mean of \\(Y\\), the regression function \\(f(X)\\), is usually unknown to us although we are interested in it. In reality, what we have is the scatter plot like Figure¬†27.7, or the sample data \\((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\), the blue points. What statistics comes into play is that we try to uncover or estimate the function from our data, get to know how \\(Y\\) is generated at a given value of \\(X\\), and use the function to predict the value of \\(Y\\) given a value of \\(X\\).\n\n\n\n\n\n\n\nFigure¬†27.7: Data with an unknown regression function\n\n\n\n\n\n\n\n Simple Linear Regression \nA regression model can be very general, considering lots of variables simultaneously and modeling a complex nonlinear regression function. We learn how to walk before how to run. So let‚Äôs start with the very basic simple linear regression. The model is called ‚Äúsimple‚Äù because there is only one predictor \\(X\\) and one response variable \\(Y\\). The model is linear because the regression function used for predicting \\(Y\\) is a linear function, i.e., \\(f(x) = \\beta_0 + \\beta_1x\\). Therefore, we use a regression line in an X-Y plane to predict the value of \\(Y\\) for a given value of \\(X = x\\). Keep in mind that here the predictor \\(X\\) is assumed known and constant. It is a variable but not a random variable. However, the response \\(Y\\) is a random variable whose value depends on not only the value of \\(x\\) through \\(f(x)\\) but also a probability distribution. We will discuss it in more detail when we talk about the model.\n\n\n \n\n\nLet‚Äôs have a short math review. A linear function \\(y = f(x) = \\beta_0 + \\beta_1 x\\) represents a straight line.\n\n\\(\\beta_1\\) is the slope of the line. It is the amount by which \\(y\\) changes when \\(x\\) increases by one unit.\n\\(\\beta_0\\) is the intercept term which is the value of \\(y\\) when \\(x = 0\\).\n\nThe linearity assumption requires that \\(\\beta_1\\) does not change as \\(x\\) changes.\n\n\n\n\n\n\n\n\n\nFigure¬†27.8: Example of a regression line\n\n\n\n\n\n\n Sample Data: Relationship Between X and Y \nBack to statistics. Remember that we want to use our sample data to estimate the unknown regression function or regression line. Suppose our data set is the \\(x\\)-\\(y\\) pairs \\((x_i, y_i), i = 1, 2, \\dots, n\\). You can see in Figure¬†27.9 that simply connecting all data points does not form a straight line because again the value \\(y\\) depends on both the regression function and some other factors that are not related to \\(x\\). So \\(y_i \\ne \\beta_0+\\beta_1x_i\\) in general even if \\(f(x) = \\beta_0+\\beta_1x\\) is the true regression function faithfully representing the linear relationship.\nNow we need an additional variable to explain the deviations from the line and complete the modeling of data generating process. The deviation can be explained by a new variable \\(\\epsilon\\) that is the error term in regression. So the value \\(y\\) is the function value \\(\\beta_0 + \\beta_1x\\) plus the value of \\(\\epsilon\\):\n\\[y_i = \\beta_0+\\beta_1x_i + \\color{red}{\\epsilon_i}\\]\n\n\n\n\n\n\n\nFigure¬†27.9: Actual relationship between X and Y isn‚Äôt perfectly linear.\n\n\n\n\nWe learned that \\(Y\\) is a random variable whose value varies with some probability distribution. We have \\(y = \\beta_0+\\beta_1x + \\epsilon\\), but \\(f(x) = \\beta_0+\\beta_1x\\) is deterministic and constant given the value of \\(x\\) because \\(\\beta_0\\) and \\(\\beta_1\\) are fixed constants although their value may not be known to us. The randomness of \\(y\\) should come from the error term \\(\\epsilon\\).\n\n\n\n\n\n\nNote\n\n\n\nAs we learned in random variable and sampling distribution, before we collect the sample, \\(Y\\) (and \\(\\epsilon\\)) is a random variable. Once data are collected, we have realized value \\(y\\) (and realized \\(\\epsilon\\)). The notation of \\(\\epsilon\\) is a little abused because \\(\\epsilon\\) could stand for a random variable or a realized value. Be careful about that and make sure you know which one it represents in the content.\n\n\nWhen we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed to be drawn from a normal distribution (for inference purpose). Its value varies and will not be exactly equal to its mean, \\(\\mu_y\\). Figure¬†27.10 illustrates this idea. At any value of \\(x\\), there is an associated normal distribution centered at the function value \\(\\beta_0+\\beta_1x\\) or \\(\\mu_{Y\\mid X = x}\\). Then we treat the value \\(y\\) as a draw from the distribution. Of course due to the randomness of the distribution, \\(y\\) will not be exactly equal to \\(\\mu_{Y\\mid X = x}\\), but its value will be somewhat around the mean. If \\(\\beta_1 &gt; 0\\), \\(\\mu_{Y\\mid X = x}\\) gets large as \\(x\\) goes up. As a result, the realized values of \\(y\\) with all levels of \\(x\\) will also have a upward linear trend centered around \\(\\mu_{Y\\mid X = x}\\).\n\n\n\n\n\n\n\n\nFigure¬†27.10: The responses, y, follows a normal distribution. Source: Introduction to the Practice of Statistics.\n\n\n\n\nIf at every level of \\(x\\), we collect \\(y\\) value again, due to randomness, we are gonna get a different set of \\(y\\)s, as shown in Figure¬†27.11 as blue points. Although the two sets of \\(y\\)s (red and blue) are different, both are centered around the mean \\(\\mu_{Y\\mid X = x}\\), and embrace the same positive linear trend.\n\n\n\n\n\n\n\nFigure¬†27.11: Second sample. Source: Introduction to the Practice of Statistics.\n\n\n\n\nFinally let‚Äôs relate the figure to the equation \\(y = \\mu_{Y\\mid X = x} + \\epsilon\\). Either red or blue points, the value \\(y\\) is decomposed into two parts, \\(\\mu_{Y\\mid X = x}\\) and \\(\\epsilon\\). They share the same baseline mean level but add a different size of random error \\(\\epsilon\\). It is the error that causes such up and downs or ‚Äúnoises‚Äù around the mean. If we ignore or wash out these noises, and let \\(\\mu_{Y\\mid X = x}\\) speak up, the true underlying function describing the linear relationship will come up. The mean of \\(Y\\) and \\(X\\) form a straight line. This regression line is what we care about. In reality we don‚Äôt see it, but we assume it is there, and it determines the base level of \\(y\\). We use the sample data to estimate this unseen line. Hopefully our estimated one is close to the true unseen one.\n\n\n\n\n\n\n\n\nFigure¬†27.12: The regression line is formed from the mean of Y and X. Source: Introduction to the Practice of Statistics.\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe never know the true regression function. In linear regression, we assume the function is linear. This assumption may be inappropriate or completely wrong. We need model adequacy checking to examine whether linearity assumption makes sense. At this moment, let‚Äôs put this issue aside, and assume the true function is linear.\n\n\n\n Simple Linear Regression Model (Population) \nWe now formally write down the simple linear regression model.\nFor the \\(i\\)-th measurement in the target population, \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\n\n\n\\(Y_i\\) is the \\(i\\)-th value of the response (random) variable.\n\n\\(X_i\\) is the \\(i\\)-th fixed known value of the predictor.\n\n\\(\\epsilon_i\\) is the \\(i\\)-th random error with the assumption \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are model or regression coefficients.\n\n\\(f(X) = \\beta_0 + \\beta_1X\\) is the population regression line that describes the true relationship between \\(X\\) and the mean of \\(Y\\) in the population.\n\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are fixed unknown parameters to be estimated from the sample data once we collect them. Once we learn \\(\\beta_0\\) and \\(\\beta_1\\), we know the regression line \\(\\beta_0 + \\beta_1X_i\\), and know how \\(X\\) affects \\(Y\\). Once we learn \\(\\sigma^2\\), we know how much \\(Y\\) is scattered and deviated from its mean.\n Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\nThe setting \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) implies several important features of the regression model.\nFirst, the mean of \\(\\epsilon_i\\) is zero. In other words, the mean of \\(Y\\) at any given level of \\(X\\) is described by the population regression line,\n\\[\\begin{align*}\n\\mu_{Y_i \\mid X_i} &= E(\\beta_0 + \\beta_1X_i + \\epsilon_i) \\\\\n&= \\beta_0 + \\beta_1X_i\n\\end{align*}\\]\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(E(c) = c\\) if \\(c\\) is a constant. For example the mean of 5 is 5. \\(E(5) = 5\\).\nFor variable or constant \\(A\\) and \\(B\\), \\(E(A+B) = E(A) + E(B)\\).\n\nSince \\(\\beta_0 + \\beta_1X_i\\) is a constant, \\(E(\\beta_0 + \\beta_1X_i + \\epsilon_i) = E(\\beta_0 + \\beta_1X_i) + E(\\epsilon_i) = \\beta_0 + \\beta_1X_i + 0 = \\beta_0 + \\beta_1X_i.\\)\n\n\nThe mean response, \\(\\mu_{Y\\mid X}\\), has a straight-line relationship with \\(X\\) given by a population regression line \\[\\mu_{Y\\mid X} = \\beta_0 + \\beta_1X\\] \n\n\n\n\n\n\n\n\nFigure¬†27.13: The straight line relationship between \\(\\mu_{Y\\mid X}\\) and \\(X\\).\n\n\n\n\n\n\n\n\n\nSecond, for any \\(i\\), \\(Var(\\epsilon_i) = \\sigma^2\\). With a fixed \\(X\\), since all the variation of \\(Y\\) comes from \\(\\epsilon\\), the variance of \\(Y_i \\mid X_i\\) is also \\(\\sigma^2\\), i.e.,\n\\[\\begin{align*}\nVar(Y_i \\mid X_i) &= Var(\\epsilon_i) = \\sigma^2\n\\end{align*}\\]\n\n\n\n\n\n\nNote\n\n\n\nFor a constant \\(c\\) and random variable \\(A\\), \\(Var(c+A) = Var(A)\\). Since \\(\\beta_0 + \\beta_1X_i\\) is a constant with no variation, \\(Var(Y_i \\mid X_i) = Var(\\beta_0 + \\beta_1X_i + \\epsilon_i) = Var(\\epsilon_i) = \\sigma^2\\).\n\n\nNote that the variance of \\(Y\\) does not depend on \\(X\\). No matter \\(X\\) is large or small, the variation of \\(Y\\) stays constant. If there are \\(n\\) distinct \\(x\\)s, there will be \\(n\\) associated distinct normal distributions, all having the same variance. \n\n\n\n\n\n\n\n\nFigure¬†27.14: \\(Y\\) has a constant variance regardless of \\(X\\).\n\n\n\n\n\n\n\n\n\nFinally, if \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\), it implies \\(Y_i \\mid X_i\\) is also normally distributed. In particular,\n\\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\]\nFor any fixed value of \\(X_i = x_i\\), the response, \\(Y_i\\), varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\). For all levels of \\(X\\), their corresponding \\(Y\\)s will be random around their mean, the regression function value at \\(X\\), to the same degree.\n\n\n\n\n\n\n\n\n\nFigure¬†27.15: The response \\(Y_i\\) follows a normal distribution.\n\n\n\n\n\n\nAgain, \\(Y_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\\) is our model assumption, and it is not necessarily true. When we perform simple liner regression analysis, we by default accept the assumption, and do the analysis based on the assumption. Our goal is to collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\). Once \\(\\beta_0\\) and \\(\\beta_1\\) are estimated, the entire regression line and function \\(f(X) = \\beta_0+\\beta_1X\\) is also estimated.\nNext section, we are going to learn how to estimate \\(\\beta_0\\) and \\(\\beta_1\\), or find the best estimated regression line.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#fitting-a-regression-line-haty-b_0-b_1x",
    "href": "model-reg.html#fitting-a-regression-line-haty-b_0-b_1x",
    "title": "27¬† Simple Linear Regression",
    "section": "\n27.3 Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n",
    "text": "27.3 Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n\n Idea of Fitting \nGiven the sample data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\) we like to ask these questions\n\nWhich sample regression line is the best?\nWhat are the best estimators, \\(b_0\\) and \\(b_1\\), for \\(\\beta_0\\) and \\(\\beta_1\\)?\n\nThe sample regression line is is \\(b_0+b_1X\\) that is used to estimate the unknown population regression line \\(\\beta_0 + \\beta_1X\\). The estimator \\(b_0\\) and \\(b_1\\) are sample statistics. The process of finding the best \\(b_0\\) and \\(b_1\\) or the sample regression line is called model fitting. We fit the simple linear regression model to the sample data. Through model fitting, we find \\(b_0\\) and \\(b_1\\) or the sample regression line that fit the data the best. The sample regression line is best representative of the sample data, and we believe the sample line will be close to the population regression line the most.\nWe are interested in \\(\\beta_0\\) and \\(\\beta_1\\) in the following sample regression model: \\[\\begin{align*}\ny_i = \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\\] or \\[E({y}_{i} \\mid x_i) = \\mu_{y|x_i} = \\beta_0 + \\beta_1~x_{i}\\] We use the sample statistics \\(b_0\\) and \\(b_1\\), which are computed from our sample data, to estimate \\(\\beta_0\\) and \\(\\beta_1\\). \\(\\hat{y}_{i} = b_0 + b_1~x_{i}\\) is called the fitted value of \\(y_i\\). It is a point on the sample regression line, and is a point estimate of the mean, \\(\\mu_{y|x_i}\\), and \\(y_i\\) itself.\n\n Ordinary Least Squares (OLS) \nGiven a data set, we could have many possible sample regression lines. In fact, there are infinitely many lines out there. Which one is the best?\n\n\n\n\n\n\n\nFigure¬†27.16: One data set can have multiple fitted regression lines.\n\n\n\n\nTo answer such question, we need to first define what ‚Äúthe best‚Äù mean. Here we want to choose \\(b_0\\) and \\(b_1\\) or the sample regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals denoted by \\(SS_{res}\\). 2\nThe residual, \\(e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)\\), is the difference between the data value \\(y_i\\) and its fitted value \\(\\hat{y}_i\\), the estimated value of \\(y_i\\) on the sample regression line. Ideally, we hope the two quantities to be as close as possible because that means the estimation is quite good. The residual \\(e_i\\) usually works as a point estimate of the error term \\(\\epsilon_i\\) in the model.\nThe best sample regression line minimizes the sum of squared residuals \\[SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2.\\]\nIf \\(b_0\\) and \\(b_1\\) are the best estimators, plug \\(e_i = y_i - (b_0 + b_1x_i)\\) into \\(SS_{res}\\), we have\n\\[\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}\\] that is the smallest comparing to any other \\(SS_{res} = \\sum_{i=1}^n(y_i - a_0 - a_1x_i)^2\\) that uses another pair of estimators \\((a_0, a_1) \\ne (b_0, b_1)\\).\nBefore we actually find \\(b_0\\) and \\(b_1\\), let‚Äôs visualize fitted values and residuals in the scatter plot.\n\n Visualizing Fitted Values and Residuals \nThe two variables are the Car Displacement in litres and Highway Miles per Gallon. The scatter plot shows a negatively correlated relationship.\n\n\n\n\n\n\n\n\nThe best sample regression line \\(\\hat{y} = b_0 + b_1 x\\) that minimizes \\(SS_{res}\\) is the navy blue straight line. The red points on the line are the fitted value of \\(y_i\\)s, or \\(\\hat{y_i} = b_0 + b_1 x_i\\).\n\n\n\n\n\n\n\n\nThe residual \\(e_i\\) is \\(y_i - \\hat{y}_i\\). In the plot, it‚Äôs the difference between the black and red points. Each vertical bar shows the magnitude of a residual. If we square the size of all vertical bars and add them up, the sum will be our \\(SS_{res}\\) value.\n\n\n\n\n\n\n\n\n Least Squares Estimates (LSE) \nIt‚Äôs time to find the best \\(b_0\\) and \\(b_1\\) that minimize \\(SS_{res}\\). We call \\(b_0\\) and \\(b_1\\) least squares estimators (LSE) and the process of finding such estimators ordinary least squares (OLS) method.\n\nMathematically, \\((b_0, b_1)\\) is the minimizer of the sum of squares\n\\[(b_0, b_1) = \\arg \\min_{\\alpha_0, \\alpha_1} \\sum_{i=1}^n(y_i - \\alpha_0 - \\alpha_1x_i)^2\\]\n\nIt is an optimization problem. I leave it to you as an exercise. The formula of LSE is\n\\[\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\]\n\\[\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = \\frac{S_{xy}}{S_{xx}} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},\\] where \\(S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2\\), \\(S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\), \\(S_{xy} = \\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})\\)\nLet‚Äôs see if we can get some intuition from the formula. First, the least squares regression line passes through the centroid \\((\\overline{x}, \\overline{y})\\) because \\(\\overline{y} = b_0 + b_1\\overline{x}\\). In addition, \\(b_1\\) is kind of like a scaled version of correlation coefficient of \\(X\\) and \\(Y\\). The correlation \\(r\\) and the slope \\(b_1\\) always have the same sign. A positive (negative) correlation implies a positive (negative) sloped regression line. The correlation \\(r\\) is unit-free. When we fit a regression model, the slope \\(b_1\\) size depends on the unit of \\(X\\) and \\(Y\\).\nNowadays we use computing software to get the estimates, but it‚Äôs good to to know the idea of OLS, and the properties of \\(b_0\\) and \\(b_1\\).\n\n\n\n\n\n\nR\nPython\n\n\n\n\nWe use the mpg data set in the ggplot2 package to demonstrate doing regression analysis in R. In particular, we grab the variable hwy as our response and disp as our predictor.\n\n\n\n# A tibble: 234 √ó 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto‚Ä¶ f        18    29 p     comp‚Ä¶\n 2 audi         a4           1.8  1999     4 manu‚Ä¶ f        21    29 p     comp‚Ä¶\n 3 audi         a4           2    2008     4 manu‚Ä¶ f        20    31 p     comp‚Ä¶\n 4 audi         a4           2    2008     4 auto‚Ä¶ f        21    30 p     comp‚Ä¶\n 5 audi         a4           2.8  1999     6 auto‚Ä¶ f        16    26 p     comp‚Ä¶\n 6 audi         a4           2.8  1999     6 manu‚Ä¶ f        18    26 p     comp‚Ä¶\n 7 audi         a4           3.1  2008     6 auto‚Ä¶ f        18    27 p     comp‚Ä¶\n 8 audi         a4 quattro   1.8  1999     4 manu‚Ä¶ 4        18    26 p     comp‚Ä¶\n 9 audi         a4 quattro   1.8  1999     4 auto‚Ä¶ 4        16    25 p     comp‚Ä¶\n10 audi         a4 quattro   2    2008     4 manu‚Ä¶ 4        20    28 p     comp‚Ä¶\n# ‚Ñπ 224 more rows\n\n\n Highway MPG hwy vs.¬†Displacement displ \nFirst we create the scatter plot.\n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\n\n\n\n\n\n\n\n Fit Simple Linear Regression \nTo fit a linear regression, we use the lm() function because linear regression is a linear model. As we learned in ANOVA Chapter 24, we write the formula hwy ~ displ, and specify the data set mpg. We save the fitted result in the object reg_fit which is a R list. The output prints \\(b_0 = 35.70\\) and \\(b_1 = -3.53\\), the least squares estimate of \\(\\beta_0\\) and \\(\\beta_1\\). So the sample regression function or regression line is \\[\\widehat{hwy} = 35.7 - 3.53 \\times disp.\\]\nFor one unit (litre) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.53 miles.\n\n\n\nreg_fit &lt;- lm(formula = hwy ~ displ, data = mpg)\nreg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n     35.698       -3.531  \n\n\n\n\n\n\n\ntypeof(reg_fit)\n\n[1] \"list\"\n\n## all elements in reg_fit\nnames(reg_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n## use $ to extract an element of a list\nreg_fit$coefficients\n\n(Intercept)       displ \n  35.697651   -3.530589 \n\n\n\n\n Fitted Values of \\(y\\) \n\n## the first 5 observed response value y\nmpg$hwy[1:5]\n\n[1] 29 29 31 30 26\n\n## the first 5 fitted value y_hat\nhead(reg_fit$fitted.values, 5)\n\n       1        2        3        4        5 \n29.34259 29.34259 28.63647 28.63647 25.81200 \n\n## the first 5 predictor value x\nmpg$displ[1:5]\n\n[1] 1.8 1.8 2.0 2.0 2.8\n\nlength(reg_fit$fitted.values)\n\n[1] 234\n\n\n\n Add a Regression Line \nTo add the fitted regression line, we just put the fitted result reg_fit into the function abline().\n\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\nabline(reg_fit, col = \"#FFCC00\", lwd = 3)\n\n\n\n\n\n\n\n\n\nWe use the mpg data set in the ggplot2 package to demonstrate doing regression analysis in Python. In particular, we grab the variable hwy as our response and disp as our predictor.\n\n\n\n  manufacturer model  displ  year  cyl       trans drv  cty  hwy fl    class\n0         audi    a4    1.8  1999    4    auto(l5)   f   18   29  p  compact\n1         audi    a4    1.8  1999    4  manual(m5)   f   21   29  p  compact\n2         audi    a4    2.0  2008    4  manual(m6)   f   20   31  p  compact\n3         audi    a4    2.0  2008    4    auto(av)   f   21   30  p  compact\n4         audi    a4    2.8  1999    6    auto(l5)   f   16   26  p  compact\n\n\n Highway MPG hwy vs.¬†Displacement displ \nFirst we create the scatter plot.\n\nplt.scatter(mpg['displ'], mpg['hwy'], \n            color='blue')\nplt.title('Highway MPG vs. Engine Displacement (litres)')\nplt.xlabel('Displacement (litres)')\nplt.ylabel('Highway MPG')\nplt.show()\n\n\n\n\n\n\n\n Fit Simple Linear Regression \nTo fit a linear regression, we use the ols() function. As we learned in ANOVA Chapter 24, we write the formula 'hwy ~ displ', and specify the data set mpg. We save the fitted result in the object reg_fit.\nreg_fit.summary() will output the OLS regression results. The output prints \\(b_0 = 35.70\\) and \\(b_1 = -3.53\\), the least squares estimate of \\(\\beta_0\\) and \\(\\beta_1\\). So the sample regression function or regression line is \\[\\widehat{hwy} = 35.7 - 3.53 \\times disp.\\]\nFor one unit (litre) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.53 miles.\n\nfrom statsmodels.formula.api import ols\n# Fit a simple linear regression model\nreg_fit = ols('hwy ~ displ', data=mpg).fit()\nreg_fit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nhwy\nR-squared:\n0.587\n\n\nModel:\nOLS\nAdj. R-squared:\n0.585\n\n\nMethod:\nLeast Squares\nF-statistic:\n329.5\n\n\nDate:\nTue, 15 Oct 2024\nProb (F-statistic):\n2.04e-46\n\n\nTime:\n10:36:32\nLog-Likelihood:\n-645.62\n\n\nNo. Observations:\n234\nAIC:\n1295.\n\n\nDf Residuals:\n232\nBIC:\n1302.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n35.6977\n0.720\n49.555\n0.000\n34.278\n37.117\n\n\ndispl\n-3.5306\n0.195\n-18.151\n0.000\n-3.914\n-3.147\n\n\n\n\nOmnibus:\n45.280\nDurbin-Watson:\n0.954\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n90.192\n\n\nSkew:\n0.961\nProb(JB):\n2.60e-20\n\n\nKurtosis:\n5.357\nCond. No.\n11.3\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n## All fields and methods in the reg_fit object\ndir(reg_fit)\n\n['HC0_se', 'HC1_se', 'HC2_se', 'HC3_se', '_HCCM', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_abat_diagonal', '_cache', '_data_attr', '_data_in_cache', '_get_robustcov_results', '_get_wald_nonlinear', '_is_nested', '_transform_predict_exog', '_use_t', '_wexog_singular_values', 'aic', 'bic', 'bse', 'centered_tss', 'compare_f_test', 'compare_lm_test', 'compare_lr_test', 'condition_number', 'conf_int', 'conf_int_el', 'cov_HC0', 'cov_HC1', 'cov_HC2', 'cov_HC3', 'cov_kwds', 'cov_params', 'cov_type', 'df_model', 'df_resid', 'diagn', 'eigenvals', 'el_test', 'ess', 'f_pvalue', 'f_test', 'fittedvalues', 'fvalue', 'get_influence', 'get_prediction', 'get_robustcov_results', 'info_criteria', 'initialize', 'k_constant', 'llf', 'load', 'model', 'mse_model', 'mse_resid', 'mse_total', 'nobs', 'normalized_cov_params', 'outlier_test', 'params', 'predict', 'pvalues', 'remove_data', 'resid', 'resid_pearson', 'rsquared', 'rsquared_adj', 'save', 'scale', 'ssr', 'summary', 'summary2', 't_test', 't_test_pairwise', 'tvalues', 'uncentered_tss', 'use_t', 'wald_test', 'wald_test_terms', 'wresid']\n\n\n\n## parameter coefficient estimates\nreg_fit.params\n\nIntercept    35.697651\ndispl        -3.530589\ndtype: float64\n\n\n Fitted Values of \\(y\\) \n\n## the first 5 observed response value y\nmpg['hwy'].head(5).values\n\narray([29, 29, 31, 30, 26])\n\n## the first 5 fitted value y_hat\nreg_fit.fittedvalues.head(5).values\n\narray([29.3425912 , 29.3425912 , 28.63647344, 28.63647344, 25.81200239])\n\n## the first 5 predictor value x\nmpg['displ'].head(5).values\n\narray([1.8, 1.8, 2. , 2. , 2.8])\n\nlen(reg_fit.fittedvalues)\n\n234\n\n\n\n Add a Regression Line \nTo add the fitted regression line, we add another plot with y values being reg_fit.fittedvalues.\n\nplt.scatter(x=mpg['displ'], y=mpg['hwy'], color='navy')\nplt.plot(mpg['displ'], reg_fit.fittedvalues, color='#FFCC00', linewidth=3)\nplt.title('Highway MPG vs. Engine Displacement (litres)')\nplt.xlabel('Displacement (litres)')\nplt.ylabel('Highway MPG')\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Estimation for \\(\\sigma^2\\) \nWe can think of \\(\\sigma^2\\) as variance around the line or the mean square (prediction) error. It is the variance of the error term. Since the residual \\(e_i\\) is the estimate of \\(\\epsilon_i\\), we can use the variance of residuals to estimate \\(\\sigma^2\\), which is the mean square residual, \\(MS_{res}\\): \\[\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2} = \\frac{\\sum_{i=1}^ne_i^2}{n-2}.\\]\n\n\n\n\n\n\nNote\n\n\n\n\nThe variance of residuals is \\(\\frac{\\sum_{i=1}^n(e_i - \\overline{e})^2}{n-2}\\). Because with OLS we have \\(\\sum_{i=1}^n e_i = 0\\), \\(\\overline{e} = 0\\), and hence \\(\\hat{\\sigma}^2 = \\frac{\\sum_{i=1}^ne_i^2}{n-2}.\\)\nThe degrees of freedom is the number of values in the final calculation of a statistic that are free to vary. It is equal to the sample size minus the number of parameters estimated. In simple linear regression, we estimate \\(\\beta_0\\) and \\(\\beta_1\\) in the calculation of \\(SS_{res}\\), so its degrees of freedom is \\(n-2\\).\n\n\n\nRemember that a sum of squares has a corresponding degrees of freedom. \\(SS_{res}\\) has \\(n-2\\) degrees of freedom.\n\\(MS_{res}\\) is often shown in computer output as \\(\\texttt{MS(Error)}\\) or \\(\\texttt{MS(Residual)}\\).\nBy the way, \\(E(MS_{res}) = \\sigma^2\\). Therefore, \\(\\hat{\\sigma}^2\\) is an unbiased estimator for \\(\\sigma^2\\).\n\n\n\nR\nPython\n\n\n\n\nPlease be careful that R provide the standard error \\(\\hat{\\sigma}\\), not \\(\\hat{\\sigma}^2\\) which is \\(MS_{res}\\).\nThe standard error \\(\\hat{\\sigma}\\) information is saved in the summary of the fitted result. In the summary output, \\(\\hat{\\sigma}\\) is shown as Residual standard error. The degrees of freedom is \\(n - 2 = 234 - 2 = 232.\\)\n\n(summ_reg_fit &lt;- summary(reg_fit))\n\n...\n\nResidual standard error: 3.836 on 232 degrees of freedom\n...\n\n\nWe can grab the \\(\\hat{\\sigma}\\) value in the summary object summ_reg_fit by summ_reg_fit$sigma.\n\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n\n[1] 3.835985\n\n\nIt is not hard to compute the \\(MS_{res}\\) directly from its definition, the sum of squared residuals divided by residual degrees of freedom.\n\n# from reg_fit\nsum(reg_fit$residuals^2) / reg_fit$df.residual\n\n[1] 14.71478\n\n\n\n\nThe standard error \\(\\hat{\\sigma}^2\\) information is saved in the fitted result, reg_fit.mse_resid, short for mean square error of residuals.\nIf we want the standard error, \\(\\hat{\\sigma} = \\sqrt{MS_{res}}\\), we need to remember to take a square root.\n\nimport numpy as np\nreg_fit.mse_resid\n\n14.71478021118735\n\nnp.sqrt(reg_fit.mse_resid)\n\n3.835984907580757\n\n\n\n# from reg_fit to compute MS_res\nnp.sum(reg_fit.resid ** 2) / reg_fit.df_resid\n\n14.714780211187357",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#confidence-intervals-and-hypothesis-testing-for-beta_0-and-beta_1",
    "href": "model-reg.html#confidence-intervals-and-hypothesis-testing-for-beta_0-and-beta_1",
    "title": "27¬† Simple Linear Regression",
    "section": "\n27.4 Confidence Intervals and Hypothesis Testing for \\(\\beta_0\\) and \\(\\beta_1\\)\n",
    "text": "27.4 Confidence Intervals and Hypothesis Testing for \\(\\beta_0\\) and \\(\\beta_1\\)\n\nWe‚Äôve obtained the point estimators of \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma^2\\). Often we‚Äôd like to do inference about the parameters, especially \\(\\beta_1\\) because it tells us how the predictor affects the response. In this section, we talk about interval estimation and testing procedure for \\(\\beta_0\\) and \\(\\beta_1\\).\n Confidence Intervals for \\(\\beta_0\\) and \\(\\beta_1\\) \nNote that \\(b_0\\) and \\(b_1\\) are functions of data. Because \\(y_i\\)s are random variables (before being sampled), so are \\(b_0\\) and \\(b_1\\). As a result, they have their sampling distribution that is used for constructing the confidence interval for \\(\\beta_0\\) and \\(\\beta_1\\) respectively. It can be show that\n\\[\\frac{b_1 - \\beta_1}{\\sqrt{\\hat{\\sigma}^2/S_{xx}}} \\sim t_{n-2}; \\quad \\frac{b_0 - \\beta_0}{\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}.\\]\nTherefore, the \\((1-\\alpha)100\\%\\) CI for \\(\\beta_1\\) is \\[b_1 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2/S_{xx}}.\\]\nThe \\((1-\\alpha)100\\%\\) CI for \\(\\beta_0\\) is \\[b_0 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}.\\]\n\n\n\n\nR\nPython\n\n\n\nTo grab the confidence interval for \\(\\beta_0\\) and \\(\\beta_1\\), we simply put the fitted result into the confint() function. By default, the confidence level is 95%, but any other level can be specified. We say we are 95% confident that one unit increase of displacement will result in a decrease in highway miles per gallon on average by 3.91 to 3.15 miles.\n\nconfint(reg_fit, level = 0.95)\n\n                2.5 %   97.5 %\n(Intercept) 34.278353 37.11695\ndispl       -3.913828 -3.14735\n\n\n\n\nTo grab the confidence interval for \\(\\beta_0\\) and \\(\\beta_1\\), we simply use the method conf_int() of the fitted object. By default, the confidence level is 95%, but any other level can be specified. We say we are 95% confident that one unit increase of displacement will result in a decrease in highway miles per gallon on average by 3.91 to 3.15 miles.\n\nreg_fit.conf_int(alpha=0.05)\n\n                   0         1\nIntercept  34.278353  37.11695\ndispl      -3.913828  -3.14735\n\n\n\n\n\n\n Hypothesis Testing \nThe hypothesis testing for \\(\\beta_0\\) and \\(\\beta_1\\) is straightforward. Here I just show the case of two-tailed tests. The one-tailed tests can be done similarly.\n \\(\\beta_1\\) \n\n\n \\(H_0: \\beta_1 = \\beta_1^0 \\quad H_1: \\beta_1 \\ne \\beta_1^0\\)  \n\nTest statistic: Under \\(H_0\\), \\[t_{test} = \\frac{b_1 - \\color{red}{\\beta_1^0}}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\\]\n\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| &gt; t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{$p$-value} = 2P(t_{n-2} &gt; |t_{test}|) &lt; \\alpha\\)\n\n\n\n \\(\\beta_0\\) \n\n\n \\(H_0: \\beta_0 = \\beta_0^0 \\quad H_1: \\beta_0 \\ne \\beta_0^0\\)  \n\nTest statistic: Under \\(H_0\\), \\[t_{test} = \\frac{b_0 - \\color{red}{\\beta_0^0}}{\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}\\]\n\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| &gt; t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{$p$-value} = 2P(t_{n-2} &gt; |t_{test}|) &lt; \\alpha\\)\n\n\n\n\n\nR\nPython\n\n\n\nThe two-sided test result for \\(\\beta_0\\) and \\(\\beta_1\\) can be found in the summary output. There is a Coeffifcients table in the middle of the output. We can extract the table by check summ_reg_fit$coefficients. The first row is for intercept and the second for the slope. The columns from left to right are LSEs, the standard errors of LSEs, the \\(t\\) test statistics, and \\(p\\)-values.\n\nsumm_reg_fit\n\n...\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  35.6977     0.7204   49.55   &lt;2e-16 ***\ndispl        -3.5306     0.1945  -18.15   &lt;2e-16 ***\n...\n\n\n\nsumm_reg_fit$coefficients\n\n             Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 35.697651  0.7203676  49.55477 2.123519e-125\ndispl       -3.530589  0.1945137 -18.15085  2.038974e-46\n\n\nNotice that the testing output is for the two-sided test with hypothesized value being equal to zero. The first row tests whether or not \\(\\beta_0 = 0\\) and the second tests whether or not \\(\\beta_1 = 0\\).\n\n\nhe two-sided test result for \\(\\beta_0\\) and \\(\\beta_1\\) can be found in the summary output. There is a Coeffifcients table in the middle of the output. The first row is for intercept and the second for the slope. The columns from left to right are LSEs, the standard errors of LSEs, the \\(t\\) test statistics, and \\(p\\)-values. The last two columns show their confidence interval.\n\nreg_fit.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nhwy\nR-squared:\n0.587\n\n\nModel:\nOLS\nAdj. R-squared:\n0.585\n\n\nMethod:\nLeast Squares\nF-statistic:\n329.5\n\n\nDate:\nTue, 15 Oct 2024\nProb (F-statistic):\n2.04e-46\n\n\nTime:\n10:36:33\nLog-Likelihood:\n-645.62\n\n\nNo. Observations:\n234\nAIC:\n1295.\n\n\nDf Residuals:\n232\nBIC:\n1302.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n35.6977\n0.720\n49.555\n0.000\n34.278\n37.117\n\n\ndispl\n-3.5306\n0.195\n-18.151\n0.000\n-3.914\n-3.147\n\n\n\n\nOmnibus:\n45.280\nDurbin-Watson:\n0.954\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n90.192\n\n\nSkew:\n0.961\nProb(JB):\n2.60e-20\n\n\nKurtosis:\n5.357\nCond. No.\n11.3\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nreg_fit.pvalues\n\nIntercept    2.123519e-125\ndispl         2.038974e-46\ndtype: float64\n\n\nNotice that the testing output and the p-values reg_fit.pvalues are for the two-sided test with hypothesized value being equal to zero. The first row tests whether or not \\(\\beta_0 = 0\\) and the second tests whether or not \\(\\beta_1 = 0\\).\n\n\n\n Interpretation of Testing Results \nSuppose we do the test  \\(H_0: \\beta_1 = 0 \\quad H_1: \\beta_1 \\ne 0\\) . Failing to reject \\(H_0: \\beta_1 = 0\\) implies there is no (statistically significant) linear relationship between (the mean of) \\(Y\\) and \\(X\\). But be careful. They could have some other type of relationship.\n\n\n\n\n\n\n\nFigure¬†27.17: Failing to reject \\(H_0\\) means there is no linear relationship between X and Y, but they could have some other type of relationship.\n\n\n\n\n\n\n\n\n\n\nIf we reject \\(H_0: \\beta_1 = 0\\), does it mean \\(X\\) and \\(Y\\) are linearly related?\n\n\n\n\n\n\n Test of Significance of Regression \nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate.\nbetter results could be obtained with a more complicated model.\n\nRejecting \\(H_0\\) doesn‚Äôt necessarily mean that a linear model is the best model. There may be some other nonlinear relationship that cannot be captured by the linear regression model. To capture that, a more sophisticated model is needed.\n\n\n\n\n\n\n\nFigure¬†27.18: Rejecting \\(H_0\\) doesn‚Äôt necessarily mean that a linear model is the best model.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#analysis-of-variance-anova-approach",
    "href": "model-reg.html#analysis-of-variance-anova-approach",
    "title": "27¬† Simple Linear Regression",
    "section": "\n27.5 Analysis of Variance (ANOVA) Approach",
    "text": "27.5 Analysis of Variance (ANOVA) Approach\nThe testing for \\(\\beta_1\\) can be done using ANOVA approach. The key idea is to separate the total deviation of \\(y\\) from its mean into the deviation of \\(y\\) explained by regression or the factor \\(x\\) and the deviation of \\(y\\) that cannot be explained by \\(x\\). Let‚Äôs learn about it.\n \\(X\\) - \\(Y\\) Relationship Explains Some Deviation \nFirst let me ask you a question.\n\n\n\n\n\n\nSuppose we only have data for \\(Y\\) and have no information about \\(X\\) or no information about the relationship between \\(X\\) and \\(Y\\). How do we predict a value of \\(Y\\)?\n\n\n\n\n\n\nIf we have sample data for \\(y\\) only, then we just have one variable. If the data have no significant pattern, we may treat every data point equally, and our best guess for \\(Y\\) could be \\(\\overline{y}\\). In this case, we use the same value to predict every value of \\(y\\), i.e., \\(\\hat{y}_i = \\overline{y}\\). This prediction is generally bad, but there is nothing we can do because we have on other information that can help us better guess the value of \\(y_i\\).\nIf there is another variable \\(X\\), and \\(X\\) and \\(Y\\) are correlated, knowing the level of \\(x\\) does help us predict the value of \\(y\\). If they are positively correlated, when \\(x\\) is large, \\(y\\) tends to be large. Thus normally we will guess its value higher than its average, leading to a smaller residual, and better prediction.\nLosing such information hurts. With no information about the relationship between \\(X\\) and \\(Y\\). The best we can do is treat \\(X\\) and \\(Y\\) as uncorrelated, as if \\(X\\) does not exist in the data. Same as one variable data of \\(y\\), in this case, \\(\\hat{y}_i = \\overline{y}\\).\nThe deviation of \\(y\\) from the sample mean is \\((y_i - \\overline{y})\\), which is viewed as the total deviation. If \\(X\\) and \\(Y\\) are linearly related, fitting a linear regression model helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided. This means \\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\) is. In other words, the regression model explains some of the deviation of \\(y\\). The relationship between \\(X\\) and \\(Y\\) is valuable and helpful in predicting \\(y\\) with the value of \\(x\\) informed.\n\n Partition of Deviation \nThe total deviation of \\(y\\) from its mean can be written as the sum of the deviation of \\(y\\) explained by regression or the factor \\(x\\) and the deviation of \\(y\\) that cannot be explained by \\(x\\).\nTotal deviation = Deviation explained by regression + unexplained deviation\nor\n\\[(y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\]\nFigure¬†27.19 illustrates the deviation partition. Look at the point \\(y_1\\). Suppose \\(y_1 = 19\\) and \\(\\overline{y} = 9\\). Then the total deviation is \\((y_i - \\overline{y}) = 19-9 = 10\\) which is represented as the red bar on the left in the figure.\nNow if the positive correlation between \\(X\\) and \\(Y\\) information come in, we can better guess the value of \\(y_1\\) using the least squares regression line \\(\\hat{y}_1 = b_0 + b_1x_1\\). Note that \\(y_1\\) is the observation whose \\(x\\) is quite large, so through the regression line, we get the fitted value \\(\\hat{y}_1\\) larger than \\(\\overline{y}\\). \\(\\hat{y}_1\\) is closer to \\(y_1\\), and the deviation \\((\\hat{y}_1 - \\overline{y})\\) is the deviation of \\(y\\) explained or captured by the regression model or the predictor \\(X\\). This corresponds to the green bar on the right of the figure. This deviation quantifies the contribution of the regression model or the predictor \\(X\\) to explaining the variation of \\(y\\).\nFinally, the remaining part of deviation that cannot be explained by \\(x\\) is \\((y_i - \\hat{y}_i)\\) which is the blue bar in the middle of the figure. Look at it carefully. It is our residual! The residual is the deviation of \\(y\\) that cannot be explained by the regression model. All the deviations that can be explained by predictor \\(x\\) has been absorbed in \\((\\hat{y}_i - \\overline{y})\\). The deviation or residual \\((y_i - \\hat{y}_i)\\) is anything that has nothing to do with \\(x\\). This is what the regression model and the predictor \\(x\\) can‚Äôt do for explaining the variation of \\(y\\) unless other factors other than \\(x\\) that are correlated with \\(y\\) are considered in the model.\n\n\n\n\n\n\n\n\nFigure¬†27.19: Explained vs.¬†unexplained deviation. Source: Applied Linear Statistical Models 5th edition.\n\n\n\n\n Sum of Squares (SS) \nThe deviation partition leads to the sum of squares identity\n\\[\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\] or\nTotal Sum of Squares \\((SS_T)\\) = Regression Sum of Squares \\((SS_R)\\) + Residual Sum of Squares \\((SS_{res})\\)\nTheir corresponding degrees of freedom is\n\\[df_T = df_R + df_{res}\\]\nor \\[\\color{blue}{(n-1) = 1 +(n-2)}\\]\nThe total sum of squares quantifies the sum of squared deviation from the mean, and \\(SS_T/df_T = \\frac{\\sum_{i=1}^n(y_i - \\overline{y})^2}{n-1}\\) is the marginal sample variance of \\(y\\) without being conditional on \\(x\\). The regression sum of squares \\(SS_R\\) is the sum of squared deviation of the fitted value from the mean, and \\(SS_{res}\\) again is the sum of squared deviation of \\(y\\) from its fitted value. The mean square residual \\(SS_{res}/df_{res} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}\\) estimates the variance about the population regression line \\(\\sigma^2\\).\n\n ANOVA for Testing Significance of Regression \n\n\n\n\n\n\nThe sum of squares and degrees of freedom identities allow us the make an ANOVA table as below. But what is this ANOVA table is for? It is used for testing overall significance of regression, that is, whether or not the regression model has explanatory power for explaining the variation of the response. When the regression model has explanatory power, all or some of its predictors are correlated with the response, and their corresponding regression coefficient is nonzero.\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSS\ndf\nMS\nF\n\n\\(p\\)-value\n\n\n\nRegression\n\\(SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2\\)\n1\n\\(MS_R = \\frac{SS_R}{1}\\)\n\\(\\frac{MS_R}{MS_{res}} = F_{test}\\)\n\\(Pr(F_{1, n-2} &gt; F_{test})\\)\n\n\nResidual\n\\(SS_{res} = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\)\n\\(n-2\\)\n\\(MS_{res} = \\frac{SS_{res}}{n-2}\\)\n\n\n\n\nTotal\n\\(SS_{T} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\)\n\\(n-1\\)\n\n\n\n\n\n\n\nIn other words, ANONA is for the test \\[\n\\begin{align*}\nH_0&: \\text{all regression coefficients except the incercept is zero}\\\\\nH_1&: \\text{At least one regression coefficient except the incercept is not zero}\n\\end{align*}\n\\]\nA larger value of \\(F_{test}\\) indicates that the regression is significant. \\(H_0\\) is rejected if\n\n\\(F_{test} &gt; F_{\\alpha, 1, n-2}\\)\n\n\\(\\text{$p$-value} = P(F_{1, n-2} &gt; F_{test}) &lt; \\alpha\\).\n\nNote that ANOVA is designed to test the \\(H_0\\) that all predictors have no value in predicting \\(y\\). In simple linear regression, there is only one predictor, so one means all. The \\(F\\)-test of ANOVA gives the exactly same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1 = 0\\).\n\n\nR\nPython\n\n\n\n\nTo get the ANOVA table, we put the fitted result into the function anova(). For simple linear regression, the ANOVA \\(F\\) test is equivalent to the marginal \\(t\\) test for \\(\\beta_1\\) where \\(H_0: \\beta_1 = 0; \\quad H_1: \\beta_1 \\ne 0\\). In particular \\(t_{test}^2 = F_{test}\\).\n\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: hwy\n           Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndispl       1 4847.8  4847.8  329.45 &lt; 2.2e-16 ***\nResiduals 232 3413.8    14.7                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsumm_reg_fit$coefficients\n\n             Estimate Std. Error   t value      Pr(&gt;|t|)\n(Intercept) 35.697651  0.7203676  49.55477 2.123519e-125\ndispl       -3.530589  0.1945137 -18.15085  2.038974e-46\n\nsumm_reg_fit$coefficients[2, 3] ^ 2\n\n[1] 329.4533\n\n\n\n\nTo get the ANOVA table, we put the fitted result into the function sm.stats.anova_lm(). For simple linear regression, the ANOVA \\(F\\) test is equivalent to the marginal \\(t\\) test for \\(\\beta_1\\) where \\(H_0: \\beta_1 = 0; \\quad H_1: \\beta_1 \\ne 0\\). In particular \\(t_{test}^2 = F_{test}\\).\n\nimport statsmodels.api as sm\nsm.stats.anova_lm(reg_fit)\n\n             df       sum_sq      mean_sq           F        PR(&gt;F)\ndispl       1.0  4847.833384  4847.833384  329.453333  2.038974e-46\nResidual  232.0  3413.829009    14.714780         NaN           NaN\n\n\n\nreg_fit.tvalues\n\nIntercept    49.554769\ndispl       -18.150849\ndtype: float64\n\nreg_fit.tvalues.iloc[1] ** 2\n\n329.45333294759047\n\nreg_fit.fvalue\n\n329.45333294759075\n\n\n\n\n\n\n Coefficient of Determination \nThe coefficient of determination or \\(R^2\\) is the proportion of the variation in \\(y\\) that is explained by the regression model. It is computed as \\[R^2 = \\frac{SS_R}{SS_T} =\\frac{SS_T - SS_{res}}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\]\n\\(R^2\\) is the proportionate reduction of total variation associated with the use of \\(X\\) , and it can be used to measure the quality of our regression or the explanatory power of regressors.\nFigure¬†27.20 shows two extreme cases.\n\n(a) \\(\\hat{y}_i = y_i\\) and \\(\\small SS_{res} =  \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = 0\\).\n(b) \\(\\hat{y}_i = \\overline{y}\\) and \\(\\small SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2  = 0\\).\n\nIn (a), fitted values are equal to true observations. The regression model explains all the variation in \\(y\\), and hence \\(R^2 = 1\\). In (b), the fitted values are all equal to the sample mean of \\(y\\) as if we don‚Äôt have information about \\(x\\) or \\(x\\) is totally useless in predicting \\(y\\). In this case, the regression model explains no variation in \\(y\\), and all variation remain unexplained. So \\(R^2 = 0\\). In the usual case, \\(R^2\\) is between zero and one.\n\n\n\n\n\n\n\nFigure¬†27.20: Examples of \\(R^2\\) being equal to 1 and 0. Source: Applied Linear Statistical Model 5th edition.\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nThe \\(R^2\\) value is shown in the summary output at the near bottom. We can also use summ_reg_fit$r.squared to get the value. The variable disp explains about 59% of the variation of hwy.\n\nsumm_reg_fit\n\n...\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \n...\n\n\n\nsumm_reg_fit$r.squared\n\n[1] 0.5867867\n\n\n\n\nThe \\(R^2\\) value is shown in the summary output on the topright. We can also use reg_fit.rsquared to get the value. The variable disp explains about 59% of the variation of hwy.\n\nreg_fit.rsquared\n\n0.586786672398904",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#prediction",
    "href": "model-reg.html#prediction",
    "title": "27¬† Simple Linear Regression",
    "section": "\n27.6 Prediction",
    "text": "27.6 Prediction\nPrediction is one of our goals when we perform regression analysis. We want to use \\(X\\) to predict \\(Y\\) by taking advantages of their relationship. This is useful when our response variable includes some personal or confidential information, and it is hard to collect such information. When the predictor data is relatively easy to be collected, we can use the predictor value to predict the response value. For example, people tend to be reluctant to let others know their annual income level, but quite happy to talk about their education or occupation. If we can discover the relationship between salary level and years of education one receives, we can use the years of education to predict the mean salary level of people with that many years of education, or predict an individual salary level given his education years.\nWhen we talk about prediction, we refer to the predictor and response whose values haven‚Äôt seen in the data. For example, if our data have three \\(x\\) values \\((3, 5, 6)\\), and its corresponding \\(y\\) values \\((10, 12, 15)\\), we want to predict the mean response value for the \\(x\\) not in the data, like \\(x = 4\\).\n Predicting the Mean Response \nWith the predictor value \\(x = x_0\\), we want to estimate the mean response \\(E(y\\mid x_0) = \\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\). For example,  the mean highway MPG \\(E(y \\mid x_0)\\) when displacement is \\(x = x_0 = 5.5\\). \nThe mean \\(\\beta_0 + \\beta_1 x_0\\) is assumed from the regression model. The problem is \\(\\beta_0\\) and \\(\\beta_1\\) are unknown. Once we fit the model, and get the LSEs, if \\(x_0\\) is within the range of \\(x\\), an unbiased point estimate of \\(E(y\\mid x_0)\\) is \\[\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0.\\]\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y\\mid x_0)\\) is \\[\\boxed{\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}.\\]\nWell, no need to memorize the formula. But could you answer the following question?\n\n\n\n\n\n\nDoes the length of the CI for \\(E(y\\mid x_0)\\) stay the same at any location of \\(x_0\\)?\n\n\n\n\n\n\nWe will have the shortest confidence interval for \\(E(y\\mid x_0)\\) when \\(x_0 = \\overline{x}\\). In general, we have better precision when the response we are interested is evaluated at the level \\(x_0\\) that is close to \\(\\overline{x}\\). The intuition is that the variation in the estimator \\(b_0 + b_1 x\\) is greater when \\(x_0\\) is far from \\(\\overline{x}\\) than when \\(x_0\\) is near \\(\\overline{x}\\).\n\n Predicting New Observations \nWe want to predict not only the mean level of response at some predictor value, but also the value of a new observation, \\(y_0\\), at \\(x = x_0\\). For example,  the highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \nDo you see the difference? Previously we care about the mean response level, but here we predict an individual response value. There are thousand of millions of cars having displacement 5.5 liters. If we‚Äôd like to know the average miles per gallon or general fuel efficiency performance for the cars with displacement 5.5, we are predicting the mean response. When we want to predict the miles per gallon of a specific car whose displacement is 5.5, we predicting a new observation. The car is an individual outcome drawn from those thousand of millions of cars.\nA point estimator for \\(y_0(x_0)\\) is \\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\] that is the same as the point estimate of the mean response. If we can only use one single value to predict an new observation, we have to ignore its variation, and use a representative point to predict where it may be located. With no further information, we would choose its center, the mean response level, as its location. That‚Äôs why the two point estimates are the same.\n\nThe \\((1-\\alpha)100\\%\\) prediction interval (PI) for \\(y_0(x_0)\\) is \\[ \\boxed{\\hat{y_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{1+ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}.\\]\nA confidence interval is for a unknown parameter. We call the interval prediction interval because \\(y_0(x_0)\\) is not a parameter, but a random variable.\n\n\n\n\n\n\nWhat is the difference between CI for \\(E(y\\mid x_0)\\) and PI for \\(y_0(x_0)\\)?\n\n\n\n\n\n\nThe PI is wider as it includes the uncertainty about \\(b_0\\), \\(b_1\\) as well as \\(y_0\\) due to error, \\(\\epsilon\\). The uncertainty about \\(b_0\\) and \\(b_1\\) is always there because their value varies from sample to sample, and this causes the uncertainty about the mean response, or the location of the distribution of \\(y\\) at any given level of \\(x\\). The PI also considers the uncertainty about the draw of the distribution of \\(y\\), measured by \\(\\sigma^2\\). Intuitively it is harder to predict a specific response value due to the additional randomness or variation within the probability distribution of \\(y\\). Given the same significance level \\(\\alpha\\), we are more uncertain about what its value is, and the uncertainty band is wider, resulting in a less precise estimation.\n\n\n\n\nR\nPython\n\n\n\nTo predict either the mean response or a new observation, we use the function predict(). The first argument is out fitted object reg_fit. Then we need to provide the predictor value for prediction in the argument newdata. Be careful that it has to be a data frame type, and variable‚Äôs name should be exactly the same as the predictor‚Äôs name in the original data set mpg when we fit the regression. If we want to get the confidence interval for the mean response, we specify ‚Äúconfidence‚Äù in the argument interval. If instead we want the prediction interval for a new response, we specify ‚Äúpredict‚Äù.\n\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 16.27941 15.35839 17.20043\n\n## PI for the new observation\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"predict\", level = 0.95)\n\n       fit      lwr      upr\n1 16.27941 8.665682 23.89314\n\n\n\n\nTo predict either the mean response or a new observation, we use the method get_prediction(). The first argument exog is the values for which you want to predict. exog means exogenous variables. Be careful that it has to be array_like type.\nIf we want to get the confidence interval for the mean response, predict.conf_int() provides that. If instead we want the prediction interval for a new response, we need to check the predict.summary_frame(). The prediction interval is saved in ‚Äòobs_ci_lower‚Äô and ‚Äòobs_ci_upper‚Äô.\n\nnew_data = pd.DataFrame({'displ': [5.5]})\npredict = reg_fit.get_prediction(new_data)\npredict.conf_int()\n\narray([[15.35839096, 17.20043427]])\n\nsummary_frame = predict.summary_frame(alpha=0.05)\nsummary_frame[['mean_ci_lower', 'mean_ci_upper']]\n\n   mean_ci_lower  mean_ci_upper\n0      15.358391      17.200434\n\nsummary_frame[['obs_ci_lower', 'obs_ci_upper']]\n\n   obs_ci_lower  obs_ci_upper\n0      8.665682     23.893144\n\n\n\n\n\nThe 95% CI for the mean hwy when disp = 5.5 is between 15.36 and 17.2, while The 95% PI for hwy of a car with disp = 5.5 is between 8.67 and 23.89. The PI is much wider than the CI because the variance of \\(y\\) for any given \\(x\\) is quite large in this example. We are quite certain about the mean miles per gallon, but we are asked to predict a car‚Äôs miles per gallon, it‚Äôs hard to provide a precise estimation because of large individual variability.\n\n\nCodepar(mar = c(3, 3, 0, 0), mgp = c(2, 0.5, 0), mfrow = c(1, 1))\n## Data and regression line\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, cex = 0.5, \n     ylim = c(7.5, 45), xlab = \"Displacement (litres)\", \n     ylab = \"Highway MPG\")\nlegend(\"topright\", c(\"CI for the mean response\", \"PI for a new observation\"), \n       col = c(\"red\", \"blue\"), lwd = c(3, 3), bty = \"n\")\nabline(reg_fit, col = \"#003366\", lwd = 3)\nsegments(x0 = 5.52, y0 = 15.35839, y1 = 17.20043, col = \"red\", lwd = 4)\nsegments(x0 = 5.48, y0 = 8.665682, y1 = 23.89314, col = \"blue\", lwd = 4)\nabline(h = 16.27941, lty = 2)\n\n\n\n\n\n\n\nFor every possible value of \\(x\\), we can obtain its corresponding CI and PI. Connecting their upper bounds and lower bounds together ends up with a red confidence band and a blue prediction band. Both are narrowest at the average value of \\(x\\), the location of the black vertical bar, although it is not clearly seen in the prediction band. As \\(x\\) is farther way from the mean, the two bands are getting wider.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#simulation-based-inference",
    "href": "model-reg.html#simulation-based-inference",
    "title": "27¬† Simple Linear Regression",
    "section": "\n27.7 Simulation-based Inference",
    "text": "27.7 Simulation-based Inference\nWe can use bootstrap to estimate the regression coefficients with uncertainty quantification without assuming or relying on normal distribution. Instead of doing distribution-based inference, we are doing simulation-based inference. Please go to Chapter 15 to review the idea of bootstrapping.\nIn simple linear regression, we perform bootstrap inference step by step as follows.\n\nBootstrap new samples \\(\\{x^b_j, y^b_j\\}_{j=1}^n\\) from the original sample \\(\\{x_i, y_i\\}_{i=1}^n\\). Note that the bootstrap sample has the same size of the original sample.\nFor each bootstrapped sample, we fit a simple linear regression model to it, and estimate the corresponding slope. If we have \\(B\\) bootstrapped samples, we then have \\(B\\) different bootstrapped estimates of slope. Same idea for the intercept.\nUse the distribution of the bootstrapped slopes to construct a confidence interval. We plot the histogram of those \\(B\\) slopes (and/or intercepts). Use it as the distribution of the slope. Then we can for example construct a 95% confidence interval for the slope by finding the 2.5% and 97.5% percentiles of the distribution.\n\nThe followings show the scatter plot of hwy vs.¬†displ from the original mpg data set and 5 bootstrapped samples with their linear regression fitted line. First, you can see that due to randomness, the bootstrapped samples are all different although they all are of the same size. Secondly, since bootstrapped samples are different, they will have different intercept and slope estimates. In other words, all the blue fitted lines from the bootstrapped samples are different. They are also different from the red fitted line obtained from the original sample.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can repeatedly create bootstrapped samples lots of times. The collection of the fitted lines or the estimated intercepts and slopes let us know their possible values and the degree of uncertainty. Same as previous inference, we find that the uncertainty about the mean response when the predictor is away from its mean is larger than the uncertainty when the predictor value is close to its mean.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnother observation is that if we get the average of the 100 intercepts and slopes, as shown in the left panel of the figure below, the mean bootstrapped fitted line (darkblue) will be pretty close to the fitted line obtained from the original sample (red). The distribution of bootstrapped sample of slope with the mean (blue) and the 95% bootstrap CI (dashed green) is shown on the right.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nAs discussed in Chapter 15, we can use the boot package and its function boot() to do bootstrappingin R.\n\nlibrary(boot)\nset.seed(2024)\ncoef_fn &lt;- function(data, index) {\n    coef(lm(hwy ~ displ, data = data, subset = index))\n}\n\nboot_lm &lt;- boot::boot(data = mpg, statistic = coef_fn, R = 100)\nboot_lm\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot::boot(data = mpg, statistic = coef_fn, R = 100)\n\n\nBootstrap Statistics :\n     original      bias    std. error\nt1* 35.697651  0.12574153   0.9733547\nt2* -3.530589 -0.03859401   0.2745006\n\n\nThe function coef_fn returns the regression coefficient estimates from the bootstrapped samples that are the statistics needed for the argument statistic in the boot() function. We generate 100 bootstrap samples, and 100 corresponding bootstrap intercepts and slopes. The bootstrapping result is saved in the object boot_lm which is a list. boot_lm$t0 saves the \\(b_0\\) and \\(b_1\\) estimates from the original data, and boot_lm$t saves the 100 bootstrap estimated intercepts and slopes. The bias shown in the printed output is the difference between the mean of bootstrap estimates and the original coefficient estimates. The std. error is derived from sd(boot_mean$t) that estimates the variance of the sample mean.\n\nboot_lm$t0\n\n(Intercept)       displ \n  35.697651   -3.530589 \n\nhead(boot_lm$t)\n\n         [,1]      [,2]\n[1,] 37.07809 -3.862234\n[2,] 35.37438 -3.421966\n[3,] 35.15120 -3.383944\n[4,] 38.05982 -4.184231\n[5,] 37.04109 -3.847217\n[6,] 35.22920 -3.335708\n\n\n\n\n\n\n\n\n\n\nWe can get the confidence intervals for intercept and slope using boot.ci() as we do in Chapter 15.\n\nboot.ci(boot.out = boot_lm, index = 1, \n        type = c(\"norm\",\"basic\", \"perc\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_lm, type = c(\"norm\", \"basic\", \"perc\"), \n    index = 1)\n\nIntervals : \nLevel      Normal              Basic              Percentile     \n95%   (33.66, 37.48 )   (33.52, 37.78 )   (33.62, 37.87 )  \nCalculations and Intervals on Original Scale\nSome basic intervals may be unstable\nSome percentile intervals may be unstable\n\nboot.ci(boot.out = boot_lm, index = 2,\n        type = c(\"norm\",\"basic\", \"perc\"))\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 100 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = boot_lm, type = c(\"norm\", \"basic\", \"perc\"), \n    index = 2)\n\nIntervals : \nLevel      Normal              Basic              Percentile     \n95%   (-4.030, -2.954 )   (-4.071, -2.941 )   (-4.121, -2.990 )  \nCalculations and Intervals on Original Scale\nSome basic intervals may be unstable\nSome percentile intervals may be unstable\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere are several ways of conducting bootstrapping in R. Here we choose to to use the boot package. Other popular packages include car, and infer packages of the tidymodels ecosystem. The car package is useful for regression analysis, and tidymodels is a trendy R framework of doing data science and machine learning.\n\n\n\n\nTo do bootstrapped regression in Python, we can use the resample() function in sklearn.utils to resample the response and predictor values in the data set with replacement. For each resmaple data, we fit the ols regression, then extract it parameter estimates. All boostrapped samples of coefficients are saved in the object boot_coefs. For better presentation, it is converted into a DataFrame.\n\n# Bootstrapping\nfrom sklearn.utils import resample\nboot_coefs = [ols('hwy ~ displ', data=resample(mpg[['displ', 'hwy']], replace=True)).fit().params for i in range(100)]\nboot_coefs = pd.DataFrame(boot_coefs)\nboot_coefs.head()\n\n   Intercept     displ\n0  34.599463 -3.171666\n1  36.953521 -3.908881\n2  36.040485 -3.671777\n3  35.873943 -3.603737\n4  36.109551 -3.692238\n\n\n\n# Histograms of bootstrap distribution\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.hist(boot_coefs['Intercept'])\nplt.title(\"Bootstrap Distribution: Intercept\")\nplt.subplot(1, 2, 2)\nplt.hist(boot_coefs['displ'])\nplt.title(\"Bootstrap Distribution: Slope\")\nplt.show()\n\n\n\n\n\n\n\nThe bootstrapped confidence intervals are obtained by finding the 2.5% and 97.5% quantiles of the bootstrapped samples.\n\nnp.percentile(a=boot_coefs['Intercept'], q=[2.5, 97.5])\n\narray([34.43496732, 37.51953612])\n\nnp.percentile(a=boot_coefs['displ'], q=[2.5, 97.5])\n\narray([-4.07076726, -3.1026541 ])",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#relationship-between-anova-and-regression",
    "href": "model-reg.html#relationship-between-anova-and-regression",
    "title": "27¬† Simple Linear Regression",
    "section": "\n27.8 Relationship between ANOVA and Regression*",
    "text": "27.8 Relationship between ANOVA and Regression*\nThe predictor in our regression example is displ, which is a numerical variable. Remember that the regression model can also have categorical predictors. In Chapter 24, we learn the ANOVA, a model for examining whether or not the factors affect the mean response level. These factors are in fact a ‚Äúvalue‚Äù of some categorical variable being studied. For example, when we examine the effect of different treatments (None, Fertilizer, Irrigation, Fertilizer and Irrigation) on the mean weights of poplar trees, we are examining the relationship between the two variables, the numerical response variable weight of poplar tree and the categorical explanatory variable treatment type.\nIn fact, when all the predictors in the regression model are categorical variables, ANOVA and regression are somewhat equivalent. Let‚Äôs first learn how to express a categorical variable in a regression model, then see why ANOVA can be viewed as a specific regression model.\n\n27.8.1 Categorical predictors and dummy variables\nSuppose now we want to see how the transmission type trans affects high miles per gallon hwy.\n\n\nR\nPython\n\n\n\n\n\n# A tibble: 234 √ó 2\n     hwy trans     \n   &lt;int&gt; &lt;chr&gt;     \n 1    29 auto(l5)  \n 2    29 manual(m5)\n 3    31 manual(m6)\n 4    30 auto(av)  \n 5    26 auto(l5)  \n 6    26 manual(m5)\n 7    27 auto(av)  \n 8    26 manual(m5)\n 9    25 auto(l5)  \n10    28 manual(m6)\n# ‚Ñπ 224 more rows\n\n\n\n\n\n\n     hwy       trans\n0     29    auto(l5)\n1     29  manual(m5)\n2     31  manual(m6)\n3     30    auto(av)\n4     26    auto(l5)\n..   ...         ...\n229   28    auto(s6)\n230   29  manual(m6)\n231   26    auto(l5)\n232   26  manual(m5)\n233   26    auto(s6)\n\n[234 rows x 2 columns]\n\n\n\n\n\nNote that trans has so many different categories or ‚Äúvalues‚Äù because of the parentheses terms. We can remove them so that the trans variable only has two categories: auto and manual, where auto means automatic transmission, and manual means manual transmission. The cleaned data set is called mpg_trans.\n\n\nR\nPython\n\n\n\n\nmpg_trans[, c(\"hwy\", \"trans\")]\n\n# A tibble: 234 √ó 2\n     hwy trans \n   &lt;int&gt; &lt;chr&gt; \n 1    29 auto  \n 2    29 manual\n 3    31 manual\n 4    30 auto  \n 5    26 auto  \n 6    26 manual\n 7    27 auto  \n 8    26 manual\n 9    25 auto  \n10    28 manual\n# ‚Ñπ 224 more rows\n\n\nBefore we fit a regression model, we need to make sure that the categorical variable is of type character or factor in R.\n\ntypeof(mpg_trans$trans)\n\n[1] \"character\"\n\n\nWe use exactly the same command lm() to fit the regression when the predictor is categorical.\n\nlm(hwy ~ trans, data = mpg_trans)\n\n\nCall:\nlm(formula = hwy ~ trans, data = mpg_trans)\n\nCoefficients:\n(Intercept)  transmanual  \n     22.293        3.486  \n\n\n\n\n\nmpg_trans[[\"hwy\", \"trans\"]]\n\n     hwy   trans\n0     29    auto\n1     29  manual\n2     31  manual\n3     30    auto\n4     26    auto\n..   ...     ...\n229   28    auto\n230   29  manual\n231   26    auto\n232   26  manual\n233   26    auto\n\n[234 rows x 2 columns]\n\n\nWe use exactly the same command ols() to fit the regression when the predictor is categorical.\n\nreg_fit_trans = ols('hwy ~ trans', data=mpg_trans).fit()\nreg_fit_trans.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\nhwy\nR-squared:\n0.076\n\n\nModel:\nOLS\nAdj. R-squared:\n0.072\n\n\nMethod:\nLeast Squares\nF-statistic:\n19.08\n\n\nDate:\nTue, 15 Oct 2024\nProb (F-statistic):\n1.89e-05\n\n\nTime:\n10:36:36\nLog-Likelihood:\n-739.78\n\n\nNo. Observations:\n234\nAIC:\n1484.\n\n\nDf Residuals:\n232\nBIC:\n1490.\n\n\nDf Model:\n1\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n22.2930\n0.458\n48.696\n0.000\n21.391\n23.195\n\n\ntrans[T.manual]\n3.4862\n0.798\n4.368\n0.000\n1.914\n5.059\n\n\n\n\nOmnibus:\n5.531\nDurbin-Watson:\n0.610\n\n\nProb(Omnibus):\n0.063\nJarque-Bera (JB):\n5.215\n\n\nSkew:\n0.350\nProb(JB):\n0.0737\n\n\nKurtosis:\n3.209\nCond. No.\n2.41\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\nreg_fit_trans.params\n\nIntercept          22.292994\ntrans[T.manual]     3.486227\ndtype: float64\n\n\n\n\n\nWriting code is a piece of cake. The more important thing is how we interpret the fitted result. Here, the variable trans has 2 categories. R will choose one as the baseline level that is used to compare to other levels, in this case, the only other variable. By default, R will choose the baseline level according to the category names alphabetically. Since a comes earlier than m, R chooses auto as the baseline level.\nThe idea is that when we fit a regression model with a categorical variable, we are learning how the (mean) response value changes when the category changes from the baseline level to another level. Notice that in the regression output, the slope coefficient label is transmanual. It is the concatenation of the categorical variable‚Äôs name trans and the non-baseline level manual. The slope tells us the changes in hwy when the transmission is changed from auto to manual.\nMathematically the fitted regression equation is\n\\[\\widehat{hwy_{i}} = 22.3 + 3.49~trans_i\\]\nNow the question is, how we define the value of \\(trans_i\\) so that the equation makes sense, and it represents what we want do to for fitting the regression. The answer is, we define the trans variable using a dummy variable or indicator variable such as\n\\[trans = \\begin{cases} 1  & \\quad \\text{if manual transmission} \\\\ 0  & \\quad \\text{if automatic transmission} \\end{cases}\\] The new dummy variable \\(trans\\) has values 0 or 1 only, and they are representing the two categories auto and manual. Furthermore, when the dummy variable is used in regression, the baseline level is always assigned value 0.\nNow we are ready to interpret our fitted regression result.\n\n\nSlope: Cars with manual transmission are expected, on average, to be 3.49 more miles per gallon than cars with auto transmission.\n\nCompare baseline level (trans = auto) to the other level (trans = manual)\n\n\n\nIntercept: Cars with auto transmission are expected, on average, to have 22.3 highway miles per gallon.\n\nBasically, the intercept value 22.3 is the average hwy MPG for cars with auto transmission because when \\(trans_i\\) is auto, \\(\\widehat{hwy_{i}} = 22.3 + 3.49~(0) = 22.3\\). The intercept + slope (22.3 + 3.49) is the average hwy MPG for cars with manual transmission because when \\(trans_i\\) is manual, \\(\\widehat{hwy_{i}} = 22.3 + 3.49~(1) = 22.3 + 3.49\\).\nIn general, when the categorical variable has \\(k\\) categories, it needs \\(k-1\\) dummy variables used in the regression model. Take regression or linear model courses for more discussion on categorical variables with more levels and dummy variables.\n\n27.8.2 ANOVA as a regression model\nWhy ANOVA can be viewed as regression? Let‚Äôs first refresh our memory of ANOVA model.\n\\[y_{ij} = \\mu + \\tau_i + \\epsilon_{ij}, \\quad j = 1, 2, \\dots, n_i, \\quad i = 1, 2, \\dots, t\\]\n\n\n\\(\\mu\\) is the overall mean across all \\(t\\) populations.\n\n\\(\\tau_i\\) is the effect due to \\(i\\)-th treatment.\n\n\\(\\epsilon_{ij}\\) is the random deviation of \\(y_{ij}\\) about the \\(i\\)-th population mean \\(\\mu_i = \\mu+ \\tau_i\\).\n\n\\(\\mu\\) and \\(\\tau_i\\) are unknown constants.\nANOVA is a linear model.\n\nThe assumptions of ANOVA are\n\n\n\\(\\epsilon_{ij}\\)s are independent and normally distributed with mean 0.\n\n\\(Var(\\epsilon_{ij}) = \\sigma^2\\) ( a constant value )\n\\(\\epsilon_{ij} \\sim N(0, \\sigma^2)\\)\n\nIn our transmission example, \\(t = 2\\). For simplicity, we can drop the subscript \\(i\\) for bookkeeping the categories or groups, and rewrite our model as\n\\[y_{j} = \\mu + \\tau_j + \\epsilon_{j}, \\quad j = 1, 2, \\dots, n, \\quad \\epsilon_j\\sim N(0, \\sigma^2).\\]\nNow \\(y_{j}\\) is the \\(j\\)th response in the data whose value is determined by the overall mean \\(\\mu\\), and the treatment effect \\(\\tau_j\\) depending on which group/category this response is in, i.e.,\n\\[\\tau_j = \\begin{cases} \\tau_m  & \\quad \\text{if } j \\text{th car has manual transmission} \\\\ \\tau_a  & \\quad \\text{if } j \\text{th car has automatic transmission} \\end{cases}\\]\nDoes this model look like a regression model? In general, for a regression model with a categorical variable, we can write the model as\n\\[y_{j} = \\beta_0 + \\beta_1x_j + \\epsilon_{j}, \\quad j = 1, 2, \\dots, n, \\quad \\epsilon_j\\sim N(0, \\sigma^2),\\] where \\[x_j = \\begin{cases} 1  & \\quad \\text{if } j \\text{th subject is in the non-baseline level} \\\\ 0  & \\quad \\text{if } j \\text{th subject is in the baseline level}  \\end{cases}\\]\nWe are done! If we let \\(\\mu = \\beta_0\\), \\(\\tau_j = \\beta_1x_j\\), i.e., \\(\\tau_m = \\beta_1\\) and \\(\\tau_a = 0\\), the two models are equivalent.\nBy default, the regression model assumes the treatment effect of the baseline group is zero. Therefore \\(\\mu\\) is in fact the baseline group mean. The regression slope \\(\\beta_1\\) is actually the treatment effect of the other group, \\(\\tau_m\\). Testing \\(H_0: \\tau_m = \\tau_a\\) is equivalent to testing whether or not \\(\\beta_1\\) is zero, \\(H_0: \\beta_1 = 0\\). In other words, the baseline group and the non-baseline group have no effect on the response value. The categorical variable does not contribute to the changes in the response, and therefore the mean response is at the same level regardless of categories or groups. Although here we use the factor having 2 categories as an example, the discussion here can be generalized to the categorical variable with more than two categories.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#exercises",
    "href": "model-reg.html#exercises",
    "title": "27¬† Simple Linear Regression",
    "section": "\n27.9 Exercises",
    "text": "27.9 Exercises\nUse the data in the table below to answer questions 1-7.\n\n\nTar\n24\n28\n21\n23\n20\n22\n20\n25\n\n\n\n\nNicotine\n1.6\n1.7\n1.2\n1.4\n1.1\n1.0\n1.3\n1.2\n\n\n\n\n\nConstruct a scatterplot using tar for the \\(x\\) axis and nicotine for the \\(y\\) axis. Does the scatterplot suggest a linear relationship between the two variables? Are they positively or negatively related?\nLet \\(y\\) be the amount of nicotine and let \\(x\\) be the amount of tar. Fit a simple linear regression to the data and identify the sample regression equation.\nWhat percentage of the variation in nicotine can be explained by the linear correlation between nicotine and tar?\nThe Raleigh brand king size cigarette is not included in the table, and it has 21 mg of tar. What is the best predicted amount of nicotine? How does the predicted amount compare to the actual amount of 1.2 mg of nicotine? What is the value of residual?\nPerform the test \\(H_0: \\beta_1 = 0\\) vs.¬†\\(H_1: \\beta_1 \\ne 0\\).\nProvide 95% confidence interval for \\(\\beta_1\\).\nGenerate the ANOVA table for the linear regression.\n\n\n\n\nCorrelation (30 points): Match each correlation to the corresponding scatterplot.\n\n\\(R = -0.65\\)\n\\(R = 0.47\\)\n\\(R = 0.03\\)\n\\(R = 0.93\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression (30 points): The following regression output is for predicting the heart weight (in g) of cats from their body weight (in kg).\n\nWrite out the simple linear regression equation.\nWhich one is the response variable and which one is the predictor (explanatory variable)?\nInterpret the slope and intercept.\n\n\n\n\n\n(Intercept)\n-0.346\n\n\nbody wt\n3.953",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-reg.html#footnotes",
    "href": "model-reg.html#footnotes",
    "title": "27¬† Simple Linear Regression",
    "section": "",
    "text": "The random variables \\(X\\) and \\(Y\\) have the population correlation coefficient defined by \\[\\rho = \\frac{E\\left[\\left(X - \\mu_X \\right)\\left(Y - \\mu_Y\\right)\\right]}{\\sigma_X\\sigma_Y},\\] where \\(\\mu_X\\) is the mean of \\(X\\), \\(\\mu_Y\\) is the mean of \\(Y\\), \\(\\sigma_X\\) is the standard deviation of \\(X\\), and \\(\\sigma_Y\\) is the standard deviation of \\(Y\\). For more details, take a probability course or read a probability book.‚Ü©Ô∏é\nWe could use another definition of ‚Äúthe best‚Äù, for example the sum of absolute value of residuals. But the sum of squared residuals is the most commonly used one.‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>27</span>¬† <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "model-mlr.html",
    "href": "model-mlr.html",
    "title": "28¬† Multiple Linear Regression*",
    "section": "",
    "text": "28.1 Why Multiple Regression?\nThe first question you may ask is why we want to use multiple regression? In practice, we often have more than one predictor in a given study, and our target response may be affected by several factors. For example, how amount of money spent on advertising on different media affect the total sales of some product? We may need more than one predictors because usually companies will spend money on several different media, not just one.\nYou may wonder, how about we just fit three separate simple linear regression models, one for each predictor. Yes, we could do that. And if we do this, we‚Äôll see that, as shown in the figure below, advertising on the 3 media is valuable because the more the money we put in, the higher sales of products we‚Äôll get. However, fitting a separate SLR model for each predictor is not satisfactory. Let‚Äôs see why.\nI hope you don‚Äôt feel‚Ä¶\nWhat I hope is‚Ä¶",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Multiple Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-mlr.html#why-multiple-regression",
    "href": "model-mlr.html#why-multiple-regression",
    "title": "28¬† Multiple Linear Regression*",
    "section": "",
    "text": "Data: total sales \\((Y)\\) and amount of money spent on advertising on TV \\((X_1)\\), YouTube \\((X_2)\\), and Instagram \\((X_3)\\).\nWe want to predict sales based on the three advertising expenditures and see which medium is more effective.\nTotal sales \\((Y)\\) and amount of money spent on advertising on YouTube (YT) \\((X_1)\\), Facebook (FB) \\((X_2)\\), Instagram (IG) \\((X_3)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredict sales based on the three advertising expenditures and see which medium is more effective.\n\n\n\n\n\n\nüëâ How to make a single prediction of sales given levels of the 3 advertising media budgets?\n\n\n How to predict the sales when the amount spent on YT is 50, 100 on FB and 30 on IG? \nIt is unclear how to make a single prediction of sales given levels of the three advertising media budgets, since each of the budgets is associated with a separate regression equation.\n\n\n\nüëâ Each regression equation ignores the other 2 media in forming coefficient estimates.\n\n The effect of FB advertising on sales may be increased or decreased when YT and IG advertising are in the model. \n\n IG advertising may have no impact on sales when YT and FB advertising are in the model. \nIf the three media budgets are correlated with each other, this can lead to very misleading estimates of the individual media effects on sales.\n\n\n\nüëçüëç Better approach: extend the SLR model so that it can directly accommodate multiple predictors.\n\n\n\n\n\n\n\n\nSource: https://memegenerator.net/instance/62248186/first-world-problems-multiple-regression-more-like-multiple-depression\n\n\n\n\n\n\n\n\n\n\nhttps://www.tldrpharmacy.com/content/how-to-be-awesome-at-biostatistics-and-literature-evaluation-part-iii",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Multiple Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-mlr.html#multiple-linear-regression-mlr-model",
    "href": "model-mlr.html#multiple-linear-regression-mlr-model",
    "title": "28¬† Multiple Linear Regression*",
    "section": "\n28.2 Multiple Linear Regression (MLR) Model",
    "text": "28.2 Multiple Linear Regression (MLR) Model\nSuppose we have \\(k\\) distinct predictors. The (population) multiple linear regression model is \\[Y_i= \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\dots + \\beta_kX_{ik} + \\epsilon_i\\]\n\n\\(X_{ij}\\): \\(j\\)-th regressor value on \\(i\\)-th measurement, \\(j = 1, \\dots, k\\).\n\\(\\beta_j\\): \\(j\\)-th coefficient quantifying the association between \\(X_j\\) and \\(Y\\).\n\nIn the advertising example, \\(k = 3\\) and \\[\\texttt{sales} = \\beta_0 + \\beta_1 \\times \\texttt{YouTube} + \\beta_2 \\times  \\texttt{Facebook} + \\beta_3 \\times \\texttt{Instagram} + \\epsilon\\] We interpret \\(\\beta_j\\), \\(j = 1, \\dots, p\\), as the average effect on \\(Y\\) of a one unit increase in \\(X_j\\), holding all other predictors fixed. Later, we will learn how to interpret the coefficients in more detail.\nThe model assumptions are same same as SLR that \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2).\\) When \\(k = 1\\), MLR is reduced to SLR.\n\n\n\n\nGiven the training sample data \\((x_{11}, \\dots, x_{1k}, y_1), (x_{21}, \\dots, x_{2k}, y_2), \\dots, (x_{n1}, \\dots, x_{nk}, y_n),\\) the sample MLR model is \\[\\begin{align}\ny_i &= \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\dots + \\beta_k x_{ik} + \\epsilon_i \\\\\n&= \\beta_0 + \\sum_{j=1}^k\\beta_j x_{ij} + \\epsilon_i, \\quad i = 1, 2, \\dots, n.\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\n\n\n\nNow I‚Äôm gonna show you what a MLR looks like when we fit it to the data. If we have two predictors, we will have a sample regression plane. If we have more than two predictors in the model, we are not able to visualize it, but the idea is the same. We will have something called hyperplane or response surface that basically play the same role as the regression plane in 2D or regression line in 1D.\nThe plot on the right is the contour plot when we project the plot onto the \\(X_1\\)-\\(X_2\\) plane. You can see that basically the higher \\(X_1\\) and/or the higher \\(X_2\\), the higher value of \\(Y\\). Moreover, you can see that the level curves are straight and parallel, meaning that the effect of \\(X_1\\) on \\(Y\\) does not change with the values of \\(X_2\\) or the effect does not depend on the level of \\(X_2\\).\n\n\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\epsilon\\)\n\\(E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember in SLR, we can have a liner model that describes a nonlinear relationship. Same in MLR. We can have a linear model that generates a nonlinear response surface!\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{12}x_1x_2 + \\epsilon\\)\nThis is in fact a linear regression model: let \\(\\beta_3 = \\beta_{12}, x_3 = x_1x_2\\).\n\\(E(y \\mid x_1, x_2) = 50 + 10x_1 + 7x_2 + 5x_1x_2\\)\nüòé ü§ì A linear model generates a nonlinear response surface!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nüòé ü§ì A linear regression model can describe a complex nonlinear relationship between the response and predictors! The following is a 2nd order model with interaction.\n\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_{2}x_2 + \\beta_{11}x_1^2 + \\beta_{22}x_2^2 + \\beta_{12}x_1x_2 + \\epsilon\\)\n\\(E(y) = 800+10x_1+7x_2 -8.5x_1^2-5x_2^2- 4x_1x_2\\)",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Multiple Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-mlr.html#estimation-of-model-parameters",
    "href": "model-mlr.html#estimation-of-model-parameters",
    "title": "28¬† Multiple Linear Regression*",
    "section": "\n28.3 Estimation of Model Parameters",
    "text": "28.3 Estimation of Model Parameters\n\n28.3.1 Least Squares Estimation (LSE)\nAs SLR, we can define the least-squares function as the sum of squares of epsilon.     The least-squares function is \\[S(\\alpha_0, \\alpha_1, \\dots, \\alpha_k) = \\sum_{i=1}^n\\left(y_i - \\alpha_0 - \\sum_{j=1}^k\\alpha_j x_{ij}\\right)^2\\] The function \\(S(\\cdot)\\) must be minimized with respect to the coefficients, i.e., \\[(b_0, b_1, \\dots, b_k) = \\underset{{\\alpha_0, \\alpha_1, \\dots, \\alpha_k}}{\\mathrm{arg \\, min}}  S(\\alpha_0, \\alpha_1, \\dots, \\alpha_k)\\]\nWe are going to choose the sample statistics \\(b_0\\), \\(b_1\\), ‚Ä¶, \\(b_k\\) as the estimates of \\(\\beta_0, \\beta_1, \\dots, \\beta_k\\) so that \\(S(.)\\) is minimized when \\(b_0\\), \\(b_1\\), ‚Ä¶, \\(b_k\\) are plugged in the function.\n\n\n\nIf we look at the geometry of least squares estimation of MLR, we have a visualization like this. Again, in SLR, different \\(b_0\\) and \\(b_1\\)s give us different sample regression lines. In MLR, suppose we have two predictors, and different \\(b_0\\) and \\(b_1\\) and \\(b_2\\) give us a different sample regression plane. Geometrically speaking, we are trying to find a sample regression plane such that the sum of the squared distance between the observations (denoted by those blue points) and the plane is minimized. For more than 2 predictor case, we are not able to visualize it because we live a 3D world, but the idea is exactly the same. And we called the regression plane a hyperplane.\n\n\n\n\n\n\n\n\n\n\n\nAgain similar to SLR, we can take derivative w.r.t \\(\\beta_0\\), \\(\\beta_1\\), to the \\(\\beta_k\\). And we are gonna have \\(p = k + 1\\) equations with \\(p\\) unknown parameters. So we can find one and only one solution to \\(\\beta_0\\), \\(\\beta_1\\), to the \\(\\beta_k\\), which are \\(b_0\\), \\(b_1\\), ‚Ä¶, \\(b_k\\). And the \\(p\\) equations are the least squares normal questions. The ordinary least squares estimators are the solutions to the normal equations.  \n\\[\\begin{align}\n\\left.\\frac{\\partial S}{\\partial\\alpha_0}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right) = 0\\\\\n\\left.\\frac{\\partial S}{\\partial\\alpha_j}\\right\\vert_{b_0, b_1, \\dots, b_k} &= -2 \\sum_{i=1}^n\\left(y_i - b_0 - \\sum_{j=1}^k b_j x_{ij}\\right)x_{ij} = 0, \\quad j = 1, 2, \\dots, k\n\\end{align}\\]\n\n\n\n\n\n\n\n\n\nBased on the MLR model and assumptions, the LS estimator \\({\\bf b} = (b_0, b_1, \\dots, b_k)'\\) is BLUE, Best Linear Unbiased Estimator. üëç\n\n Linear : Each \\(b_j\\) is a linear combination of \\(y_1, \\dots, y_n\\).\n Unbiased : Each \\(b_j\\) is normally distributed with mean \\(\\beta_j\\).\n Best : Each \\(b_j\\) has the minimum variance, comparing to all other unbiased estimator for \\(\\beta_j\\) that is a linear combo of \\(y_1, \\dots, y_n\\).\n\n\n Example: Least Squares Estimation \n\nIn this chapter, we use Delivery Time Data of Example 3.1 from Introduction to Linear Regression Analysis, 6th edition to demo MLR.\n\n\nR\nPython\n\n\n\n\n# Load the data set\ndelivery &lt;- read.csv(file = \"./data/data-ex-3-1.csv\", header = TRUE)\ndelivery_data &lt;- delivery[, -1]\ncolnames(delivery_data) &lt;- c(\"time\", \"cases\", \"distance\")\ndelivery_data\n\n   time cases distance\n1  16.7     7      560\n2  11.5     3      220\n3  12.0     3      340\n4  14.9     4       80\n5  13.8     6      150\n6  18.1     7      330\n7   8.0     2      110\n8  17.8     7      210\n9  79.2    30     1460\n10 21.5     5      605\n11 40.3    16      688\n12 21.0    10      215\n13 13.5     4      255\n14 19.8     6      462\n15 24.0     9      448\n16 29.0    10      776\n17 15.3     6      200\n18 19.0     7      132\n19  9.5     3       36\n20 35.1    17      770\n21 17.9    10      140\n22 52.3    26      810\n23 18.8     9      450\n24 19.8     8      635\n25 10.8     4      150\n\n\n\n\n\nimport pandas as pd\ndelivery_data = pd.read_csv('./data/delivery_data.csv')\ndelivery_data\n\n     time  cases  distance\n0   16.68      7       560\n1   11.50      3       220\n2   12.03      3       340\n3   14.88      4        80\n4   13.75      6       150\n5   18.11      7       330\n6    8.00      2       110\n7   17.83      7       210\n8   79.24     30      1460\n9   21.50      5       605\n10  40.33     16       688\n11  21.00     10       215\n12  13.50      4       255\n13  19.75      6       462\n14  24.00      9       448\n15  29.00     10       776\n16  15.35      6       200\n17  19.00      7       132\n18   9.50      3        36\n19  35.10     17       770\n20  17.90     10       140\n21  52.32     26       810\n22  18.75      9       450\n23  19.83      8       635\n24  10.75      4       150\n\n\n\n\n\n\n\\(y\\): the amount of time required by the route driver to stock the vending machines with beverages\n\\(x_1\\): the number of cases stocked\n\\(x_2\\): the distance walked by the driver\nGoal: fit a MLR model \\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\epsilon\\) to the amount of time required by the route driver to service the vending machines\n\n\n\n\n\n\n\nNote\n\n\n\nAlways get to know your data set before you fit any statistical or machine learning model to the data.\n\n\nEach plot shows the relationship between a pair of variables.\n\n\nR\nPython\n\n\n\n\npairs(delivery_data)\n\n\n\n\n\n\n\n\nSometimes a 3D scatterplot is useful in visualizing the relationship between the response and the regressors when there are only two regressors.\n\nCodepar(mgp = c(2, 0.8, 0), las = 1, mar = c(4, 4, 0, 0))\nlibrary(scatterplot3d)\nscatterplot3d(x = delivery_data$cases, y = delivery_data$distance, z = delivery_data$time,\n              xlab =\"cases\", ylab = \"distance\", zlab = \"time\",\n              xlim = c(2, 30), ylim = c(36, 1640), zlim = c(8, 80),\n              box = TRUE, color = \"blue\", mar = c(3, 3, 0, 2), angle = 30, pch = 16)\n\n\n\n\n\n\n\nTo fit the MLR model, we again use lm(). In the furmula argument, we use + to add predictors. coef(delivery_lm) or delivery_lm$coef can be used to grab the LS estimates of coefficients.\n\ndelivery_lm &lt;- lm(time ~ cases + distance, data = delivery_data)\ncoef(delivery_lm)\n\n(Intercept)       cases    distance \n     2.3412      1.6159      0.0144 \n\n\n\n\n\nimport matplotlib.pyplot as plt\npd.plotting.scatter_matrix(delivery_data, figsize=(8, 8))\n\narray([[&lt;Axes: xlabel='time', ylabel='time'&gt;,\n        &lt;Axes: xlabel='cases', ylabel='time'&gt;,\n        &lt;Axes: xlabel='distance', ylabel='time'&gt;],\n       [&lt;Axes: xlabel='time', ylabel='cases'&gt;,\n        &lt;Axes: xlabel='cases', ylabel='cases'&gt;,\n        &lt;Axes: xlabel='distance', ylabel='cases'&gt;],\n       [&lt;Axes: xlabel='time', ylabel='distance'&gt;,\n        &lt;Axes: xlabel='cases', ylabel='distance'&gt;,\n        &lt;Axes: xlabel='distance', ylabel='distance'&gt;]], dtype=object)\n\nplt.show()\n\n\n\n\n\n\n\nSometimes a 3D scatterplot is useful in visualizing the relationship between the response and the regressors when there are only two regressors.\n\nCodefig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(delivery_data['cases'], \n           delivery_data['distance'], \n           delivery_data['time'])\nax.set_xlabel('Cases')\nax.set_ylabel('Distance')\nax.set_zlabel('Time')\nplt.show()\n\n\n\n\n\n\n\nTo fit the MLR model, we again use ols(). In the furmula argument, we use + to add predictors. delivery_ols.params can be used to grab the LS estimates of coefficients.\n\nfrom statsmodels.formula.api import ols\ndelivery_ols = ols(formula='time ~ cases + distance', data=delivery_data).fit()\ndelivery_ols.params\n\nIntercept    2.341231\ncases        1.615907\ndistance     0.014385\ndtype: float64\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is another function OLS() that takes response vector y and design matrix X as input arguments to fit the linear regression model.\n\nimport statsmodels.api as sm\nX = delivery_data[['cases', 'distance']]\nX = sm.add_constant(X)  # Adds a constant term to the predictor\ny = delivery_data['time']\ndelivery_OLS = sm.OLS(y, X).fit()\n\n\n\n\n\n\n\n\\[\\hat{y} = 2.34 + 1.62x_1 + 0.014x_2\\] Interpretation of coefficients needs additional attention.\n\n\\(b_1\\): All else held constant, for one case of product stocked increase, we expect the delivery time to be longer, on average, by 1.62 minutes.\n\\(b_2\\): All else held constant, one additional foot walked by the driver causes the delivery time, on average, to be 0.014 minutes longer.\n\\(b_0\\): The delivery time with no number of cases of product stocked and no distance walked by the driver is expected to be 2.34 minutes. (Make sense?!)\n\nWhen we interpret slopes or the effect of any predictor on the response in MLR, for example, \\(x_1\\), it needs to be measured on the same scale, meaning that all other predictors should not change because any change in them will change the response value too, and this response change is not due to \\(x_1\\), and not measured or explained by \\(b_1\\).\nFor regression we usually don‚Äôt pay much attention to \\(\\beta_0\\) because quite often, it does not have natural physical meaning. Still, depending on your research questions, you may be interested in the intercept term. For example, you may want to know your response value when your predictor, temperature, is at value zero. Moreover, it is quite often that we normalize/standardize our variables before we fit MLR. If that is the case, the intercept means the average response level when the predictors are at their average level.\n\nThe LS fitted regression plane is shown below.\n\n\n\n\n\n\n\n\n\n28.3.2 Estimation of \\(\\sigma^2\\)\n\nSame as SLR, the estimate of \\(\\sigma^2\\), denoted as \\(\\hat{\\sigma}^2\\) or \\(s^2\\), is the mean square residual of the model.\nRemember that the sum of squares residual is \\(SS_{res} = \\sum_{i=1}^ne_i^2 = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\). Then the mean square residual is \\(SS_{res}\\) divided by its degrees of freedom: \\(MS_{res} = \\frac{SS_{res}}{n - p}\\) with \\(p = k + 1\\). Note that the degrees of freedom is \\(n - p\\) where \\(p\\) is the number of beta coefficients in the model. When \\(p = 2\\), it goes back to the SLR case.\n\\(S^2 = MS_{res}\\) is unbiased for \\(\\sigma^2\\), i.e., \\(E[MS_{res}] = \\sigma^2\\). Keep in mind that \\(S^2 = MS_{res}\\) is a random variable. Before data are collected \\(Y_i\\) and \\(\\hat{Y}_i\\) are assumed random variables, and \\(S^2\\), a function of random variables, will be a random variable too.\n\\(\\hat{\\sigma}^2 = MS_{res}\\) is model dependent. Its value varies with change of the model. If our model is specified correctly, \\(\\hat{\\sigma}^2\\) depends only on our data quality. If our data have lots of noise itself, there is nothing we can do.\n\\(\\hat{\\sigma}^2\\) of SLR may be quite larger than the \\(\\hat{\\sigma}^2\\) of MLR if the predictors in MLR capture a lots of variation of \\(y\\) that cannot be explained by the only predictor in the SLR, and are treated as noises or unexplained variation. Remember \\(S^2\\) measures the variation or the size of the unexplained noise about the fitted regression line/hyperplane, and we prefer a small residual mean square.\n\n\n\n\n\n\n Example: Estimation of \\(\\sigma^2\\) \n\n\nR\nPython\n\n\n\nHere I show three methods to obtain the \\(\\sigma^2\\) estimate. The method 1 first compute the summary of the fitted result, which is saved as a list, then get the element sigma that is \\(\\hat{\\sigma}\\). If you look at the summary output, the value of \\(\\hat{\\sigma}\\) is shown in the row: Residual standard error: 3.26 on 22 degrees of freedom\n\n## method 1\nsumm_delivery &lt;- summary(delivery_lm)  ## check names(summ_delivery)\nsumm_delivery$sigma ^ 2\n\n[1] 10.6\n\nsummary(delivery_lm)\n\n\nCall:\nlm(formula = time ~ cases + distance, data = delivery_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.788 -0.663  0.436  1.157  7.420 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.34123    1.09673    2.13  0.04417 *  \ncases        1.61591    0.17073    9.46  3.3e-09 ***\ndistance     0.01438    0.00361    3.98  0.00063 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.26 on 22 degrees of freedom\nMultiple R-squared:  0.96,  Adjusted R-squared:  0.956 \nF-statistic:  261 on 2 and 22 DF,  p-value: 4.69e-16\n\n\nThe second method simply uses the definition of \\(MS_{res}\\). We first calculate \\(SS_{res}\\), then divide it by \\(n-3\\) because in this example, we have 2 predictors \\((k = 2)\\), and 3 coefficients \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\).\n\n## method 2\nn &lt;- length(delivery_lm$residuals)\n(SS_res &lt;- sum(delivery_lm$residuals * delivery_lm$residuals))\n\n[1] 234\n\nSS_res / (n - 3)\n\n[1] 10.6\n\n\nThe third method also uses the definition of \\(MS_{res}\\). The difference is that here we use \\(\\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\) instead of \\(\\sum_{i=1}^ne_i^2\\).\n\n## method 3\n(SS_res1 &lt;- sum((delivery_data$time - delivery_lm$fitted.values) ^ 2))\n\n[1] 234\n\nSS_res1 / (n - 3)\n\n[1] 10.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere I show three methods to obtain the \\(\\sigma^2\\) estimate. The method 1 obtains the mean square error of residual from the fitted object.\n\n# Method 1: Residual standard error\ndelivery_ols.mse_resid\n\n10.624167155479675\n\n\nThe second method simply uses the definition of \\(MS_{res}\\). We first calculate \\(SS_{res}\\), then divide it by \\(n-3\\) because in this example, we have 2 predictors \\((k = 2)\\), and 3 coefficients \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\beta_2\\).\n\n# Method 2: Residual Sum of Squares (RSS) and variance estimation\nn = len(delivery_ols.resid)\nimport numpy as np\nSS_res = np.sum(delivery_ols.resid ** 2)\nSS_res\n\n233.73167742055284\n\nSS_res / (n - 3)\n\n10.624167155479675\n\n\nThe third method also uses the definition of \\(MS_{res}\\). The difference is that here we use \\(\\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\) instead of \\(\\sum_{i=1}^ne_i^2\\).\n\n# Method 3: Another way to calculate RSS and variance estimation\nSS_res1 = np.sum((delivery_data['time'] - delivery_ols.fittedvalues) ** 2)\nSS_res1\n\n233.73167742055284\n\nSS_res1 / (n - 3)\n\n10.624167155479675\n\n\n\n\n\n\n28.3.3 Wald CI for Coefficients\nThe \\((1-\\alpha)100\\%\\) Wald CI for \\(\\beta_j\\), \\(j = 0, 1, \\dots, k\\) is \\[\\left(b_j- t_{\\alpha/2, n-p}~se(b_j), \\quad b_j + t_{\\alpha/2, n-p}~ se(b_j)\\right)\\] where \\(se(b_j)\\) is the standard error of \\(b_j\\). The formula come from the fact that each of the statistics \\(\\frac{b_j - \\beta_j}{se(b_j)}, j = 0, 1, 2, \\dots, k\\) follows \\(t_{n-p}\\) distribution. \\(se(b_j)\\) is a function of \\(\\hat{\\sigma}^2\\) and \\(x_{ij}s\\).\n\n Example: Wald CI for Coefficients \n\n\nR\nPython\n\n\n\nWe simply use confint()command with the fitted result put inside to obtain the CI for coefficients.\n\n(ci &lt;- confint(delivery_lm))\n\n              2.5 % 97.5 %\n(Intercept) 0.06675 4.6157\ncases       1.26182 1.9700\ndistance    0.00689 0.0219\n\n\n\n\n\nci = delivery_ols.conf_int()\nci\n\n                  0         1\nIntercept  0.066752  4.615710\ncases      1.261825  1.969990\ndistance   0.006892  0.021878\n\n\n\n\n\nThese are marginal CIs separately for each \\(b_j\\). These interval estimates do not take correlation of coefficients into account. One coefficient may be higher when another is lower. We can only use the intervals one at a time when doing interval estimation. When we want to do interval estimation for several coefficients together at the same time, these intervals are not that accurate.\n\n\n\n\n\n\nCovariance of random variables \\(X\\) and \\(Y\\), \\(\\cov(X, Y)\\) is defined as \\[\\small \\cov(X, Y) = E[(X - E(X))(Y - E(Y))]\\] \n\n\n\n\n\nThe covariance matrix of the coefficient vector \\({\\bf b} = (b_0, b_1, b_2)'\\) is \\[\\scriptsize \\begin{align} \\cov({\\bf b}) &= \\begin{bmatrix} \\cov(b_0, b_0) & \\cov(b_0, b_1) & \\cov(b_0, b_2) \\\\ \\cov(b_1, b_0) & \\cov(b_1, b_1) & \\cov(b_1, b_2) \\\\ \\cov(b_2, b_0) & \\cov(b_2, b_1) & \\cov(b_2, b_2) \\end{bmatrix} \\end{align}\\]\n\nThe correlation matrix of the coefficient vector \\({\\bf b} = (b_0, b_1, b_2)'\\) is \\[\\scriptsize \\begin{align} \\cor({\\bf b}) &= \\begin{bmatrix} 1 & r_{01} & r_{02} \\\\ r_{10} & 1 & r_{12} \\\\ r_{20} & r_{21} & 1 \\end{bmatrix} \\end{align}\\]\n\n\n In fact, all \\(b_j\\) are correlated! \n\n\n\n\n Example: Correlated Coefficients \n\n\nR\nPython\n\n\n\nWe use vcov() to obtain the variance-covariance matrix of \\(b_j\\)s. The square root of its diagonal terms are \\(se(b_0)\\), \\(se(b_1)\\), and \\(se(b_2)\\) respectively.\n\n## variance-covariance matrix\n(V &lt;- vcov(delivery_lm))\n\n            (Intercept)     cases  distance\n(Intercept)    1.202817 -0.047263 -8.89e-04\ncases         -0.047263  0.029150 -5.08e-04\ndistance      -0.000889 -0.000508  1.31e-05\n\n## standard error\nsqrt(diag(V))\n\n(Intercept)       cases    distance \n    1.09673     0.17073     0.00361 \n\n\n\n\nWe can convert the covariance matrix into a correlation matrix using cov2cor(). Clearly \\(b_1\\) and \\(b_2\\) are negatively correlated. The individual CI previously obtained ignores the correlation between \\(b_j\\)s.\n\n## correlation matrix\ncov2cor(V)\n\n            (Intercept)  cases distance\n(Intercept)       1.000 -0.252   -0.224\ncases            -0.252  1.000   -0.824\ndistance         -0.224 -0.824    1.000\n\n\n\n\nWe use cov_params() to obtain the variance-covariance matrix of \\(b_j\\)s. The square root of its diagonal terms are \\(se(b_0)\\), \\(se(b_1)\\), and \\(se(b_2)\\) respectively.\n\n## variance-covariance matrix\nV = delivery_ols.cov_params()\nV\n\n           Intercept     cases  distance\nIntercept   1.202817 -0.047263 -0.000889\ncases      -0.047263  0.029150 -0.000508\ndistance   -0.000889 -0.000508  0.000013\n\n## standard error\nnp.sqrt(np.diag(V))\n\narray([1.09673017, 0.17073492, 0.00361309])\n\n\nWe can convert the covariance matrix into a correlation matrix using sm.stats.moment_helpers.cov2corr(). Clearly \\(b_1\\) and \\(b_2\\) are negatively correlated. The individual CI previously obtained ignores the correlation between \\(b_j\\)s.\n\nimport statsmodels.api as sm\n## correlation matrix\nsm.stats.moment_helpers.cov2corr(V)\n\narray([[ 1.        , -0.25240355, -0.22433649],\n       [-0.25240355,  1.        , -0.824215  ],\n       [-0.22433649, -0.824215  ,  1.        ]])\n\n\n\n\n\nHow do we specify a confidence level that applies simultaneously to a set of interval estimates? For example, a \\(95\\%\\) confidence ‚Äúinterval‚Äù for both \\(b_1\\) and \\(b_2\\)?\n\nThe \\((1-\\alpha)100\\%\\) CI for a set of \\(b_j\\)s will be an elliptically-shaped region! The blue region below is the 95% confidence region for \\(\\beta_1\\) and \\(\\beta_2\\). The black dashed lines indicate the 95% Wald CI for \\(\\beta_1\\) and \\(\\beta_2\\).\nWith repeated sampling, 95% of such ellipses will simultaneously include \\(\\beta_1\\) and \\(\\beta_2\\), if the fitted model is correct and normality holds. The orientation of the ellipse reflects the negative correlation between the estimates. Contrast the 95% confidence ellipse with the marginal 95% confidence intervals, also shown on the plot. Some points within the marginal intervals (red point) ‚Äî with smaller values for both of the coefficients, for example ‚Äî are implausible according to the joint region. Similarly, the joint region includes values of the coefficient for cases (black point), for example, that are excluded from the marginal interval.\n\n\n\nCodepar(mgp = c(2.8, 0.9, 0), mar = c(4, 4, 2, 0))\n## confidence region\ncar::confidenceEllipse(\n    delivery_lm, \n    levels = 0.95, fill = TRUE,\n    which.coef = c(\"cases\", \"distance\"), \n    main = expression(\n        paste(\"95% Confidence Region for \", \n              beta[1], \" and \",  beta[2])\n        )\n    )\n## marginal CI for cases\nabline(v = ci[2, ], lty = 2, lwd = 2)  \n## marginal CI for distance\nabline(h = ci[3, ], lty = 2, lwd = 2)\npoints(x = 1.4, y = 0.01, col = \"red\", cex = 2, pch = 16)\npoints(x = 2, y = 0.008, col = \"black\", cex = 2, pch = 16)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28.3.4 CI for the Mean Response \\(E(y \\mid {\\bf x}_0)\\)\n\nThe fitted value at a point \\({\\bf x}_0 = (1, x_{01}, x_{02}, \\dots, x_{0k})'\\) is \\[\\hat{y}_0 = b_0 + b_1x_{01} + \\cdots + b_kx_{0k}\\]\nThis is an unbiased estimator for \\(E(y \\mid {\\bf x}_0)\\). \nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y \\mid {\\bf x}_0)\\) is \\[\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} ~ se(\\hat{y}_0), \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} ~ se(\\hat{y}_0)\\right).\\] It is from the fact that \\(\\frac{\\hat{y}_0 - E(y|{\\bf x}_0)}{se(\\hat{y}_0)}\\sim t_{n-p}\\) where \\(se(\\hat{y}_0)\\) is a function of \\(\\hat{\\sigma}\\), \\(x_{ij}\\)s, and \\({\\bf x}_0\\).\n\n Example: CI for the Mean Response \n\n\nR\nPython\n\n\n\nWe learned how to use predict() in the previous chapter. For MLR, we need to specify the 2 predictor values cases = 8, and distance = 275, and save it as a data frame in the newdata argument. Note that the name cases and distance should be exactly the same as their column name of the data delivery_data.\n\npredict(delivery_lm,\n        newdata = data.frame(cases = 8, distance = 275),\n        interval = \"confidence\", level = 0.95)\n\n   fit  lwr  upr\n1 19.2 17.7 20.8\n\n\n\n\nWe learned how to use get_prediction() in the previous chapter. For MLR, we need to specify the 2 predictor values cases = 8, and distance = 275, and save it as a DataFrame in the exog argument. Note that the name cases and distance should be exactly the same as their column name of the data delivery_data.\n\nnew_data = pd.DataFrame({'cases': [8], 'distance': [275]})\npredict = delivery_ols.get_prediction(exog=new_data)\npredict.conf_int()\n\narray([[17.65389505, 20.79473705]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28.3.5 PI for New Observations\nOften we also want to predict the future observation \\(y_0\\) when \\({\\bf x} = {\\bf x}_0\\). A point estimate is \\(\\hat{y}_0 = b_0 + b_1x_{01} + \\cdots + b_kx_{0k}\\), same as the point estimate of the mean response. When we can only use one single value to predict the mean response or a new observation value, \\(\\hat{y}_0\\) is the best we can do. The \\((1-\\alpha)100\\%\\) PI for \\(y_0\\) is \\[\\left(\\hat{y}_0 - t_{\\alpha/2, n-p} ~se(y_0 - \\hat{y}_0), \\quad \\hat{y}_0 + t_{\\alpha/2, n-p} ~se(y_0 - \\hat{y}_0)\\right)\\]\n\n Example: PI for New Observations \n\n\nR\nPython\n\n\n\nWhen predicting a new observation with uncertainty, we use interval = \"predict\". The interpretation is the same as the one in SLR.\n\npredict(delivery_lm,\n        newdata = data.frame(cases = 8, distance = 275),\n        interval = \"predict\", level = 0.95)\n\n   fit  lwr  upr\n1 19.2 12.3 26.2\n\n\n\n\nWhen predicting a new observation with uncertainty, we use summary_frame(), then grab ‚Äòobs_ci_lower‚Äô and ‚Äòobs_ci_upper‚Äô from the output. The interpretation is the same as the one in SLR.\n\nsummary_frame = predict.summary_frame(alpha=0.05)\nsummary_frame[['obs_ci_lower', 'obs_ci_upper']]\n\n   obs_ci_lower  obs_ci_upper\n0     12.284559     26.164073\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n28.3.6 Predictor Effect Plots\nA complete picture of the regression surface requires drawing a \\(p\\)-dimensional graph.\nPredictor effect plots look at 1 or 2D plots for each predictor.\nTo plot the predictor effect plot for \\(x_j\\), \\(x_1\\) for example, we\n\nfix the values of all other predictors (\\(x_2\\) in the example)\nsubstitute these fixed values into the fitted regression equation.\n\nUsually we fix all other predictors (\\(x_2\\) in the example) at their average. In our example, we have \\(\\hat{y} = 2.34 + 1.62 ~x_1 + 0.014 (409.28)\\). Note that the slope of \\(x_1\\) would be the same for any choice of fixed values of other predictors, while the intercept depends on the values of other predictors.\n\n\n\n\n\n\n\n Example: Predictor Effect Plots \n\n\nR\nPython\n\n\n\nThe command predictorEffects() in the effects package is used to generate the predictor effect plots.\n\nlibrary(effects)\nplot(effects::predictorEffects(mod = delivery_lm))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe shaded area represents pointwise 95% confidence interval about the fitted line, without correction for simultaneous statistical inference. The short vertical lines at the bottom indicate the values of predictors. The interval length is larger when the predictor value is away from its average. Since our model is linear, the mean of time increases linearly as cases or distance increases.\n\n\nSo far to my knowledge there is no Python function for creating predictor effect plots. However, statsmodels does offer other useful regression plots.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Multiple Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-mlr.html#hypothesis-testing",
    "href": "model-mlr.html#hypothesis-testing",
    "title": "28¬† Multiple Linear Regression*",
    "section": "\n28.4 Hypothesis Testing",
    "text": "28.4 Hypothesis Testing\nIn this section we talk about two tests: Test for significance of regression and Tests on individual coefficients. In fact, one can test whether or not the coefficients form any linear combination relationship, called the general linear hypotheses. We leave this part of discussion in the Regression Analysis course.\n\n\n\n\n\n\n\n\n\n\n\n\n28.4.1 Test for Significance of Regression\nTest for significance determines if there is a linear relationship between the response and any of the regressor variables. In other words, we are testing\n \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0 \\quad H_1: \\beta_j \\ne 0 \\text{ for at least one } j\\) \nAs long as there is one predictor has a significant impact on the response, \\(H_0\\) should be rejected. When \\(\\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0\\), it means that the regrssion model has no explanatory power on explaining any variation of the response from the regressors put in the regression model. The regression model is not helping at all. To interpret it more precisely, when we fail to \\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0\\), there are three possibilities. First, it could mean that all the predictors have no relationship with the response. Second, remember our model is a linear model, and \\(\\beta_j\\)s measure linear effect of \\(x_j\\)s on \\(y\\). It may just mean there is no linear relationship between any \\(x_j\\) and \\(y\\) given all other regressors are in the model. Some other types of relationship may exist between the predictors and the response. Third, we make a Type II error that \\(H_0\\) is actually false, and at least one regressor is linearly related to the response, again given all other predictors are in the model.\n\n\n\n\n\nThe test is usually conducted by checking the ANOVA table of MLR as shown below. When \\(k=1\\), it becomes the ANOVA table of SLR.\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSS\ndf\nMS\nF\n\n\\(p\\)-value\n\n\n\nRegression\n\\(SS_R\\)\n\\(k\\)\n\\(MS_R\\)\n\\(\\frac{MS_R}{MS_{res}} = F_{test}\\)\n\\(P(F_{k, n-k-1} &gt; F_{test})\\)\n\n\nResidual\n\\(SS_{res}\\)\n\\(n-k-1\\)\n\\(MS_{res}\\)\n\n\n\n\nTotal\n\\(SS_{T}\\)\n\\(n-1\\)\n\n\n\n\n\n\nWe reject \\(H_0\\) if \\(F_{test} &gt; F_{\\alpha, k, n - k - 1}\\). In SLR, test for significance is the same as testing individual coefficient \\(\\beta_1 = 0\\) because there is only one predictor, and testing its corresponding coefficient is equivalent to testing all the coefficients.\nWe may reject the null when the truth is all beta‚Äôs are nonzero, or only one single beta is nonzero. And we may need further post hoc analysis to see which coefficients are nonzero.\nIf there is only one nonzero coefficient, and we reject the null, basically, the nearly all the variation of \\(Y\\) is explained by the corresponding predictor that has the nonzero beta coefficient. We can still say the model is significant because that particular predictor has a significant effect on explaining \\(y\\).\n\n\n\n\n\n\n\n\n\n\n Example: Test for Significance \n \\(H_0: \\beta_{1} = \\beta_{2} = 0 \\quad H_1: \\beta_j \\ne 0 \\text{ for at least one } j\\) \n\n\nR\nPython\n\n\n\nIf we check the summary of the fitted result, it shows some t values and p-values, but no ANOVA table shows up.\n\nsumm_delivery\n\n...\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.34123    1.09673    2.13  0.04417 *  \ncases        1.61591    0.17073    9.46  3.3e-09 ***\ndistance     0.01438    0.00361    3.98  0.00063 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.26 on 22 degrees of freedom\nMultiple R-squared:  0.96,  Adjusted R-squared:  0.956 \nF-statistic:  261 on 2 and 22 DF,  p-value: 4.69e-16\n...\n\n\n\n\nIf we check the summary of the fitted result, it shows some t values and p-values, but no ANOVA table shows up.\n\ndelivery_ols.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ntime\nR-squared:\n0.960\n\n\nModel:\nOLS\nAdj. R-squared:\n0.956\n\n\nMethod:\nLeast Squares\nF-statistic:\n261.2\n\n\nDate:\nTue, 15 Oct 2024\nProb (F-statistic):\n4.69e-16\n\n\nTime:\n10:36:45\nLog-Likelihood:\n-63.415\n\n\nNo. Observations:\n25\nAIC:\n132.8\n\n\nDf Residuals:\n22\nBIC:\n136.5\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.3412\n1.097\n2.135\n0.044\n0.067\n4.616\n\n\ncases\n1.6159\n0.171\n9.464\n0.000\n1.262\n1.970\n\n\ndistance\n0.0144\n0.004\n3.981\n0.001\n0.007\n0.022\n\n\n\n\nOmnibus:\n0.421\nDurbin-Watson:\n1.170\n\n\nProb(Omnibus):\n0.810\nJarque-Bera (JB):\n0.010\n\n\nSkew:\n0.032\nProb(JB):\n0.995\n\n\nKurtosis:\n3.073\nCond. No.\n873.\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nHow do we obtain the ANOVA Table?\n\n\n\n\n\n\n\n\n\n\n\nSource of Variation\nSS\ndf\nMS\nF\n\n\\(p\\)-value\n\n\n\nRegression\n\\(SS_R\\)\n\\(k\\)\n\\(MS_R\\)\n\\(\\frac{MS_R}{MS_{res}} = F_{test}\\)\n\\(P(F_{k, n-k-1} &gt; F_{test})\\)\n\n\nResidual\n\\(SS_{res}\\)\n\\(n-k-1\\)\n\\(MS_{res}\\)\n\n\n\n\nTotal\n\\(SS_{T}\\)\n\\(n-1\\)\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nWe may think using anova(delivery_lm) as we do for SLR.\n\nanova(delivery_lm) ## This is for sequential F-test\n\nAnalysis of Variance Table\n\nResponse: time\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)    \ncases      1   5382    5382   506.6 &lt; 2e-16 ***\ndistance   1    168     168    15.8 0.00063 ***\nResiduals 22    234      11                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nWe may think using sm.stats.anova_lm(delivery_ols) as we do for SLR.\n\nsm.stats.anova_lm(delivery_ols)\n\n            df       sum_sq      mean_sq           F        PR(&gt;F)\ncases      1.0  5382.408797  5382.408797  506.619363  1.112549e-16\ndistance   1.0   168.402126   168.402126   15.850854  6.312469e-04\nResidual  22.0   233.731677    10.624167         NaN           NaN\n\n\n\n\n\nUnfortunately, this ANOVA table is so called Type-I ANOVA table in literature for a sequential F-test, which is NOT what we want.\n\n\n\n\n\n\nTo obtain the correct ANOVA table, we need to view the test in a different way. Testing coefficients is like model comparison: We are comparing two models, the full model having all the predictors \\(x_1, \\dots, x_k\\) in the model with zero or nonzero \\(\\beta\\) coefficients with the null model under \\(H_0\\). Mathematically,\n\nFull model: \\(y = \\beta_0 + \\beta_1x_1 + \\cdots + \\beta_kx_k + \\epsilon\\)\nNull model: \\(y = \\beta_0 + \\epsilon\\) because \\(\\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0\\) under \\(H_0\\). The null model is the model with the intercept only.\n\nThe idea is that we are trying to see how close the full model fitting is close to the null model fitting. If \\(H_0\\) is true, then using the full model will be more likely to have similar result to the null models result because the full model will have all the coefficients being negligibly zero. The two models are more likely to be indistinguishable. When \\(H_0\\) is not true, and some \\(\\beta_j\\)s are away from zero, the two models will have pretty different fitting results.\n\n\n\n\nIn our example,\n\nFull model: including both cases and distance predictors\nNull model: no predictors \\((\\beta_1 = \\beta_2 = 0)\\)\n\nThe ANOVA table for the example is\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\nTo generate the ANOVA table using R, we first create the null model and save its fitted result. When there is no predictors in the model, we write time ~ 1 where 1 represents the intercept term. Then we are comparing the two model fitted results using anova(null_lm, delivery_lm).\n\n## regression with intercept only\nnull_lm &lt;- lm(time ~ 1, data = delivery_data)\nanova(null_lm, delivery_lm)\n\n...\nAnalysis of Variance Table\n\nModel 1: time ~ 1\nModel 2: time ~ cases + distance\n  Res.Df  RSS Df Sum of Sq   F  Pr(&gt;F)    \n1     24 5785                             \n2     22  234  2      5551 261 4.7e-16 ***\n...\n\n\n\n\n\nNotice that the output is not exactly the same as the ANOVA table, but both are equivalent. We just need to carefully and correctly interpret the R output. The first row shows the \\(SS_T\\) value in the RSS column. RSS is the residual sum of squares \\(SS_{res}\\). The first model (Model 1) is the null model. Because there is no predictors, all variation of \\(y\\) is due to random errors, and therefore there is no \\(SS_R\\), and \\(SS_{res} = SS_T\\).\nThe second row provides information about \\(SS_R\\) and \\(SS_{res}\\) of the full model (Model 2). \\(SS_R\\) is shown in the column Sum of Sq, and \\(SS_{res}\\) again in the column RSS.\nIn Row 2, Res.Df is the residual degrees of freedom, and Df is the regression degrees of freedom. The value 24 in Row 1, the residual degrees of freedom of the null model, is worked as the total degrees of freedom, the same idea that its \\(SS_{res}\\) works as \\(SS_T\\).\n\n\nTo generate the ANOVA table using Python, we first create the null model and save its fitted result. When there is no predictors in the model, we write time ~ 1 where 1 represents the intercept term. Then we are comparing the two model fitted results using sm.stats.anova_lm(null_ols, delivery_ols).\n\n## regression with intercept only\nnull_ols = ols('time ~ 1', data=delivery_data).fit()\nsm.stats.anova_lm(null_ols, delivery_ols)\n\n   df_resid          ssr  df_diff      ss_diff           F        Pr(&gt;F)\n0      24.0  5784.542600      0.0          NaN         NaN           NaN\n1      22.0   233.731677      2.0  5550.810923  261.235109  4.687422e-16\n\n\nNotice that the output is not exactly the same as the ANOVA table, but both are equivalent. We just need to carefully and correctly interpret the Python output. The first row shows the \\(SS_T\\) value in the ssr column. ssr is the sum of squares residual \\(SS_{res}\\). The first model (Model 1) is the null model. Because there is no predictors, all variation of \\(y\\) is due to random errors, and therefore there is no \\(SS_R\\), and \\(SS_{res} = SS_T\\).\nThe second row provides information about \\(SS_R\\) and \\(SS_{res}\\) of the full model (Model 2). \\(SS_R\\) is shown in the column ss_diff, and \\(SS_{res}\\) again in the column ssr.\nIn Row 2, df_resid is the residual degrees of freedom, and df_diff is the regression degrees of freedom. The value 24 in Row 1, the residual degrees of freedom of the null model, is worked as the total degrees of freedom, the same idea that its \\(SS_{res}\\) works as \\(SS_T\\).\n\n\n\nThe output does not provide the mean square information, but it can be easily calculated.\n\n\n\n\n28.4.2 \\(R^2\\) and Adjusted \\(R^2\\)\n\nWe learn in SLR that \\(R^2 = \\frac{SS_R}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\). The \\(R^2\\) statistic can also be calculated and used in MLR. The size of \\(R^2\\) assesses how well the regression model fits the data‚Äîthe larger the \\(R^2\\), the better the fit. Specifically, \\(R^2\\) measures the proportion of variability in \\(Y\\) that is explained by the regression model or the \\(k\\) predictors. It‚Äôs important to note that adding an additional predictor to the model always increases \\(R^2\\). This happens because the model with \\(k+1\\) predictors will have \\(SS_R\\) that is greater than or at least equal to the \\(SS_R\\) of the model with only \\(k\\) predictors, provided that the \\(k\\) predictors are included among the \\(k+1\\) predictors in the larger model.\nThe model with one additional predictor always gets a higher \\(R^2\\) even the new predictor has no explanatory power or useless in predicting \\(y\\). This happens because the additional predictor may capture random noise in the data, leading to a decrease in \\(SS_{res}\\). If we rely solely on \\(R^2\\) to compare models that include various predictors, we might always end up selecting the full model with all possible predictors, even if many of them contribute little or nothing to the actual fitting or prediction.\nWhile this approach might yield a model that fits the current data very well, it poses significant risks if our goal is to predict new, unseen future data. Including too many variables in the regression model can lead to overfitting‚Äîa situation where the model fits the sample data exceptionally well but performs poorly when predicting the mean response or new response values for a different dataset. Overfitting results in a model that captures noise and idiosyncrasies of the sample data, rather than the underlying patterns, making it less generalizable and less reliable for future predictions.\nA complex or a larger model has several other disadvantages. First, it‚Äôs more difficult to interpret your model and results. It reduces the interpretability. Second, the computing or running time is usually longer, and sometimes much longer depending on the order of complexity. Also, the data or the model itself may consume lots of memory spaces.\n\n\n\nOccam‚Äôs Razor: Don‚Äôt use a complex model if a simpler model can perform equally well!\n\nThere are other criterion or metrics for accessing model adequacy or model fit that may be better than \\(R^2\\), for example, adjusted \\(R^2\\), \\(R^2_{adj}\\).\n\\[R^2_{adj} = 1 - \\frac{SS_{res}/(n-p)}{SS_T/(n-1)}\\]\n\\(R^2_{adj}\\) applies a penalty (through \\(p\\)) for number of variables included in the model. It is not the more the better anymore. Adjusted \\(R^2\\) doesn‚Äôt increase if the new variable provide very little information for prediction. Adjusted \\(R^2\\) will only increase on adding a variable to the model if the addition of the regressor reduces \\(MS_{res}\\). The new added variable must show that it can contribute to explaining the variation of \\(y\\) sufficiently large, so that its contribution is bigger than the price we pay for hiring this guy. This makes adjusted \\(R^2\\) a preferable metric for model selection in multiple regression models.\n\n\n\n\n\nFor a model with 3 predictors, \\(SS_{res} = 90\\), \\(SS_T = 245\\), and \\(n = 15\\). \\[R^2_{adj} = 1 - \\frac{90/(15-4)}{245/(15-1)} = 0.53\\]\nThe 4-th regressor is added into the model, and \\(SS_{res} = 88\\) (always decreases). Then \\[R^2_{adj} = 1 - \\frac{88/(15-5)}{245/(15-1)} = 0.49\\]\n\nThe new added regressor should have explanatory power for \\(y\\) large enough, so that \\(MS_{res}\\) is decreased. In this case, adding the 4th regressor does not decrease \\(SS_{res}\\) enough to convince us to put it in the model.\n\n\n Example: \\(R^2\\) and Adjusted \\(R^2\\) \n\n\nR\nPython\n\n\n\nTo get \\(R^2\\) and adjusted \\(R^2\\) in R, we check the summary output. They can also be extracted from the summary list.\n\nsumm_delivery\n\n...\n\nResidual standard error: 3.26 on 22 degrees of freedom\nMultiple R-squared:  0.96,  Adjusted R-squared:  0.956 \n...\n\n\n\nsumm_delivery$r.squared\n\n[1] 0.96\n\nsumm_delivery$adj.r.squared\n\n[1] 0.956\n\n\n\n\nTo get \\(R^2\\) and adjusted \\(R^2\\) in Python, we check the summary output which is shown at the topright corner. They can also be extracted from the fitted object.\n\ndelivery_ols.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ntime\nR-squared:\n0.960\n\n\nModel:\nOLS\nAdj. R-squared:\n0.956\n\n\nMethod:\nLeast Squares\nF-statistic:\n261.2\n\n\nDate:\nTue, 15 Oct 2024\nProb (F-statistic):\n4.69e-16\n\n\nTime:\n10:36:46\nLog-Likelihood:\n-63.415\n\n\nNo. Observations:\n25\nAIC:\n132.8\n\n\nDf Residuals:\n22\nBIC:\n136.5\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.3412\n1.097\n2.135\n0.044\n0.067\n4.616\n\n\ncases\n1.6159\n0.171\n9.464\n0.000\n1.262\n1.970\n\n\ndistance\n0.0144\n0.004\n3.981\n0.001\n0.007\n0.022\n\n\n\n\nOmnibus:\n0.421\nDurbin-Watson:\n1.170\n\n\nProb(Omnibus):\n0.810\nJarque-Bera (JB):\n0.010\n\n\nSkew:\n0.032\nProb(JB):\n0.995\n\n\nKurtosis:\n3.073\nCond. No.\n873.\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\ndelivery_ols.rsquared\n\n0.9595937494832257\n\ndelivery_ols.rsquared_adj\n\n0.9559204539817008\n\n\n\n\n\n\n28.4.3 Tests on Individual Regression Coefficients\nThis is the hypothesis test on any single regression coefficient:\n \\(H_0: \\beta_{j} = 0 \\quad H_1: \\beta_j \\ne 0\\) \nIt is a \\(t\\) test with the test statistic \\(t_{test} = \\frac{b_j}{se(b_j)}\\). We reject \\(H_0\\) if \\(|t_{test}| &gt; t_{\\alpha/2, n-k-1}\\). This is a partial or marginal test: a test of the contribution of \\(X_j\\) given ALL other regressors in the model.\nWe can also do a one-side test for sure. But I am not sure if there is a R function to do a one-sided test. But we can always compute the test statistic or the p-value ourselves.\n\n\n Example: Tests on Individual Coefficients \nSuppose we would like to assess the effect of \\(x_2\\) (distance) given that \\(x_1\\) (cases) is in the model.\n \\(H_0: \\beta_{2} = 0 \\quad H_1: \\beta_2 \\ne 0\\) \n\n\nR\nPython\n\n\n\nThe marginal test results are shown in the summary of fitted result. The t test statistic value is 3.98, and p-value is close to zero, concluding that \\(\\beta_2 \\ne 0\\).\n\nsumm_delivery$coefficients\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)   2.3412    1.09673    2.13 4.42e-02\ncases         1.6159    0.17073    9.46 3.25e-09\ndistance      0.0144    0.00361    3.98 6.31e-04\n\n\n\n\nThe marginal test results are shown in the summary of fitted result. The t test statistic value is 3.98, and p-value is close to zero, concluding that \\(\\beta_2 \\ne 0\\).\n\ndelivery_ols.summary()\n\n\nOLS Regression Results\n\n\nDep. Variable:\ntime\nR-squared:\n0.960\n\n\nModel:\nOLS\nAdj. R-squared:\n0.956\n\n\nMethod:\nLeast Squares\nF-statistic:\n261.2\n\n\nDate:\nTue, 15 Oct 2024\nProb (F-statistic):\n4.69e-16\n\n\nTime:\n10:36:46\nLog-Likelihood:\n-63.415\n\n\nNo. Observations:\n25\nAIC:\n132.8\n\n\nDf Residuals:\n22\nBIC:\n136.5\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n2.3412\n1.097\n2.135\n0.044\n0.067\n4.616\n\n\ncases\n1.6159\n0.171\n9.464\n0.000\n1.262\n1.970\n\n\ndistance\n0.0144\n0.004\n3.981\n0.001\n0.007\n0.022\n\n\n\n\nOmnibus:\n0.421\nDurbin-Watson:\n1.170\n\n\nProb(Omnibus):\n0.810\nJarque-Bera (JB):\n0.010\n\n\nSkew:\n0.032\nProb(JB):\n0.995\n\n\nKurtosis:\n3.073\nCond. No.\n873.\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nEach piece of information can be obtained from the fiited object.\n\ndelivery_ols.params\n\nIntercept    2.341231\ncases        1.615907\ndistance     0.014385\ndtype: float64\n\ndelivery_ols.bse\n\nIntercept    1.096730\ncases        0.170735\ndistance     0.003613\ndtype: float64\n\ndelivery_ols.tvalues\n\nIntercept    2.134738\ncases        9.464421\ndistance     3.981313\ndtype: float64\n\ndelivery_ols.pvalues\n\nIntercept    4.417012e-02\ncases        3.254932e-09\ndistance     6.312469e-04\ndtype: float64",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Multiple Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-mlr.html#inference-pitfalls",
    "href": "model-mlr.html#inference-pitfalls",
    "title": "28¬† Multiple Linear Regression*",
    "section": "\n28.5 Inference Pitfalls",
    "text": "28.5 Inference Pitfalls\n\n\nThe test \\(H_0:\\beta_j = 0\\) will always be rejected as long as the sample size is large enough, even \\(x_j\\) has a very small effect on \\(y\\).\n\nConsider the practical significance of the result, not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values.\n\n\n\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0:\\beta_j = 0\\).\n\nDON‚ÄôT immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\n\nIn MLR, it‚Äôs easy to inadvertently extrapolate since the regressors jointly define the region containing the data. We can define the smallest convex set containing all of the original \\(n\\) regressor points, as the regressor variable hull. Any point outside the hull can be viewed as an extrapolate point. A point within the ranges of \\(x_1\\) and \\(x_2\\) may not be necessarily a interpolation point, as shown the blue point in the figure.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>28</span>¬† <span class='chapter-title'>Multiple Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-logistic.html",
    "href": "model-logistic.html",
    "title": "29¬† Logistic Regression",
    "section": "",
    "text": "29.1 Regression vs.¬†Classification\nLinear regression assumes that the response \\(Y\\) is numerical (quantitative). In many situations, however, the response we would like to infer or predict on is categorical (qualitative), eye color, car brand, true vs.¬†fake news for example.\nA process of predicting categorical response is known as classification. There are many classification tools, or classifiers used to predict a categorical response. For instance, logistic regression is a classifier.\nRegression Function \\(f(x)\\) vs.¬†Classifier \\(C(x)\\)\nLike regression, we have our predictor inputs \\(X_1, \\dots, X_p\\), through a classifier \\(C(x)\\) that takes all the predictors and then generate a predicted value of \\(y\\), which is categorical. For example, we collect a person‚Äôs nationality, race, hair color, build a classifier, then predict his eye color.\nHere, the classifier \\(C(x)\\) in classification problem is analogous to the regression function \\(f(x) = \\beta_0 + \\beta_1X_1 + \\cdots + \\beta_pX_p\\) in the regression problem. The classifier \\(C(x)\\) is used to predict a categorical variable \\(y\\) and regression function \\(f(x)\\) is used to predict the value of a numerical variable \\(y\\).\nIn machine learning, it usually separates regression and classification apart, one for numerical response, the other for categorical response. But the term regression in general means any relationship between response variables and predictors, including both numerical and categorical responses. So the definition of regression in machine learning is a little bit narrow scoped. A regression model in general can deal with either numerical response or the regression problem in machine learning, or categorical response or the classification problem. That‚Äôs why we can use a logistic regression to do classification problems.\nFigure¬†29.1: Difference between classification and regression (https://daviddalpiaz.github.io/r4sl/classification-overview.html)\nClassification Example\nWhy Not Linear Regression?\nBefore we jump into logistic regression, one question is, why not just fit a linear regression model? That is, we define 0 as not default and 1 as default, and treat 0 and 1 as numerical values, and run a linear regression model, like\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), \\(\\, X =\\) credit card balance, and\n\\[Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}\\]\nIn fact, the predicted valued of \\(Y\\), \\(\\hat{Y} = b_0 + b_1X\\), estimates \\(P(Y = 1 \\mid X) = P(default \\mid balance)\\) which is actually what we want. So what‚Äôs the issue of linear regression? Look at the figure below. You can see that this is not like usual scatter plot when \\(y\\) is numerical, this scatter plot is pretty strange that \\(y\\) is either 0 or 1, the red and green points. This blue line is our linear regression line. Any point on the line is the estimated probability of default given a value of \\(X\\). Do you see any issues of using linear regression to fit this type of data?\nFigure¬†29.3: Graphical illustration of why a simple linear regression model won‚Äôt work for Default ~ Balance\nFirst, probability is always between 0 and 1, but some estimates here are outside the \\([0, 1]\\) interval. For example, when the credit card balance is below 500, the estimated probability of default is negative, which is not allowed mathematically.\nAlso, we assume the probability of default is linearly increasing with credit card balance, which is generally not true in reality.\nIn addition, the dummy variable approach \\((Y = 0, 1)\\) cannot be easily extended to \\(Y\\) with more than two categories. If we have 3 categories with nominal level of measurements, like car brand and eye color, coding the categories with 1, 2, 3 forces them to be ordered, the forces them to have the same difference. It does not make sense.\nAs a result, we need to use a model that is appropriate for categorical responses, like logistic regression.\nWhy Logistic Regression?\nSome classification methods, logistic regression for example, first predict the probability of each category of \\(Y\\). Then, the logistic regression predicts the probability of default using an S-shaped curve.\nYou can see the estimated probability curve is always between 0 and 1 and higher balance leads to higher chance of default. If our cut-off or threshold probability is 0.5, \\(y\\) will be labeled as default when the balance is over about 2000 dollars. Our goal is to learn how to generate this predicted probability curve using logistic regression.\nFigure¬†29.4: Graphical illustration of why a logistic regression model works better for Default ~ Balance",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "model-logistic.html#regression-vs.-classification",
    "href": "model-logistic.html#regression-vs.-classification",
    "title": "29¬† Logistic Regression",
    "section": "",
    "text": "Normal vs.¬†COVID vs.¬†Smoker‚Äôs Lungs\n\n\n\n\n\n\n\n\n\n\n\nFake vs.¬†Fact\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredict whether people will default on their credit card payment, where \\((Y)\\) is yes or no, based on their monthly credit card balance, \\((X)\\).\nWe use the sample data \\(\\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\) to build a classifier.\n\n\n\n\n\n\n\n\n\n\nFigure¬†29.2: Boxplot of Default vs.¬†Balance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the problem with this dummy variable approach?",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "model-logistic.html#introduction-to-logistic-regression",
    "href": "model-logistic.html#introduction-to-logistic-regression",
    "title": "29¬† Logistic Regression",
    "section": "\n29.2 Introduction to Logistic Regression",
    "text": "29.2 Introduction to Logistic Regression\n Binary Responses \nThe story starts with the binary response. The idea is that we treat each default \\((y = 1)\\) and not default \\((y = 0)\\), as success and failure arising from separate Bernoulli trials.\n\n\n\n\n\n\nWhat is a Bernoulli trial?\n\n\n\n\nA Bernoulli trial is a special case of a binomial trial when the number of trials is \\(m = 1\\).\n\nThere are exactly two possible outcomes, ‚Äúsuccess‚Äù and ‚Äúfailure‚Äù.\nThe probability of success, \\(\\pi\\), is constant.\n\n\n\n\n\n\n\n\n\n\n\nIn the default credit card example,\n\n\n\n\nDo we have exactly two outcomes?\nDo we have constant probability? \\(P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?\\)\n\n\n\n\n Nonconstant Probability\n\n\n\nTwo outcomes: Default \\((y = 1)\\) and Not Default \\((y = 0)\\)\nThe probability of success, \\(\\pi\\), changes with the value of predictor, \\(X\\)!\nWith a different value of \\(x_i\\), each Bernoulli trial outcome, \\(y_i\\), has a different probability of success, \\(\\pi_i\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\[ y_i \\mid x_i \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i)) = binomial(m=1,\\pi = \\pi(x_i)) \\]\n\n\n\\(X =\\) balance. \\(x_1 = 2000\\) has a larger \\(\\pi_1 = \\pi(2000)\\) than \\(\\pi_2 = \\pi(500)\\) with \\(x_2 = 500\\) because credit cards with a higher balance are more likely to default.\n\nThe idea of nonconstant probability in logistic regression is similar to the idea of different mean response value in linear regression. In linear regression, the mean response level is linearly affected by the predictor‚Äôs value, \\(E(Y \\mid X) = \\beta_0 + \\beta_1X\\). In logistic regression, \\(\\pi(x) = P(Y = 1 \\mid X = x)\\) is a function of \\(X\\). In our example, the higher value of \\(X\\), the higher value of \\(\\pi\\), although their relationship is not linear. The fact that the regressor \\(X\\) affects the mean response level or the probability of the response belonging to some category is the reason why we use regression. Through the relationship between the response and regressors, knowing \\(X\\) better helps us know the value of \\(Y\\).\nBecause of this Bernoulli assumption, the logistic regression is also called binomial regression.\n\n Logistic Regression \nNow it‚Äôs time to see what the logistic regression is. Logistic regression models a binary response \\((Y)\\) using predictors \\(X_1, \\dots, X_k\\).\n\n\n\\(k = 1\\): simple logistic regression\n\n\\(k &gt; 1\\): multiple logistic regression\n\nBut remember, we are not predicting \\(Y\\) directly. Instead, our goal is to use predictors \\(X_1, \\dots, X_k\\) to estimate the probability of success \\(\\pi\\) of the Bernoulli variable \\(Y\\). And if \\(\\pi &gt; threshold\\), say 0.5, \\(\\hat{Y} = 1\\), if \\(\\pi &lt; threshold\\), \\(\\hat{Y} = 0\\). But how?\nNow that it‚Äôs not good to use \\(y = 0, 1\\) in the regression, and predicting a probability having value between 0 and 1 sounds difficult. How about this. we transform \\(\\pi \\in (0, 1)\\) into another variable say \\(\\eta \\in (-\\infty, \\infty)\\) living on the whole real line. So that we can reasonably fit a linear regression on \\(\\eta\\) because the linear predictor could be any value.\nIn the logistic regression, we use the logit function:\n \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) \nwhich is in fact the log odds. This transformation is monotone. The higher \\(\\pi\\) is, the larger \\(\\eta\\) is. When \\(\\pi\\) approaches to zero, \\(\\eta\\) approaches to \\(-\\infty\\), and when \\(\\pi\\) approaches to 1, \\(\\eta\\) approaches to \\(\\infty\\).\n\n\n\n\n\n\n\n\n\n\nFigure¬†29.5: Graphical illustration of the logit function\n\n\n\n\nThen we can assume \\(\\eta\\) is a linear function of \\(x\\):\n\\[\\eta(x) =\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x\\] By doing so, we connect \\(x\\) and \\(\\pi\\) (and \\(\\eta\\)) together, and we can learn how \\(x\\) affect \\(\\pi\\) (and \\(\\eta\\)) through the coefficients \\(\\beta_0\\) and \\(\\beta_1\\).\nWell, the question is, if we obtain the \\(\\beta_0\\) and \\(\\beta_1\\) estimates, and therefore obtain \\(\\eta\\) estimated value, how do we obtain \\(\\pi(x)\\), our real interest? We can consider transform \\(\\eta\\) back to \\(\\pi\\) using the inverse function of the logit function, which is the logistic function.\n\nSo the logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\). But the logistic function denoted as  \\[\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]  takes a value \\(\\eta \\in (-\\infty, \\infty)\\) and maps it to a value \\(\\pi \\in (0, 1)\\).\nSo once \\(\\eta\\) is estimated by the linear predictor, we use the logistic function to transform \\(\\eta\\) back to the probability. The figure below shows a logistic function. This logistic function is a function of \\(x\\), and it describes the probability \\(\\pi\\). This S-shaped curve is what we want to estimate given the data, and we use this curve to do classification on \\(y\\).\n\n\n\n\n\n\n\nFigure¬†29.6: Graphical illustration of the logistic function",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "model-logistic.html#simple-logistic-regression-model",
    "href": "model-logistic.html#simple-logistic-regression-model",
    "title": "29¬† Logistic Regression",
    "section": "\n29.3 Simple Logistic Regression Model",
    "text": "29.3 Simple Logistic Regression Model\nTo sum up, here shows a simple logistic regression model: For \\(i = 1, \\dots, n\\) and with one predictor \\(X\\),\n\\[(Y_i \\mid X = x_i) \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i))\\] \\[\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi(x_i)}{1 - \\pi(x_i)} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i}\\]\nEach \\(Y_i\\) is a Bernoulli variable with the probability of success \\(\\pi_i(x_i)\\) which depends on its corresponding regressor \\(x_i\\) value. The nonlinear relationship between \\(\\pi_i(x_i)\\) and \\(x_i\\) is described through a linear relationship on its logit transformation. Put it another way, we have \\[\\small \\pi_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{i})}{1+\\exp(\\beta_0+\\beta_1 x_{i})} = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}\\]\nOnce we get the estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\), we can simply plug them into the logistic function to get the estimated probability \\(\\hat{\\pi}_i\\).\n\\[\\small \\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i} )}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i})}\\] If we consider a sequence of possible values of \\(x_i\\), and their estimated probability \\(\\hat{\\pi}_i\\), the collection of \\(\\{ x_i \\}\\) and \\(\\{ \\hat{\\pi}_i \\}\\) will give us the S-shaped probability curve.\n\n Probability Curve \n\n\n\nThe relationship between \\(\\pi(x)\\) and \\(x\\) is not linear! \\[\\pi(x) = \\frac{\\exp(\\beta_0+\\beta_1 x)}{1+\\exp(\\beta_0+\\beta_1 x)}\\]\nBecause of the S-shaped curve, the amount that \\(\\pi(x)\\) changes due to a one-unit change in \\(x\\) depends on the current value of \\(x\\). \\(\\pi(x = 2000)\\) would change much larger than \\(\\pi(x = 1000)\\) with one-unit change in \\(x\\).\nRegardless of the value of \\(x\\), if \\(\\beta_1 &gt; 0\\), increasing \\(x\\) will increase \\(\\pi(x)\\) because the slope of the S-shape curve is positive.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Interpretation of Coefficients \n\nThe ratio \\(\\frac{\\pi}{1-\\pi} \\in (0, \\infty)\\) is called the odds of having \\(y=1\\).\nExample: If 1 in 5 people will default, the odds is 1/4 since \\(\\pi = 0.2\\) implies an odds of \\(0.2/(1‚àí0.2) = 1/4\\).\n\n\\[\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x\\]\n\nIncreasing \\(x\\) by one unit changes the log-odds by \\(\\beta_1\\), or it multiplies the odds by \\(e^{\\beta_1}\\).\n\n\n\n\n\n\n\nNote\n\n\n\n\n\\(\\beta_1\\) does not correspond to the change in \\(\\pi(x)\\) associated with a one-unit increase in \\(x\\).\n\\(\\beta_1\\) is the change in log odds associated with one-unit increase in \\(x\\).\n\n\n\n Example: Logistic Regression \n\n\nWe use the body data to demonstrate the implementation of logistic regression. The response variable is GENDER, and the predictor is HEIGHT. We would like to use logistic regression to predict whether a person is male or female using the height information of that person.\nIn the data,\n\nGENDER = 1 if male\nGENDER = 0 if female\n\nThe unit of HEIGHT is centimeter (cm) (1 cm = 0.3937 in).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\nbody &lt;- read.table(\"./data/body.txt\", header = TRUE)\nhead(body)\n\n  AGE GENDER PULSE SYSTOLIC DIASTOLIC HDL LDL WHITE  RED PLATE WEIGHT HEIGHT\n1  43      0    80      100        70  73  68   8.7 4.80   319   98.6  172.0\n2  57      1    84      112        70  35 116   4.9 4.73   187   96.9  186.0\n3  38      0    94      134        94  36 223   6.9 4.47   297  108.2  154.4\n4  80      1    74      126        64  37  83   7.5 4.32   170   73.1  160.5\n5  34      1    50      114        68  50 104   6.1 4.95   140   83.1  179.0\n6  77      1    60      134        60  55  75   5.7 3.95   192   86.5  166.7\n  WAIST ARM_CIRC  BMI\n1 120.4     40.7 33.3\n2 107.8     37.0 28.0\n3 120.3     44.3 45.4\n4  97.2     30.3 28.4\n5  95.1     34.0 25.9\n6 112.0     31.4 31.1\n\n\n\n\n\nimport pandas as pd\nbody = pd.read_table(\"./data/body.txt\", sep='\\s+')\nbody.head()\n\n   AGE  GENDER  PULSE  SYSTOLIC  ...  HEIGHT  WAIST  ARM_CIRC   BMI\n0   43       0     80       100  ...   172.0  120.4      40.7  33.3\n1   57       1     84       112  ...   186.0  107.8      37.0  28.0\n2   38       0     94       134  ...   154.4  120.3      44.3  45.4\n3   80       1     74       126  ...   160.5   97.2      30.3  28.4\n4   34       1     50       114  ...   179.0   95.1      34.0  25.9\n\n[5 rows x 15 columns]\n\n\n\n\n\nAgain, we are not using gender to predict someone‚Äôs height, which is usually done by linear regression. Instead, our response variable is a binary categorical variable, and we are doing classification with a numeric predictor.\n\n Data Summary \nA basic data summary by gender tells us that height plays a role in distinguish male from female, so using height to classify gender would be helpful.\n\n\n\n\nR\nPython\n\n\n\n\ntable(body$GENDER)\n\n\n  0   1 \n147 153 \n\nsummary(body[body$GENDER == 1, ]$HEIGHT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  155.0   169.1   173.8   174.1   179.4   193.3 \n\nsummary(body[body$GENDER == 0, ]$HEIGHT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  134.5   156.5   162.2   161.7   166.8   181.4 \n\n\n\n\n\n\n\nboxplot(body$HEIGHT ~ body$GENDER)\n\n\n\n\n\n\n\n\n\n\n\n\n# Show the distribution of GENDER\nbody['GENDER'].value_counts()\n\nGENDER\n1    153\n0    147\nName: count, dtype: int64\n\nbody[body['GENDER'] == 1]['HEIGHT'].describe()\n\ncount    153.000000\nmean     174.124837\nstd        7.100975\nmin      155.000000\n25%      169.100000\n50%      173.800000\n75%      179.400000\nmax      193.300000\nName: HEIGHT, dtype: float64\n\nbody[body['GENDER'] == 0]['HEIGHT'].describe()\n\ncount    147.000000\nmean     161.687075\nstd        7.482960\nmin      134.500000\n25%      156.500000\n50%      162.200000\n75%      166.750000\nmax      181.400000\nName: HEIGHT, dtype: float64\n\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure()\nsns.boxplot(x='GENDER', y='HEIGHT', data=body)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n Model Fitting \n\n\nR\nPython\n\n\n\nWe use the function glm() in R to fit a logistic regression model. ‚Äúglm‚Äù means generalized linear model (GLM). Linear regression is a linear model, and it is a normal model because the response variable is normally distributed given \\(x\\). The models for non-normal response variables generalizes the linear regression model, and hence belong to GLM. The logistic regression whose response is Bernoulli or binomial distributed is a GLM. There are other GLMs such as Poisson regression and gamma regression. They are often discussed in the second course of Linear Models or a separate Generalized Linear Models course.\nIn the function, we use the same formula syntax as lm(), and one additional job we need to do is specify the family of the GLM. family = \"binomial\" should be used because the logistic regression is a binomial regression.\n\nlogit_fit &lt;- glm(GENDER ~ HEIGHT, data = body, family = \"binomial\")\n(summ_logit_fit &lt;- summary(logit_fit))\n\n\nCall:\nglm(formula = GENDER ~ HEIGHT, family = \"binomial\", data = body)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -40.54809    4.63084  -8.756   &lt;2e-16 ***\nHEIGHT        0.24173    0.02758   8.764   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 415.77  on 299  degrees of freedom\nResidual deviance: 251.50  on 298  degrees of freedom\nAIC: 255.5\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nsumm_logit_fit$coefficients\n\n               Estimate Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -40.5480864 4.63083742 -8.756102 2.021182e-18\nHEIGHT        0.2417325 0.02758399  8.763507 1.892674e-18\n\n\n\n\nWe use the function statsmodels.formula.api.logit() in Python to fit a logistic regression model. In the function, we use exactly the same syntax as ols().\n\nfrom statsmodels.formula.api import logit\nlogit_fit = logit(formula='GENDER ~ HEIGHT', data=body).fit()\n\nOptimization terminated successfully.\n         Current function value: 0.419162\n         Iterations 7\n\nlogit_fit.summary()\n\n\nLogit Regression Results\n\n\nDep. Variable:\nGENDER\nNo. Observations:\n300\n\n\nModel:\nLogit\nDf Residuals:\n298\n\n\nMethod:\nMLE\nDf Model:\n1\n\n\nDate:\nTue, 15 Oct 2024\nPseudo R-squ.:\n0.3951\n\n\nTime:\n10:37:11\nLog-Likelihood:\n-125.75\n\n\nconverged:\nTrue\nLL-Null:\n-207.88\n\n\nCovariance Type:\nnonrobust\nLLR p-value:\n1.320e-37\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-40.5481\n4.631\n-8.756\n0.000\n-49.625\n-31.471\n\n\nHEIGHT\n0.2417\n0.028\n8.763\n0.000\n0.188\n0.296\n\n\n\n\n\nlogit_fit.params\n\nIntercept   -40.548086\nHEIGHT        0.241733\ndtype: float64\n\n\n\n\n\n\n\n\nNote\n\n\n\nThere is another function Logit() that uses the response vector and design matrix to perform logistic regression.\n\nX = body[['HEIGHT']]\ny = body['GENDER']\nimport statsmodels.api as sm\nlogit_model_fit = sm.Logit(y, sm.add_constant(X)).fit()\nlogit_model_fit.summary()\n\n\n\n\n\n\nBased on the fitted result, we learn that \\[\\hat{\\eta}(x) = \\ln \\left( \\frac{\\hat{\\pi}(x)}{1 - \\hat{\\pi}(x)}\\right) = \\ln (\\text{odds}_{x}) =  \\hat{\\beta}_0 + \\hat{\\beta}_1x = -40.55 + 0.24 \\times \\text{HEIGHT}\\]\n\nSince \\(\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)\\), \\(\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x}) = \\ln \\left( \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} \\right)\\)\nA one centimeter increase in HEIGHT increases the log odds of being male by 0.24 units.\nThe odds ratio, \\(\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.24} = 1.273\\). Therefore, the odds of being male increases by 27.3% with an additional one centimeter of HEIGHT.\n\n Prediction \nKnowing how the predictor affects the chance of response belonging to some category is one goal. But in classification, we usually focus more on prediction. For example, we may want to know Pr(GENDER = 1) when HEIGHT is 170 cm.\n\n\nOnce we obtain the coefficient estimates and the predictor value, we just need to plug them into the logistic function to obtain the probability we want.\n\\[ \\hat{\\pi}(x = 170) = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x)}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x)} = \\frac{\\exp(-40.55+0.24 \\times 170)}{1+\\exp(-40.55+0.24 \\times 170)} = 0.633 = 63.3\\%\\]\nThrough the formula, we learn that from our model, when a person is 170cm tall, the probability that this person is male is about 63/3%.\n\n\nR\nPython\n\n\n\npredict(logit_fit, type = \"response\") gives us a vector of \\(\\hat{\\pi}(x_i)\\) of all responses in the data. When type = \"link\", predict() gives us \\(\\hat{\\eta}(x_i)\\) because in literature, \\(\\eta(x) = \\ln \\left( \\frac{\\hat{\\pi}(x)}{1 - \\hat{\\pi}(x)}\\right)\\) is called the link function. To predict a specific probability at some \\(x\\) value, we use newdata = data.frame(HEIGHT = 170). Remember the variable name should be exactly the same as the name in the original data set.\n\npi_hat &lt;- predict(logit_fit, type = \"response\")\neta_hat &lt;- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\n\n\npredict(logit_fit, newdata = data.frame(HEIGHT = 170), type = \"response\")\n\n        1 \n0.6333105 \n\npredict(logit_fit, newdata = data.frame(HEIGHT = 170), type = \"link\")\n\n        1 \n0.5464453 \n\n\n\n\nlogit_fit.predict(which='mean') gives us a vector of \\(\\hat{\\pi}(x_i)\\) of all responses in the data because the probability is the mean of the Bernoulli distribution. When which='linear', predict() gives us \\(\\hat{\\eta}(x_i)\\) which is a linear function of \\(x\\). To predict a specific probability at some \\(x\\) value, we use newinput = pd.DataFrame({'HEIGHT': [170]}) and get_prediction(newinput). Remember the variable name should be exactly the same as the name in the original data set.\n\npi_hat = logit_fit.predict(which='mean')\neta_hat = logit_fit.predict(which='linear') ## default gives us b0 + b1*x\n\n\n# Create new data for prediction\nnewinput = pd.DataFrame({'HEIGHT': [170]})\npredict = logit_fit.get_prediction(newinput)\npredict.predicted\n\narray([0.63331048])\n\npredict.linpred\n\narray([0.54644529])\n\n\n\n\n\n Probability Curve \n\n\n\n\n\n\nWhat is the probability of being male when the HEIGHT is 160 cm? What about when the HEIGHTis 180 cm?\n\n\n\n\n\n\n\n\nR\nPython\n\n\n\n\npredict(logit_fit, newdata = data.frame(HEIGHT = c(160, 170, 180)), type = \"response\")\n\n        1         2         3 \n0.1334399 0.6333105 0.9509103 \n\n\n\n\n\n# Make predictions for new HEIGHT values\nnewheight = pd.DataFrame({'HEIGHT': [160, 170, 180]})\npred = logit_fit.get_prediction(newheight)\npred.predicted\n\narray([0.13343992, 0.63331048, 0.95091031])\n\n\n\n\n\nThe blue S-shaped curve is the estimated probability curve. It can be plotted using pi_hat.\n\n\n\n\n\n\n\n\n\n\n\n\n 160 cm, Pr(male) = 0.13\n 170 cm, Pr(male) = 0.63\n 180 cm, Pr(male) = 0.95",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "model-logistic.html#evaluation-metrics",
    "href": "model-logistic.html#evaluation-metrics",
    "title": "29¬† Logistic Regression",
    "section": "\n29.4 Evaluation Metrics",
    "text": "29.4 Evaluation Metrics\n\nHaving probabilities is just a intermediate step. Most of the time our final goal is to classify our response, doing a binary decision. Given a predicted probability, we may correctly classify the label, or mis-classify the label. To know whether or not our model does a good job on classification, we need some evaluation metrics.\nFirst, we can use a tool called a confusion matrix to display all possible outcomes in a binary classification scenario. In this context, let‚Äôs designate 0 as Negative and 1 as Positive, regardless of what these values represent. If the actual truth is 0 and we classify the response as 0, this outcome is a True Negative (TN), meaning we made a correct decision. However, if we classify the response as 1 instead, we commit an error, resulting in a False Positive (FP). In this case, the true condition is Negative, but our model incorrectly predicts it as Positive.\nSimilarly, if the truth is 1 and we correctly predict it as 1, the classification is accurate and is called a True Positive (TP). However, if we incorrectly label a response as Negative when it is actually Positive, we make a False Negative (FN).\n\n\n\n\n\n\n\n\n0\n1\n\n\n\nLabeled 0\nTrue Negative (TN)\nFalse Negative (FN)\n\n\nLabeled 1\nFalse Positive (FP)\nTrue Positive (TP)\n\n\n\nA good classifier accurately identifies a high number of TNs and TPs while minimizing FNs and FPs. Commonly used performance measures include Sensitivity (True Positive Rate), Specificity (True Negative Rate), and Accuracy.\n\nSensitivity (True Positive Rate, TPR) \\(= P( \\text{Labeled 1} \\mid \\text{1}) = \\frac{TP}{TP+FN}\\) measures the classifier‚Äôs ability to correctly identify positive cases or make correct discoveries when the actual responses are truly Positive. It reflects how well the model detects actual positives.\nSpecificity (True Negative Rate, TNR) \\(= P( \\text{Labeled 0} \\mid \\text{0}) = \\frac{TN}{FP+TN}\\) measures the percentage of correctly labeled negatives among the actual negative responses. It indicates how effectively the model avoids false alarms by accurately identifying negatives.\nAccuracy \\(= \\frac{TP + TN}{TP+FN+FP+TN}\\) considers both negatives and positives, measuring the overall proportion of correct classifications across the entire dataset. It provides a general assessment of the classifier‚Äôs performance by accounting for the total number of correct predictions out of all predictions made. \nMore on Wiki page\n\n\n Example: Confusion Matrix \n\n\nR\nPython\n\n\n\nTo produce a confusion matrix, we need two things: the true response labels, and the classification from the model. In the example, the true labels are just body$GENDER data. For the classification result, we need to make a binary decision from the estimated probabilities obtained from the model. Here we use 0.5 as the threshold. If \\(\\hat{\\pi}(x) &gt; 0.5\\), then we label the response as 1, and 0 otherwise. We then can simply use table() function to generate a confusion matrix.\n\nprob &lt;- predict(logit_fit, type = \"response\")\n\n## true observations\ngender_true &lt;- body$GENDER\n\n## predicted labels\ngender_predict &lt;- (prob &gt; 0.5) * 1\n\n## confusion matrix\ntable(gender_predict, gender_true)\n\n              gender_true\ngender_predict   0   1\n             0 118  29\n             1  29 124\n\n\n\n\nTo produce a confusion matrix, we need two things: the true response labels, and the classification from the model. In the example, the true labels are just body['GENDER'] data. For the classification result, we need to make a binary decision from the estimated probabilities obtained from the model. Here we use 0.5 as the threshold. If \\(\\hat{\\pi}(x) &gt; 0.5\\), then we label the response as 1, and 0 otherwise. We then can simply use confusion_matrix() function from sklearn.metrics to generate a confusion matrix.\n\nprob = logit_fit.predict()\n\n## true observations\ngender_true = body['GENDER']\n\n## predicted labels\ngender_predict = (prob &gt; 0.5).astype(int)\n\n# Confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(gender_true, gender_predict)\n\narray([[118,  29],\n       [ 29, 124]])\n\n\n\n\n\nThe true positive rate is 124/(124+29) = 0.81, and the true negative rate is 118/(118+29) = 0.803. The overall accuracy is (124 + 118) /(124 + 29 + 29 + 118) = 0.807. \n\nA commonly used visualization tool for assessing the classification performance is the receiver operating characteristic (ROC) curve. 1 This curve plots True Positive Rate (Sensitivity) vs.¬†False Positive Rate (1 - Specificity). The confusion matrix provides the classification result for a single threshold, typically 0.5 in many cases. However, by adjusting the threshold, we can obtain different classification outcomes. For example, if \\(\\hat{\\pi}(x) = 0.57\\), and the cutoff is set at 0.5, \\(\\hat{y} = 1\\) (Positive). However, if we increase the cutoff to 0.6, \\(\\hat{y} = 0\\) (Negative). The ROC curve helps visualize how the classifier‚Äôs performance changes across various thresholds, allowing us to evaluate its overall effectiveness.\nThe ROC curve effectively illustrates how sensitivity and specificity change across every possible cutoff value between 0 and 1. When the cutoff is small, it becomes more likely that \\(\\hat{\\pi}(x) &gt; cutoff\\), resulting in more Positives and fewer Negatives. As a result, sensitivity increases (TP \\(\\uparrow\\) and FN \\(\\downarrow\\)) while specificity decreases (TN \\(\\downarrow\\) and FP \\(\\uparrow\\)), causing 1 - specificity to rise. Conversely, when the cutoff is large, it is more likely that \\(\\hat{\\pi}(x) &lt; cutoff\\), resulting in fewer Positives and more Negatives. This leads to a decrease in sensitivity (TP \\(\\downarrow\\) and FN \\(\\uparrow\\)) and an increase in specificity (TN \\(\\uparrow\\) and FP \\(\\downarrow\\)), so 1 - specificity is lower.\nThis dynamic explains why the two endpoints of the ROC curve are positioned at the top-right and bottom-left corners of the graph. The top-right corner represents a scenario with high sensitivity and low specificity (low cutoff), while the bottom-left corner represents high specificity and low sensitivity (high cutoff).\nThe ideal classification result occurs when TPR is 1 and FPR is 0, corresponding to the top-left point of the ROC plot. A superior ROC curve would closely resemble a right-angle shape (red-dashed), with the ‚Äúknee‚Äù of the curve near this perfect point. Building on this concept, a commonly used performance measure derived from the ROC curve is the Area Under the Curve (AUC). The AUC quantifies the overall ability of the model to distinguish between classes. The larger the AUC, the better the classification performance, with a value of 1 representing perfect classification and a value of 0.5 indicating no better than random chance (black-dashed).\n\n\n\n\n\n\n\nFigure¬†29.7: ROC curve for Gender ~ Height",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "model-logistic.html#exercises",
    "href": "model-logistic.html#exercises",
    "title": "29¬† Logistic Regression",
    "section": "\n29.5 Exercises",
    "text": "29.5 Exercises\n\nThe following logistic regression equation is used for predicting whether a bear is male or female. The value of \\(\\pi\\) is the probability that the bear is male. \\[\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = 2.3 - 0.0573 (\\text{Length}) + 0.00842(\\text{Weight})\\]\n\nIdentify the predictor and response variables. Which of these are dummy variables?\nGiven that the variable Lengthis in the model, does a heavier weight increase or decrease the probability that the bear is a male? Please explain.\nThe given regression equation has an overall p-value of 0.218. What does that suggest about the quality of predictions made using the regression equation?\nUse a length of 60 in. and a weight of 300 lb to find the probability that the bear is a male. Also, what is the probability that the bear is a female?",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "model-logistic.html#footnotes",
    "href": "model-logistic.html#footnotes",
    "title": "29¬† Logistic Regression",
    "section": "",
    "text": "R packages for ROC curves: ROCR and pROC, yardstick of Tidymodels‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>29</span>¬† <span class='chapter-title'>Logistic Regression</span>"
    ]
  },
  {
    "objectID": "model-bayes.html",
    "href": "model-bayes.html",
    "title": "30¬† Bayesian Linear Regression*",
    "section": "",
    "text": "30.1 Bayesian Simple Linear Regression Model\nIn simple linear regression, we have\n\\[Y_i \\mid \\beta_0, \\beta_1, \\sigma \\stackrel{ind}{\\sim}  N\\left(\\mu_i, \\sigma^2 \\right) \\quad \\text{with} \\quad \\mu_i = \\beta_0 + \\beta_1 X_i\\]\nWe assume or thought the response values are generated from the normal distribution whose mean depends on their corresponding regressor‚Äôs value. In Bayesian inference, this normal data model is our likelihood \\(L(\\btheta = (\\beta_0, \\beta_1, \\sigma) \\mid \\mathcal{D} = \\{\\by, \\bx \\})\\), as it evaluates how the data are compatible with different possible values of parameters.\nHere, in SLR, we have three unknown parameters \\(\\beta_0, \\beta_1, \\sigma\\) to be estimated, and we use \\(\\btheta\\) to denote a unknown parameter vector including the three. \\(\\mathcal{D}\\) is the collected data set including both response \\(\\by\\) vector and regressor \\(\\bx\\) vector.\nTo be a Bayesian, what do we do next? Bayesians only have one trick. We assign priors to the unknown parameters, then do the posterior inference using Bayes‚Äô rule! The idea is simple, but the big question is HOW. There are countless approaches to construct priors for \\(\\beta_0, \\beta_1\\), and \\(\\sigma\\). For simplicity, we can assume \\(\\beta_0, \\beta_1\\), and \\(\\sigma\\) are independent. That is, their joint probability distribution is the product of their own marginal distribution: \\(\\pi(\\btheta) = \\pi(\\beta_0, \\beta_1, \\sigma) = \\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma)\\).\nThe next question is what are \\(\\pi(\\beta_0)\\), \\(\\pi(\\beta_1)\\), and \\(\\pi(\\sigma)\\)? \\(\\beta_0\\) and \\(\\beta_1\\) can technically take any values in the real line. We can embrace the normal distribution because its support is the whole real line, and it is actually a conjugate prior for the SLR with normally distributed errors. Let‚Äôs assume\n\\[\\begin{align} \\beta_0 \\sim N(m_0, s_0^2), \\\\\n\\beta_1 \\sim N(m_1, s_1^2). \\end{align}\\]\nHere \\(m_0\\) and \\(m_1\\) are prior means, and \\(s_0^2\\) and \\(s_1^2\\) are prior variances. Before new data or evidence come in, we think \\(\\beta_0\\) and \\(\\beta_1\\) are sort of located at \\(m_0\\) and \\(m_1\\), respectively. Of course we are not 100% sure or correct. \\(s_0^2\\) and \\(s_1^2\\) measure how uncertain we feel about the location of \\(\\beta_0\\) and \\(\\beta_1\\).\nBy definition, \\(\\sigma\\) must be positive practically. Therefore, a distribution having a positive support is more appropriate to reflect the possible values of \\(\\sigma\\). One popular distribution used to describe \\(\\sigma\\) is the exponential distribution denoted as\n\\[\\begin{align} \\sigma \\sim \\text{Exp}(\\lambda) \\end{align}\\] whose density function is \\(\\pi(\\sigma) = \\lambda e^{-\\lambda \\sigma}\\) with \\(\\lambda &gt; 0\\). The mean and variance are \\(\\E(\\sigma) = 1/\\lambda\\), and \\(\\Var(\\sigma) = 1/\\lambda^2\\). The parameter \\(\\lambda\\) controls the size of mean and variance. The larger \\(\\lambda\\) is, the smaller mean and variance will be\nOnce the prior distributions are specified, with the data likelihood model, the Bayesian simple linear regression model is complete.\n\\[\\begin{align} Y_i \\mid \\beta_0, \\beta_1, \\sigma &\\stackrel{ind}{\\sim}  N\\left(\\mu_i, \\sigma^2 \\right) \\quad \\text{with} \\quad \\mu_i = \\beta_0 + \\beta_1 X_i \\\\\n\\beta_0 &\\sim N(m_0, s_0^2) \\\\\n\\beta_1 &\\sim N(m_1, s_1^2) \\\\\n\\sigma &\\sim \\text{Exp}(\\lambda) \\end{align}\\]\nNote that in this model, we introduce other parameters, the parameters in the prior distributions, \\(m_0\\), \\(s_0^2\\), \\(m_1\\), \\(s_1^2\\), and \\(\\lambda\\). Some call them hyperparameters. The posterior inference results will depend on those values, so before we compute the posterior distribution, we need to specify those values properly, so that the prior distributions faithfully convey our knowledge and belief about \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\). The process of finding appropriate hyperparameters to faithfully describe and represents our knowledge is called prior parameter tuning. We explain how we tune the parameters using the following example.\nExample: Capital Bikeshare bayesrules::bikes Data in Washington, D.C.\nThe data we use for illustration is bayesrules::bikes data set. In particular, we check the relationship between the number of bikeshare rides rides and what the temperature feels like (degrees Fahrenheit) temp_feel.\nRows: 500\nColumns: 2\n$ rides     &lt;int&gt; 654, 1229, 1454, 1518, 1362, 891, 1280, 1220, 1137, 1368, 13‚Ä¶\n$ temp_feel &lt;dbl&gt; 64.7, 49.0, 51.1, 52.6, 50.8, 46.6, 45.6, 49.2, 46.4, 45.6, ‚Ä¶",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Bayesian Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-bayes.html#bayesian-simple-linear-regression-model",
    "href": "model-bayes.html#bayesian-simple-linear-regression-model",
    "title": "30¬† Bayesian Linear Regression*",
    "section": "",
    "text": "30.1.1 Tuning Prior Models\nPrior understanding 1:\n\nOn an average temperature day, say 65 or 70 degrees, there are typically around 5000 riders, though this average could be somewhere between 3000 and 7000.\n\nThis piece of information gives us an idea of what \\(m_0\\) and \\(s_0\\) should be, but the information has been centered because it provides the typical situation.\nWe can consider the centered intercept, \\(\\beta_{0c}\\) that reflects the typical ridership at the typical temperature. \\(\\beta_{0c}\\) will be centered at the typical number of rides 5000, with standard deviation about 1000 because the mean \\(\\pm\\) 2 \\(\\times\\) SD \\(= (3000, 7000)\\). So the prior for the centered intercept is \\(\\beta_{0c} \\sim N(5000, 1000^2)\\). Graphically speaking, we believe when temperature is around 65 to 70 degrees, the number of rides is about 5000, and interaction of the two dashed lines in the figure.\n\n\n\n\n\n\n\n\n\nPrior understanding 2:\n\nFor every one degree increase in temperature, ridership typically increases by 100 rides, though this average increase could be as low as 20 or as high as 180.\n\nThis information tells us about \\(\\beta_1\\). On average, the slope is 100, and its standard deviation is around 40 because the mean \\(\\pm\\) 2 \\(\\times\\) SD \\(= (20, 180)\\). Therefore we can have \\(\\beta_{1} \\sim N(100, 40^2)\\).\nGraphically speaking, with the centered intercept, we now have the prior regression line, the relationship between rides and temp_feel that we believe it typically has. Note that the prior regression line passes the the centered intercept.\n\n\n\n\n\n\n\n\n\nNote that this red line represents just the typical case we believe. There are other (in fact infinitely many) possibilities of \\(\\beta_{0c}\\) and \\(\\beta_1\\) being considered in our brain that are captured in the prior distributions. We can plot some possible combinations of \\(\\beta_{0c}\\) and \\(\\beta_1\\), and see the prior plausible model lines \\(\\mu_{Y|X} = \\beta_0 + \\beta_1 X\\).\n\n\n\n\n\n\n\n\n\n\nPrior understanding 3:\n\nAt any given temperature, daily ridership will tend to vary with a moderate standard deviation of 1250 rides.\n\nThis information is about \\(\\sigma\\). With \\(\\pi(\\sigma) = \\text{Exp}(\\lambda)\\). We can set \\(\\lambda\\) so that the prior mean is 1250, as informed by the description.\n\\(\\sigma \\sim \\text{Exp}(0.0008)\\) because \\(\\E(\\sigma) = 1/\\lambda = 1/0.0008 = 1250\\)\nWe basically complete the tuning, and all prior distributions are clearly specified. We can generate some prior data points, the data we believe it should look like if we were collecting them, based on the prior model. The following is the simulated prior data when the regression line comes from the prior mean level of \\(\\beta_{0c}\\) and \\(\\beta_1\\).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Bayesian Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-bayes.html#posterior-inference",
    "href": "model-bayes.html#posterior-inference",
    "title": "30¬† Bayesian Linear Regression*",
    "section": "\n30.2 Posterior Inference",
    "text": "30.2 Posterior Inference\n\n30.2.1 Posterior distribution and sampling\nOnce the Bayesian model is fully specified, we can start doing posterior inference by obtaining the posterior distribution of unknown parameters. We use Bayes rule to update our prior understanding of the relationship between ridership and temperature using data information provided by likelihood.\nThe joint posterior distribution is\n\\[\\pi(\\beta_0, \\beta_1, \\sigma \\mid \\by) = \\frac{L(\\beta_0, \\beta_1, \\sigma \\mid \\by)\\pi(\\beta_0, \\beta_1, \\sigma)}{p(\\by)}\\] where\n\n\\(L(\\beta_0, \\beta_1, \\sigma \\mid \\by) = p(\\by \\mid \\beta_0, \\beta_1, \\sigma) = \\prod_{i=1}^np(y_i \\mid \\beta_0, \\beta_1, \\sigma)\\). It means that given the parameters, and the data generating mechanism, data points \\(y_i\\)s are independent each other.\n\\(\\pi(\\beta_0, \\beta_1, \\sigma) = \\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma)\\). Priors are assumed independent.\n\\(p(\\by) = \\int \\int \\int \\left[\\prod_{i=1}^np(y_i \\mid \\beta_0, \\beta_1, \\sigma)\\right]\\pi(\\beta_0)\\pi(\\beta_1)\\pi(\\sigma) ~ d\\beta_0d\\beta_1d\\sigma\\). This is marginal likelihood that integrates all the parameters out. It is a function of data only. It works as a normalizing constant, guaranteeing that the posterior distribution \\(\\pi(\\beta_0, \\beta_1, \\sigma \\mid \\by)\\) is a valid probability distribution.\n\nAs you can see, \\(p(\\by)\\) looks much more complicated than the one we learned in Chapter 22. When there are many parameters to be estimated, and their prior distributions are complex, it is difficult or impossible to obtain this integral by hand. That is why when doing Bayesian inference, we usually need to use some algorithms to help us derive the posterior distribution, either exactly or approximately.\nThere are lots of ways to generate/approximate the posterior distribution of parameters. They are discussed in Bayesian statistics course. The method used here is Markov chain Monte Carlo (MCMC). Briefly speaking, MCMC is an algorithm that repeatedly draws the samples of the parameters, and eventually the collection of those draws well represents the posterior distribution of parameters. Again, the details are discussed in a Bayesian analysis course.\n\nHere we use the R rstanarm package that uses RStan syntax1 to do Bayesian inference for applied regression models (arm).\nThe function helping us draw posterior samples is stan_glm(). The formula is exactly the same as lm(). The family is gaussian because we have a normal data model. prior_intercept is for \\(\\beta_{0c}\\), prior for \\(\\beta_1\\) and prior_aux for \\(\\sigma\\). Here, 4 Markov chains are generated. In other words, 4 distinct collection of posterior samples will be generated, each having 10000 draws or iterations. Each iteration draws a posterior sample of the \\(\\beta_0\\), \\(\\beta_1\\), and \\(\\sigma\\).\n\nbike_model &lt;- rstanarm::stan_glm(rides ~ temp_feel, data = bikes,\n                                 family = gaussian(),\n                                 prior_intercept = normal(5000, 1000),\n                                 prior = normal(100, 40), \n                                 prior_aux = exponential(0.0008),\n                                 chains = 4, iter = 5000*2, seed = 2024)\n\n\n\nAfter tossing out the first half of Markov chain values from the warm-up or burn-in phase, stan_glm() simulation produces four parallel chains of length 5000 for each model parameter:\n\\(\\{ \\beta_0^{(1)}, \\beta_0^{(2)}, \\dots, \\beta_0^{(5000)} \\}\\), \\(\\{ \\beta_1^{(1)}, \\beta_1^{(2)}, \\dots, \\beta_1^{(5000)} \\}\\), \\(\\{ \\sigma^{(1)}, \\sigma^{(2)}, \\dots, \\sigma^{(5000)} \\}\\)\n\n30.2.2 Convergence diagnostics\nWhen we use MCMC to do posterior inference, we need to make sure that the algorithm converges. In other words, the posterior samples drawn by the algorithm should reach a stable distribution, and the distribution basically won‚Äôt change much as samples are continuously collected. One way to check the convergence is to check the traceplot, seeing how the sample values move as the iteration goes. If the algorithm reaches the convergence, the traceplots from the 4 independent runs should be mixed and overlapped very well because the 4 set of samples should represent the same posterior distribution. The traceplots obtained from the model fit are mixed well.\n\n# Trace plots of parallel chains\nbayesplot::mcmc_trace(bike_model, size = 0.1)\n\n\n\n\n\n\n\n\nWhen MCMC just starts, the drawn samples are usually not representative of the posterior distribution. Because of this, we usually don‚Äôt include the samples drawn in the early iterations in the final collection of samples used for representing the posterior distribution and for analysis. The following two animations show that it takes a while and several iterations to reach a more stable distribution.\n\n\n\n\nSource: https://blog.stata.com/2016/11/15/introduction-to-bayesian-statistics-part-2-mcmc-and-the-metropolis-hastings-algorithm/\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe density curves estimated from the posterior samples approximate posterior distributions. We observe that these four chains produce nearly indistinguishable posterior approximations. This provides evidence that our simulation is stable and sufficiently long ‚Äì running the chains for more iterations likely wouldn‚Äôt produce drastically different or improved posterior approximations.\n\n# Density plots of parallel chains\nbayesplot::mcmc_dens_overlay(bike_model)\n\n\n\n\n\n\n\n\n\n\n\nIn addition to visualization, we can use some numeric measures to diagnose convergence. Both effective sample size and R-hat are close to 1, indicating that the chains are stable, mixing quickly, and behaving much like an independent sample.\n\n# Effective sample size ratio\nbayesplot::neff_ratio(bike_model)\n\n(Intercept)   temp_feel       sigma \n      1.023       1.021       0.996 \n\n# Rhat\nbayesplot::rhat(bike_model)\n\n(Intercept)   temp_feel       sigma \n          1           1           1 \n\n\n\nThere‚Äôs no magic rule for interpreting this ratio, and it should be utilized alongside other diagnostics such as the trace plot. That said, we might be suspicious of a Markov chain for which the effective sample size ratio is less than 0.1, i.e., the effective sample size is less than 10% of the actual sample size. An R-hat ratio greater than 1.05 raises some red flags about the stability of the simulation. \n\nThe followings provide two scenarios that MCMC hasn‚Äôt reached convergence.\n\n\n\n\nHighly Autocorrelated Chain: Effective size is small, not many independent samples that are representative of the true posterior distribution.\n\nRun longer and thinning the chain\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSlow Convergence: Need wait longer to have the chain reached a stable mixing zone that are representative of the true posterior distribution.\n\nSet a longer burn-in or warm-up period\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n30.2.3 Posterior summary and analysis\nOnce posterior samples are collected, we can calculate important summary statistics, for example posterior mean, and the 80% credible interval. The two are shown in the first column and the last two columns in the table below.\n\n# Posterior summary statistics\ntidy(bike_model, effects = c(\"fixed\", \"aux\"),\n     conf.int = TRUE, conf.level = 0.80)\n\n# A tibble: 4 √ó 5\n  term        estimate std.error conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -2194.     355.    -2648.    -1747. \n2 temp_feel       82.2      5.08     75.7      88.7\n3 sigma         1282.      40.6    1231.     1335. \n4 mean_PPD      3485.      80.7    3384.     3591. \n\n\nThe posterior relationship (on average) is \\(-2196 + 82.2 X\\). Note that before data come in, we believe one degree increase will increase 100 rides on average. However, data tell us the effect of temperature is not as large as we thought. We can directly quantify the uncertainty about \\(\\beta_1\\) by looking at its credible interval. The 80% credible interval for \\(\\beta_1\\) is \\((75.7, 88.5)\\). The interval is much more intuitive than the confidence interval. We say, given the data, the probability that \\(\\beta_1\\) is between 75.7 and 88.5 is 80%., i.e., \\(P(\\beta_1 \\in(75.7, 88.5) \\mid \\by, \\bx) = 80\\%\\).\n\nThe posterior samples are saved in bike_model_df.\n\n# Store the 4 chains for each parameter in 1 data frame\nbike_model_df &lt;- as.data.frame(bike_model)\nnrow(bike_model_df)\n\n[1] 20000\n\nhead(bike_model_df)\n\n  (Intercept) temp_feel sigma\n1       -1993      77.7  1289\n2       -1818      75.9  1292\n3       -2003      78.3  1305\n4       -2515      87.4  1250\n5       -2519      88.0  1293\n6       -1666      74.5  1294\n\n\nHow do we learn whether or not \\(\\beta_1 &gt; 0\\) is zero without using the frequentist hypothesis testing approach? Since we have the posterior samples, we have (at least well approximated) the entire posterior distribution. As a result, we can sort of quantify the probability \\(P(\\beta_1 &gt; 0 \\mid \\by, \\bx)\\). How? We just need to calculate the relative frequency of \\(\\beta_1\\) that is greater than 0.\n\\[P(\\beta_1 &gt; 0 \\mid \\by, \\bx) \\approx \\frac{1}{20000}\\sum_{t=1}^{20000} \\mathbf{1}\\left(\\beta_1^{(t)} &gt; 0\\right)\\]\n\nsum(bike_model_df$temp_feel &gt; 0) / nrow(bike_model_df)\n\n[1] 1\n\n\nInstead of making a dichotomous judgement, in the Bayesian world, we provide the probability of something happening. Anything is not just black or white. We live in a world full of different gray, a continuum showing the degree of our credence. In our example, the probability that \\(\\beta_1 &gt; 0\\) is basically one. We are pretty certain that temperature has a positive effect on the number of bikeshare rides.\n\nThe figure below shows 100 posterior regression lines, one corresponding to one pair of \\((\\beta_0^{(t)}, \\beta_1^{(t)})\\).\n\n\n\n\n\n\n\n\n\nFor each posterior draw \\(\\{\\beta_0^{(t)}, \\beta_1^{(t)}, \\sigma^{(t)} \\}_{t = 1}^{20000}\\), we have the posterior predictive distribution \\[Y_i^{(t)} \\sim N\\left(\\beta_0^{(t)} + \\beta_1^{(t)}X_i, \\, (\\sigma^{(t)})^2\\right)\\]\nWe then can have 20000 different posterior predictive response distributions, and 20000 different posterior predictive response data sets. Those data are what we think the data will look like if the true data generating mechanism is one of the regression model with parameters \\(\\{\\beta_0^{(t)}, \\beta_1^{(t)}, \\sigma^{(t)} \\}\\).",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Bayesian Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-bayes.html#resources",
    "href": "model-bayes.html#resources",
    "title": "30¬† Bayesian Linear Regression*",
    "section": "\n30.3 Resources",
    "text": "30.3 Resources\n\nBayes Rules! An Introduction to Applied Bayesian Modeling\nStatistical Rethinking\nA Student‚Äôs Guide to Bayesian Statistics\nBayesian Data Analysis",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Bayesian Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-bayes.html#footnotes",
    "href": "model-bayes.html#footnotes",
    "title": "30¬† Bayesian Linear Regression*",
    "section": "",
    "text": "RStan is the R interface to Stan.‚Ü©Ô∏é",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>30</span>¬† <span class='chapter-title'>Bayesian Linear Regression*</span>"
    ]
  },
  {
    "objectID": "model-survival.html",
    "href": "model-survival.html",
    "title": "31¬† Survival Analysis",
    "section": "",
    "text": "31.1 Life Table\nAn example of a life table is shown in Figure¬†31.1. The entire report can be downloaded at CDC Publications and Information Products.\nThe death rates for the various age groups that were in effect in the year 2018 continue to remain in effect during the entire lives of the 100,000 hypothetical people assumed to be present at age 0. That is, we pretend that a population of 100,000 people is born in the year 2018, and they each live their entire lives in a word with the same constant death rates that were present in the year 2018.\nFigure¬†31.1: Life table for total population in the United States in 2018\nNote that mortality experiences are different for various gender and race groups, so it is common to have tables for specific groups. For example, Figure¬†31.2 below is a table for females in the United States.\nFigure¬†31.2: Life table for females in the United States in 2018\nFigure¬†31.3: The life table lists its basis year and number of hypothetical individuals\nFigure¬†31.4: The first column lists the age intervals of the individuals\nFigure¬†31.5: The second column lists the probability of dying between two ages\nFigure¬†31.6: The third column lists the number of individuals alive at the beginning of the age interval\nFigure¬†31.7: The fourth column lists the number of individuals who die during a given age interval\nFigure¬†31.8: The fifth column lists the total number of person-years lived within a given age interval\nFigure¬†31.9: The fifth column lists the total number of person-years lived above a given age\nFigure¬†31.10: The final column lists the expectation of life at a given age\nExample: Probability of Dying\nWe can use Figure¬†31.1 to find the probability of a person dying between age of 15 and 20.\n\\[\\begin{align*} Pr(\\text{die in } [15, 20)) &= Pr([15, 16) \\cup [16, 17) \\cup \\cdots \\cup [19, 20)) \\\\ &= Pr([15, 16) + Pr([16, 17)) + \\cdots + Pr([19, 20)) \\\\ &= 0.000214 + 0.000253 + 0.000292 + 0.000329 + 0.000365 = 0.001453 \\end{align*}\\]\n\\[\\begin{align*} Pr(\\text{surviving between 15th and 20th birthdays}) &= \\frac{\\text{Number of people alive on their 20th birthday}}{\\text{Number of people alive on their 15th birthday}} \\\\ &= \\frac{99,151}{99,296} \\\\ &= 0.99854 \\end{align*}\\]\n\\[Pr(\\text{die in } [15, 20)) = 1-Pr(\\text{survive in } [15, 20)) = 1 - 0.99854 = 0.00146\\]",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "model-survival.html#life-table",
    "href": "model-survival.html#life-table",
    "title": "31¬† Survival Analysis",
    "section": "",
    "text": "A period (current) life table describes mortality and longevity data for a hypothetical cohort (generation).\n\nThe data is computed with the assumption that the conditions affecting mortality in a particular basis year (such as 2018) remain the same throughout the lives of everyone in the hypothetical cohort.\n\nFor example, a 1-year-old toddler and an elderly 70-year-old live their entire life in a world with the same constant death rates that were present in a given year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe basis year for the mortality rate in this table is 2018, as is highlighted in Figure¬†31.3.\nThis life table has data for a cohort of 100,000 hypothetical people.\n\n\n\nThe age ranges chosen for this life table include the following classes: \\([0, 1)\\), \\([1, 2)\\), \\([2, 3)\\), ‚Ä¶ \\([99, 100)\\), \\([100, \\infty)\\).\n\n\n\nThe probabilities of dying during the age interval are listed in the 1st column of the life table.\nFor example, in Figure¬†31.5, there is a 0.000367 probability of someone dying between their 1st birthday and their 2nd birthday.\n\n\n\nThe number of people alive at the beginning of the age interval is listed in column 2.\nAs Figure¬†31.6 displays, among the 100,000 hypothetical people who were born, 99,435 of them are alive on their 1st birthday.\n\n\n\nThe number of people who died during the age interval is listed in column 3.\n\n\n\n\n\n\n\nHow is this column related to the previous two columns?\n\n\n\n\n\n\n\n\nThe total number of years lived during the age interval by those who were alive at the beginning of the age interval is listed in the fourth column.\nFor example, the 100,000 people who were present at age 0 lived a total of 99,505 years (Figure¬†31.8).\nIf none of those people had died, this entry would have been 100,000 years.\n\n\n\nThe sixth column is similar to the fifth, but lists the total number of years lived during the age interval and all of the following age intervals as well.\n\n\n\nThe final column lists the expected remaining lifetime in years, measured from the beginning of the age interval (Figure¬†31.10).\n\n\n\n\n\n\n\nWhy does the age interval of 1-2 have an expected remaining lifetime of 78.2 years?",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "model-survival.html#applications-of-life-tables",
    "href": "model-survival.html#applications-of-life-tables",
    "title": "31¬† Survival Analysis",
    "section": "\n31.2 Applications of Life Tables",
    "text": "31.2 Applications of Life Tables\n Social Security \n\n\nThere were 3,600,000 births in the U.S. in 2020. If the age for receiving full Social Security payment is 67, how many of those born in 2020 are expected to be alive on their 67th birthday? Check the report!\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmong 100,000 people born, we expect 81,637 of them will survive to their 67th birthday. Therefore, we expect that \\(3,600,000 \\times 0.81637 = 2,938,932\\) people born in 2020 will receive their full Social Security payment.\n\n Hypothesis Testing \n\n\nBack to our opening story. For one city, there are 5000 people who reach their 16th birthday. 25 of them die before their 17th birthday. Do we have sufficient evidence to conclude that this number of deaths is significantly high?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAccording to the life table, the probability of dying for the age interval of 16-17 is 0.000405.\nThis is a \\(H_1\\) claim. We are going to test  \\(\\small \\begin{align} &H_0: \\pi = 0.000405 \\\\ &H_1: \\pi &gt; 0.000405\\end{align}\\) \n\\(\\hat{\\pi} = 25/5000 = 0.005\\).\n\\(z = \\frac{0.005 - 0.000405}{\\sqrt{\\frac{(0.000405)(0.999595)}{5000}}} = 16.15\\)\n\\(P\\)-value \\(\\approx 0\\).\nThere is sufficient evidence to conclude that the proportion of deaths is significantly higher than the proportion that is usually expected for this age interval.",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "model-survival.html#kaplan-meier-survival-analysis",
    "href": "model-survival.html#kaplan-meier-survival-analysis",
    "title": "31¬† Survival Analysis",
    "section": "\n31.3 Kaplan-Meier Survival Analysis",
    "text": "31.3 Kaplan-Meier Survival Analysis\n Survival Analysis \n\n\n\nThe life table method is based on fixed time intervals.\nThe Kaplan-Meier method\n\nis based on intervals that vary according to the times of survival to some particular terminating event.\nis used to describe the probability of surviving for a specific period of time.\n\n\n What is the probability of surviving for 5 more years after cancer chemotherapy? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Survival Time \n\nThe time lapse from the beginning of observation to the time of terminating event is considered the survival time (Figure¬†31.11).\n\n\n\n\n\n\n\n\nFigure¬†31.11: Graph of survival time\n\n\n\n\n\n Survivor \n\nA survivor is a subject that successfully lasted throughout a particular time period.\n\n\n\n\n\n\n\nNote\n\n\n\n\nA survivor does not necessarily mean living.\n\nA patient trying to stop smoking is a survivor if smoking has not resumed.\nYour iPhone that worked for some particular length of time can be considered a survivor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensored Data \n\n\nSurvival times are censored data if the subjects\n\nsurvive past the end of the study\nare dropped from the study for reasons not related to the terminating event being studied.\n\n\nWe have censored data for subject A and C. Subject A dropped from the study in June before the study ends in December. Subject C is still alive at the end of study.\n\n\n\n\n\n\n\n\nFigure¬†31.12: Illustration of censored data (https://unc.live/3K1ph8f)\n\n\n\n\n\n Example: Medication Treatment for Quitting Smoking \n\n\n\n\n\n\n\n\n\n\nDay\nStatus (0 = censored, 1 = Smoke Again)\nNumber of Patients\nPatients Not Smoking\nProportion Not Smoking\nCumulative Proportion Not Smoking\n\n\n\n1\n0\n\n\n\n\n\n\n3\n1\n4\n3\n3/4 = 0.75\n0.75\n\n\n4\n1\n3\n2\n2/3 = 0.67\n0.5\n\n\n7\n1\n2\n1\n1/2 = 0.5\n0.25\n\n\n21\n1\n1\n0\n0\n0\n\n\n\n\n‚ÄúSurviving‚Äù means the patient has NOT resumed smoking.\nAs shown in Figure¬†31.13, the Subject 1 disliked the medication and dropped out of the study on day one.\n\nThe table above also provides information regarding the study.\n\n2nd row: Subject 2 resumed smoking 3 days after the start of the program.\n3rd row: Cumulative Proportion is \\(0.5 = (3/4)(2/3)\\)\n4th row: Cumulative Proportion is \\(0.25 = (3/4)(2/3)(1/2)\\)\n5th row: Cumulative Proportion is \\(0 = (3/4)(2/3)(1/2)(0)\\)\n\n\n\n\n\n\n\n\n\n\nFigure¬†31.13: Survival time for five subjects receiving the medication treatment\n\n\n\n\n\n\n\n Example: Counseling Treatment for Quitting Smoking \n\n\n\n\n\n\n\n\n\n\nDay\nStatus  (0 = censored, 1 = Smoke Again)\nNumber of Patients\nPatients Not Smoking\nProportion Not Smoking\nCumulative Proportion Not Smoking\n\n\n\n2\n1\n10\n9\n9/10\n0.9\n\n\n4\n1\n9\n8\n8/9\n0.8\n\n\n5\n0\n\n\n\n\n\n\n8\n1\n7\n6\n6/7\n0.686\n\n\n9\n1\n6\n5\n5/6\n0.571\n\n\n12\n0\n\n\n\n\n\n\n14\n1\n4\n3\n3/4\n0.429\n\n\n22\n1\n3\n2\n2/3\n0.286\n\n\n24\n0\n\n\n\n\n\n\n28\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is the cumulative proportion on Day 8 0.686?\n\n\n\n\\[0.686 = (9/10)(8/9)(6/7)\\]\n\nOn Day 5 a patient dropped out, so we don‚Äôt know whether he resumed smoking on Day 8 or not.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure¬†31.14: Survival time for ten subjects receiving the counseling treatment\n\n\n\n\n\n\n\n Kaplan-Meier Analysis \n\n\n\n\n\n\nWhich treatment is better for quitting smoking?\n\n\n\n\n\n\nIt is often more insightful to create graphs that facilitate the understanding of survival data. The Kaplan-Meier cumulative survival curves shown below are constructed using survival times and the cumulative proportions of patients who remained non-smokers.\nThese curves indicate that the proportion of survivors (patients who had not resumed smoking) is generally higher for those in the counseling program compared to those in the medication program, suggesting that the counseling program yielded better results. However, it is also evident that neither program achieved very high survival rates, indicating that neither approach was particularly effective in helping patients successfully quit smoking.\n\n\n\n\n\n\n\nFigure¬†31.15: Data from the medication treatment group and the counseling treatment group are compared using a Kaplan-Meier plot",
    "crumbs": [
      "Statistical Models",
      "<span class='chapter-number'>31</span>¬† <span class='chapter-title'>Survival Analysis</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html",
    "href": "a-r_prog.html",
    "title": "Appendix A ‚Äî R programming",
    "section": "",
    "text": "A.1 Arithmetic and Logical Operators\n2 + 3 / (5 * 4) ^ 2\n\n[1] 2.0075\n\n5 == 5.00\n\n[1] TRUE\n\n# 5 and 5L are of the same value too\n# 5 is of type double; 5L is integer\n5 == 5L\n\n[1] TRUE\n\ntypeof(5L)\n\n[1] \"integer\"\n\n!TRUE == FALSE\n\n[1] TRUE\nType coercion: When doing AND/OR comparisons, all nonzero values are treated as TRUE and 0 as FALSE.\n-5 | 0\n\n[1] TRUE\n\n1 & 1\n\n[1] TRUE\n\n2 | 0\n\n[1] TRUE",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#math-functions",
    "href": "a-r_prog.html#math-functions",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.2 Math Functions",
    "text": "A.2 Math Functions\nMath functions in R are built-in.\n\nsqrt(144)\n\n[1] 12\n\nexp(1)\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nlog(32, base = 2)\n\n[1] 5\n\nabs(-7)\n\n[1] 7",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#variables-and-assignment",
    "href": "a-r_prog.html#variables-and-assignment",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.3 Variables and Assignment",
    "text": "A.3 Variables and Assignment\nUse &lt;- to do assignment. Why\n\n## we create an object, value 5, \n## and call it x, which is a variable\nx &lt;- 5\nx\n\n[1] 5\n\n(x &lt;- x + 6)\n\n[1] 11\n\nx == 5\n\n[1] FALSE\n\nlog(x)\n\n[1] 2.397895",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#object-types",
    "href": "a-r_prog.html#object-types",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.4 Object Types",
    "text": "A.4 Object Types\ncharacter, double, integer and logical.\n\ntypeof(5)\n\n[1] \"double\"\n\n\n\ntypeof(5L)\n\n[1] \"integer\"\n\n\n\ntypeof(\"I_love_data_science!\")\n\n[1] \"character\"\n\n\n\ntypeof(1 &gt; 3)\n\n[1] \"logical\"\n\n\n\nis.double(5L)\n\n[1] FALSE",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---vector",
    "href": "a-r_prog.html#data-structure---vector",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.5 Data Structure - Vector",
    "text": "A.5 Data Structure - Vector\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable defined previously is a scalar value, or in fact a (atomic) vector of length one.\n\n\nA.5.1 (Atomic) Vector\n\nTo create a vector, use c(), short for concatenate or combine.\nAll elements of a vector must be of the same type.\n\n\n(dbl_vec &lt;- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(int_vec &lt;- c(1L, 6L, 10L))\n\n[1]  1  6 10\n\n## TRUE and FALSE can be written as T and F\n(log_vec &lt;- c(TRUE, FALSE, F))  \n\n[1]  TRUE FALSE FALSE\n\n(chr_vec &lt;- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5\n\n\n\nA.5.2 Sequence of Numbers\n\nUse : to create a sequence of integers.\nUse seq() to create a sequence of numbers of type double with more options. \n\n\n(vec &lt;- 1:5) \n\n[1] 1 2 3 4 5\n\ntypeof(vec)\n\n[1] \"integer\"\n\n# a sequence of numbers from 1 to 10 with increment 2\n(seq_vec &lt;- seq(from = 1, to = 10, by = 2))\n\n[1] 1 3 5 7 9\n\ntypeof(seq_vec)\n\n[1] \"double\"\n\n\n\n# a sequence of numbers from 1 to 10\n# with 12 elements\nseq(from = 1, to = 10, length.out = 12)\n\n [1]  1.000000  1.818182  2.636364  3.454545  4.272727  5.090909  5.909091\n [8]  6.727273  7.545455  8.363636  9.181818 10.000000\n\n\n\nA.5.3 Operations on Vectors\n\nWe can do any operations on vectors as we do on a scalar variable (vector of length 1).\n\n\n# Create two vectors\nv1 &lt;- c(3, 8)\nv2 &lt;- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10\n\n\n\nA.5.4 Recycling of Vectors\n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\n\nv1 &lt;- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 &lt;- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16\n\n\n\nA.5.5 Subsetting Vectors\n\nTo extract element(s) in a vector, we use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The 3rd element\nv1[3] \n\n[1] 4\n\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\nv1[1:2]\n\n[1] 3 8\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---factor",
    "href": "a-r_prog.html#data-structure---factor",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.6 Data Structure - Factor",
    "text": "A.6 Data Structure - Factor\n\nA vector of type factor can be ordered in a meaningful way.\nCreate a factor by factor(). It is a type of integer, not character. üò≤ üôÑ\n\n\n## Create a factor from a character vector using function factor()\n(fac &lt;- factor(c(\"med\", \"high\", \"low\")))\n\n[1] med  high low \nLevels: high low med\n\ntypeof(fac)  ## The type is integer.\n\n[1] \"integer\"\n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\n\n\norder_fac &lt;- factor(c(\"med\", \"high\", \"low\"),\n                    levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1\n\n\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically.\n\n[1] \"high\" \"low\"  \"med\"",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---list",
    "href": "a-r_prog.html#data-structure---list",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.7 Data Structure - List",
    "text": "A.7 Data Structure - List\n\n\n\n\n\n\n\n\n\nLists are different from (atomic) vectors: Elements can be of any type, including lists. (Generic vectors)\nConstruct a list by using list().\n\n\n\n\n## a list of 3 elements of different types\nx_lst &lt;- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\n\n\n$idx\n[1] 1 2 3\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1]  TRUE FALSE\n\n\n\n\nstr(x_lst)\n\nList of 3\n $ idx: int [1:3] 1 2 3\n $    : chr \"a\"\n $    : logi [1:2] TRUE FALSE\n\nnames(x_lst)\n\n[1] \"idx\" \"\"    \"\"   \n\nlength(x_lst)\n\n[1] 3\n\n\n\n\n\nA.7.1 Subsetting a list\n\n\nReturn an  element  of a list\n\n## subset by name (a vector)\nx_lst$idx  \n\n[1] 1 2 3\n\n## subset by indexing (a vector)\nx_lst[[1]]  \n\n[1] 1 2 3\n\ntypeof(x_lst$idx)\n\n[1] \"integer\"\n\n\n\n\nReturn a  sub-list  of a list\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n\n$idx\n[1] 1 2 3\n\n## subset by indexing (still a list)\nx_lst[1]  \n\n$idx\n[1] 1 2 3\n\ntypeof(x_lst[\"idx\"])\n\n[1] \"list\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIf list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\n‚Äî @RLangTip, https://twitter.com/RLangTip/status/268375867468681216",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---matrix",
    "href": "a-r_prog.html#data-structure---matrix",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.8 Data Structure - Matrix",
    "text": "A.8 Data Structure - Matrix\n\nA matrix is a two-dimensional analog of a vector with attribute dim.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat &lt;- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n\n# elements are arranged by row\nmatrix(data = 1:6, \n       nrow = 3, \n       ncol = 2, \n       byrow = TRUE) #&lt;&lt;\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nattributes(mat)\n\n$dim\n[1] 3 2\n\n\n\nA.8.1 Row and Column Names\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## assign row names and column names\nrownames(mat) &lt;- c(\"A\", \"B\", \"C\")\ncolnames(mat) &lt;- c(\"a\", \"b\")\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\n\n\n\nrownames(mat)\n\n[1] \"A\" \"B\" \"C\"\n\ncolnames(mat)\n\n[1] \"a\" \"b\"\n\nattributes(mat)\n\n$dim\n[1] 3 2\n\n$dimnames\n$dimnames[[1]]\n[1] \"A\" \"B\" \"C\"\n\n$dimnames[[2]]\n[1] \"a\" \"b\"\n\n\n\n\n\nA.8.2 Subsetting a Matrix\n\nUse the same indexing approach as vectors on rows and columns.\nUse comma , to separate row and column index.\n\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\nA B C \n4 5 6 \n\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\na b \n2 5 \n\n## The 1st and 3rd rows and the 1st column\nmat[c(1, 3), 1] \n\nA C \n1 3 \n\n\n\n\n\nA.8.3 Binding Matrices\n\ncbind() (binding matrices by adding columns)\nrbind() (binding matrices by adding rows)\nWhen matrices are combined by columns (rows), they should have the same number of rows (columns).\n\n\n\n\nmat\n\n  a b\nA 1 4\nB 2 5\nC 3 6\n\nmat_c &lt;- matrix(data = c(7,0,0,8,2,6), \n                nrow = 3, ncol = 2)\n## should have the same number of rows\ncbind(mat, mat_c)  \n\n  a b    \nA 1 4 7 8\nB 2 5 0 2\nC 3 6 0 6\n\n\n\n\nmat_r &lt;- matrix(data = 1:4, \n                nrow = 2, \n                ncol = 2)\n## should have the same number of columns\nrbind(mat, mat_r)  \n\n  a b\nA 1 4\nB 2 5\nC 3 6\n  1 3\n  2 4",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#data-structure---data-frame",
    "href": "a-r_prog.html#data-structure---data-frame",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.9 Data Structure - Data Frame",
    "text": "A.9 Data Structure - Data Frame\n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nMore general than matrix: Different columns can have different types.\nUse data.frame() that takes named vectors as input ‚Äúelement‚Äù.\n\n\n\n\n## data frame w/ an dbl column named age\n## and char column named gender.\n(df &lt;- data.frame(age = c(19, 21, 40), \n                  gen = c(\"m\",\"f\", \"m\")))\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n## a data frame has a list structure\nstr(df)  \n\n'data.frame':   3 obs. of  2 variables:\n $ age: num  19 21 40\n $ gen: chr  \"m\" \"f\" \"m\"\n\n\n\n\n## must set column names\n## or they are ugly and non-recognizable\ndata.frame(c(19,21,40), c(\"m\",\"f\",\"m\")) \n\n  c.19..21..40. c..m....f....m..\n1            19                m\n2            21                f\n3            40                m\n\n\n\n\n\nA.9.1 Properties of data frames\nData frame has properties of matrix and list.\n\n\n\nnames(df)  ## df as a list\n\n[1] \"age\" \"gen\"\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\" \"gen\"\n\nlength(df) ## df as a list\n\n[1] 2\n\nncol(df) ## df as a matrix\n\n[1] 2\n\ndim(df) ## df as a matrix\n\n[1] 3 2\n\n\n\n\n## rbind() and cbind() can be used on df\ndf_r &lt;- data.frame(age = 10, \n                   gen = \"f\")\nrbind(df, df_r)\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n4  10   f\n\ndf_c &lt;- \n    data.frame(col = c(\"red\",\"blue\",\"gray\"))\n(df_new &lt;- cbind(df, df_c))\n\n  age gen  col\n1  19   m  red\n2  21   f blue\n3  40   m gray\n\n\n\n\n\nA.9.2 Subsetting a data frame\nCan use either list or matrix subsetting methods.\n\n\n\ndf_new\n\n  age gen  col\n1  19   m  red\n2  21   f blue\n3  40   m gray\n\n## Subset rows\ndf_new[c(1, 3), ]\n\n  age gen  col\n1  19   m  red\n3  40   m gray\n\n## select the row where age == 21\ndf_new[df_new$age == 21, ]\n\n  age gen  col\n2  21   f blue\n\n\n\n\n## Subset columns\n## like a list\ndf_new$age\n\n[1] 19 21 40\n\ndf_new[c(\"age\", \"gen\")]\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n## like a matrix\ndf_new[, c(\"age\", \"gen\")]\n\n  age gen\n1  19   m\n2  21   f\n3  40   m\n\n\n\n\n\ndf_new[c(1, 3), ]\n\n  age gen  col\n1  19   m  red\n3  40   m gray\n\nstr(df[\"age\"])  ## a data frame with one column\n\n'data.frame':   3 obs. of  1 variable:\n $ age: num  19 21 40\n\nstr(df[, \"age\"])  ## becomes a vector by default\n\n num [1:3] 19 21 40",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#special-objects",
    "href": "a-r_prog.html#special-objects",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.10 Special Objects",
    "text": "A.10 Special Objects",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#plotting",
    "href": "a-r_prog.html#plotting",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.11 Plotting",
    "text": "A.11 Plotting\n\nA.11.1 Scatter plot\n\n\n\nmtcars[1:15, 1:4]\n\n                    mpg cyl  disp  hp\nMazda RX4          21.0   6 160.0 110\nMazda RX4 Wag      21.0   6 160.0 110\nDatsun 710         22.8   4 108.0  93\nHornet 4 Drive     21.4   6 258.0 110\nHornet Sportabout  18.7   8 360.0 175\nValiant            18.1   6 225.0 105\nDuster 360         14.3   8 360.0 245\nMerc 240D          24.4   4 146.7  62\nMerc 230           22.8   4 140.8  95\nMerc 280           19.2   6 167.6 123\nMerc 280C          17.8   6 167.6 123\nMerc 450SE         16.4   8 275.8 180\nMerc 450SL         17.3   8 275.8 180\nMerc 450SLC        15.2   8 275.8 180\nCadillac Fleetwood 10.4   8 472.0 205\n\n\n\n\nplot(x = mtcars$mpg, y = mtcars$hp, \n     xlab  = \"Miles per gallon\", \n     ylab = \"Horsepower\", \n     main = \"Scatter plot\", \n     col = \"red\", \n     pch = 5, las = 1)\n\n\n\n\n\n\n\n\n\n\nA.11.2 Argument pch\n\nplot(x = 1:25, y = rep(1, 25), pch = 1:25, xlab = \"\", ylab = \"\", main = \"pch\", axes = FALSE)\naxis(1, at = 1:25, cex.axis = 0.5)\n\n\n\n\n\n\n\n\nThe default is pch = 1\n\nA.11.3 R Subplots\n\npar(mfrow = c(1, 2))\nplot(x = mtcars$mpg, y = mtcars$hp, xlab = \"mpg\")\nplot(x = mtcars$mpg, y = mtcars$weight, xlab = \"mpg\")\n\n\n\n\n\n\n\n\nA.11.4 Boxplot\n\npar(mar = c(4, 4, 0, 0))\nboxplot(mpg ~ cyl, \n        data = mtcars, \n        col = c(\"blue\", \"green\", \"red\"), \n        las = 1, \n        horizontal = TRUE,\n        xlab = \"Miles per gallon\", \n        ylab = \"Number of cylinders\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA.11.5 Histogram\nhist() decides the class intervals/with based on breaks. If not provided, R chooses one.\n\nhist(mtcars$wt, \n     breaks = 20, \n     col = \"#003366\", \n     border = \"#FFCC00\", \n     xlab = \"weights\", \n     main = \"Histogram of weights\",\n     las = 1)\n\n\n\n\n\n\n\n\nBesides color names, you can also use hex number to specify colors. Pretty handy.\n\nA.11.6 Barplot\n\n(counts &lt;- table(mtcars$gear)) \n\n\n 3  4  5 \n15 12  5 \n\n\n\nmy_bar &lt;- barplot(counts, \n                  main = \"Car Distribution\", \n                  xlab = \"Number of Gears\", \n                  las = 1)\ntext(x = my_bar, y = counts - 0.8, \n     labels = counts, \n     cex = 0.8)\n\n\n\n\n\n\n\n\nA.11.7 Pie chart\n\nPie charts are used for categorical variables, especially when we want to know percentage of each category.\nThe first argument is the frequency table, and you can add labels to each category.\n\n\n(percent &lt;- round(counts / sum(counts) * 100, 2))\n\n\n    3     4     5 \n46.88 37.50 15.62 \n\n(labels &lt;- paste0(3:5, \" gears: \", percent, \"%\"))\n\n[1] \"3 gears: 46.88%\" \"4 gears: 37.5%\"  \"5 gears: 15.62%\"\n\n\n\npie(x = counts, labels = labels,\n    main = \"Pie Chart\", \n    col = 2:4, \n    radius = 1)\n\n\n\n\n\n\n\n\nA.11.8 2D imaging\n\nThe image() function displays the values in a matrix using color.\n\n\nmatrix(1:30, 6, 5)\n\n     [,1] [,2] [,3] [,4] [,5]\n[1,]    1    7   13   19   25\n[2,]    2    8   14   20   26\n[3,]    3    9   15   21   27\n[4,]    4   10   16   22   28\n[5,]    5   11   17   23   29\n[6,]    6   12   18   24   30\n\nimage(matrix(1:30, 6, 5))\n\n\n\n\n\n\n\n\nlibrary(fields)\n\nLoading required package: spam\n\n\nSpam version 2.10-0 (2023-10-23) is loaded.\nType 'help( Spam)' or 'demo( spam)' for a short introduction \nand overview of this package.\nHelp for individual functions is also obtained by adding the\nsuffix '.spam' to the function name, e.g. 'help( chol.spam)'.\n\n\n\nAttaching package: 'spam'\n\n\nThe following objects are masked from 'package:base':\n\n    backsolve, forwardsolve\n\n\nLoading required package: viridisLite\n\n\n\nTry help(fields) to get started.\n\nstr(volcano)\n\n num [1:87, 1:61] 100 101 102 103 104 105 105 106 107 108 ...\n\nimage.plot(volcano)\n\n\n\n\n\n\n\n\nx &lt;- 10*(1:nrow(volcano))\ny &lt;- 10*(1:ncol(volcano))\nimage(x, y, volcano, col = hcl.colors(100, \"terrain\"), axes = FALSE)\ncontour(x, y, volcano, levels = seq(90, 200, by = 5),\n        add = TRUE, col = \"brown\")\naxis(1, at = seq(100, 800, by = 100))\naxis(2, at = seq(100, 600, by = 100))\nbox()\ntitle(main = \"Maunga Whau Volcano\", font.main = 4)\n\n\n\n\n\n\n\n\nA.11.9 3D scatter plot\n\nlibrary(scatterplot3d)\nscatterplot3d(x = mtcars$wt, y = mtcars$disp, z = mtcars$mpg, \n              xlab = \"Weights\", ylab = \"Displacement\", zlab = \"Miles per gallon\", \n              main = \"3D Scatter Plot\",\n              pch = 16, color = \"steelblue\")\n\n\n\n\n\n\n\n\nA.11.10 Perspective plot\n\n# Exaggerate the relief\nz &lt;- 2 * volcano      \n# 10 meter spacing (S to N)\nx &lt;- 10 * (1:nrow(z))   \n# 10 meter spacing (E to W)\ny &lt;- 10 * (1:ncol(z))   \npar(bg = \"slategray\")\npersp(x, y, z, theta = 135, phi = 30, col = \"green3\", scale = FALSE,\n      ltheta = -120, shade = 0.75, border = NA, box = FALSE)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#special-objects-1",
    "href": "a-r_prog.html#special-objects-1",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.12 Special Objects",
    "text": "A.12 Special Objects\n\nA.12.1 NA\n\nNA means Not Available, which is a logical constant of length 1 for a missing value indicator.\nEach type of vector has its own missing value. They all are reserved words.\nYou can always use NA and it will be converted to the correct type.\n\n\n\n\nNA            # logical\n\n[1] NA\n\nNA_integer_   # integer\n\n[1] NA\n\nNA_real_      # double\n\n[1] NA\n\nNA_character_ # character\n\n[1] NA\n\n\n\n\n\n## The NA in the vector x is NA_real_\nx &lt;- c(NA, 0, 1)\ntypeof(x)  \n\n[1] \"double\"\n\nis.na(x)\n\n[1]  TRUE FALSE FALSE\n\n\n\n\n\nA.12.2 NULL\n\n\nNULL represents the null object, an object representing nothing.\n\nNULL is a reserved word and can also be the product of importing data with unknown data type.\n\nNULL typically behaves like a vector of length 0.\n\n\n\n\ny &lt;- c(NA, NULL, \"\")\ny\n\n[1] NA \"\"\n\n## only first element is evaluated...\nis.null(y) \n\n[1] FALSE\n\n\n\n\n## a missing value is a value we don't know.\n## It is something.\nis.null(NA)\n\n[1] FALSE\n\nis.null(NULL)\n\n[1] TRUE\n\n# empty character is something, not nothing!\nis.null(\"\")\n\n[1] FALSE\n\n\n\n\n\nA.12.3 NaN, Inf, and -Inf\n\nIntegers have one special value: NA, while doubles have four: NA, NaN, Inf and -Inf.\nNaN means Not a Number.\nAll three special values NaN, Inf and -Inf can arise during division.\n\n\nc(-1, 0, 1) / 0\n\n[1] -Inf  NaN  Inf\n\n\n\nAvoid using ==. Use functions is.finite(), is.infinite(), and is.nan().\n\n\n\n\nis.finite(0)\n\n[1] TRUE\n\nis.nan(0/0)\n\n[1] TRUE\n\n\n\n\nis.infinite(7.8/1e-307)\n\n[1] FALSE\n\nis.infinite(7.8/1e-308)\n\n[1] TRUE\n\n\n\n\n\nA.12.4 Helper Functions\n\nNaN is a missing value too.\nThere should be something there, but it‚Äôs Not Available to us because of invalid operation.\n\n\n\n\n0\nInf\nNA\nNaN\n\n\n\nis.finite()\nv\n\n\n\n\n\nis.infinite()\n\nv\n\n\n\n\nis.na()\n\n\nv\nv\n\n\nis.nan()\n\n\n\nv\n\n\n\nA.12.5 NA, NULL, NaN comparison\n\n\n\nclass(NULL); class(NA); class(NaN)\n\n[1] \"NULL\"\n\n\n[1] \"logical\"\n\n\n[1] \"numeric\"\n\nNULL &gt; 5; NA &gt; 5; NaN &gt; 5\n\nlogical(0)\n\n\n[1] NA\n\n\n[1] NA\n\nlength(NULL); length(NA); length(NaN)\n\n[1] 0\n\n\n[1] 1\n\n\n[1] 1\n\n\n\n\n(vx &lt;- c(3, NULL, 5)); (vy &lt;- c(3, NA, 5)); (vz &lt;- c(3, NaN, 5))\n\n[1] 3 5\n\n\n[1]  3 NA  5\n\n\n[1]   3 NaN   5\n\nsum(vx)  # NULL isn't a problem cuz it doesn't exist\n\n[1] 8\n\nsum(vy)\n\n[1] NA\n\nsum(vy, na.rm = TRUE)\n\n[1] 8\n\nsum(vz)\n\n[1] NaN\n\nsum(vz, na.rm = TRUE)\n\n[1] 8",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#conditions",
    "href": "a-r_prog.html#conditions",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.13 Conditions",
    "text": "A.13 Conditions\n\n# The condition must evaluate to either TRUE or FALSE.\nif (condition) {\n  # code executed when condition is TRUE\n} else {\n  # code executed when condition is FALSE\n}\n\n\nif (c(TRUE, FALSE)) {}\n#&gt; Warning in if (c(TRUE, FALSE)) {: the condition has length &gt; 1 and only the\n#&gt; first element will be used\n#&gt; NULL\n\nif (NA) {}\n#&gt; Error in if (NA) {: missing value where TRUE/FALSE needed\n\n\nYou can use || (or) and && (and) to combine multiple logical expressions.\n\n\nif (cond1 || cond2) {\n  # code executed when condition is TRUE\n}\n\n\nThe basic If-else statement structure is that we write if then put a condition inside a pair of parenthesis, then use curly braces to wrap the code to be run when the condition is TRUE.\nIf we want to have the code to be run when the condition is FALSE, we add else and another pair of curly braces.\ncurly braces is not necessary if you just have one line of code to be run.\nThe condition must evaluate to either one TRUE or one FALSE.\nIf it‚Äôs a vector, you‚Äôll get a warning message, and only the first element will be used.\nIf it‚Äôs an NA, you‚Äôll get an error.\n\n\nif (this) {\n    # do that\n} else if (that) {\n    # do something else\n} else {\n    # \n}\n\n\nif (celsius &lt;= 0) {\n    \"freezing\"\n} else if (celsius &lt;= 10) {\n    \"cold\"\n} else if (celsius &lt;= 20) {\n   \"cool\"\n} else if (celsius &lt;= 30) {\n    \"warm\"\n} else {\n    \"hot\"\n}\n\n\nIf we want to have multiple conditions, we add the word else if.\nThe code below is an example of converting numerical data to categorical data, freezing, cold, warm and hot.\nIt‚Äôs not the best way to do conversion.\n\n\nIf you end up with a very long series of chained if-else statements, rewrite it!\n\n\n\n\n\nhttps://speakerdeck.com/jennybc/code-smells-and-feels\n\n\n\n\nIf you end up with a very long series of chained if-else statements, rewrite it! Don‚Äôt confuse readers and especially yourself.\nWe have a function called ‚Äúget same data‚Äù.\nWhen you read the code in a linear fashion from top to bottom, you are falling down and down into conditions that were like a long time ago that you saw what you are actually checking.\nHere is another way to write the exactly the same function. ‚Ä¶.. and now I am on the happy path. I get the data.\nSo if I open a file, and I know that something is gone wrong and checking all these things, I am much happier to be facing this than that!\nI think our brain cannot process too many layers. When we are trying to analyze so many layers, we just get lost.\n\n\n\n\n\nhttps://speakerdeck.com/jennybc/code-smells-and-feels\n\n\n\n\nSo there is no else, there is only if!\nBack, on the left, every if has an else.\nOn the right, we have no else. And this makes your code much more readable!",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#functions",
    "href": "a-r_prog.html#functions",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.14 Functions",
    "text": "A.14 Functions\n\nWrite a function whenever you‚Äôve copied and pasted your code more than twice.\n\nThree key steps/components:\n\npick a name for the function.\nlist the inputs, or arguments, to the function inside function.\nplace the code you have developed in body of the function.\n\n\n\n\nfunction_name &lt;- function(arg1, arg2, ...) {\n    ## body\n    return(something)\n}\n\n\nadd_number &lt;- function(a, b) {\n    c &lt;- a + b\n    return(c)\n}\n\nn1 &lt;- 9\nn2 &lt;- 18\nadd_number(n1, n2)\n\n[1] 27",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-r_prog.html#loops",
    "href": "a-r_prog.html#loops",
    "title": "Appendix A ‚Äî R programming",
    "section": "\nA.15 Loops",
    "text": "A.15 Loops\n\nA.15.1 for loops\n\n## Syntax\nfor (value in that) {\n    this\n}\n\n\nfor (i in 1:5) {\n    print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\nfor (x in c(\"My\", \"1st\", \"for\", \"loop\")) {\n    print(x)\n}\n\n[1] \"My\"\n[1] \"1st\"\n[1] \"for\"\n[1] \"loop\"\n\n\n\nA.15.2 while loops\n\nwhile (condition) {\n    # body\n}\n\n\nYou can rewrite any for loop as a while loop, but you can‚Äôt rewrite every while loop as a for loop.\n\n\nfor (i in seq_along(x)) {\n    # body\n}\n\n# Equivalent to\ni &lt;- 1\nwhile (i &lt;= length(x)) {\n    # body\n    i &lt;- i + 1 \n}\n\n\n\n\n\n\n\n\nWe find how many tries it takes to get 5 heads in a row:\n\n\n## a function that sample one from \"T\" or \"H\"\nflip &lt;- function() sample(c(\"T\", \"H\"), 1)\n\nflips &lt;- 0; nheads &lt;- 0\n\nwhile (nheads &lt; 5) {\n    if (flip() == \"H\") {\n        nheads &lt;- nheads + 1\n    } else {\n        nheads &lt;- 0\n    }\n    flips &lt;- flips + 1\n}\n\nflips\n\n[1] 5",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>R programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html",
    "href": "a-py_prog.html",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "",
    "text": "B.1 Arithmetic and Logical Operators\n2 + 3 / (5 * 4) ** 2\n\n2.0075\n\n5 == 5.00\n\nTrue\n\n5 == int(5)\n\nTrue\n\ntype(int(5))\n\n&lt;class 'int'&gt;\n\nnot True == False\n\nTrue\nbool() converts nonzero numbers to True and zero to False\n-5 | 0\n\n-5\n\n1 & 1\n\n1\n\nbool(2) | bool(0)\n\nTrue",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#math-functions",
    "href": "a-py_prog.html#math-functions",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.2 Math Functions",
    "text": "B.2 Math Functions\nNeed to import math library in Python.\n\nimport math\nmath.sqrt(144)\n\n12.0\n\nmath.exp(1)\n\n2.718281828459045\n\nmath.sin(math.pi/2)\n\n1.0\n\nmath.log(32, 2)\n\n5.0\n\nabs(-7)\n\n7\n\n\n\n# python comment",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#variables-and-assignment",
    "href": "a-py_prog.html#variables-and-assignment",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.3 Variables and Assignment",
    "text": "B.3 Variables and Assignment\n\nx = 5\nx\n\n5\n\nx = x + 6\nx\n\n11\n\nx == 5\n\nFalse\n\nmath.log(x)\n\n2.3978952727983707",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#object-types",
    "href": "a-py_prog.html#object-types",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.4 Object Types",
    "text": "B.4 Object Types\nstr, float, int and bool.\n\ntype(5.0)\n\n&lt;class 'float'&gt;\n\ntype(5)\n\n&lt;class 'int'&gt;\n\ntype(\"I_love_data_science!\")\n\n&lt;class 'str'&gt;\n\ntype(1 &gt; 3)\n\n&lt;class 'bool'&gt;\n\ntype(5) is float\n\nFalse",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#data-structure---lists",
    "href": "a-py_prog.html#data-structure---lists",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.5 Data Structure - Lists",
    "text": "B.5 Data Structure - Lists\n\nB.5.1 Lists\n\nPython has numbers and strings, but no built-in vector structure.\nTo create a sequence type of structure, we can use a list that can save several elements in an single object.\nTo create a list in Python, we use [].\n\n\nlst_num = [0, 2, 4] \nlst_num\n\n[0, 2, 4]\n\ntype(lst_num)\n\n&lt;class 'list'&gt;\n\nlen(lst_num)\n\n3\n\n\nList elements can have different types!\n\nB.5.2 Subsetting lists\n\nlst = ['data', 'math', 34, True]\nlst\n\n['data', 'math', 34, True]\n\n\n\nIndexing in Python always starts at 0!\n\n0: the 1st element\n\n\nlst\n\n['data', 'math', 34, True]\n\nlst[0]\n\n'data'\n\ntype(lst[0]) ## not a list\n\n&lt;class 'str'&gt;\n\n\n\n\n-1: the last element\n\n\nlst[-2]\n\n34\n\n\n\n\n[a:b]: the (a+1)-th to b-th elements\n\n\nlst[1:4]\n\n['math', 34, True]\n\ntype(lst[1:4]) ## a list\n\n&lt;class 'list'&gt;\n\n\n\n\n[a:]: elements from the (a+1)-th to the last\n\n\nlst[2:]\n\n[34, True]\n\n\nWhat does lst[0:1] return? Is it a list?\n\nB.5.3 Lists are mutable\n\nLists are changed in place!\n\n\nlst[1]\n\n'math'\n\nlst[1] = \"stats\"\nlst\n\n['data', 'stats', 34, True]\n\n\n\nlst[2:] = [False, 77]\nlst\n\n['data', 'stats', False, 77]\n\n\nIf we change any element value in a list, the list itself will be changed as well.\n\nB.5.4 List operations and methods list.method()\n\nThis is a common syntax in Python. We start with a Python object of some type, then type dot followed by any method specifically for this particular data type or structure for operations.\n\n## Concatenation\nlst_num + lst\n\n[0, 2, 4, 'data', 'stats', False, 77]\n\n\n\n## Repetition\nlst_num * 3 \n\n[0, 2, 4, 0, 2, 4, 0, 2, 4]\n\n\n\n## Membership\n34 in lst\n\nFalse\n\n\n\n## Appends \"cat\" to lst\nlst.append(\"cat\")\nlst\n\n['data', 'stats', False, 77, 'cat']\n\n\n\n## Removes and returns last object from list\nlst.pop()\n\n'cat'\n\nlst\n\n['data', 'stats', False, 77]\n\n\n\n## Removes object from list\nlst.remove(\"stats\")\nlst\n\n['data', False, 77]\n\n\n\n## Reverses objects of list in place\nlst.reverse()\nlst\n\n[77, False, 'data']",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#data-structure---tuples",
    "href": "a-py_prog.html#data-structure---tuples",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.6 Data Structure - Tuples",
    "text": "B.6 Data Structure - Tuples\n\nTuples work exactly like lists except they are immutable, i.e., they can‚Äôt be changed in place.\nTo create a tuple, we use ().\n\n\ntup = ('data', 'math', 34, True)\ntup\n\n('data', 'math', 34, True)\n\ntype(tup)\n\n&lt;class 'tuple'&gt;\n\nlen(tup)\n\n4\n\n\n\ntup[2:]\n\n(34, True)\n\ntup[-2]\n\n34\n\n\n\ntup[1] = \"stats\"  ## does not work!\n# TypeError: 'tuple' object does not support item assignment\n\n\ntup\n\n('data', 'math', 34, True)\n\n\n\nB.6.1 Tuples functions and methods\nLists have more methods than tuples because lists are more flexible.\n\n# Converts a list into tuple\ntuple(lst_num)\n\n(0, 2, 4)\n\n\n\n# number of occurance of \"data\"\ntup.count(\"data\")\n\n1\n\n\n\n# first index of \"data\"\ntup.index(\"data\")\n\n0",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#data-structure---dictionaries",
    "href": "a-py_prog.html#data-structure---dictionaries",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.7 Data Structure - Dictionaries",
    "text": "B.7 Data Structure - Dictionaries\n\nA dictionary consists of key-value pairs.\nA dictionary is mutable, i.e., the values can be changed in place and more key-value pairs can be added.\nTo create a dictionary, we use {\"key name\": value}.\nThe value can be accessed by the key in the dictionary.\n\n\ndic = {'Name': 'Ivy', 'Age': 7, 'Class': 'First'}\n\n\ndic['Age']\n\n7\n\n\n\ndic['age']  ## does not work\n\n\ndic['Age'] = 9\ndic['Class'] = 'Third'\ndic\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third'}\n\n\n\nB.7.1 Properties of dictionaries\n\nPython will use the last assignment!\n\n\ndic1 = {'Name': 'Ivy', 'Age': 7, 'Name': 'Liya'}\ndic1['Name']\n\n'Liya'\n\n\n\nKeys are unique and immutable.\nA key can be a tuple, but CANNOT be a list.\n\n\n## The first key is a tuple!\ndic2 = {('First', 'Last'): 'Ivy Lee', 'Age': 7}\ndic2[('First', 'Last')]\n\n'Ivy Lee'\n\n\n\n## does not work\ndic2 = {['First', 'Last']: 'Ivy Lee', 'Age': 7}\ndic2[['First', 'Last']]\n\n\nB.7.2 Disctionary methods\n\ndic\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third'}\n\n\n\n## Returns list of dictionary dict's keys\ndic.keys()\n\ndict_keys(['Name', 'Age', 'Class'])\n\n\n\n## Returns list of dictionary dict's values\ndic.values()\n\ndict_values(['Ivy', 9, 'Third'])\n\n\n\n## Returns a list of dict's (key, value) tuple pairs\ndic.items()\n\ndict_items([('Name', 'Ivy'), ('Age', 9), ('Class', 'Third')])\n\n\n\n## Adds dictionary dic2's key-values pairs in to dic\ndic2 = {'Gender': 'female'}\ndic.update(dic2)\ndic\n\n{'Name': 'Ivy', 'Age': 9, 'Class': 'Third', 'Gender': 'female'}\n\n\n\n## Removes all elements of dictionary dict\ndic.clear()\ndic\n\n{}",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#python-data-structures-for-data-science",
    "href": "a-py_prog.html#python-data-structures-for-data-science",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.8 Python Data Structures for Data Science",
    "text": "B.8 Python Data Structures for Data Science\n\nPython built-in data structures are not specifically for data science.\nTo use more data science friendly functions and structures, such as array or data frame, Python relies on packages NumPy and pandas.\n\n\nB.8.1 Installing NumPy and pandas\nIn your RStudio project, run\n\nlibrary(reticulate)\nvirtualenv_create(\"myenv\")\n\nGo to Tools &gt; Global Options &gt; Python &gt; Select &gt; Virtual Environments\n\n\n\n\nYou may need to restart R session. Do it, and in the new R session, run\n\nlibrary(reticulate)\npy_install(c(\"numpy\", \"pandas\", \"matplotlib\"))\n\nRun the following Python code, and make sure everything goes well.\n\nimport numpy as np\nimport pandas as pd\nv1 = np.array([3, 8])\nv1\ndf = pd.DataFrame({\"col\": ['red', 'blue', 'green']})\ndf",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#pandas",
    "href": "a-py_prog.html#pandas",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.9 Pandas",
    "text": "B.9 Pandas\npandas is a Python library that provides data structures, manipulation and analysis tools for data science.\n\nimport numpy as np\nimport pandas as pd\n\n\nB.9.1 Pandas series from a list\n\n# import pandas as pd\na = [1, 7, 2]\ns = pd.Series(a)\nprint(s)\n\n0    1\n1    7\n2    2\ndtype: int64\n\n\n\nprint(s[0])\n\n1\n\n\n\n## index used as naming \ns = pd.Series(a, index = [\"x\", \"y\", \"z\"])\nprint(s)\n\nx    1\ny    7\nz    2\ndtype: int64\n\n\n\nprint(s[\"y\"])\n\n7\n\n\n\nB.9.2 Pandas series from a dictionary\n\ngrade = {\"math\": 99, \"stats\": 97, \"cs\": 66}\ns = pd.Series(grade)\nprint(s)\n\nmath     99\nstats    97\ncs       66\ndtype: int64\n\n\n\ngrade = {\"math\": 99, \"stats\": 97, \"cs\": 66}\n\n## index used as subsetting \ns = pd.Series(grade, index = [\"stats\", \"cs\"])\nprint(s)\n\nstats    97\ncs       66\ndtype: int64\n\n\n\nHow do we create a named vector in R?\n\n\ngrade &lt;- c(\"math\" = 99, \"stats\" = 97, \"cs\" = 66)\n\n\nB.9.3 Pandas data frame\n\n\nCreate a data frame from a dictionary\n\n\ndata = {\"math\": [99, 65, 87], \"stats\": [92, 48, 88], \"cs\": [50, 88, 94]}\n\ndf = pd.DataFrame(data)\nprint(df) \n\n   math  stats  cs\n0    99     92  50\n1    65     48  88\n2    87     88  94\n\n\n\nRow and column names\n\n\ndf.index = [\"s1\", \"s2\", \"s3\"]\ndf.columns = [\"Math\", \"Stat\", \"CS\"]\ndf\n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\nB.9.4 Subsetting columns\n\nIn Python, [] returns Series, [[]] returns DataFrame!\nIn R, [] returns tibble/data frame, [[]] returns vector!\n\nBy Names\n\n## Series\ndf[\"Math\"]\n\ns1    99\ns2    65\ns3    87\nName: Math, dtype: int64\n\ntype(df[\"Math\"])\n\n&lt;class 'pandas.core.series.Series'&gt;\n\n\nBy Index\n\n# ## DataFrame\ndf[[\"Math\"]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\ntype(df[[\"Math\"]])\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\n\n\n\ndf[[\"Math\", \"CS\"]]\n\n    Math  CS\ns1    99  50\ns2    65  88\ns3    87  94\n\n\n\nisinstance(df[[‚ÄúMath‚Äù]], pd.DataFrame)\n\n\nB.9.5 Subsetting rows DataFrame.iloc\n\n\n\ninteger-location based indexing for selection by position\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\n## first row Series\ndf.iloc[0] \n\nMath    99\nStat    92\nCS      50\nName: s1, dtype: int64\n\n\n\n## first row DataFrame\ndf.iloc[[0]]\n\n    Math  Stat  CS\ns1    99    92  50\n\n\n\n## first 2 rows\ndf.iloc[[0, 1]]\n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\n\n\n\n## 1st and 3rd row\ndf.iloc[[True, False, True]]\n\n    Math  Stat  CS\ns1    99    92  50\ns3    87    88  94\n\n\n\nB.9.6 Subsetting rows and columns DataFrame.iloc\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\n## (1, 3) row and (1, 3) col\ndf.iloc[[0, 2], [0, 2]]\n\n    Math  CS\ns1    99  50\ns3    87  94\n\n\n\n## all rows and 1st col\ndf.iloc[:, [True, False, False]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\n\n\ndf.iloc[0:2, 1:3]\n\n    Stat  CS\ns1    92  50\ns2    48  88\n\n\n\nB.9.7 Subsetting rows and columns DataFrame.loc\n\nAccess a group of rows and columns by label(s)\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.loc['s1', \"CS\"]\n\n50\n\n\n\n## all rows and 1st col\ndf.loc['s1':'s3', [True, False, False]]\n\n    Math\ns1    99\ns2    65\ns3    87\n\n\n\ndf.loc['s2', ['Math', 'Stat']]\n\nMath    65\nStat    48\nName: s2, dtype: int64\n\n\n\nB.9.8 Obtaining a single cell value DataFrame.iat/ DataFrame.at\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.iat[1, 2]\n\n88\n\n\n\ndf.iloc[0].iat[1]\n\n92\n\n\n\ndf.at['s2', 'Stat']\n\n48\n\n\n\ndf.loc['s1'].at['Stat']\n\n92\n\n\n\nB.9.9 New columns DataFrame.insert and new rows pd.concat\n\n\ndf \n\n    Math  Stat  CS\ns1    99    92  50\ns2    65    48  88\ns3    87    88  94\n\n\n\ndf.insert(loc = 2, \n          column = \"Chem\", \n          value = [77, 89, 76])\ndf\n\n    Math  Stat  Chem  CS\ns1    99    92    77  50\ns2    65    48    89  88\ns3    87    88    76  94\n\n\n\ndf1 = pd.DataFrame({\n    \"Math\": 88, \n    \"Stat\": 99, \n    \"Chem\": 0, \n    \"CS\": 100\n    }, index = ['s4'])\n\n\npd.concat(objs = [df, df1])\n\n    Math  Stat  Chem   CS\ns1    99    92    77   50\ns2    65    48    89   88\ns3    87    88    76   94\ns4    88    99     0  100\n\n\n\n\npd.concat(objs = [df, df1], \n          ignore_index = True)",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#numpy",
    "href": "a-py_prog.html#numpy",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.10 NumPy",
    "text": "B.10 NumPy\n\nB.10.1 NumPy for arrays/matrices\nNumPy is used to work with arrays/matrices.\n\nThe array object in NumPy is called ndarray.\nUse array() to create an array.\n\n\nrange(0, 5, 1) # a seq of number from 0 to 4 with increment of 1\n\nrange(0, 5)\n\nlist(range(0, 5, 1))\n\n[0, 1, 2, 3, 4]\n\n\n\nimport numpy as np\narr = np.array(range(0, 5, 1)) ## One-dim array \narr\n\narray([0, 1, 2, 3, 4])\n\ntype(arr)\n\n&lt;class 'numpy.ndarray'&gt;\n\n\n\nB.10.2 1D array (vector) and 2D array (matrix)\n\n\nnp.arange: Efficient way to create a one-dim array of sequence of numbers\n\n\nnp.arange(2, 5)\n\narray([2, 3, 4])\n\nnp.arange(6, 0, -1)\n\narray([6, 5, 4, 3, 2, 1])\n\n\n\n2D array\n\n\nnp.array([[1, 2, 3], [4, 5, 6]])\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\n\nnp.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])\n\narray([[[1, 2, 3],\n        [4, 5, 6]],\n\n       [[1, 2, 3],\n        [4, 5, 6]]])\n\n\n\nB.10.3 np.reshape()\n\n\narr2 = np.arange(8).reshape(2, 4)\narr2\n\narray([[0, 1, 2, 3],\n       [4, 5, 6, 7]])\n\narr2.shape  \n\n(2, 4)\n\n\n\narr2.ndim\n\n2\n\n\n\narr2.size\n\n8\n\n\n\nB.10.4 Stacking arrays\n\na = np.array([1, 2, 3, 4]).reshape(2, 2)\nb = np.array([5, 6, 7, 8]).reshape(2, 2)\n\nnp.vstack((a, b))\n\narray([[1, 2],\n       [3, 4],\n       [5, 6],\n       [7, 8]])\n\n\n\nnp.hstack((a, b))\n\narray([[1, 2, 5, 6],\n       [3, 4, 7, 8]])",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#plotting",
    "href": "a-py_prog.html#plotting",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.11 Plotting",
    "text": "B.11 Plotting\nmatplotlib.markers\n\npch = np.array(['.', ',', 'o', 'v', '^', '&lt;', '&gt;', '1', '2', '3', '4', '8', 's', 'p', 'P', '*', 'h', 'H', '+', 'x', 'X', 'D', 'd', '|', '_'])\n#all types of maker\npch_len = pch.shape[0]\nx = np.array([i for i in range(1, pch_len+1)])\ny = np.ones(pch_len)\n\n\nplt.figure(0)\nfor i in range(0, pch_len):\n    plt.plot(x[i],y[i],pch[i])\n\n\n\n\n\n\n\n\nB.11.1 Scatterplot\n\nCodemtcars = pd.read_csv('./data/mtcars.csv')\nmtcars.iloc[0:15,0:4]\n\n     mpg  cyl   disp   hp\n0   21.0    6  160.0  110\n1   21.0    6  160.0  110\n2   22.8    4  108.0   93\n3   21.4    6  258.0  110\n4   18.7    8  360.0  175\n5   18.1    6  225.0  105\n6   14.3    8  360.0  245\n7   24.4    4  146.7   62\n8   22.8    4  140.8   95\n9   19.2    6  167.6  123\n10  17.8    6  167.6  123\n11  16.4    8  275.8  180\n12  17.3    8  275.8  180\n13  15.2    8  275.8  180\n14  10.4    8  472.0  205\n\n\n\nimport matplotlib.pyplot as plt\nplt.scatter(x = mtcars.mpg, y = mtcars.hp, color = \"r\")\nplt.xlabel(\"Miles per gallon\")\nplt.ylabel(\"Horsepower\")\nplt.title(\"Scatter plot\")\n\n\n\n\n\n\n\n\nB.11.2 Subplots\nThe command plt.scatter() is used for creating one single plot. If multiple subplots are wanted in one single call, one can use the format\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.scatter(x, y)\nax2.plot(x, y)\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.scatter(x = mtcars.mpg, y = mtcars.hp)\nax2.scatter(x = mtcars.hp, y = mtcars.disp)\n\n\n\n\n\n\n\n\nCheck Creating multiple subplots using plt.subplots for more details.\n\nB.11.3 Boxplot\n\nCodecyl_index = np.sort(np.unique(np.array(mtcars.cyl)))\ncyl_shape = cyl_index.shape[0]\ncyl_list = []\nfor i in range (0, cyl_shape):\n    cyl_list.append(np.array(mtcars[mtcars.cyl == cyl_index[i]].mpg))\n\n\n\nplt.boxplot(cyl_list, vert=False, tick_labels=[4, 6, 8])\n\n{'whiskers': [&lt;matplotlib.lines.Line2D object at 0x169998860&gt;, &lt;matplotlib.lines.Line2D object at 0x169998b30&gt;, &lt;matplotlib.lines.Line2D object at 0x169999af0&gt;, &lt;matplotlib.lines.Line2D object at 0x169999dc0&gt;, &lt;matplotlib.lines.Line2D object at 0x16999ac90&gt;, &lt;matplotlib.lines.Line2D object at 0x16999af30&gt;], 'caps': [&lt;matplotlib.lines.Line2D object at 0x169998da0&gt;, &lt;matplotlib.lines.Line2D object at 0x169999040&gt;, &lt;matplotlib.lines.Line2D object at 0x16999a000&gt;, &lt;matplotlib.lines.Line2D object at 0x16999a2d0&gt;, &lt;matplotlib.lines.Line2D object at 0x16999b200&gt;, &lt;matplotlib.lines.Line2D object at 0x16999b4d0&gt;], 'boxes': [&lt;matplotlib.lines.Line2D object at 0x169998740&gt;, &lt;matplotlib.lines.Line2D object at 0x169999820&gt;, &lt;matplotlib.lines.Line2D object at 0x16999aa50&gt;], 'medians': [&lt;matplotlib.lines.Line2D object at 0x169999340&gt;, &lt;matplotlib.lines.Line2D object at 0x16999a540&gt;, &lt;matplotlib.lines.Line2D object at 0x16999b7a0&gt;], 'fliers': [&lt;matplotlib.lines.Line2D object at 0x1699995e0&gt;, &lt;matplotlib.lines.Line2D object at 0x16999a7e0&gt;, &lt;matplotlib.lines.Line2D object at 0x16999ba40&gt;], 'means': []}\n\nplt.xlabel(\"Miles per gallon\")\nplt.ylabel(\"Number of cylinders\")\n\n\n\n\n\n\n\n\nB.11.4 Histogram\n\nplt.hist(mtcars.wt, \n         bins = 19, \n         color=\"#003366\",\n         edgecolor=\"#FFCC00\")\nplt.xlabel(\"weights\")\nplt.title(\"Histogram of weights\")\n\n\n\n\n\n\n\n\nB.11.5 Barplot\n\ncount_py = mtcars.value_counts('gear')\ncount_py\n\ngear\n3    15\n4    12\n5     5\nName: count, dtype: int64\n\n\n\nplt.bar(count_py.index, count_py)\nplt.xlabel(\"Number of Gears\")\nplt.title(\"Car Distribution\")\n\n\n\n\n\n\n\n\nB.11.6 Pie chart\n\npercent = round(count_py / sum(count_py) * 100, 2)\ntexts = [str(percent.index[k]) + \" gear \" + str(percent.array[k]) + \"%\" for k in range(0,3)]\n\n\nplt.pie(count_py, labels = texts, colors = ['r', 'g', 'b'])\n\n([&lt;matplotlib.patches.Wedge object at 0x16e5bdd90&gt;, &lt;matplotlib.patches.Wedge object at 0x16e5572f0&gt;, &lt;matplotlib.patches.Wedge object at 0x16e5fc320&gt;], [Text(0.10781885436251686, 1.0947031993394165, '3 gear 46.88%'), Text(-0.6111272563215624, -0.9146165735327998, '4 gear 37.5%'), Text(0.9701133907831904, -0.5185364105085978, '5 gear 15.62%')])\n\nplt.title(\"Pie Charts\")\n\n\n\n\n\n\n\n\nB.11.7 2D Imaging\nIn Python,\n\nmat_img = np.reshape(np.array(range(1,31)), [6,5], \"F\")\nmat_img\n\narray([[ 1,  7, 13, 19, 25],\n       [ 2,  8, 14, 20, 26],\n       [ 3,  9, 15, 21, 27],\n       [ 4, 10, 16, 22, 28],\n       [ 5, 11, 17, 23, 29],\n       [ 6, 12, 18, 24, 30]])\n\nplt.imshow(mat_img, cmap = 'Oranges')\n\n\n\n\n\n\n\n\nvolcano = pd.read_csv('./data/volcano.csv', index_col=0)\nx = 10*np.arange(1,volcano.shape[0]+1)\ny = 10*np.arange(1,volcano.shape[1]+1)\nX,Y = np. meshgrid(x,y)\nvt = volcano.transpose()\nprint(vt.shape)\n\n(61, 87)\n\nprint(X.shape)\n\n(61, 87)\n\nprint(Y.shape)\n\n(61, 87)\n\n\n\nfig, ax = plt.subplots()\nIM = ax.matshow(vt, alpha =1, cmap='terrain')\nCS = ax.contour(vt, levels=np.arange(90,200,5))\nax.clabel(CS, inline=True, fontsize=10)\nax.set_title('Maunga Whau Volcano')\n\n\n\n\n\n\n\n\nB.11.8 3D scatterplot\nIn Python,\n\nfig = plt.figure(figsize=(12, 12))\nax = fig.add_subplot(projection='3d')\n\nax.scatter(xs = mtcars.wt, ys = mtcars.disp, zs = mtcars.mpg)\nax.set_xlabel('Weights')\nax.set_ylabel(\"Displacement\")\nax.set_zlabel(\"Miles per gallon\")\nax.set_title(\"3D Scatter Plot\")\n\n\n\n\n\n\n\n\nB.11.9 Perspective plot\nIn Python,\n\nx = 10*np.arange(1,volcano.shape[0]+1)\ny = 10*np.arange(1,volcano.shape[1]+1)\nvt = volcano.transpose()\nZ = 10*vt\nX,Y = np. meshgrid(x,y)\n\nprint(Z.shape)\n\n(61, 87)\n\nprint(X.shape)\n\n(61, 87)\n\nprint(Y.shape)\n\n(61, 87)\n\n\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n# Plot the surface.\nax.plot_surface(X, Y, Z, cmap = 'Greens')",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#special-objects",
    "href": "a-py_prog.html#special-objects",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.12 Special Objects",
    "text": "B.12 Special Objects\nIn python, NA, NaN and NULL are not that distinguishable, comparing to R.\n\nNaN can be used as a numerical value on mathematical operations, while None cannot (or at least shouldn‚Äôt).\nNaN is a numeric value, as defined in IEEE 754 floating-point standard.\nNone is an internal Python type (NoneType) and would be more like ‚Äúinexistent‚Äù or ‚Äúempty‚Äù than ‚Äúnumerically invalid‚Äù in this context.\n\n\na = np.array([None, 0.9, 10])\ntype(a)\n\n&lt;class 'numpy.ndarray'&gt;\n\na == None\n\narray([ True, False, False])\n\nlen(a)\n\n3\n\nprint(type(a[0]))\n\n&lt;class 'NoneType'&gt;\n\n\n\nNone == None\n\nTrue\n\n'' == None\n\nFalse\n\n\n\na1 = np.array([-1,0,1])/0\n\n&lt;string&gt;:1: RuntimeWarning: divide by zero encountered in divide\n&lt;string&gt;:1: RuntimeWarning: invalid value encountered in divide\n\na1\n\narray([-inf,  nan,  inf])\n\n\n\nmath.isfinite(0)\n\nTrue\n\nmath.isnan(float(\"nan\"))\n\nTrue\n\npd.isna(float(\"nan\"))\n\nTrue\n\nnp.isnan(float(\"nan\"))\n\nTrue\n\n\n\nmath.isfinite(7.8/1e-307)\n\nTrue\n\nmath.isfinite(7.8/1e-308)\n\nFalse\n\n\n\ntype(None)\n\n&lt;class 'NoneType'&gt;\n\n\n\n## TypeError: '&gt;' not supported between instances of 'NoneType' and 'int'\nNone &gt; 5\n\n\n## TypeError: object of type 'NoneType' has no len()\nlen(None)\n\n\nfloat(\"NaN\") &gt; 5\n\nFalse\n\n\n\nv_none = np.array([3, None, 5])\nv_none\n\narray([3, None, 5], dtype=object)\n\nv_nan = np.array([3, float(\"NaN\"), 5])\nv_nan\n\narray([ 3., nan,  5.])\n\n\n\n# TypeError: unsupported operand type(s) for +: 'int' and 'NoneType'\nsum(v_none)\n\n\nsum(v_nan)\n\nnan",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#conditions",
    "href": "a-py_prog.html#conditions",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.13 Conditions",
    "text": "B.13 Conditions\n\nif condition:\n    # code executed when condition is true\nelse:\n    # code executed when condition is false\n\n\na = 5\nb = 20\nif a &gt; 4 or b &gt; 4:\n    print('a &gt; 4 or b &gt; 4')\n\na &gt; 4 or b &gt; 4\n\nif a &gt; 4 and b &gt; 4:\n    print('a &gt; 4 and b &gt; 4')\n\na &gt; 4 and b &gt; 4\n\n\n\nif (a &gt; 4) | (b &gt; 4):\n    print('a &gt; 4 or b &gt; 4')\n\na &gt; 4 or b &gt; 4\n\nif (a &gt; 4) & (b &gt; 4):\n    print('a &gt; 4 and b &gt; 4')\n\na &gt; 4 and b &gt; 4",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#multiple-conditions",
    "href": "a-py_prog.html#multiple-conditions",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.14 Multiple conditions",
    "text": "B.14 Multiple conditions\n\nif condition A:\n    # do that\nelif condition B:\n    # do something else\nelse:\n    # \n\n\nrd = np.random.randint(100)\nprint(rd)\n\n22\n\nif rd &lt;= 20:\n    print(\"rd &lt;= 20\")\nelif rd &gt; 20 and rd &lt;= 40:\n    print('rd &gt; 20 and rd &lt;= 40')\nelif rd &gt; 40 and rd &lt;= 60:\n    print('rd &gt; 40 and rd &lt;= 60')\nelif rd &gt; 60 and rd &lt;= 80:\n    print('rd &gt; 60 and rd &lt;= 80')\nelif rd &gt; 80 and rd &lt;= 100:\n    print('rd &gt; 80 and rd &lt;= 100')\n\nrd &gt; 20 and rd &lt;= 40",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#functions",
    "href": "a-py_prog.html#functions",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.15 Functions",
    "text": "B.15 Functions\n\ndef function_name(arg1, arg2, ...):\n    ## body\n    return(something)\n\n\ndef add_number(a, b):\n    c = a + b\n    return c\n\nn1 = 9\nn2 = 18\nadd_number(n1, n2)\n\n27",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "a-py_prog.html#loops",
    "href": "a-py_prog.html#loops",
    "title": "Appendix B ‚Äî Python Programming",
    "section": "\nB.16 Loops",
    "text": "B.16 Loops\n\nB.16.1 for loops\n\nPython\nfor value in that:\n    # do this\n\n\nfor i in range(5):\n    print('for', i)\n\nfor 0\nfor 1\nfor 2\nfor 3\nfor 4\n\n\n\nfor i in ['My', '1st', 'for', 'loop']:\n    print(i)\n\nMy\n1st\nfor\nloop\n\n\n\nB.16.2 while loops\n\nwhile (condition):\n    # do this\n\n\ni = 1\nwhile(i &lt; 5):\n    print('while',i)\n    i = i + 1\n\nwhile 1\nwhile 2\nwhile 3\nwhile 4\n\n\n\nnp.random.seed(86)\ndef flip():\n    return np.random.choice(['T','H'], 1)\n    \nflips = 0\nnheads = 0\n\nwhile(nheads &lt; 3):\n    if flip() == \"H\":\n        nheads += 1\n    else:\n        nheads = 0\n    flips += 1\n    \nflips\n\n9",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>B</span>¬† <span class='chapter-title'>Python Programming</span>"
    ]
  },
  {
    "objectID": "prob-disc.html#python",
    "href": "prob-disc.html#python",
    "title": "10¬† Discrete Probability Distributions",
    "section": "\n10.3 Python",
    "text": "10.3 Python\nIn practice, we are not gonna calculate probabilities of binomial or other commonly used probability distributions. Instead, we use computing software. In Python we can use the methods in the scipy.stats.binom module to calculate probabilities or generate values from the distributions. In general, for some distribution, short for dist, Python has the following functions\n\n\ndist.pmf(k, ...): calculate probability value \\(P(X = k)\\).\n\n\npmf means probability mass function.\n\n\n\ndist.pdf(x, ...): calculate probability density \\(f(x)\\).\n\n\npdf means probability density function.\n\n\n\ndist.cdf(k, ...): calculate \\(P(X \\le k)\\).\n\n\ncdf means cumulative distribution function.\n\n\n\ndist.sf(k, ...): calculate \\(P(X &gt; k)\\).\n\n\nsf means survival function.\n\n\n\ndist.ppf(q, ...): obtain quantile of probability \\(q\\).\n\n\nppf means percent point function.\n\n\n\ndist.rvs(n, ...): generate \\(n\\) random numbers.\n\n\nrvs means random variables.\n\n\n\nWhen we use these functions, the dist is replaced with the shortened name of the distribution we consider. For example, we use binom.pmf(k, ...) to do the calculation for the binomial distribution.\nFor the binomial distribution, we use binom.pmf(k, n, p) to compute \\(P(X = k)\\), where n is the number of trials and p is the probability of success. Besides k, we need to provide the values of n and p because remember that they are the parameters of the binomial distribution. Without them, we cannot have a specific binomial distribution, and its probability cannot be calculated.\nTo obtain \\(P(X = 6)\\) where \\(X \\sim binomial(n=15, \\pi=0.2)\\), in Python, we do\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import binom\n\n\n# Binomial distribution calculations\n# 1. P(X = 6)\nbinom.pmf(k=6, n=15, p=0.2)\n\n0.04299262263296005\n\n\nTo answer the second question \\(P(X \\ge 6)\\), we can use the function binom.cdf(k, n, p) that calculates \\(P(X \\le k)\\) for \\(X \\sim bionomial(n = \\texttt{n}, \\pi = \\texttt{p})\\). Notice that \\(P(X &gt;= 6) = 1 - P(X &lt;= 5)\\), so in Python, we can do\n\n# 2. P(X &gt;= 6) = 1 - P(X &lt;= 5)\n1 - binom.cdf(k=5, n=15, p=0.2)\n\n0.06105142961766408\n\n\nThe function binom.cdf(q, n, p) calculates the probability \\(P(X \\le k)\\) which is the left or lower tail part of the distribution. We can use binom.sf(q, n, p) to calculate \\(P(X &gt; q)\\) which is the right or upper tail part of the distribution. Since \\(P(X \\ge 6) = P(X &gt; 5)\\), in Python we can do\n\n# Alternatively, using the upper tail probability\nbinom.sf(5, n=15, p=0.2)\n\n0.061051429617664056\n\n\nNotice that we use k=5 instead of k=6 because we want \\(P(X &gt; 5)\\). The probability value is the same as the value before.\nBelow is an example of how to generate the binomial probability distribution as a graph. We use the stem plot plt.stem().\n\nnp.arange(0, 16)\n\narray([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n\nplt.stem(np.arange(0, 16), binom.pmf(np.arange(0, 16), n=15, p=0.2), \n         basefmt=\" \")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(X = x)\")\nplt.title(\"Binomial(15, 0.2)\")\nplt.show()\n\n\n\n\n\n\n\nHere a sequence of integers 0 to 15 are created and put in the x-axis. np.arange(0, 16) is a way to generate a sequence of numbers from 0 to 15. Again 0 is included but 16 is not. Then binom.pmf(np.arange(0, 16), n=15, p=0.2) is used to create probabilities of \\(binomial(15, 0.2)\\) for each integer.\nSince \\(n = 15\\) and \\(\\pi = 0.2\\), the mean is \\((15)(0.2) = 3\\). For the binomial distribution, it means that the number of success is more likely to be happened around \\(x = 3\\). It is very uncommon to see that more than 10 drivers have alcohol level above the legal limit.\n:::",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Discrete Probability Distributions</span>"
    ]
  },
  {
    "objectID": "prob-disc.html#section",
    "href": "prob-disc.html#section",
    "title": "Observable JS Binomial Distribution",
    "section": "\n10.4 \n",
    "text": "10.4 \n\ntitle: ‚ÄúObservable JS Poisson Distribution‚Äù execute: echo: false ‚Äî\n\nviewof params_pois = Inputs.form([\n      Inputs.range([0.1, 20], {value: 2, step: 0.1, label: tex`\\lambda:`}),\n      Inputs.range([1, 40], {value: 1, step: 1, label: \"quantile:\"})\n    ])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Assumptions and Properties of Poisson Variables \nAs the binomial distribution, the Poisson distribution comes from the Poisson experiment having the following properties and assumptions:\n\nüëâ Events occur one at a time; two or more events do not occur at the same time or in the same space or spot. For example, one cannot say two patients arrived at ICU at the same time. There must be a way to separate one event from the other, and one can always know which patient arrives at ICU earlier.\nüëâ The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a nonoverlapping time period or region of space. For example, number of patients arriving at ICU between 2 PM and 3 PM has nothing to do with the number of patients arriving at ICU between 8 PM and 9 PM because the two time periods have no overlap.\nüëâ \\(\\lambda\\) is constant for any period or region. For example, the mean number of patients arriving at ICU between 2 PM and 3 PM is the same as the mean number of patients arriving at ICU between 8 PM and 9 PM. This assumption is pretty strong, and usually violated in reality. If you want to use Poisson distribution to build your statistical model, use it with additional care.\n\n\n\n\n\n\n\nWhat are the differences between Binomial and Poisson distributions?\n\n\n\nThe Poisson distribution\n\nis determined by one single parameter \\(\\lambda\\)\nhas possible values \\(x = 0, 1, 2, \\dots\\) with no upper limit (countable), while a binomial variable has possible values \\(0, 1, 2, \\dots, n\\) (finite)\n\n\n\n\n Example \n\n\nLast year there were 4200 births at the University of Wisconsin Hospital. Let \\(X\\) be the number of births in a given day at the center, and assume \\(X \\sim Poisson(\\lambda)\\). Find\n\n\\(\\lambda\\), the mean number of births per day.\nthe probability that on a randomly selected day, there are exactly 10 births.\n\\(P(X &gt; 10)\\)?\n\n\n\n\n\n\nSource: Unsplash kaushal mishra\n\n\n\n\n\n\n\\(\\small \\lambda = \\frac{\\text{Number of birth in a year}}{\\text{Number of days}} = \\frac{4200}{365} = 11.5.\\) There are totally 4200 births in one year, so on average there are 11.5 per day. According to how we define \\(X\\), the time unit is a day, not a year. The parameter \\(\\lambda\\) and \\(X\\) should have the same time unit.\n\\(\\small P(X = 10 \\mid \\lambda = 11.5) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = \\frac{11.5^{10} e^{-11.5}}{10!} = 0.113.\\)\n\\(\\small P(X &gt; 10) = p(11) + p(12) + \\dots + p(20) + \\dots\\) (No end!) \\(\\small P(X &gt; 10) = 1 - P(X \\le 10) = 1 - (p(1) + p(2) + \\dots + p(10))\\).\n\nDid you see how tedious it is to calculate the Poisson probabilities even using a calculator? I know you are waiting for R/Python implementation!\n\n\n\n\n\n\nR\nPython\n\n\n\nInstead of using dbinom() and pbinom(), for Poisson distribution, we replace binom with pois, and use dpois(x, lambda) and ppois(q, lambda) to calculate the probabilities.\nWith lambda being the mean of Poisson distribution, and \\(X\\sim Poisson(\\lambda)\\), we use\n\ndpois(x, lambda) to compute \\(P(X = x \\mid \\lambda)\\)\nppois(q, lambda) to compute \\(P(X \\le q \\mid \\lambda)\\)\nppois(q, lambda, lower.tail = FALSE) to compute \\(P(X &gt; q \\mid \\lambda)\\)\n\n\n\n[1] 11.50685\n\n\n[1] 0.112834\n\n\n\n\n[1] 0.5990436\n\n\n[1] 0.5990436\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBelow is an example of how to generate the Poisson probability distribution as a graph.\n\nplot(0:24, dpois(0:24, lambda = lam), type = 'h', \n     lwd = 5, ylab = \"P(X = x)\", xlab = \"x\", \n     main = \"Poisson(11.5)\")\n\n\n\n\n\n\n\nBe careful that the Poisson \\(X\\) has no upper limit; the graph is truncated at \\(x = 24\\). Strictly speaking, the graph does not accurately display the \\(Poisson(11.5)\\) distribution. Since the mean is 11.5, we know that it is very unlikely to have a very large \\(x\\), that is \\(P(X = x)\\) is very close to zero for \\(x &gt; 25\\). As the binomial distribution, the number of occurrences is more likely to be around its mean number 11.5, and the chance is decaying as the number is away from the mean.\n\n\nInstead of using binom.pnf() and binom.cdf(), for Poisson distribution, we replace binom with poisson, and use poisson.pmf(k, mu) and poisson.cdf(k, mu) to calculate the probabilities.\nWith lambda being the mean of Poisson distribution, and \\(X\\sim Poisson(\\lambda)\\), we use\n\npoisson.pmf(k, mu = lambda) to compute \\(P(X = k \\mid \\lambda)\\)\npoisson.cdf(k, mu = lambda) to compute \\(P(X \\le k \\mid \\lambda)\\)\npoisson.sf(k, mu = lambda) to compute \\(P(X &gt; k \\mid \\lambda)\\)\n\n\nfrom scipy.stats import poisson\n\n\n\n11.506849315068493\n\n\n0.1128340209466802\n\n\n\n\n0.5990435715682069\n\n\n0.5990435715682069\n\n\nBelow is an example of how to generate the Poisson probability distribution as a graph.\n\nplt.stem(np.arange(0, 25), poisson.pmf(np.arange(0, 25), mu=lam), \n         basefmt=\" \")\nplt.xlabel(\"x\")\nplt.ylabel(\"P(X = x)\")\nplt.title(\"Poisson(11.5)\")\nplt.show()\n\n\n\n\n\n\n\nBe careful that the Poisson \\(X\\) has no upper limit; the graph is truncated at \\(x = 24\\). Strictly speaking, the graph does not accurately display the \\(Poisson(11.5)\\) distribution. Since the mean is 11.5, we know that it is very unlikely to have a very large \\(x\\), that is \\(P(X = x)\\) is very close to zero for \\(x &gt; 25\\). As the binomial distribution, the number of occurrences is more likely to be around its mean number 11.5, and the chance is decaying as the number is away from the mean.",
    "crumbs": [
      "Probability",
      "<span class='chapter-number'>10</span>¬† <span class='chapter-title'>Observable JS Poisson Distribution</span>"
    ]
  }
]