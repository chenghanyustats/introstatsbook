[
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "Introduction to Statistics",
    "section": "Welcome",
    "text": "Welcome\nThis is the website for my introductory statistics book. This book serves as a main reference book for my MATH 4720 Statistical Methods and MATH 4740 Biostatistical Methods at Marquette University.1 Some topics can also be discussed in an introductory data science course. Youâ€™ll learn basic probability and statistical concepts as well as data analysis techniques such as linear regression using R computing software.2 The book balances the following aspects of statistics:\n\nmathematical derivation and statistical computation\ndistribution-based and simulation-based inferences\nmethodology and applications\nclassical/frequentist and Bayesian approaches\n\nThe course materials are borrowed from several existing statistics books and notes, especially from the following books:\n\nOpenIntro Statsitics (data focused)\nIntroduction to Modern Statistics (computation focused)\nAn Introduction to Statistical Methods & Data Analysis, 7th edition (mathematics focused)3"
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Introduction to Statistics",
    "section": "License",
    "text": "License\nThis website is (and will always be) free to use, and is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 4.0 License. If youâ€™d like to give back, please consider reporting a typo or leaving a pull request at github.com/chenghanyustats/introstatsbook."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Statistics and Data",
    "section": "",
    "text": "What is Statistics? What is Data Science? What are data?"
  },
  {
    "objectID": "intro-stats.html#what-is-statistics",
    "href": "intro-stats.html#what-is-statistics",
    "title": "1Â  Science of Data and Data Science",
    "section": "\n1.1 What is Statistics?",
    "text": "1.1 What is Statistics?\n\nStatistics can be defined in a variety of ways, and there doesnâ€™t seem to be one definition that describes it best.\nFor our purposes, statistics can be generally divided into two overarching categories.\n\nStatistics as a set of numeric records\nStatistics as a discipline\n\n\n\n Statistics as a Set of Numeric Records \n\nIn ordinary conversations, the word statistics is used as a term to indicate a set or collection of numeric records.\nFor example, FigureÂ 1.1 below shows Michael Jordanâ€™s career statistics from his time in the NBA.\n\n\n\n\n\nFigureÂ 1.1: Example of statistics as a set of numeric records (Source: https://www.nba.com/stats/player/893/career)\n\n\n\n\n\nHowever, this is just one way of defining statistics.\n\n\n Statistics as a Discipline \n\nAs previously stated, other definitions of statistics exist including the one shown in FigureÂ 1.2.\n\n\n\n\n\nFigureÂ 1.2: Statistics Shirt (Source: shorturl.at/vEMNS)\n\n\n\n\n\nThis definition emphasizes the idea that with the same data, different statistical methods may produce different results and lead to different conclusions.\nWiki lists a more formal definition of statistics in FigureÂ 1.3 below.\n\n\n\n\n\nFigureÂ 1.3: More formal definition of statistics (Source:https://en.wikipedia.org/wiki/Statistics)\n\n\n\n\n\n\nStatistics can also be described as a Science of Data that uses statistical thinking, methods and models.\n\n\nðŸ¤” But wait, if statistics is a science of data, then what is DATA SCIENCEâ“"
  },
  {
    "objectID": "intro-stats.html#difference-between-statistics-and-data-science",
    "href": "intro-stats.html#difference-between-statistics-and-data-science",
    "title": "1Â  Science of Data and Data Science",
    "section": "\n1.2 Difference between Statistics and Data Science",
    "text": "1.2 Difference between Statistics and Data Science\n\n Data Science \n\nBecause of their shared attributes, many find it hard to differentiate between statistics and data science.\nThe tweets below poke fun at the lack of clarity surrounding the definition of data science/data scientists (FigureÂ 1.4).\n\n\n\n\n\nFigureÂ 1.4: Tweets about what Data Science is\n\n\n\nA more formal definition of data science can be found on Investopedia.\nThis site defines Data Science as a field of applied mathematics and statistics that provides useful information based on large amounts of complex data or big data.\nAlthough this definition is helpful for understanding data science, Dan Ariely, a famous behavioral economist at Duke, joked about their use of the term big data in his tweet below (FigureÂ 1.5).\n\n\n\nFigureÂ 1.5: Professor Ariely on Big Data\n\n\n\nMore information can be gathered about the differences between these two fields from looking at the courses offered in the Statistics Department at UC Santa Cruz.\nFrom FigureÂ 1.6 below, one can see that statistics primarily focuses on data analysis, methods and models.\n\n\n\n\n\nFigureÂ 1.6: Courses offered by the Department of Statistics at UC Santa Cruz (Source: https://courses.soe.ucsc.edu/)\n\n\n\n\n\nThis statistics department, in particular, doesnâ€™t talk a lot about data collection, organization, data presentation or data visualization.\nAlthough statistics does not focus on these concepts, they are encompassed within the field of data science.\nThe data science process includes the collection, organization, analysis, interpretation and presentation of data (FigureÂ 1.7).\n\n\n\n\n\nFigureÂ 1.7: The data science process created at Harvard by Joe Blitzstein and Hanspeter Pfister\n\n\n\n\n\nIn typical statistics departments, there isnâ€™t much instruction or research done on data collection, cleaning, storage, database management, and data visualization.\nBecause statistics continues to focus on data analysis and modeling, Data Science now addresses these other processes that statistics passes over."
  },
  {
    "objectID": "intro-stats.html#what-will-we-learn-in-this-course",
    "href": "intro-stats.html#what-will-we-learn-in-this-course",
    "title": "1Â  Science of Data and Data Science",
    "section": "\n1.3 What Will We Learn In this Course?",
    "text": "1.3 What Will We Learn In this Course?\n\nBelow, the main topics of this course, as well as brief descriptions, are listed in the order in which they will be covered. .\n\n\n\n\n\n\n\n\n\n\nWe will spend most of our time talking about probability and the statistical inference methods that are circled on the list above.\nThis course will also include a focus on the statistical methods for analyzing data.\nIn summary, we will learn useful information\n\nabout the population we are interested in\nfrom our sample data\n\nthrough statistical inferential methods, including estimation and testing\n\n\n\n\n\n\n\n\nFigureÂ 1.8: Illustration of obtaining sample data from a population"
  },
  {
    "objectID": "intro-data.html#data",
    "href": "intro-data.html#data",
    "title": "2Â  Data",
    "section": "\n2.1 Data",
    "text": "2.1 Data\n What is Data? \n\nBecause statistics is a science of data, we first need to understand what data is.\n\nData can be described as a set of objects on which we observe or measure one or more characteristics.\n\n\nObjects are individuals, observations, subjects or cases in statistical studies.\nA characteristic or attribute is also called a variable because it varies from one object to another.\n\n\nWe usually store a data set in a matrix form that has rows and columns.\n\nEach row corresponds to a unique case or observational unit.\nEach column represents a characteristic or variable.\n\n\nThis structure allows new cases to be added as rows or new variables to be added as columns.\n\n\n Example \n\n\nFigureÂ 2.1 below is a data set of Marquette basketball players stored in matrix form.\nThe objects are individuals or players in the data and each have their own associated row.\nEach player has several characteristics or attributes shown in the columns associated with them.\n\nThese include jersey number, class, position, height, weight, hometown and high school.\nThese characteristics can also be referred to as variables because they vary from one player to another.\n\n\n\n\n\n\n\nFigureÂ 2.1: Data set of 2019 Marquette menâ€™s basketball players"
  },
  {
    "objectID": "intro-data.html#population-and-sample",
    "href": "intro-data.html#population-and-sample",
    "title": "2Â  Data",
    "section": "\n2.2 Population and Sample",
    "text": "2.2 Population and Sample\n Target Population \n\nThe first step in conducting a study is to identify questions to be investigated.\nA clear research question is helpful in identifying\n\nwhat cases should be studied (row)\nwhat variables are important (column)\n\n\nOnce the research question is determined, it is important to identify the target population to be investigated.\nThe target population is the complete collection of data weâ€™d like to make inference about.\n\n GPA Example \n\n\n\n\nResearch Question: What is the average GPA of currently enrolled Marquette undergraduate students?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Target Population: All Marquette undergraduate students that are currently enrolled.\nBecause all Marquette undergrads that are currently enrolled are the complete collection of data weâ€™d like to make inference about, each currently enrolled Marquette undergrad is an object.\nAverage GPA is the variable or population property we would like to make an inference about.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nStudents who are not currently enrolled or students that have already graduated are not our interest, so they shouldnâ€™t be a part of target population.\n\n\n\n Heart Disease Example \n\n\n\nDoes a new drug reduce mortality in patients with severe heart disease?\n\n\n\n\n\n\n\n\n\n\n\n Target Population: All people with severe heart disease. \nMortality is the variable or population property we would like to make an inference about.\n\n\n\n\n Sample Data \n\nIn some cases itâ€™s possible to collect data of all the cases we are interested in.\nHowever, most of the time it is either expensive or too time consuming to collect data for every case in a population.\nWhat if we tried to collect data on the average GPA of all students in Illinois? The U.S.? The world? ðŸ˜± ðŸ˜± ðŸ˜±\n\n\n\n\nThe solution to this problem is sampling.\nA sample is a subset of cases selected from a population.\nWe are not able to collect the average GPA of every member of the population, but we can collect a sample from that population which has fewer objects (FigureÂ 13.1).\nWe can then compute the average GPA of the sample data.\n\n\n\n\n\n\n\n\nFigureÂ 2.2: Sampling from the population reduces the number of objects from which to collect data.\n\n\n\n\n\n\n\nOur hope is that the average GPA of the sample is close to the average GPA of the population, which is our main interest.\nFor the sampleâ€™s average GPA to be close to populationâ€™s average GPA, we want the sample to look like the population such that they share similar attributes including GPA.\n\n\n Good Sample vs.Â Bad Sample \n\n\n\n\n\n\nIs this 4720/5720 class a sample of the target population Marquette students?\n\n\n\n\n\n\n\n\n\n\nFigureÂ 2.3: Majors of students in this 4740/5740 class\n\n\n\n\n\n\n\n\n\n\nIs this 4720/5720 class a â€œgoodâ€ sample of the target population?\n\n\n\n\n\n\n\nThe sample is convenient to be collected, but as FigureÂ 2.3 shows, it is NOT representative of the population.\nBecause this class is primarily composed of STEM majors, it may not share the attributes necessary with the target population for the two to share a similar average GPA.\nTherefore, we call this a biased sample.\n\nThe average GPA of the class may differ greatly from the average GPA of all MU undergrads.\n\n\n\n\n\n\n\nFigureÂ 2.4: Sampling from a class of mostly STEM students is not representative of the entire population.\n\n\n\n\n\n\n\nAs shown in FigureÂ 2.5, the average GPA differs based on studentsâ€™ majors.\nBecause this class consists of mostly STEM majors, it is likely that the average GPA of its students is not the same as the average GPA of all MU undergraduates.\n\nFigureÂ 2.4 depicts that sampling needs to be done appropriately to ensure the sample is representative of the population.\n\n\n\n\n\n\n\n\nFigureÂ 2.5: UC Berkeley average GPAs by major\n\n\n\n\n\n\n\n How do we collect and why do we need a representative sample? \n\nWe always seek to randomly select a sample from a population.\nRandom sampling usually give us a representative sample, as long as the sample size, or the number of objects in the sample, is not too small.\nIt is important to collect samples this way, because many statistical methods are based on the randomness assumption."
  },
  {
    "objectID": "intro-data.html#data-collection",
    "href": "intro-data.html#data-collection",
    "title": "2Â  Data",
    "section": "\n2.3 Data Collection",
    "text": "2.3 Data Collection\n Two Types of Studies to Collect Sample Data \n\nThere are two types of studies that are used to collect data: observational studies and experimental studies.\nAn observational study is a study in which those collecting the data observe and measure characteristics/variables, but do NOT attempt to modify or intervene with the subjects being studied.\n\n Example: Sample from 1ï¸âƒ£ the heart disease and 2ï¸âƒ£ heart disease-free populations and record the fat content of the diets for the two groups. \n\n\nIn an experimental study, some treatment(s) is applied and then those collecting data proceed to observe its responses or effects on the individuals (experimental units).\n\n Example: Assign volunteers to one of several diets with different levels of dietary fat (treatments) and compare the treatments with respect to the incidence of heart disease after a period of time. \n\n\n\n\n\n\n\n\n\nObservational or Experimental?\n\n\n\n\nRandomly select 40 males and 40 females to see the difference in blood pressure levels between male and female.\nTest the effects of a new drug by randomly dividing patients into 3 groups (high dosage, low dosage, placebo).\n\n\n\n Limitation of Observational Studies: Confounding Variables \n\nA confounder is a variable NOT included in a study that affects the variables in the study.\nFor example, a person observes past data that shows that increases in ice cream sales are associated with increases in drownings and concludes that eating ice cream causes drownings. ðŸ˜±ðŸ˜•â‰ï¸\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is the confounder that is not in the data but affects ice cream sales and the number of drownings?\n\n\n\n\nTemperature\n\n\n\n\nAs temperature increases, ice cream sales increase and the number of drownings also rises because more people go swimming (FigureÂ 2.6).\n\n Causal Relationships \n\nMaking causal conclusions based on experimental data is often more reasonable than making the same causal conclusions based on observational data.\nObservational studies are generally only sufficient to show associations, not causality.\n\n\n\n\n\nFigureÂ 2.6: Temperature acting as a confounder\n\n\n\n\n\n Types of Random Samples \n\nAs previously mentioned, many statistical methods are based on the randomness assumption.\nTherefore, itâ€™s important to understand what a random sample is and how to collect it.\nIn a random sample, each member of a population is equally likely to be selected.\n\n Simple Random Sample \n\nFor a simple random sample (SRS), every possible sample of sample size \\(n\\) has the same chance to be chosen.\n\nExample: If I were to sample 100 students from all 10,000 Marquette students, I would randomly assign each student a number (from 1 to 10,000) and then randomly select 100 numbers.\n\n\n\n\n\n\n\nFigureÂ 2.7: Simple Random Sample\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 2.8: Simple random sample from a population of 15 (https://research-methodology.net/sampling-in-primary-data-collection/random-sampling/)\n\n\n\n\n\n\n Stratified Random Sample \n\nFor stratified sampling, subdivide the population into different subgroups (strata) that share the same characteristics, then draw a simple random sample from each subgroup.\n\nHomogeneous within strata; Non-homogeneous between strata (FigureÂ 2.9)\n\n\n\n\n\nFigureÂ 2.9: Stratified Sampling\n\n\n\n\n\n\nExample: Divide Marquette students into groups by colleges, then perform a SRS for each group (FigureÂ 2.10).\n\n\n\n\n\nFigureÂ 2.10: Stratified sampling of Marquette Students\n\n\n\n\n Cluster Sampling \n\nFor cluster sampling, divide the population into clusters, then randomly select some of those clusters, and then choose all the members from those selected clusters.\n\nHomogeneous between clusters; Non-homogeneous within clusters (FigureÂ 2.11)\n\n\n\n\n\nFigureÂ 2.11: Cluster Sampling\n\n\n\n\n\n\nExample: Study 4720 studentsâ€™ drinking habits by dividing the students into 9 groups, and then randomly selecting 3 and interviewing all of the students in each of those clusters (FigureÂ 2.12).\n\n\n\n\n\nFigureÂ 2.12: Cluster sampling of Marquette students"
  },
  {
    "objectID": "intro-data.html#data-type",
    "href": "intro-data.html#data-type",
    "title": "2Â  Data",
    "section": "\n2.4 Data Type",
    "text": "2.4 Data Type\n\n\n\n\nFigureÂ 2.13: Types of Data\n\n\n\n\n Categorical vs.Â Numerical Variables \n\nA categorical variable provides non-numerical information which can be placed in one (and only one) category from two or more categories.\n\nGender (Male ðŸ‘¨, Female ðŸ‘©, Trans ðŸ³ï¸â€ðŸŒˆ) \nClass (Freshman, Sophomore, Junior, Senior, Graduate) \nCountry (USA ðŸ‡ºðŸ‡¸, Canada ðŸ‡¨ðŸ‡¦, UK ðŸ‡¬ðŸ‡§, Germany ðŸ‡©ðŸ‡ª, Japan ðŸ‡¯ðŸ‡µ, Korea ðŸ‡°ðŸ‡·) \n\n\nA numerical variable is recorded in a numerical value representing counts or measurements.\n\n GPA \n The number of relationships youâ€™ve had \n Height \n\n\n\n Numerical Variables \n\nNumerical variables can be discrete or continuous.\nA discrete variable takes on values of a finite or countable number.\nA continuous variable takes on values anywhere over a particular range without gaps or jumps.\n\n GPA is continuous because it can be any value between 0 and 4. \n The number of relationships youâ€™ve had is discrete because you can count the number and it is finite.\n Height is continuous because it can be any number within a range. \n\n\n\n Categorical Variables \n\nCategorical variables are usually recorded as numbers.\nGender (Male = 0, Female = 1, Trans = 2) \nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5) \nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301) \nUnited Airlines boarding groups\n\nThe numbers represent categories only; differences between them are meaningless.\n\nCanada - USA = 101 - 100 = 1?\nGraduate - Sophomore = 5 - 2 = 3 = Junior?\n\n\nWe need to learn the level of measurements to know which arithmetic operations are meaningful.\n\n\n Levels of Measurements \n Nominal and Ordinal for Categorical Variables \n\n\nNominal: The data can NOT be ordered in a meaningful or natural way.\n\n\nGender (Male = 0, Female = 1, Trans = 2)  is nominal because Male, Female and Trans cannot be ordered.\n\nCountry (USA = 100, Canada = 101, UK = 200, Germany = 201, Japan = 300, Korea = 301)  is nominal.\n\n\n\nOrdinal: The data can be arranged in some meaningful order, but differences between data values can NOT be determined or are meaningless.\n\n\nClass (Freshman = 1, Sophomore = 2, Junior = 3, Senior = 4, Graduate = 5)  is ordinal because Sophomore is one class higher than Freshman.\n\n\n\n Interval and Ratio for Numerical Variables \n\n\nInterval: The data have meaningful differences between any two values but the data do NOT have a natural zero or starting point. The data can do \\(\\color{red} +\\) and \\(\\color{red} -\\), but canâ€™t reasonably do \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\n\nTemperature is interval because \\(80^{\\circ}\\)F is 40 degrees higher than \\(40^{\\circ}\\)F \\((80-40=40)\\), but \\(0^{\\circ}\\) does not mean NO heat and \\(80^{\\circ}\\)F is NOT twice as hot as \\(40^{\\circ}\\)F.\n\n\n\nRatio: The data have both meaningful differences and ratios, and there is a natural zero starting point that indicates none of the quantity. The data can do \\(\\color{red} +\\), \\(\\color{red} -\\), \\(\\color{red} \\times\\) and \\(\\color{red} \\div\\).\n\n\nDistance is ratio because \\(80\\) miles is twice as far as \\(40\\) miles \\((80/40 = 2)\\), and \\(0\\) mile means no distance.\n\n\n\n\n Converting Numerical to Categorical \n\nYouâ€™ve already seen an example of this with the class grading scale (FigureÂ 2.14).\n\n\n\n\n\n\nGrade\n\n\nPercentage\n\n\n\n\n\nA\n\n\n[94, 100]\n\n\n\n\nA-\n\n\n[90, 94)\n\n\n\n\nB+\n\n\n[87, 90)\n\n\n\n\nB\n\n\n[83, 87)\n\n\n\n\nB-\n\n\n[80, 83)\n\n\n\n\nC+\n\n\n[77, 80)\n\n\n\n\nC\n\n\n[73, 77)\n\n\n\n\nC-\n\n\n[70, 73)\n\n\n\n\nD+\n\n\n[65, 70)\n\n\n\n\nD\n\n\n[60, 65)\n\n\n\n\nF\n\n\n[0, 60)\n\n\n\n\nFigureÂ 2.14: Grading scale for this class\n\n\n\n\n Practice \n\n\n\n\n\n\nYour turn!\n\n\n\nIdentify the data type of each variable in the Marquette menâ€™s basketball player data\n\n\n\n\n\n\nFigureÂ 2.15: 2019 Marquette menâ€™s basketball player data set"
  },
  {
    "objectID": "intro-data.html#exercises",
    "href": "intro-data.html#exercises",
    "title": "2Â  Data",
    "section": "\n2.5 Exercises",
    "text": "2.5 Exercises\n\n\nData Type: Identify each of the following as numerical or categorical data.\n\nThe names of the companies that manufacture paper towels\nThe colors of cars\nThe heights of football players\n\n\n\nLevel of Measurements: Identify the level of measurement used in each of the following.\n\nThe weights of people in a sample of people living in Milwaukee.\nA physicianâ€™s descriptions of â€œabstains from smoking, light smoker, moderate smoker, heavy smoker.â€\nFlower classifications of â€œrose, tulip, daisy.â€\nSuzy measures time in days, with 0 corresponding to her birth date. The day before her birth is -1, the day after her birth is +1, and so on. Suzy has converted the dates of major historical events to her numbering system. What is the level of measurement of these numbers?\n\n\n\nDiscrete vs Continuous: Determine whether the data are discrete or continuous.\n\nThe length of stay (in days) for each COVID patient in Wisconsin.\nSeveral subjects are randomly selected and their heights are recorded.\nFrom a data set, we see that a male had an arm circumference of 31.28 cm.\nA sample of married couples is randomly selected and the number of animals in each family is recorded.\n\n\n\nSampling Method: Identify which of these types of sampling is used: random, stratified, or cluster.\n\nDr.Â Yu surveys his statistics class by identifying groups of males and females, then randomly selecting 7 students from each of those two groups.\nDr.Â Yu conducts a survey by randomly selecting 5 different sports teams at Marquette and surveying all of the student-athletes on those teams.\n427 subjects were randomly assigned to (1) meditation or (2) no mediation group to study the effectiveness of this mindfulness activity on lowering blood pressure.\n\n\n\nStudy Type: Determine whether the study is an experiment or an observational study, then identify a major problem with this study.\n\nIn a survey conducted by USA Today, 998 Internet users chose to respond to the question:â€œHow often do you seek medical advice online?â€ 42% of the respondents said â€œfrequently.â€\nThe Physiciansâ€™ Health Study involved 21,045 female physicians. Based on random selections, 11,224 of them were treated with aspirin and other other 9,821 were given placebos. The study was stopped early because it became clear that aspirin did not reduce the risk of myocardial infarctions by a substantial amount."
  },
  {
    "objectID": "intro-r.html#lets-get-equipped-with-our-tools",
    "href": "intro-r.html#lets-get-equipped-with-our-tools",
    "title": "3Â  Tool foR Data",
    "section": "\n3.1 Letâ€™s get equipped with our tools!",
    "text": "3.1 Letâ€™s get equipped with our tools!\n Integrated Development Environment \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nR and Python are programming languages.\n\nPosit Cloud offers two integrated development environments (IDE).\n\nRStudio\nJupyterLab/Jupyter Notebook\n\n\nThese IDEs are software for efficiently writing computer programs.\n\n R and Posit \n\n\n\n\n\n\n\n\n\n\n\n\nR: free open-source programming language ðŸ“ˆ\nR is mainly for doing data science with strength in statistical modeling, computing and data visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\nPosit: interface for R, Python, etc. called an IDE (integrated development environment), e.g.Â â€œI write R code in the RStudio IDEâ€.\nPosit is not a requirement for programming with R, but itâ€™s commonly used by R developers, statisticians and data scientists.\n\n\n\n\n The R User Interface \n\nRStudio IDE includes\n\na viewable environment, a file browser, data viewer and a plotting pane. ðŸ‘\nalso features integrated help, syntax highlighting, context-aware tab completion and more! ðŸ˜„\n\n\n\n\n\n\nR\n\n\n\n\n\nFigureÂ 3.1: R Console\n\n\n\n\n\n\nRStudio\n\n\n\n\n\nFigureÂ 3.2: RStudio Console\n\n\n\n\n\n\n\n â˜ï¸ Posit Cloud - Statistics w/o hardware hassles \n\nðŸ˜Ž You can implement R/Python programs without installing R/Python and the IDE on your laptop!\nðŸ˜Ž Posit Cloud lets you do, share and learn data science online for free!\n\n\n\nðŸ˜ž Getting everything ready locally: Lots of friction\n\nDownload and install R/Python\nDownload and install IDE\nInstall wanted R/Python packages:\n\ntidymodels\ntidyverse\nNumPy\nâ€¦\n\n\nLoad these packages\nDownload and install tools like Git\n\n\nðŸ¤“ Posit Cloud: Much less friction\n\n\n\n\n\n\n\n\n\nGo to https://posit.cloud/\nLog in\n\n\n>hello R!\n\n\n\n\n Install Posit Cloud \n\n\n\n\n\n\nLab Time!\n\n\n\n\n\nStep 1: In the Posit website https://posit.co/, choose Products > Posit Cloud as shown below.\n\n\n\n\n\n\n\nFigureÂ 3.3: Posit website\n\n\n\n\n\n\n\n\n\n\nLab Time!\n\n\n\n\n\nStep 2: Click GET STARTED.\n\nStep 3: Free1 > Sign Up. Please sign up with GitHub if you have one or use your Marquette email address.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n New Projects \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo create a new project, click New Project in the top right corner as shown above.\n\n\n Workspaces \n\nWhen you create an account on Posit Cloud, you get a workspace of your own.\nYou can add a new workspace (click + New Space in sidebar) and control its permissions.\n\n\n\n\n\n\n\n\n\n\n First R Code in Posit Cloud! \n\n\n\n\n\n\nLab Time!\n\n\n\n\nIn the bar, click the desired workspace.\nClick New Project > New RStudio Project to get into the IDE.\nClick Untitled Project and give your project a nice name, math-4720 for example.\nIn the Console pane, write your first R code: a string \"Hello WoRld!\" or math 2 + 4.\nChange the editor theme: Tools > Global Options > Appearance\n\n\n\n\n\n\n\n\nFigureÂ 3.4: How to change the editor theme\n\n\n\n\n\n More Tips \n\nFor more help, read the Posit Cloud guide.\n\n\n\n\n\nFigureÂ 3.5: Posit Cloud Guide"
  },
  {
    "objectID": "intro-r.html#working-in-posit-cloud",
    "href": "intro-r.html#working-in-posit-cloud",
    "title": "3Â  Tool foR Data",
    "section": "\n3.2 Working in Posit Cloud",
    "text": "3.2 Working in Posit Cloud\n Panes \n\n\n\n\nFigureÂ 3.6: RStudio Panes\n\n\n\n\n\n Source Pane \n R Script \n\nAn R script is a .R file that contains R code.\nTo create an R script, go to File > New > R Script, or click the green-plus icon on the top left corner and select R Script.\n\n\n\n\n\nFigureÂ 3.7: Creating an R script\n\n\n\n\n Python Script \n\nA Python script is a .py file that contains Python code.\nTo create a Python script, go to File > New > Python Script, or click the green-plus icon on the topleft corner and select Python Script.\n\n\n\n\n\nFigureÂ 3.8: Creating a Python script\n\n\n\n\n Run Code \n\n\n Run : run the current line or selection of code.\n\n\nctrl + enter (Win) or cmd + enter (Mac)\n\n\n\n Icon to the right of Run : re-run the previous code.\n\n\nalt + ctrl + p (Win) or option + cmd + p (Mac)\n\n\n\n Source : run all the code in the R script.\n\n\nshift + ctrl + s (Win) or shift + cmd + s (Mac)\n\n\n\n Source with Echo : run all the code in the R script with the code printed in the console.\n\n\nshift + ctrl + enter (Win) or shift + cmd + enter (Mac)\n\n\n\n\n\n\n\nFigureÂ 3.9: Running R Code\n\n\n\n\n Run Python Code \n\nRunning Python code may require you to update some packages. Please say YES!\n\nWhen you run the Python code in the R console, the console will switch from R to Python.\nType quit in the Python console to switch back to the R console.\n\n\n\n\n\n\n\n\n\n\n Environment Tab \n\nThe (global) environment is where we are currently working.\nAnything created or imported into the current R/Python session is stored in our environment and shown in the Environment tab.\nAfter we run the R script from FigureÂ 3.7, the following objects are stored in the environment:\n\nData set mtcars\n\nObject x storing integer values 1 to 10.\nObject y storing three numeric values 3, 5, 9.\n\n\n\n\n\n\n\nFigureÂ 3.10: Environment Pane\n\n\n\n\n\nAfter we run the Python script from FigureÂ 3.8, the following object is stored in the environment:\n\nObject b storing a string Hello World!\n\n\n\n\n\n\n\n\n\n\n\n\n\n History Tab \n\nThe History tab keeps a record of all previous commands.\n\n\nSave icon: save all history to a file\n\nTo Console: send the selected commands to the console.\n\nTo Source : inserted the selected commands into the current script.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn the console pane, use â¬†ï¸ to show the previous commands.\n\n\n\n R Packages ðŸ“¦ \n\nWhen we start an R session, only built-in packages like base, stats, graphics, etc. are available.\nInstalling packages is an easy way to get access to other data and functions.\n\n\n!\n\n\n Installing R Packages \n\n\n\nTo install a package, such as the ggplot2 package, we use the command\n\n\ninstall.packages(\"ggplot2\")\n\n\nA different option in the right-bottom pane is Packages > Install.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Loading R Packages ðŸ“¦ \n\n\n\nTo use any function or data in ggplot2, we write ggplot2:: followed by the name of the function or data.\n\n\nggplot2::ggplot(ggplot2::mpg, \n                ggplot2::aes(\n                    x = displ, \n                    y = hwy, \n                    colour = class)\n                ) + \n    ggplot2::geom_point()\n\n\n\n\n\n\n\nQuestion\n\n\n\nWhat happens when you run the code shown below?\n\nggplot(mpg, aes(x = displ, \n                y = hwy, \n                colour = class)) + \n    geom_point()\n\n\n\n\n\n\n\nWe can load the package into our R session using library().\n\nWith library(ggplot2), R knows the function and data are from the ggplot2 package.\n\n\nlibrary(ggplot2)\nggplot(mpg, aes(x = displ, \n                y = hwy, \n                colour = class)) + \n    geom_point()\n\n\n\n\n Help \n\nWhat if you donâ€™t know how a function works or what a data set is about â“\n\nðŸ‘‰ Simply type ? followed by the data name or function name to get more information.\n\n\n\n\n?mean\n?mtcars\n\n\n\n\n\n\n\nWhat does the function mean() do? What is the size of mpg?\n\n\n\n\n\n\n\nA document will show up in the Help tab, teaching you how to use the function or explaining the data set.\n\nAn example is shown for the mpg data set in FigureÂ 3.11 below.\n\n\n\n\n\n\n\nFigureÂ 3.11: Document explaining the mpg data set in the Help tab\n\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nWhat is the size of mtcars data?\nType mtcars and hit enter in the console to see the data set.\nDiscuss the data type of each variable.\nType mtcars[, 1] and hit enter in the console and discuss what you see."
  },
  {
    "objectID": "intro-r.html#install-r-and-r-studio-locally-to-your-computer",
    "href": "intro-r.html#install-r-and-r-studio-locally-to-your-computer",
    "title": "3Â  Tool foR Data",
    "section": "\n3.3 Install R and R Studio Locally to Your Computer",
    "text": "3.3 Install R and R Studio Locally to Your Computer\n Install R \n Step 1 \n\nGo to https://cloud.r-project.org.\nClick Download R for [your operating system].\n\n\n\nFigureÂ 3.12: Downloading R\n\n\n Step 2 \n\nIf you are a Mac user, you should see the page shown below in FigureÂ 3.13.\nYou are recommended to download and install the latest version of R (now R-4.2.1) if your OS version allows to do so.\nOtherwise, choose a previous version, such as R-3.6.3.\n\n\n\nFigureÂ 3.13: Downloading R for Mac\n\n\n\nIf you are a Windows user, after clicking Download R for Windows, please choose base version and then click Download R-4.2.1 for Windows.\n\n Step 3 \n\nOnce you successfully install R, when you open R, you should be able to see the following R terminal or console:\n\n\n\n\nWindows\n\n\n\nFigureÂ 3.14: Windows R Console\n\n\n\n\n\n\nMac\n\n\n\nFigureÂ 3.15: Mac R Console\n\n\n\n\n Welcome to the R World! \n\nNow you are ready to use R for statistical computation.\nYou can use R like a calculator.\n\nAfter typing your formula, simply hit enter and you get the answer!\n\n\n\n\n1 + 2\n\n[1] 3\n\n30 * 42 / 3\n\n[1] 420\n\nlog(5) - exp(3) * sqrt(7)\n\n[1] -51.5319\n\n\n\n Install RStudio \n Step 1 \n\nIn the RStudio website, please choose Products > RStudio as shown in FigureÂ 3.16.\n\n\n\nFigureÂ 3.16: R Studio Website\n\n\n Step 2 \n\nChoose RStudio Desktop and click DOWNLOAD RSTUDIO DESKTOP for the free version.\n\n\n\n\n\nFigureÂ 3.17: Downloading RStudio Desktop\n\n\n\n\n Step 3 \n\nClick DOWNLOAD RSTUDIO FOR [YOUR SYSTEM] (FigureÂ 3.18).\nFollow the standard installation steps and you should get the software.\nMake sure that R is installed successfully on your computer before you download and install RStudio.\n\n\n\n\n\n\n\nNote\n\n\n\nThe latest version of RStudio is 2022.07.1+554.\n\n\n\n\n\n\nFigureÂ 3.18: Latest version of RStudio\n\n\n\n\n\n RStudio Screen \n\nWhen you open RStudio, you should see something similar to FigureÂ 3.19 below.\nIf you do, congratulations!\nYou can now do any statistical computation in R using RStudio locally on your computer.\n\n\n\n\n\nFigureÂ 3.19: R Studio Screen"
  },
  {
    "objectID": "intro-r.html#operators",
    "href": "intro-r.html#operators",
    "title": "3Â  Tool foR Data",
    "section": "\n3.4 Operators",
    "text": "3.4 Operators\n R is a Calculator \n Arithmetic Operators \n\n\n\n\nFigureÂ 3.20: Table of arithmetic operators\n\n\n\n\n Examples \n\n2 + 3 * 5 + 4\n\n[1] 21\n\n2 + 3 * (5 + 4)\n\n[1] 29\n\n\n\nWe have to do the operation in the parentheses first, as the PEMDAS rule describes in FigureÂ 3.21 below.\n\n\n\n\n\nFigureÂ 3.21: Order of operations\n\n\n\n\n\n R Does Comparisons \n Logical Operators \n\n\n\n\nFigureÂ 3.22: Table of logical operators\n\n\n\n\n Examples \n\n\n\n5 <= 5\n\n[1] TRUE\n\n5 <= 4\n\n[1] FALSE\n\n# Is 5 is NOT equal to 5? FALSE\n5 != 5\n\n[1] FALSE\n\n\n\n\n\n\n## Is TRUE not equal to FALSE?\nTRUE != FALSE\n\n[1] TRUE\n\n## Is not TRUE equal to FALSE?\n!TRUE == FALSE\n\n[1] TRUE\n\n## TRUE if either one is TRUE or both are TRUE\nTRUE | FALSE\n\n[1] TRUE\n\n\n\n\n\n\n\n\n\n\nWhat does TRUE & FALSE return?\n\n\n\n\n\n\n\n Built-in Functions \n\nR has lots of built-in functions, especially for mathematics, probability and statistics.\n\n\n\n\n\nFigureÂ 3.23: R Built-in functions\n\n\n\n\n Examples \n\n\n\nsqrt(144)\n\n[1] 12\n\nexp(1)  ## Euler's number\n\n[1] 2.718282\n\nsin(pi/2)\n\n[1] 1\n\nabs(-7)\n\n[1] 7\n\n\n\n\n\n\nfactorial(5)\n\n[1] 120\n\n## without specifying base value\n## it is a natural log with base e\nlog(100)\n\n[1] 4.60517\n\n## log function and we specify base = 2\nlog(100, base = 10)\n\n[1] 2\n\n\n\n\n\n Commenting \n\n\n\n\n\n\nYouâ€™ve seen comments a lot! How do we write a comment in R?\n\n\n\n\n\n\n\nUse # to add a comment so that the text after # is not read as an R command.\nWriting (good) comments is highly recommended.\nComments help readers, and more importantly yourself, understand what the code is doing.\nThey should explain the why, not the what.\n\n\n\n\n\n\n\nhttps://www.reddit.com/r/ProgrammerHumor /comments/8w54mx/code_comments_be_like/\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Objects and Funtions in R \n\n Everything that exists is an object.  Everything that happens is a function call. \nâ€“ John Chambers, the creator of the S programming language.\n\n\nWe have made lots of things happen!\nEven arithmetic and logical operators are functions!\n\n\n`+`(x = 2, y = 3)\n\n[1] 5\n\n`&`(TRUE, FALSE)\n\n[1] FALSE\n\n\n\n Creating Variables \n\nA variable stores a value that can be changed according to our need.\nUse the <- operator to assign a value to the variable. (Highly recommendedðŸ‘)\n\n\nx <- 5  ## we create an object, value 5, and call it x, which is a variable.\nx  ## type the variable name to see the value stored in the object x\n\n[1] 5\n\n\n\n\n(x <- x + 6)  # We can reassign any value to the variable we created\n\n[1] 11\n\nx == 5  # We can perform any operations on variables\n\n[1] FALSE\n\nlog(x) # Variables can also be used in any built-in functions\n\n[1] 2.397895\n\n\n\n\n\n\n\n\n\n\n\n Bad Naming \n\nâŒ Unless you have a very good reason, donâ€™t create a variable whose name is the same as any R built-in constant or function!\nðŸ˜Ÿ It causes lots of confusion when your code is long and when others read it.\n\n\n## THIS IS BAD CODING! DON'T DO THIS!\npi  ## pi is a built-in constant\n\n[1] 3.141593\n\n(pi <- 20)\n\n[1] 20\n\nabs ## abs is a built-in function\n\nfunction (x)  .Primitive(\"abs\")\n\n(abs <- abs(pi))\n\n[1] 20"
  },
  {
    "objectID": "intro-r.html#object-types",
    "href": "intro-r.html#object-types",
    "title": "3Â  Tool foR Data",
    "section": "\n3.5 Object Types",
    "text": "3.5 Object Types\n Types of Variables \n\nUse typeof() to check which type a variable belongs to.\nCommon types include character, double, integer and logical.\nTo check if itâ€™s of a specific type, use is.character(), is.double(), is.integer(), is.logical().\n\n\n\n\ntypeof(5)\n\n[1] \"double\"\n\ntypeof(5L)\n\n[1] \"integer\"\n\ntypeof(\"I_love_stats!\")\n\n[1] \"character\"\n\n\n\n\n\n\ntypeof(1 > 3)\n\n[1] \"logical\"\n\nis.double(5L)\n\n[1] FALSE\n\n\n\n\n Variable Types in R and in Statistics \n\nType character and logical correspond to categorical variables.\n\nType logical is a special type of categorical variables that has only two categories (binary).\n\nWe usually call it a binary variable.\n\n\n\n\nType double and integer correspond to numerical variables. (an exception later)\n\nType double is for continuous variables\nType integer is for discrete variables."
  },
  {
    "objectID": "intro-r.html#r-data-structures",
    "href": "intro-r.html#r-data-structures",
    "title": "3Â  Tool foR Data",
    "section": "\n3.6 R Data Structures",
    "text": "3.6 R Data Structures\n (Atomic) Vector \n\nTo create a vector, use c(), which is short for concatenate or combine.\n\nAll elements of a vector must be of the same type.\n\n\n\n\n(dbl_vec <- c(1, 2.5, 4.5)) \n\n[1] 1.0 2.5 4.5\n\n(int_vec <- c(1L, 6L, 10L))\n\n[1]  1  6 10\n\n## TRUE and FALSE can be written as T and F\n(log_vec <- c(TRUE, FALSE, F))  \n\n[1]  TRUE FALSE FALSE\n\n(chr_vec <- c(\"pretty\", \"girl\"))\n\n[1] \"pretty\" \"girl\"  \n\n\n\n\n\n\n## check how many elements in a vector\nlength(dbl_vec) \n\n[1] 3\n\n## check a compact description of \n## any R data structure\nstr(dbl_vec) \n\n num [1:3] 1 2.5 4.5\n\n\n\n\n Operations on Vectors \n\nWe can do the same operations on vectors that we do on a scalar variable (vector of length 1).\n\n\n\n\n# Create two vectors\nv1 <- c(3, 8)\nv2 <- c(4, 100) \n\n## All operations happen element-wisely\n# Vector addition\nv1 + v2\n\n[1]   7 108\n\n# Vector subtraction\nv1 - v2\n\n[1]  -1 -92\n\n\n\n\n\n\n# Vector multiplication\nv1 * v2\n\n[1]  12 800\n\n# Vector division\nv1 / v2\n\n[1] 0.75 0.08\n\nsqrt(v2)\n\n[1]  2 10\n\n\n\n\n Recycling of Vectors \n\nIf we apply arithmetic operations to two vectors of unequal length, the elements of the shorter vector will be recycled to complete the operations.  \n\n\n\nv1 <- c(3, 8, 4, 5)\n# The following 2 operations are the same\nv1 * 2\n\n[1]  6 16  8 10\n\nv1 * c(2, 2, 2, 2)\n\n[1]  6 16  8 10\n\nv3 <- c(4, 11)\nv1 + v3  ## v3 becomes c(4, 11, 4, 11) when doing the operation\n\n[1]  7 19  8 16\n\n\n Subsetting Vectors \n\nTo extract element(s) in a vector, use a pair of brackets [] with element indexing.\nThe indexing starts with 1.\n\n\n\n\nv1\n\n[1] 3 8 4 5\n\nv2\n\n[1]   4 100\n\n## The first element\nv1[1] \n\n[1] 3\n\n## The second element\nv2[2]  \n\n[1] 100\n\n\n\n\n\n\nv1[c(1, 3)]\n\n[1] 3 4\n\n## extract all except a few elements\n## put a negative sign before the vector of \n## indices\nv1[-c(2, 3)] \n\n[1] 3 5\n\n\n\n\n\n Factor \n\nA vector of type factor can be ordered in a meaningful way.\n\nCreate a factor by factor().\nIt is a type of integer, not character. ðŸ˜² ðŸ™„\n\n\nfac <- factor(c(\"med\", \"high\", \"low\"))\ntypeof(fac)\n\n[1] \"integer\"\n\nlevels(fac) ## Each level represents an integer, ordered from the vector alphabetically.\n\n[1] \"high\" \"low\"  \"med\" \n\nstr(fac)  ## The integers show the level each element in vector fac belongs to.\n\n Factor w/ 3 levels \"high\",\"low\",\"med\": 3 1 2\n\norder_fac <- factor(c(\"med\", \"high\", \"low\"), levels = c(\"low\", \"med\", \"high\"))\nstr(order_fac)\n\n Factor w/ 3 levels \"low\",\"med\",\"high\": 2 3 1\n\n\n\n List (Generic Vectors) \n\nLists are different from vectors.\n\nElements can be of any type, including lists.\n\n\nConstruct a list by using list() instead of c().\n\n\n\n\n## a list of 3 elements of different types\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\nx_lst\n\n$idx\n[1] 1 2 3\n\n[[2]]\n[1] \"a\"\n\n[[3]]\n[1]  TRUE FALSE\n\n\n\n\n\n\nstr(x_lst)\n\nList of 3\n $ idx: int [1:3] 1 2 3\n $    : chr \"a\"\n $    : logi [1:2] TRUE FALSE\n\nnames(x_lst)\n\n[1] \"idx\" \"\"    \"\"   \n\nlength(x_lst)\n\n[1] 3\n\n\n\n\n Subsetting a List \n\nx_lst <- list(idx = 1:3, \n              \"a\", \n              c(TRUE, FALSE))\n\n\n\n\nReturn an  element  of a list\n\n\n## subset by name (a vector)\nx_lst$idx  \n\n[1] 1 2 3\n\n## subset by indexing (a vector)\nx_lst[[1]]  \n\n[1] 1 2 3\n\ntypeof(x_lst$idx)\n\n[1] \"integer\"\n\n\n\n\n\n\nReturn a  sub-list  of a list\n\n\n## subset by name (still a list)\nx_lst[\"idx\"]  \n\n$idx\n[1] 1 2 3\n\n## subset by indexing (still a list)\nx_lst[1]  \n\n$idx\n[1] 1 2 3\n\ntypeof(x_lst[\"idx\"])\n\n[1] \"list\"\n\n\n\n\n\n\n\n\n\nFigureÂ 3.24: Condiment analogy for subsetting lists\n\n\n\n\n\n\nIf list x is a train carrying objects, then x[[5]] is the object in car 5; x[4:6] is a train of cars 4-6.\nâ€” @RLangTip, https://twitter.com/RLangTip/status/268375867468681216\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 3.25: Train analogy for subsetting a list\n\n\n\n\n\n Matrix \n\nA matrix is a two-dimensional analog of a vector.\nUse command matrix() to create a matrix.\n\n\n## Create a 3 by 2 matrix called mat\n(mat <- matrix(data = 1:6, nrow = 3, ncol = 2)) \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\ndim(mat); nrow(mat); ncol(mat)\n\n[1] 3 2\n\n\n[1] 3\n\n\n[1] 2\n\n\n Subsetting a Matrix \n\nTo extract a sub-matrix, use the same indexing approach as vectors.\nUse comma , to separate the row and column index.\n\n\nmat[2, 2] extracts the element of the second row and second column.\n\n\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n## all rows and 2nd column\n## leave row index blank\n## specify 2 in coln index\nmat[, 2]\n\n[1] 4 5 6\n\n\n\n\n\n\n## 2nd row and all columns\nmat[2, ] \n\n[1] 2 5\n\n## The 1st and 3rd rows\nmat[c(1, 3), ] \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    3    6\n\n\n\n\n Binding Matrices \n\nWe can generalize c() used in vectors to cbind() (binding matrices by adding columns) and rbind() (binding matrices by adding rows) for matrices.\nWhen matrices are combined by columns, they should have the same number of rows.\nWhen matrices are combined by rows, they should have the same number of columns.\n\n\n\n\nmat\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\nmat_c <- matrix(data = c(7, 0, 0, 8, 2, 6), \n                nrow = 3, ncol = 2)\n## should have the same number of rows\ncbind(mat, mat_c)  \n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7    8\n[2,]    2    5    0    2\n[3,]    3    6    0    6\n\n\n\n\n\n\nmat_r <- matrix(data = 1:4, \n                nrow = 2, \n                ncol = 2)\n## should have the same number of columns\nrbind(mat, mat_r)  \n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n[4,]    1    3\n[5,]    2    4\n\n\n\n\n\n Data Frame: The Most Common Way of Storing Data \n\nA data frame is of type list of equal-length vectors, having a 2-dimensional structure.\nIt is more general than a matrix.\n\nDifferent columns can have different types.\n\n\nTo create a data frame, use data.frame() that takes named vectors as input.\n\n\n\n\n## data frame w/ an dbl column named  \n## and char column named grade.\n(df <- data.frame(age = c(19,21,40), \n                  gender = c(\"m\",\"f\",\"m\")))\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n## a data frame has a list structure\nstr(df)  \n\n'data.frame':   3 obs. of  2 variables:\n $ age   : num  19 21 40\n $ gender: chr  \"m\" \"f\" \"m\"\n\n\n\n\n\n\n## must set column names\n## or they are ugly and non-recognizable\ndata.frame(c(19, 21, 40), c(\"m\",\"f\", \"m\")) \n\n  c.19..21..40. c..m....f....m..\n1            19                m\n2            21                f\n3            40                m\n\n\n\n\n Properties of Data Frames \n\nData frame has properties of matrix and list.\n\n\n\n\nnames(df)  ## df as a list\n\n[1] \"age\"    \"gender\"\n\ncolnames(df)  ## df as a matrix\n\n[1] \"age\"    \"gender\"\n\nlength(df) ## df as a list\n\n[1] 2\n\nncol(df) ## df as a matrix\n\n[1] 2\n\ndim(df) ## df as a matrix\n\n[1] 3 2\n\n\n\n\n\n\n## rbind() and cbind() can be used on df\n\ndf_r <- data.frame(age = 10, \n                   gender = \"f\")\nrbind(df, df_r)\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n4  10      f\n\ndf_c <- \n    data.frame(col = c(\"red\",\"blue\",\"gray\"))\n(df_new <- cbind(df, df_c))\n\n  age gender  col\n1  19      m  red\n2  21      f blue\n3  40      m gray\n\n\n\n\n Subsetting a Data Frame \n\nWhen we subset data frames, we can use either list or matrix subsetting methods.\n\n\n\n\ndf_new\n\n  age gender  col\n1  19      m  red\n2  21      f blue\n3  40      m gray\n\n## Subset rows\ndf_new[c(1, 3), ]\n\n  age gender  col\n1  19      m  red\n3  40      m gray\n\n## select the row where age == 21\ndf_new[df_new$age == 21, ]\n\n  age gender  col\n2  21      f blue\n\n\n\n\n\n\n## Subset columns\n## like a list\ndf_new$age\n\n[1] 19 21 40\n\ndf_new[c(\"age\", \"gender\")]\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n## like a matrix\ndf_new[, c(\"age\", \"gender\")]\n\n  age gender\n1  19      m\n2  21      f\n3  40      m\n\n\n\n\n\n\n\n\n\n\n\nYou can do it!\n\n\n\n\nCreate a vector object called x that has 5 elements 3, 6, 2, 9, 14.\nCompute the average of elements of x.\nSubset the mtcars data set by selecting variables mpg and disp.\nSelect the cars (rows) in mtcars that have 4 cylinders."
  },
  {
    "objectID": "intro-r.html#exercises",
    "href": "intro-r.html#exercises",
    "title": "3Â  Tool foR Data",
    "section": "\n3.7 Exercises",
    "text": "3.7 Exercises\n\n# ==============================================================================\n## Vector\n# ==============================================================================\npoker_vec <- c(170, -20, 50, -140, 210)\nroulette_vec <- c(-30, -40, 70, -340, 20)\ndays_vec <- c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\")\nnames(poker_vec) <- days_vec\nnames(roulette_vec) <- days_vec\n\n\nVector\n\nThe code above shows a Marquette student poker and roulette winnings from Monday to Friday. Copy and paste them into your R and complete problem 1.\n\nAssign to the variable total_daily how much you won or lost on each day in total (poker and roulette combined).\nCalculate the winnings overall total_week. Print it out.\n\n\n\n# ==============================================================================\n## Factor\n# ==============================================================================\n# Create speed_vector\nspeed_vec <- c(\"medium\", \"low\", \"low\", \"medium\", \"high\")\n\n\nFactor\n\n\n\nspeed_vec above should be converted to an ordinal factor since its categories have a natural ordering. Create an ordered factor vector speed_fac by completing the code below. Set ordered to TRUE, and set levels to c(\"low\", \"medium\", \"high\"). Print speed_fac.\n\n\n_________ <- factor(________, ordered = ______, \n                    levels = ______________________)\n\n\n\n# ==============================================================================\n## Data frame\n# ==============================================================================\n# Definition of vectors\nname <- c(\"Mercury\", \"Venus\", \"Earth\", \"Mars\", \"Jupiter\", \"Saturn\", \n          \"Uranus\", \"Neptune\")\ntype <- c(\"Terrestrial planet\", \"Terrestrial planet\", \"Terrestrial planet\", \n          \"Terrestrial planet\", \"Gas giant\", \"Gas giant\", \n          \"Gas giant\", \"Gas giant\")\ndiameter <- c(0.375, 0.947, 1, 0.537, 11.219, 9.349, 4.018, 3.843)\nrotation <- c(57.63, -242.03, 1, 1.05, 0.42, 0.44, -0.73, 0.65)\nrings <- c(FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, TRUE)\n\n\nData Frame\n\nData frames have properties of lists and matrices, so we skip lists and matrices and focus on data frames. You want to construct a data frame that describes the main characteristics of eight planets in our solar system. You feel confident enough to create the necessary vectors: name, type, diameter, rotation and rings that have already been coded up as above. The first element in each of these vectors corresponds to the first observation.\n\nUse the function data.frame() to construct a data frame. Pass the vectors name, type, diameter, rotation and rings as arguments to data.frame(), in this order. Call the resulting data frame planets_df.\n\n\n________ <- data.frame(______, ______, ______, ______, ______)\n\n\nUse str() to investigate the structure of the new planets_df variable. Which are categorical (qualitative) variables and which are numerical (quantitative) variables? For those that are categorical, are they nominal or ordinal? For those numerical variables, are they interval or ratio level? discrete or continuous?\nFrom planets_df, select the diameter of Mercury: this is the value at the first row and the third column. Simply print out the result.\nFrom planets_df, select all data on Mars (the fourth row). Simply print out the result.\nSelect and print out the first 5 values in the diameter column of planets_df.\nUse $ to select the rings variable from planets_df.\nUse (f) to select all columns for planets that have rings."
  },
  {
    "objectID": "data-graphics.html#frequency-table-for-categorical-variable",
    "href": "data-graphics.html#frequency-table-for-categorical-variable",
    "title": "4Â  Data Visualization",
    "section": "\n4.1 Frequency Table for Categorical Variable",
    "text": "4.1 Frequency Table for Categorical Variable\n\nA frequency table (frequency distribution) lists variable values individually for categorical data along with their corresponding number of times occurred in the data (frequencies or counts).\nBelow is an example of a frequency table for categorical data with \\(n\\) being the total number of data values.\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\n\\(C_1\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(C_2\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\nâ€¦\nâ€¦\nâ€¦\n\n\n\\(C_k\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\nHere is another example of a categorical variable color that has three categories.\n\n\n\nCategory name\nFrequency\nRelative Frequency\n\n\n\nRed ðŸ”´\n8\n8/50 = 0.16\n\n\nBlue ðŸ”µ\n26\n26/50 = 0.52\n\n\nBlack âš«\n16\n16/50 = 0.32\n\n\n\n Categorical Frequency Table in R \nloan50\n\nBelow is the loan50 data set from the openintro package in R.\n\n\n# install.packages(\"openintro\")\nlibrary(openintro)\nstr(loan50)\n\ntibble [50 Ã— 18] (S3: tbl_df/tbl/data.frame)\n $ state                  : Factor w/ 51 levels \"\",\"AK\",\"AL\",\"AR\",..: 32 6 41 6 36 16 35 25 11 11 ...\n $ emp_length             : num [1:50] 3 10 NA 0 4 6 2 10 6 3 ...\n $ term                   : num [1:50] 60 36 36 36 60 36 36 36 60 60 ...\n $ homeownership          : Factor w/ 3 levels \"rent\",\"mortgage\",..: 1 1 2 1 2 2 1 2 1 2 ...\n $ annual_income          : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 80000 ...\n $ verified_income        : Factor w/ 4 levels \"\",\"Not Verified\",..: 2 2 4 2 2 3 3 2 2 3 ...\n $ debt_to_income         : num [1:50] 0.558 1.306 1.056 0.574 0.238 ...\n $ total_credit_limit     : int [1:50] 95131 51929 301373 59890 422619 349825 15980 258439 87705 330394 ...\n $ total_credit_utilized  : int [1:50] 32894 78341 79221 43076 60490 72162 2872 28073 23715 32036 ...\n $ num_cc_carrying_balance: int [1:50] 8 2 14 10 2 4 1 3 10 4 ...\n $ loan_purpose           : Factor w/ 14 levels \"\",\"car\",\"credit_card\",..: 4 3 4 3 5 5 4 3 3 4 ...\n $ loan_amount            : int [1:50] 22000 6000 25000 6000 25000 6400 3000 14500 10000 18500 ...\n $ grade                  : Factor w/ 8 levels \"\",\"A\",\"B\",\"C\",..: 3 3 6 3 3 3 5 2 2 4 ...\n $ interest_rate          : num [1:50] 10.9 9.92 26.3 9.92 9.43 ...\n $ public_record_bankrupt : int [1:50] 0 1 0 0 0 0 0 0 0 1 ...\n $ loan_status            : Factor w/ 7 levels \"\",\"Charged Off\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ has_second_income      : logi [1:50] FALSE FALSE FALSE FALSE FALSE FALSE ...\n $ total_income           : num [1:50] 59000 60000 75000 75000 254000 67000 28800 80000 34000 192000 ...\n\n\n\nhomeownership\n\nThe values as well as the frequency table for the variable homeownership from the loan50 data set are shown below.\n\n\n# 50 values (rent, mortgage, own) of categorical homeownership in loan50 data\n(x <- loan50$homeownership)\n\n [1] rent     rent     mortgage rent     mortgage mortgage rent     mortgage\n [9] rent     mortgage rent     mortgage rent     mortgage rent     mortgage\n[17] rent     rent     rent     mortgage mortgage mortgage mortgage rent    \n[25] mortgage rent     mortgage own      mortgage mortgage rent     mortgage\n[33] mortgage rent     rent     own      mortgage rent     mortgage rent    \n[41] mortgage rent     rent     mortgage mortgage mortgage mortgage rent    \n[49] own      mortgage\nLevels: rent mortgage own\n\n## frequency table\ntable(x)\n\nx\n    rent mortgage      own \n      21       26        3 \n\n\n\n\n\n\n\n\n\nIf we want to create a frequency table shown in definition, which R data structure we can use?\n\n\n\n\n\n\n\nfreq <- table(x)\nrel_freq <- freq / sum(freq)\ncbind(freq, rel_freq)\n\n         freq rel_freq\nrent       21     0.42\nmortgage   26     0.52\nown         3     0.06\n\n\n\n Visualizing a Frequency Table \n Bar Chart \n\nBelow is a bar chart that visualizes the homeownership frequency table.\n\n\nbarplot(height = table(x), main = \"Bar Chart\", xlab = \"Homeownership\")\n\n\n\n\n Pie Chart \n\nThe homeownership frequency table can also be visualized using a pie chart.\n\n\npie(x = table(x), main = \"Pie Chart\")"
  },
  {
    "objectID": "data-graphics.html#frequency-distribution-for-numerical-variables",
    "href": "data-graphics.html#frequency-distribution-for-numerical-variables",
    "title": "4Â  Data Visualization",
    "section": "\n4.2 Frequency Distribution for Numerical Variables",
    "text": "4.2 Frequency Distribution for Numerical Variables\n\nTo create a frequency distribution for numerical variables, one must\n\nDivide the data into \\(k\\) non-overlapping groups of intervals (classes).\nConvert the data into \\(k\\) categories with an associated class interval.\nCount the number of measurements falling in a given class interval (class frequency).\n\n\n\n\n\nClass\nClass Interval\nFrequency\nRelative Frequency\n\n\n\n\\(1\\)\n\\([a_1, a_2]\\)\n\\(f_1\\)\n\\(f_1/n\\)\n\n\n\\(2\\)\n\\((a_2, a_3]\\)\n\\(f_2\\)\n\\(f_2/n\\)\n\n\nâ€¦\nâ€¦\nâ€¦\nâ€¦\n\n\n\\(k\\)\n\\((a_k, a_{k+1}]\\)\n\\(f_k\\)\n\\(f_k/n\\)\n\n\n\n\n\n\\((a_2 - a_1) = (a_3 - a_2) = \\cdots = (a_{k+1} - a_k)\\). All class widths are the same!\n\n\n\n\n\n\n\nCan our grade conversion be used for creating a frequency distribution?\n\n\n\n\nNo, because the class widths are not all the same as seen in FigureÂ 4.2.\n\n\n\n\n\n\n\n\nGrade\n\n\nPercentage\n\n\n\n\n\nA\n\n\n[94, 100]\n\n\n\n\nA-\n\n\n[90, 94)\n\n\n\n\nB+\n\n\n[87, 90)\n\n\n\n\nB\n\n\n[83, 87)\n\n\n\n\nB-\n\n\n[80, 83)\n\n\n\n\nC+\n\n\n[77, 80)\n\n\n\n\nC\n\n\n[73, 77)\n\n\n\n\nC-\n\n\n[70, 73)\n\n\n\n\nD+\n\n\n[65, 70)\n\n\n\n\nD\n\n\n[60, 65)\n\n\n\n\nF\n\n\n[0, 60)\n\n\n\n\nFigureÂ 4.2: Grading scale for this class\n\n\n\n\n Interest Rate Data loan50 [OI] \n\nBelow is data for the interest rate variable in the loan 50 data set.\n\n\n(int_rate <- round(loan50$interest_rate, 1))\n\n [1] 10.9  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n[16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n[31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n[46] 10.4 21.4 10.9  9.4  6.1\n\n\n\n\n\n\n\n\n\n\n Frequency Distribution of Interest Rate \n\n\n\n\n Class Class_Intvl Freq Rel_Freq\n     1     5%-7.5%   11     0.22\n     2    7.5%-10%   15     0.30\n     3   10%-12.5%    8     0.16\n     4   12.5%-15%    5     0.10\n     5   15%-17.5%    4     0.08\n     6   17.5%-20%    4     0.08\n     7   20%-22.5%    1     0.02\n     8   22.5%-25%    1     0.02\n     9   25%-27.5%    1     0.02\n\n\n\nrange(int_rate)\n\n[1]  5.3 26.3\n\n\n\n\nAll class widths are the same (2.5%)!\nThe number of classes should not be too big or too small.\nThe lower limit of the 1st class should not be greater than the minimum value of the data.\n\nThe lower limit of the 1st class is 5%, which is less than the minimum value of 5.3%.\n\n\nThe upper limit of the last class should not be smaller than the maximum value of the data.\n\nThe upper limit of the last class is 27.5%, which is greater than the maximum value of 26.3%.\n\n\n\n\n\n\n\n\n\n\n\nHow do we choose the number of classes or the class width?\n\n\n\nR decides the number of classes for us when we visualize the frequency distribution by a histogram.\n\n\n\n Visualizing Frequency Distribution by a Histogram \n\n\n\nUse default breaks (no need to specify)\n\n\nhist(x = int_rate, \n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Defualt)\")\n\n\n\n\n\n\n\n\nUse customized breaks\n\n\nclass_boundary\n\n [1]  5.0  7.5 10.0 12.5 15.0 17.5 20.0 22.5 25.0 27.5\n\nhist(x = int_rate, \n     breaks = class_boundary, #<<\n     xlab = \"Interest Rate (%)\",\n     main = \"Hist. of Int. Rate (Ours)\")\n\n\n\n\n\n\n Skewness \n\nKey characteristics of distributions include the shape, center and spread.\nSkewness provides a way to summarize the shape of a distribution.\n\n\n\n\n\nFigureÂ 4.3: Distribution characteristics\n\n\n\n\n\n\n\n\n\n\n\n\n\nIs the interest rate histogram left skewed or right skewed?\n\n\n\n\n\n\n\n\n\n\nFigureÂ 4.4: Interest Rate Histogram\n\n\n\n\n\n\n\n\n\nFigureÂ 4.5: Trick for remembering skewness (Biostatistics for the Biological and Health Sciences p.53)\n\n\n\n\n\n\n\n Scatterplot for Two Numerical Variables \n\n\nA scatterplot provides a case-by-case view of data for two numerical variables.\nBelow is a scatterplot of Loan Amount vs.Â Total Income from the loan 50 data.\n\n\nplot(x = loan50$total_income, y = loan50$loan_amount,\n     xlab = \"Total Income\", ylab = \"Loan Amount\",\n     pch = 16, col = 4)"
  },
  {
    "objectID": "data-graphics.html#exercises",
    "href": "data-graphics.html#exercises",
    "title": "4Â  Data Visualization",
    "section": "\n4.3 Exercises",
    "text": "4.3 Exercises\nIn the following, we will be using the data set mtcars to do some data summary and graphics. First load the data set into your R session by the command data(mtcars). The data set is like\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nPlease see ?mtcars for the description of the data set.\n\nUse the function pie() to create a pie chart for the number of carburetors (carb). What the number of carburetors has the most frequencies in the data?\nUse the function barplot() to create a bar chart for the number of cylinders (cyl). What the number of cylinders has the most frequencies in the data?\nUse the function hist() to generate a histogram of the gross horsepower (hp). Is it right or left-skewed?\nUse the function plot() to create a scatter plot of weight (wt) vs.Â miles per gallon (mpg). As the weight increases, does the miles per gallon tend to increase or decrease?"
  },
  {
    "objectID": "data-numerics.html#measures-of-center",
    "href": "data-numerics.html#measures-of-center",
    "title": "5Â  Data Sample Statistics",
    "section": "\n5.1 Measures of Center",
    "text": "5.1 Measures of Center\n Mean \n\nThe (arithmetic) mean or average is calculated by adding up all of the values and then dividing by the total number of them.\nThe population mean is denoted as \\(\\mu\\).\nLet \\(x_1, x_2, \\dots, x_n\\) denote the measurements observed in a sample of size \\(n\\).\n\nThe sample mean is defined as\n\n\n\n\n\\[\\overline{x} = \\frac{\\sum_{i=1}^{n} x_i}{n} = \\frac{x_1 + x_2 + \\dots + x_n}{n}\\]\n\n\nFor the interest rate example,\n\n\n\\[\\overline{x} = \\frac{10.9\\% + 9.9\\% + \\cdots + 6.1\\%}{50} = 11.56\\%\\]\n\n Calculate Mean in R \n\n\n\n\nmean(int_rate)\n\n[1] 11.558\n\n\n Balancing Point \n\nThink of the mean as the balancing point of the distribution.\n\n\n\n\n\n\n\nFigureÂ 5.1: Mean as a balancing point for interest rate example\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Median \n\nThe median is the middle value when data values are sorted.\nHalf of the values are less than or equal to the median, and the other half are greater than the median.\nTo find the median, we first sort the values.\nIf \\(n\\) is odd, the median is located in the exact middle of the ordered values.\n\n Data: (0, 2, 10, 14, 8) \n Sorted Data: (0, 2, 8, 10, 14) \n\n The median is \\(8\\) .\n\n\nIf \\(n\\) is even, the median is the average of the two middle numbers.\n\n Data: (0, 2, 10, 14, 8, 12) \n Sorted Data: (0, 2, 8, 10, 12, 14) \n\n The median is \\(\\frac{8 + 10}{2} = 9\\) .\n\n\n\n Calculate Median in R \n\nThere are two ways to calculate the median in R.\n\n\nmedian(int_rate)  ## Compute the median using command median()\n\n[1] 9.9\n\n\n\n## Compute the median using definition\n(sort_rate <- sort(int_rate))  ## sort data\n\n [1]  5.3  5.3  5.3  6.1  6.1  6.1  6.7  6.7  7.3  7.3  7.3  8.0  8.0  8.0  8.0\n[16]  9.4  9.4  9.4  9.4  9.4  9.9  9.9  9.9  9.9  9.9  9.9 10.4 10.4 10.9 10.9\n[31] 10.9 10.9 10.9 12.0 12.6 12.6 12.6 14.1 15.0 16.0 17.1 17.1 17.1 18.1 18.4\n[46] 19.4 20.0 21.4 24.9 26.3\n\nlength(int_rate)  ## Check sample size is odd or even\n\n[1] 50\n\n(sort_rate[25] + sort_rate[26]) / 2  ## Verify the answer\n\n[1] 9.9\n\n\n\n\n\n\n\n\nWarning\n\n\n\n\nBe sure to sort the data first if computing the median using its definition.\n\n\n\n\n(int_rate[25] + int_rate[26]) / 2  ## Using un-sorted data leads to a wrong answer!!\n\n[1] 8.1\n\n\n\n Mode \n\nThe mode is the value that occurs most frequently.\nFor continuous numerical data, it is common for there not to be any observations that share the same value.\nA more practical definition is that a mode is represented by a prominent peak in the distribution.\n\n Calculate Mode in R \n\n## Create a frequency table \n(table_data <- table(int_rate))\n\nint_rate\n 5.3  6.1  6.7  7.3    8  9.4  9.9 10.4 10.9   12 12.6 14.1   15   16 17.1 18.1 \n   3    3    2    3    4    5    6    2    5    1    3    1    1    1    3    1 \n18.4 19.4   20 21.4 24.9 26.3 \n   1    1    1    1    1    1 \n\n\n\n## Sort the table to find the mode that occurs most frequently\n## the number that happens most frequently will be the first one\nsort(table_data, decreasing = TRUE)\n\nint_rate\n 9.9  9.4 10.9    8  5.3  6.1  7.3 12.6 17.1  6.7 10.4   12 14.1   15   16 18.1 \n   6    5    5    4    3    3    3    3    3    2    2    1    1    1    1    1 \n18.4 19.4   20 21.4 24.9 26.3 \n   1    1    1    1    1    1"
  },
  {
    "objectID": "data-numerics.html#comparison-of-mean-median-and-mode",
    "href": "data-numerics.html#comparison-of-mean-median-and-mode",
    "title": "5Â  Data Sample Statistics",
    "section": "\n5.2 Comparison of Mean, Median and Mode",
    "text": "5.2 Comparison of Mean, Median and Mode\n\nThe mode is applicable for both categorical and numerical data, while the median and mean work for numerical data only.\nIt is also possible to have more than one mode, but there is only one median and one mean.\nThe mean is sensitive to extreme values (outliers).\nThe median and mode are more robust than the mean.\n\nBeing more robust means these measures of center are more resistant to the addition of extreme values to the data.\nAn example in R is shown below:\n\n\n\n\ndata_extreme\n\n [1] 90.0  9.9 26.3  9.9  9.4  9.9 17.1  6.1  8.0 12.6 17.1  5.3  7.3  5.3  8.0\n[16] 24.9 18.1 10.4  8.0 19.4 14.1 20.0  9.4  9.9 10.9  5.3  6.7 15.0 12.0 12.6\n[31] 10.9  9.4  9.9  7.3 18.4 17.1  8.0  6.1  6.7  7.3 12.6 16.0 10.9  9.9  9.4\n[46] 10.4 21.4 10.9  9.4  6.1\n\n\n\nmean(data_extreme)  ## Large mean! Original mean is 11.56\n\n[1] 13.14\n\nmedian(data_extreme)  ## Median does not change!\n\n[1] 9.9\n\nnames(sort(table(data_extreme), decreasing = TRUE))[1] ## Mode does not change either!\n\n[1] \"9.9\"\n\n\n\nBelow is a figure that shows the differences in where the mean, median, and mode lie for skewed distributions vs.Â symmetric distributions.\n\n\n\n\n\nFigureÂ 5.2: Comparison of mean, median, and mode for symmetrical vs.Â skewed distributions"
  },
  {
    "objectID": "data-numerics.html#measures-of-variation",
    "href": "data-numerics.html#measures-of-variation",
    "title": "5Â  Data Sample Statistics",
    "section": "\n5.3 Measures of Variation",
    "text": "5.3 Measures of Variation\n\nMeasures of variation, just like measures of center, affect the shape of the distribution (FigureÂ 5.3).\n\n\n\n\n\nFigureÂ 5.3: Effects of variation on the shape of distributions\n\n\n\n\n\n p-th percentile \n\n\n\nThe p-th percentile (quantile) is a data value such that\n\nat most \\(p\\%\\) of the values are below it\nat most \\((1-p)\\%\\) of the values are above it\n\n\n\n\n\n\n\n\n\nThere are two data sets with the same mean 20.\n\n\n\n\nOne data set has 99-th percentile = 30, and 1-st percentile = 10.\nThe other has 99-th percentile = 40, and 1-st percentile = 0.\nWhich data set has larger variation?\n\n\n\n\n\n\n\n\nFigureÂ 5.4: Percentiles for ACT scores (https://en.wikipedia.org/wiki/ACT_(test))\n\n\n\n\n\n\n\n Interquartile Range (IQR) \n\n\nFirst Quartile (Q1): the 25-th percentile\n\nSecond Quartile (Q2): the 50-th percentile (Median)\n\nThird Quartile (Q3): the 75-th percentile\n\nInterquartile Range (IQR): Q3 - Q1\n\n\n\n\n## Use quantile() to find any percentile \n## through specifying the probability\nquantile(x = int_rate, \n         probs = c(0.25, 0.5, 0.75))\n\n   25%    50%    75% \n 8.000  9.900 13.725 \n\n## IQR by definition\nquantile(x = int_rate, probs = 0.75) - \n  quantile(x = int_rate, probs = 0.25) \n\n  75% \n5.725 \n\n\n\n\n\n\n## IQR()\nIQR(int_rate)  \n\n[1] 5.725\n\n## summary() to get the numeric summary\nsummary(int_rate)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   5.30    8.00    9.90   11.56   13.72   26.30 \n\n\n\n\n\n\n\n\n\n\nDoes a larger IQR means more or less variation?\n\n\n\n\n\n\n\n Variance and Standard Deviation \n\nThe distance of an observation from its mean, \\(x_i - \\overline{x}\\), is its deviation.\n\nSample Variance is defined as \\[ s^2 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1} \\]\n\n\nSample Standard Deviation (SD) is defined as the square root of the variance. \\[ s = \\sqrt{\\frac{\\sum_{i=1}^n(x_i - \\overline{x})^2}{n-1}} \\]\n\nThe corresponding population variance and SD are denoted as \\(\\sigma^2\\) and \\(\\sigma\\) respectively.\nThe variance is the average of squared deviation from the sample mean \\(\\overline{x}\\) or the mean squared deviation from the mean.\nThe standard deviation is the root mean squared deviation from the mean.\n\nIt measures, on average, how far the data spread out around the average.\n\n\n\n Compute Variance and SD in R \n\nvar(int_rate)\n\n[1] 25.54942\n\nsqrt(var(int_rate))\n\n[1] 5.054644\n\nsd(int_rate)\n\n[1] 5.054644"
  },
  {
    "objectID": "data-numerics.html#visualizing-data-variation",
    "href": "data-numerics.html#visualizing-data-variation",
    "title": "5Â  Data Sample Statistics",
    "section": "\n5.4 Visualizing Data Variation",
    "text": "5.4 Visualizing Data Variation\n Boxplot \n\nWhen plotting the whiskers for a boxplot,\n\nthe minimum is the minimal value that is not a potential outlier.\nthe maximum is the maximal value that is not a potential outlier.\n\n\n\n\n\n\n\nFigureÂ 5.5: Example of a boxplot (https://www.leansigmacorporation.com/box-plot-with-minitab/)\n\n\n\n\n Interest Rate Boxplot \n\nBelow is the boxplot for the interest rate data (FigureÂ 5.6).\n\n\n\n\n\nFigureÂ 5.6: Boxplot for interest rate example\n\n\n\n\n Boxplot in R \n\n\n\nboxplot(int_rate,ylab =\"Interest Rate (%)\")\n\n\n\n\n\n\n\n\nsort(int_rate, decreasing = TRUE)[1:5]\n\n[1] 26.3 24.9 21.4 20.0 19.4\n\nsort(int_rate)[1:5]\n\n[1] 5.3 5.3 5.3 6.1 6.1\n\nQ3 <- quantile(int_rate, probs = 0.75, \n               names = FALSE)\nQ1 <- quantile(int_rate, probs = 0.25, \n               names = FALSE)\nIQR <- Q3 - Q1\nQ1 - 1.5 * IQR\n\n[1] -0.5875\n\nQ3 + 1.5 * IQR\n\n[1] 22.3125"
  },
  {
    "objectID": "data-numerics.html#exercises",
    "href": "data-numerics.html#exercises",
    "title": "5Â  Data Sample Statistics",
    "section": "\n5.5 Exercises",
    "text": "5.5 Exercises\n\nIn the following, we will be using the data set mtcars to do some data summary and graphics. First load the data set into your R session by the command data(mtcars). The data set is like\n\n\nhead(mtcars)\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\n\nPlease see ?mtcars for the description of the data set.\n\nUse the function boxplot() to generate a boxplot of 1/4 mile time (qsec). Are there any outliers?\nCompute the mean, median and standard deviation of displacement (disp).\n\n\n\nMean and standard deviation (SD): For each part, compare data (1) and (2) based on their mean and SDs. You donâ€™t need to calculate these statistics, but compare (1) and (2) by stating which one has a larger mean/SD or they have the same mean/SD. Explain your reasoning.\n\n\n-30, 0, 0, 0, 15, 25, 25\n-50, 0, 0, 0, 15, 20, 25\n\n\n\n0, 1, 3, 5, 7\n21, 23, 25, 27, 29\n\n\n\n100, 200, 300, 400, 500\n0, 50, 350, 500, 600\n\n\n\n\nSkewness: Facebook data indicate that \\(50\\%\\) of Facebook users have 130 or more friends, and that the average friend count of users is 115. What do these findings suggest about the shape (right-skewed, left-skewed, symmetric) of the distribution of number of friends of Facebook users? Please explain."
  },
  {
    "objectID": "prob-define.html#language-of-uncertainty",
    "href": "prob-define.html#language-of-uncertainty",
    "title": "6Â  Definition of Probability",
    "section": "\n6.1 Language of Uncertainty",
    "text": "6.1 Language of Uncertainty\n Why Study Probability \nIt goes without saying that we live in a world full of chances and uncertainty. People do care about chances and usually make decisions given the information about some uncertain situations. FigureÂ 6.1 shows the Google search result of â€œwhat are chancesâ€ on my personal computer on September 19, 2022. It looks like people are curious about getting pregnant, getting COVID of course at that time, and getting struck by lightning or tornado. And of course, it is the chance or uncertainty that makes so many games so fun, amusing and even additive, for example, the board game monopoly, and the gambling machines like slots and blackjacks.\n\n\n\n\nFigureÂ 6.1: Google search result of â€œwhat are chancesâ€.\n\n\n\n\n\n\n\n\n\n\nFigureÂ 6.2: Monopoly baord game\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 6.3: Slots\n\n\n\n\n\n\n\n\nFigureÂ 6.4: Blackjacks\n\n\n\n\n\n\nApparently most people want to quantify uncertainty about something happening, or measure the chances of some event happening for better decision making. The question is, how? We need a consistent way of measuring chances and uncertainty so that our society can be operated in order. Thanks to great enthusiasm for gambling, the mathematical study of chances starts quite early, and nowadays, the most formal and rigorous way of studying chances is to use probability. Therefore, we could probably view probability as the language of uncertainty.\nMost of the time, we cannot do statistical inference or prediction using machine learning without probability because we usually assume our data at hand are random realizations from some targeted population that are described by a probability distribution. In other words, we are doing data analysis in the world of uncertainty. For example, we are interested in the mean height of Marquette female students, which is usually assumed to be bell-shaped distributed. By chances, our collected sample of girls may all have heights below 5â€™4â€, which is not representative of the target population, and the sample mean is far from the true population mean height being interested. Examining the chance of getting such biased data set becomes important and helps us quantify the plausibility of the numerical results we obtain from the data.\nAs a result, before jumping into statistical inference or data analysis, we need to have basic understanding of probability definitions, rules, and operations that are frequently used in the data science community.\n\n Why Probability Before Statistics? \nAlthough probability and statistics are closely related and usually taught in one single introductory course, they are two distinct subjects. Since the randomness of sampling from the target population, statistical inference and data analysis usually involve uncertainty quantification, and hence probability is used in the analysis.\nIn a typical statistical analysis, we assume the target population, for example the Marquette studentsâ€™ height follows some probability distribution, and the collected sample data are the realized data points from the distribution. With this, the population probability distribution is a mechanism that generates data. In probability theory, we examine how this mechanism generates data, and how the observed data will behave given the fact that they are from the probabilistic data generating process. For example, we assume the height follows some probability distribution, and we are curious about the probability that the sample mean is larger than 5â€™10â€, or that the sample mean is between 5â€™6â€ and 5â€™11â€.\nIn the probability theory, the process generating the data is assumed known and we are interested in properties of observations. However, in reality, the data generating process, or the target population distribution, is unknown to us, and is what we would like to infer to using the sample data we collect. For example, we want to estimate the unknown mean height of Marquette students using the data of 100 Marquette students sampled at random from the unknown target population distribution. This is what statistical inference is all about. For statistics, we observe the data (sample) and are interested in determining what process generates such data (population). These principles are illustrated below in FigureÂ 6.5.\n\n\n\n\nFigureÂ 6.5: Relationship between probability and statistical inference\n\n\n\n\nEven though the data generating process is fixed and unchanged every time a data set is collected, the data replicates are all different due to the random sampling from the population probability distribution. Such randomness creates the uncertainty about how we do the inference about the population properties because a different data set represents only a part of, and probably biased, information about the the whole distribution. As a result, when doing inference, we prefer probabilistic statements to deterministic statements."
  },
  {
    "objectID": "prob-define.html#interpretation-of-probability",
    "href": "prob-define.html#interpretation-of-probability",
    "title": "6Â  Definition of Probability",
    "section": "\n6.2 Interpretation of Probability",
    "text": "6.2 Interpretation of Probability\n Relative Frequency \nThere are several ways of interpreting probability. The first interpretation is relative frequency. Formally speaking, the probability that some outcome of a process will be obtained is interpreted as the relative frequency with which that outcome would be obtained if the process were repeated a large number of times independently under similar conditions.\nThink about the following scenario. Your mom gave you a unfair coin, but she does not know the probability of getting heads when one tosses the coin. How do you obtain, or at least approximate the probability? Well, we can use the concept of relative frequency. First, we decide how many times we want to flip the coin. Each time after flipping the coin, we record either heads or tails shows up. Once we are done all the flips, we count the frequency or the total number of times heads shows up among all the flips. To obtain the probability of getting heads, we calculate the relative frequency, the ratio of the frequency of heads to the number of tosses.\nBelow is an example depicting the relative frequency of flipping a coin and getting heads or tails.\n\n\n\n\n      Frequency Relative Frequency\nHeads         4                0.4\nTails         6                0.6\nTotal        10                1.0\n---------------------\n      Frequency Relative Frequency\nHeads       514              0.514\nTails       486              0.486\nTotal      1000              1.000\n---------------------\n\n\n\n\nWhen we flip the coin 10 times, 4 of them end up being heads, and the probability of getting heads is 40%. You may be skeptical of the result, and want to have more replicates. Some day you have lots of spare time, and you decide to flip the coin 1000 times. The relative frequency, or the probability of getting heads, now becomes 51.4%.\n\n\n\n\n\n\nDo you see any issues with relative frequency probability?\n\n\n\n\n\n\nApparently, we donâ€™t know the true probability if it does exist. And as you learned in the coin-flipping example, we donâ€™t have one unique answer for that if we use relative frequency as the way of interpreting probability. In fact, there are some issues when we treat relative frequency as probability.\n Issues with Relative Frequency \n\nðŸ˜• How large of a number is large enough?\n\nThere is no correct answer for how many replicates of the experiment we should have. We may think 10 times is not enough, but how about 1000 times? one million times? How well the relative frequency approximates the true probability depends on the complexity of the experiments. In general, the larger the number of times of repeating the process, the better the approximation of the relative frequency. This is the result of the so-called law of large numbers that is discussed in ChapterÂ 12.\n\nðŸ˜• What is the meaning of â€œunder similar conditionsâ€?\n\nIn the definition, the experiment or process needs to be repeated â€œunder similar conditionsâ€. What does that mean? The definition itself is not rigorous enough. Do we need to control the airflow when the experiment is conducted? How about temperature? Can your mom and you take turns to flip the coin? Be honest, there is no answer for that.\n\nðŸ˜• Is the relative frequency reliable under identical conditions?\n\nCan we trust the relative frequency when the experiment is conducted under identical conditions? If there is a skilled person who can control whether heads or tails shows up every time he flips a coin, should we believe the relative frequency is a good approximation to the true probability?\n\nðŸ‘‰ We can only obtain an approximation instead of exact value for the probability.\n\nYou may already find out that the relative frequency is only an approximation instead of exact value for the probability. If we want to get the true probability (if it does exist), we need to get the relative frequency whose process is repeated infinitely many of times, which is unrealistic. Such probability stems from the frequentist philosophy that interprets probability as the long-run relative frequency of a repeatable experiment.\n\nðŸ˜‚ How do you compute the probability that Chicago Cubs win the World Series next year?\n\nIn the real world and our daily lives, lots of times we want to compute the probability of something happening where the something cannot be a repeatable process or experiment. For example, it is impossible to compute the probability that Chicago Cubs win the World Series next year because we would never to able to obtain the relative frequency of Chicago Cubs winning the World Series next year.\n\n\n\n\nSource: https://media.giphy.com/media/EKURBxKKkw0uY/giphy.gif\n\n\n\n\n\n Classical Approach \nAnother interpretation of probability follows the classical approach, whose probability is based on the concept of equally likely outcomes. If the outcome of some process must be one of \\(n\\) different outcomes, the probability of each outcome is simply \\(1/n\\). For example, if you toss a fair coin (2 outcomes) ðŸª™, the probability of getting heads is 1/2. If you roll a well-balanced die (6 outcomes) ðŸŽ², the probability of each outcome being shown is 1/6. If you draw one from a deck of cards (52 outcomes) ðŸƒ, the probability of each card being drawn is 1/52.\n\n\n\n\n\n\nDo you see any issues with classical probability?\n\n\n\n\n\n\nIt wouldnâ€™t make sense to say that the probability that [you name it] wins the World Series next year is 1/30. Even though there are 30 teams in the MLB, each team is not equally likely to win the World Series. Donâ€™t you agree?!\n\n Subjective Approach \nThe last interpretation of probability we discuss here is the subjective approach, whose probability is assigned or estimated using peopleâ€™s knowledge, beliefs and information about the data generating process. In this case, it is a personâ€™s subjective probability of an outcome, rather than the true probability of that outcome. For example, I think â€œthe probability that the Milwaukee Brewers win the World Series this year is 30%â€. My probability that the Milwaukee Brewers win the World Series this year is likely to be different from an ESPN analystâ€™s probability.\n\n\n\n\n\n\nSource: https://www.nj.com/yankees/2020/02/mlb-rumors-espns-sunday-night-baseball-will-feature-a-lot-of-ex-yankee-alex-rodriguez-and-not-much-else.html\n\n\n\n\n\n\n\n\n\nSource: Wiki: ESPN Major League Baseball\n\n\n\n\n\n\nHere, a probability measures the relative plausibility of some event or outcome, and such probability stems from the so-called Bayesian philosophy. With this, we can claim that candidate A has a 0.9 probability of winning because the probability represents our plausibility or belief about the winning chance of the candidate A. A Bayesian statistician would say based on analysis the candidate A is 9 times more likely to win than to lose. For a statistician with the frequentist philosophy, he might say the statement is wrong or there is no such claim. Or he might weirdly say in long-run hypothetical repetitions of the election, candidate A would win roughly 90% of the time.\n\n\n\n\n\n\n\nNote\n\n\n\nProbability operations and rules do NOT depend on the interpretation of probability!"
  },
  {
    "objectID": "prob-rule.html#probability-operations-and-rules",
    "href": "prob-rule.html#probability-operations-and-rules",
    "title": "7Â  Probability Rules",
    "section": "\n7.1 Probability Operations and Rules",
    "text": "7.1 Probability Operations and Rules\n Experiments, Events and Sample Space \nProbability starts with the concept of sets. When we calculate the probability of â€œsomethingâ€, that something is represented as a set, which is a collection of some outcomes generated by an process or experiment associated to the something we are interested. To denote a set, we usually use a pair of curly braces { }, and the elements of the set is put inside the braces, each separated by a comma, for example, {red, green, yellow} is a set with three color elements.\nHere we define some terminology that are commonly used in set and probability concepts.\n\nAn experiment is any process in which the possible outcomes can be identified ahead of time.\n\nThe key words is ahead of time. For example, we know what is the result of flipping a coin, which is heads or tails showing up, before we actually do it. Therefore, flipping a coin is an experiment. Similarly, before we roll a six-sided die, we already know the possible outcome of doing that, which is 1, 2, 3, 4, 5, 6, so rolling a die is also an experiment.\n\nAn event is a set of possible outcomes of the experiment.\n\nGenerally there are two or more potential outcomes for some experiment. Any collection of those outcomes is called an event. For example, there are 6 possible outcomes for rolling a die, 1, 2, 3, 4, 5, 6. Then any collection of those 6 numbers is an event. So â€œAn odd number showing upâ€ which corresponds to the collection {1, 3, 5} is an event. â€œAn even number showing upâ€ that is represented by {2, 4, 6} is also an event.\n\nThe sample space \\((\\mathcal{S})\\) of an experiment is the collection of ALL possible outcomes of the experiment.\n\nBased on the definition, the sample space the largest set associated with an experiment because it collects all possible outcomes. In other words, when an experiment is conducted, no matter what outcome shows up, it is always in the sample space.\n\n\n\n\n\n\nIs the sample space also an event?\n\n\n\n\nYes, the sample space itself is an event because it is also a set of possible outcomes of the experiment.\n\n\n\nThe table below provides a summary of experiments flipping a coin and rolling a die.\n\n\n\n\n\n\n\n\nExperiment\nPossible Outcomes\nSome Events\nSample Space\n\n\n\nFlip a coin ðŸª™\nHeads, Tails\n{Heads}, {Heads, Tails}, â€¦\n{Heads, Tails}\n\n\nRoll a die ðŸŽ²\n1, 2, 3, 4, 5, 6\n{1, 3, 5}, {2, 4, 6}, {2}, {3, 4, 5, 6}, â€¦\n{1, 2, 3, 4, 5, 6}\n\n\n\n\n Set Concept: Example of Rolling a six-side balanced die\n\n\n\n\n\n\nTip\n\n\n\nDraw a Venn Diagram every time you get stuck! Venn diagram is a very useful tool for identifying a set, so I encourage you to draw a venn diagram when you get stuck on complicated set operations.\n\n\n\n\n The complement  of an event (set) \\(A\\), is denoted  \\(A^c\\) . It is the set of all outcomes (elements) of \\(\\mathcal{S}\\) in which \\(A\\) does not occur. For the die example, let \\(A\\) be an event that a number greater than 2 is rolled. Then \\(A = \\{3, 4, 5, 6\\}\\) and \\(A^c = \\{1, 2\\}\\).\n\n\n\n\n\n\n\n\n\n\n\nThe  union \\((A \\cup B)\\)  is the set of all outcomes of \\(\\mathcal{S}\\) in \\(A\\) or \\(B\\).  For the die example, let \\(B\\) be an event that an even number is rolled. Then \\(B = \\{2, 4, 6\\}\\) and \\(A \\cup B = \\{2, 3, 4, 5, 6\\}\\).  Basically, as long as an element is in either \\(A\\) or \\(B\\), not necessarily in both \\(A\\) and \\(B\\), the element is collected in the set \\((A \\cup B)\\).\n\n\n\n\n\n\n\n\n\n\nThe  intersection \\((A \\cap B)\\)  is the set of all outcomes of \\(\\mathcal{S}\\) in both \\(A\\) and \\(B\\).  For the die example, \\(A \\cap B = \\{4, 6\\}\\). Basically, for any element in the set \\((A \\cap B)\\), it must be an element of \\(A\\) and of \\(B\\).\n\n\n\n\n\n\n\n\n\n\n\n\\(A\\) and \\(B\\) are disjoint or mutually exclusive if they have no outcomes in common \\((A \\cap B = \\emptyset)\\). \\(\\emptyset\\) means an empty set, \\(\\{\\}\\), i.e., no elements in the set. If the two events are disjoint, it means that they cannot occur at the same time for one single trial of the experiment.  For the die example, let \\(C\\) be an event that an odd number is obtained. Then \\(C = \\{1, 3, 5\\}\\) and \\(B \\cap C = \\emptyset\\).  When we roll a die one time, we cannot get a number that is odd and even at the same time.\n\n\n\n\n\n\n\n\n\n\nThe containment \\((A \\subset B)\\) means every elements of \\(A\\) also belongs to \\(B\\). In other words, if \\(A\\) occurs, then so does \\(B\\).  For the die example, let \\(B\\) be the event that an even number is obtained and \\(D\\) be the event that a number greater than 1 is obtained. Then \\(B = \\{2, 4, 6\\}\\) and \\(D = \\{2, 3, 4, 5, 6\\}\\).  In this case, every even number is greater than one, therefore \\(B\\) is a subset of \\(D\\), i.e., \\(B \\subset D\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nKeep in mind that every event (set) being considered is a subset of the sample space \\((\\mathcal{S})\\) because it only makes sense to discuss the events that are possibly to be occurred. As a result, for any event \\(A\\), \\(A \\subset \\mathcal{S}\\).\n\n\n\n Probability Rules \nWe have learned how to represent an event in terms of sets and set operations. Here we are going to learn several probability rules for events.\nFirst, we denote the probability of an event \\(A\\) on a sample space \\(\\mathcal{S}\\) as \\(P(A)\\).\n\n\n\n\n\n\nTip\n\n\n\nTreat the probability of an event as the area of the event in the Venn diagram.\n\n\nIn order to have a coherent and logically consistent set of probability rules, we need some axioms that are self-evident to anyone. The three axioms are as follows.\n\n\\(P(\\mathcal{S}) = 1\\)\nFor any event \\(A\\), \\(P(A) \\ge 0\\)\n\nIf \\(A\\) and \\(B\\) are disjoint or mutually exclusive, \\(P(A \\cup B) = P(A) + P(B)\\)\n\n\nIf we treat the sample space as an event, the probability of the entire sample space is always equal to one because this event must happen every time the experiment is conducted. In the Venn diagram, the sample space is the entire rectangle, and in probability, we presume the area of the rectangle is one.\nBecause any event is a collection of some outcomes that could possibly occur or not occur, its probability is greater than or equal to zero. Any probability cannot be negative. It is clearly shown in terms of Venn diagram because an area of any shape of an object is greater than or equal to zero.\nFinally, if the two events are disjoint, the probability of the union of the two is just the sum of their own probability. For example, if the probability of getting a green M&M is 20% and that of getting a blue M&M is 15%, then the probability of getting a green or blue M&M is 20% + 15% = 35%. It is clearly shown in the Venn diagram too because the total area of the two disjoint events in the sample space is the sum of the individual area.\nWith the three axioms, the entire probability operation system can be constructed. Some basic properties are listed here.\n\n\n\\(P(\\emptyset) = 0\\).\n\\(0 \\le P(A) \\le 1\\)\n\nAddition Rule: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\)\n\n\\(P(A^c) = 1 - P(A)\\)\nIf \\(A \\subset B\\), then \\(P(A) \\le P(B)\\)\n\n\nThe empty set does not contain any possible outcomes of an experiment. Because some outcome must be occurred after an experiment is conducted, it is impossible for some event to happen without any outcome involved. Therefore, the probability of the empty set is zero. In terms of Venn diagram, an empty set is a set with area zero because it does not occupy any part (outcome) of the entire sample space.\nSince every event being considered must be a subset of the sample space, the area of any event is smaller than the area of the sample space which is one. Therefore, for any event \\(A\\), \\(P(A) \\le 1\\). \\(P(A) = 1\\) if and only if \\(A = \\mathcal{S}\\).\n\n\nThe addition rule can be clearly understood using the Venn diagram. FigureÂ 7.1 shows how \\(P(A \\cup B)\\) is expressed by \\(P(A)\\), \\(P(B)\\) and \\(P(A \\cap B)\\). To get the area of \\(A \\cup B\\), we can first consider the sum of the area of \\(A\\) and \\(B\\), the left and right circles. However, the middle part which is \\(A \\cap B\\) is counted twice when we take the sum, so the one piece of area of \\(A \\cap B\\) should be removed from the sum of the area.\n\n\n\n\nFigureÂ 7.1: Venn Diagram depiction of the addition rule\n\n\n\n\nIn fact, the third axiom is a special case of the addition rule. FigureÂ 7.2 illustrates the case. Since the two events are disjoint, \\(A \\cap B = \\emptyset\\), and the area of \\(A \\cap B\\), or \\(P(A \\cap B)\\) is zero.\n\n\n\n\n\nFigureÂ 7.2: Venn Diagram depiction of the addition rule for the disjoint case\n\n\n\n\nSince \\(A \\cup A^c = \\mathcal{S}\\) and \\(A \\cap A^c = \\emptyset\\), we have \\[P(A \\cup A^c) = P(\\mathcal{S}) = P(A) + P(A^c)\\] Therefore, \\(P(A) + P(A^c) = 1\\) and \\(P(A^c) = 1 - P(A)\\).\nIf \\(A \\subset B\\), the area of \\(A\\) is smaller or equal to the area of \\(B\\). Therefore, \\(P(A) \\le P(B)\\).\n Example: M&M Colors\n\nThe makers of the M&Ms report that their plain M&Ms are composed of\n\n15% Yellow, 10% Red, 20% Orange, 25% Blue, 15% Green and 15% Brown\n\n\n\n\n\n\n\n\n\nSource: Unsplash: Robert Anasch\n\n\n\n\n\n\n\n\n\n\n\nIf you randomly select an M&M, what is the probability of the following?\n\n\n\n\nIt is brown.\nIt is red or green.\nIt is not blue.\nIt is red and brown.\n\n\n\n\n\n\n\nSolution\n\n\\(P(\\mathrm{Brown}) = 0.15\\)\n\\(\\begin{align} P(\\mathrm{Red} \\cup \\mathrm{Green}) &= P(\\mathrm{Red}) + P(\\mathrm{Green}) - P(\\mathrm{Red} \\cap \\mathrm{Green}) \\\\ &= 0.10 + 0.15 - 0 = 0.25 \\end{align}\\)\n\\(P(\\text{Not Blue}) = 1 - P(\\text{Blue}) = 1 - 0.25 = 0.75\\)\n\\(P(\\text{Red and Brown}) = P(\\emptyset) = 0\\)\n\n\n\n\n\n\n\n\n\nWhich interpretation of probability is used in this question?"
  },
  {
    "objectID": "prob-rule.html#conditional-probability-and-independence",
    "href": "prob-rule.html#conditional-probability-and-independence",
    "title": "7Â  Probability Rules",
    "section": "\n7.2 Conditional Probability and Independence",
    "text": "7.2 Conditional Probability and Independence\n Conditional Probability \nQuite often people are interested in the probability of something happening given the fact that some other event has been occurred or some information about the experiment or its outcomes have been known. In this case, we can calculate the conditional probability that takes the occurred event or known information into account. The conditional probability would be more appropriate for quantifying uncertainty about what we are interested because knowing some event being occurred is a piece of valuable information that helps us properly adjust the chance of something happening.\nBy definition, the conditional probability of \\(A\\) given \\(B\\) is \\[ P(A \\mid  B) = \\frac{P(A \\cap B)}{P(B)}.\\]\nThe vertical bar â€œ\\(\\mid\\)â€ is read as given or conditional on. Therefore, we consider the event \\(A\\) not in the entire sample space, but the event \\(A\\) that is conditional on the event \\(B\\), or \\(A \\mid B\\). In other words, we donâ€™t consider all possible parts of \\(A\\). Instead, we only care about the part of \\(A\\) for which \\(B\\) has already occurred.\nThe formula is well defined when \\(P(B) > 0\\) and undefined if \\(P(B) = 0\\). Intuitively, one cannot calculate the probability of \\(A\\) given \\(B\\) when \\(B\\) is not occurred by any chance. FigureÂ 7.3 illustrates the conditional probability of \\(A\\) given \\(B\\) using the Venn diagram. When we compute the probability of \\(A\\), the information about \\(B\\) have been given, and the probability of \\(A\\) is adjusted, according to this information. The conditional probability is the ratio of \\(P(A \\cap B)\\) to \\(P(B)\\).\n\n\n\n\n\n\n\n\n\n\nFigureÂ 7.3: Venn Diagram illustration of the conditional probability of A given B\n\n\n\n\nSo what is the difference between \\(P(A)\\) and \\(P(A \\mid B)\\)? How knowing event \\(B\\) occurred affects the probability of \\(A\\)? Well, FigureÂ 7.4 describes the difference. When we calculate \\(P(A)\\), we assume we donâ€™t have any specific information at hand. What we can base on is the entire sample space because we need to take all possible outcomes into account, and see if any outcome is related to the event \\(A\\). Therefore, although we just write \\(P(A)\\), the probability is actually calculated conditional on the entire sample space, i.e., \\(P(A \\mid \\mathcal{S})\\), which is the ratio of area of \\(A\\) to the area of \\(\\mathcal{S}\\) that is one. Such probability is usually called unconditional or marginal probability because being conditional on \\(\\mathcal{S}\\) is like being not conditional on some specific event, and â€œmarginalâ€ means all other possible events or outcomes have been â€œmarginalizedâ€ out, and only event \\(A\\) is of our interest.\nNow if we know \\(B\\) has occurred, we donâ€™t need to consider the entire sample space any more. Instead, we can focus only on \\(B\\) since we know \\(B\\) has occurred, and anything not related to \\(B\\), or \\(B^c\\) becomes irrelevant. We shrink the search pool from \\(\\mathcal{S}\\) to the smaller space \\(B\\). To find \\(P(A \\mid B)\\), we just need to find how large part of \\(B\\) that also belongs to \\(A\\). Intuitively speaking, the event \\(B\\) has become our new sample space that are smaller than the original \\(\\mathcal{S}\\). We calculate the probability of \\(A\\) based on \\(B\\), not \\(\\mathcal{S}\\). Since \\(B\\) is the new sample space, we can treat \\(P(B)\\) as one. In other words, any probability conditional on \\(B\\) is scaled up by \\(1 / P(B)\\) so that \\(P(B) \\times \\frac{1}{P(B)} = 1\\). This is why \\(P(A \\cap B)\\) is multiplied by \\(1 / P(B)\\) in the conditional probability formula.\nHere is an example of new sample space or search pool. Suppose we would like to calculate the probability of a woman who is greater than 20 years old in a certain area. When we donâ€™t have any background information about the woman, to compute the probability, we need to base on the entire female population of interest. But if we do know that the woman has two children, we shrink our focus on the pool of women who have two children, and compute the proportion of the women pool that is over 20 years of age.\n\n\n\n\n\n\n\n\nFigureÂ 7.4: Difference Between \\(P(A)\\) and \\(P(A \\mid B)\\)\n\n\n\n\nThe conditional probability formula lead to the multiplication rule: \\[P(A \\cap B) = P(A \\mid  B)P(B) = P(B \\mid  A)P(A)\\] The rule is a rearranged form of the formula by multiplying both sides by \\(P(B)\\). Notice that \\(P(B \\mid A) = \\frac{P(B \\cap A)}{P(A)}\\) and hence \\(P(B \\mid A)P(A) = P(B \\cap A) = P(A \\cap B)\\).\n Example: Peanut Butter and Jelly\n\nSuppose 80% of people like peanut butter, 89% like jelly and 78% like both. Given that a randomly sampled person likes peanut butter, what is the probability that she also likes jelly?\n\n\n\n\n\n\n\n\n\n\n\n\nWe want \\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)}\\).\nFrom the problem we have \\(P(PB) = 0.8\\), \\(P(J) = 0.89\\) and \\(P(PB \\cap J) = 0.78\\)\n\n\\(P(J\\mid PB) = \\frac{P(PB \\cap J)}{P(PB)} = \\frac{0.78}{0.8} = 0.975\\)\nIf we donâ€™t know if the person loves peanut butter, the probability that he or she loves jelly is 89%.\nIf we do know she loves peanut butter, the probability that he or she loves jelly is going up to 97.5%.\n\n\n\n\n Independence \nIn the previous example, we learn that whether a person loves peanut butter affects the probability that she loves jelly. This piece of information is relevant, and the two events â€œlove peanut butterâ€ and â€œlove jellyâ€ are dependent each other because the one event will affect the chance of the other event happening. Uncovering the association or dependence is important for statistical inference because it helps statisticians better pin down the probability of being interest.\nFormally speaking, event \\(A\\) and \\(B\\) are independent if \\(\\begin{align} P(A \\mid B) &= P(A) \\text{ or }\\\\ P(B \\mid A) &= P(B) \\text{ or } \\\\P(A\\cap B) &= P(A)P(B)\\end{align}\\) \\(\\text{ for } P(A) > 0 \\text{ and } P(B) > 0.\\)\nIntuitively, this means that knowing \\(B\\) occurs does not change the probability that \\(A\\) occurs and vice versa. The information about \\(B\\) is irrelevant to probability of \\(A\\).\nHere is a question. Can we compute \\(P(A \\cap B)\\) if we only know \\(P(A)\\) and \\(P(B)\\)? The answer is no. We cannot compute \\(P(A \\cap B)\\) because we donâ€™t know if \\(A\\) and \\(B\\) are independent. We can only do so if \\(A\\) and \\(B\\) are independent. In general, we need to consider the dependence of two events and use the multiplication rule \\(P(A \\cap B) = P(A \\mid B)P(B)\\).\nFigureÂ 7.5 explain independence using the Venn diagram. Independence means that the ratio of area of \\(A\\) to area of \\(\\mathcal{S}\\) is the same as the ratio of area of \\(A \\cap B\\) to area of \\(B\\). Look at the case of non-independence on the right, the two events (circles) in particular. The area of \\(A \\cap B\\) is very close to the area of \\(B\\) because the two events are quite overlapped each other. It means that the ratio of area of \\(A \\cap B\\) to area of \\(B\\) is pretty close to one. So if we know \\(B\\) has occurred, there will be a very high chance that \\(A\\) would happen as well. In this case, the information about \\(B\\) does matter, and affect probability of \\(A\\). The idea is that the two events describe pretty much similar set of outcomes for an experiment. As a result, when one occurs, we are pretty sure the other happens as well.\n\n\n\n\n\n\n\n\n\n\nFigureÂ 7.5: Venn Diagram Explanation of Independence\n\n\n\n\n Independence Example \n\n\n\n\n\n\nAssuming that events \\(A\\) and \\(B\\) are independent and that \\(P(A) = 0.3\\) and \\(P(B) = 0.7\\).\n\n\n\n\n\n\\(P(A \\cap B)\\)?\n\n\\(P(A \\cup B)\\)?\n\n\\(P(A \\mid B)\\)?\n\n\n\nSolution\n\n\\(P(A \\cap B) = P(A)P(B)=0.21\\)\n\\(P(A \\cup B) = P(A)+P(B)-P(A\\cap B) = 0.3+0.7-0.21=0.79\\)\n\\(P(A \\mid B) = P(A) = 0.3\\)"
  },
  {
    "objectID": "prob-rule.html#bayes-formula",
    "href": "prob-rule.html#bayes-formula",
    "title": "7Â  Probability Rules",
    "section": "\n7.3 Bayesâ€™ Formula",
    "text": "7.3 Bayesâ€™ Formula\n Why Bayesâ€™ Formula? \n\nOften, we know \\(P(B \\mid A)\\), but we are much more interested in \\(P(A \\mid B)\\).\nExample: Diagnostic tests provide \\(P(\\text{positive test result} \\mid \\text{COVID})\\), but we are also interested in \\(P(\\text{COVID} \\mid \\text{positive test result})\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBayesâ€™ formula provides a way to find \\(P(A \\mid B)\\) from \\(P(B \\mid A)\\)\n\n\n\n Formula \n\nIf \\(A\\) and \\(B\\) are events whose probabilities are not 0 or 1, then \\[\\begin{align*} P(A \\mid B) &= \\frac{P(A \\cap B)}{P(B)} \\quad ( \\text{def. of cond. prob.}) \\\\ &= \\frac{P(A \\cap B)}{P((B \\cap A) \\cup (B \\cap A^c))} \\quad ( \\text{partition } B) \\\\ &= \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)}  \\quad ( \\text{multiplication rule}) \\end{align*}\\]\n\n\n\n\n\n\nFigureÂ 7.6: Venn Diagram illustration for Bayesâ€™ formula\n\n\n\n\n Example: Passing Rate \n\n\nAfter taking MATH 4720, \\(80\\%\\) of students understand the Bayesâ€™ formula.\n\nOf those who understood the Bayesâ€™ formula,\n\n\n\\(95\\%\\) passed\n\n\nOf those who did not understand the Bayesâ€™ formula,\n\n\n\\(60\\%\\) passed\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCalculate the probability that a student understands the Bayesâ€™ formula given the fact that she passed.\n\n\n\n\n\n\n\n\n Step 1: Formulate what we would like to compute \n\n\\(P(\\text{understood} \\mid \\text{passed})\\)\n\n\n\n Step 2: Define relevant events in the formula: \\(A\\), \\(A^c\\) and \\(B\\) \n\nLet \\(A =\\) understood and \\(B =\\) passed. Then \\(A^c =\\) didnâ€™t understand and \\(P(\\text{understood} \\mid \\text{passed}) = P(A \\mid B)\\).\n\n\n\n Step 3: Find probabilities in the Bayesâ€™ formula using provided information. \n\n\\(P(B \\mid A) = P(\\text{passed} \\mid \\text{understood}) = 0.95\\)\n\n\\(P(B \\mid A^c) = P(\\text{passed} \\mid \\text{didn't understand}) = 0.6\\)\n\n\\(P(A) = P(\\text{understood}) = 0.8\\)\n\\(P(A^c) = 1 - P(A) = 0.2\\)\n\n\n\n Step 4: Apply Bayesâ€™ formula. \n\n\\(\\small P(\\text{understood} \\mid \\text{passed}) = P(A \\mid B) = \\frac{P(B \\mid A)P(A)}{P(B \\mid A)P(A) + P(B \\mid A^c)P(A^c)} = \\frac{(0.95)(0.8)}{(0.95)(0.8) + (0.6)(0.2)} = 0.86\\)\n\n\n\n Tree Diagram Illustration \n\n\n\\(80\\%\\) of students understand the Bayesâ€™ formula.\nOf those who understood the Bayesâ€™ formula, \\(95\\%\\) passed (\\(5\\%\\) failed).\nOf those who did not understand the formula, \\(60\\%\\) passed (\\(40\\%\\) failed).\n\n\n\n\n\n\n\nFigureÂ 7.7: Tree Diagram illustration of Passing Rate example\n\n\n\n\n\n\n\n\\[\\begin{align*} & P(\\text{yes} \\mid \\text{pass}) \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass})} \\\\ &= \\frac{P(\\text{yes and }  \\text{pass})}{P(\\text{pass and yes}) + P(\\text{pass and no})}\\\\ &= \\frac{P(\\text{pass | yes})P(\\text{yes})}{P(\\text{pass | yes})P(\\text{yes}) + P(\\text{pass | no})P(\\text{no})} \\\\ &= \\frac{0.76}{0.76 + 0.12} = 0.86 \\end{align*}\\]"
  },
  {
    "objectID": "prob-rule.html#exercises",
    "href": "prob-rule.html#exercises",
    "title": "7Â  Probability Rules",
    "section": "\n7.4 Exercises",
    "text": "7.4 Exercises\n\nA Pew Research survey asked 2,422 randomly sampled registered voters their political affiliation (Republican, Democrat, or Independent) and whether or not they identify as swing voters. 38% of respondents identified as Independent, 25% identified as swing voters, and 13% identified as both.\n\nAre being Independent and being a swing voter disjoint, i.e.Â mutually exclusive?\nWhat percent of voters are Independent but not swing voters?\nWhat percent of voters are Independent or swing voters?\nWhat percent of voters are neither Independent nor swing voters?\nIs the event that someone is a swing voter independent of the event that someone is a political Independent?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEarth is warming\nNot warming\nDonâ€™t Know/Refuse\nTotal\n\n\n\nConservative Republican\n0.11\n0.20\n0.02\n0.33\n\n\nMod/Lib Republican\n0.06\n0.06\n0.01\n0.13\n\n\nMod/Cons Democrat\n0.25\n0.07\n0.02\n0.34\n\n\nLiberal Democrat\n0.18\n0.01\n0.01\n0.20\n\n\nTotal\n0.60\n0.34\n0.06\n1.00\n\n\n\n\nA Pew Research poll asked 1,423 Americans, â€œFrom what youâ€™ve read and heard, is there solid evidence that the average temperature on earth has been getting warmer over the past few decades, or not?â€. The table above shows the distribution of responses by party and ideology, where the counts have been replaced with relative frequencies.\n\nAre believing that the earth is warming and being a liberal Democrat mutually exclusive?\nWhat is the probability that a randomly chosen respondent believes the earth is warming or is a Mod/Cons Democrat?\nWhat is the probability that a randomly chosen respondent believes the earth is warming given that he is a Mod/Cons Democrat?\nWhat is the probability that a randomly chosen respondent believes the earth is warming given that he is a Mod/Lib Republican?\nDoes it appear that whether or not a respondent believes the earth is warming is independent of their party and ideology? Explain your reasoning.\n\n\nAfter an MATH 4740/MSSC 5740 course, 73% of students could successfully construct scatter plots using R. Of those who could construct scatter plots, 84% passed, while only 62% of those students who could not construct scatter plots passed. Calculate the probability that a student is able to construct a scatter plot if it is known that she passed."
  },
  {
    "objectID": "prob-rv.html#recap",
    "href": "prob-rv.html#recap",
    "title": "8Â  Random Variables",
    "section": "\n8.1 Recap",
    "text": "8.1 Recap\n\nA variable in a data set is a characteristic that varies from one object to another.\n\nA variable can be either categorical or numerical.\nNumerical variables can be either discrete or continuous.\n\n\nA random variable, usually written as \\(X\\) 1, is a variable whose possible values are numerical outcomes determined by the chance or randomness of a procedure or experiment.\n\n \\(X\\) = # of heads after flipping a coin twice. \n \\(X\\) = # of accidents in W. Wisconsin Ave. per day.\n\n\nA random variable has a probability distribution associated with it, accounting for its randomness.\n\n[1] Usually in statistics, a capital \\(X\\) represents a random variable and a small \\(x\\) represents a realized value of \\(X\\)."
  },
  {
    "objectID": "prob-rv.html#discrete-and-continuous-random-variables",
    "href": "prob-rv.html#discrete-and-continuous-random-variables",
    "title": "8Â  Random Variables",
    "section": "\n8.2 Discrete and Continuous Random Variables",
    "text": "8.2 Discrete and Continuous Random Variables\n\nA discrete random variable takes on a finite or countable number of values.\n\n The number of relationships youâ€™ve ever had is discrete variable because we can count the number and it is finite.\nIf we can further determine the probability that the number is 0, 1, 2 or any possible number, it is a discrete random variable.\n\n\nA continuous random variable has infinitely many values, and the collection of values is uncountable.\n\n Height is a continuous variable because it can be any number within a range. \nIf we have a way to quantify the probability that the height is from any value \\(a\\) to any value \\(b\\), it is a continuous random variable.\n\n\n\nFigureÂ 8.1 below shows the many different types of probability distributions that exist.\n\n\n\n\n\nFigureÂ 8.1: Probability distributions a statistician should know (https://github.com/rasmusab/distribution_diagrams)"
  },
  {
    "objectID": "prob-disc.html#introduction",
    "href": "prob-disc.html#introduction",
    "title": "9Â  Discrete Probability Distributions",
    "section": "\n9.1 Introduction",
    "text": "9.1 Introduction\n\nThe probability (mass) function of a discrete random variable (rv) \\(X\\) is a function \\(P(X = x)\\) (or \\(p(x)\\)) that assigns a probability to every possible number \\(x\\).\nThe probability distribution for a discrete r.v. \\(X\\) displays its probability function.\nThe display can be a table, graph or mathematical formula of \\(P(X = x)\\).\n\n Example:ðŸª™ðŸª™ Toss a fair coin twice independently where \\(X\\) is the number of heads. \n\nThe probability distribution of \\(X\\) as a table is\n\n\n\n\n\n\n\n x \n    0 \n    1 \n    2 \n  \n\n P(X = x) \n    0.25 \n    0.5 \n    0.25 \n  \n\n\n\n\n\n\n\n\n\n\nðŸ‘‰ \\(\\{X = x\\}\\) corresponds to an event of some experiment.\n\n\n\n\nWhat is the event that \\(\\{X = 0\\}\\) corresponds to?\nHow do we determine \\(P(X = 0)\\), \\(P(X=1)\\) and \\(P(X=2)\\) ?\n\n\n\n\n\n\n\n\nFigureÂ 9.1: Discrete probability distribution of two coin flips as a graph\n\n\n\n\n\n\n\\(0 \\le P(X = x) \\le 1\\) for every value \\(x\\) of \\(X\\).\n\n \\(x = 0, 1, 2\\) \n\n\n\n\\(\\sum_{x}P(X=x) = 1\\), where \\(x\\) assumes all possible values.\n\n \\(P(X=0) + P(X = 1) + P(X = 2) = 1\\) \n\n\nThe probabilities for a discrete r.v. are additive because \\(\\{X = a\\}\\) and \\(\\{X = b\\}\\) are disjoint for any possible values \\(a \\ne b\\).\n\n \\(P(X = 1 \\text{ or } 2) = P(\\{X = 1\\} \\cup \\{X = 2\\}) = P(X = 1) + P(X = 2)\\). \n\n\n\n\n Mean \n\nSuppose \\(X\\) takes values \\(x_1, \\dots, x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\).\nThe mean or expected value of \\(X\\) is the sum of each outcome multiplied by its corresponding probability: \\[E(X) := x_1 \\times P(X = x_1) + \\dots + x_k \\times P(X = x_k) = \\sum_{i=1}^kx_iP(X=x_i)\\]\n\nThe Greek letter \\(\\mu\\) may also be used in place of the notation \\(E(X)\\).\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe mean of a discrete random variable \\(X\\) is a weighted average.\nThe possible values, \\(x\\), are weighted by their corresponding probability.\n\n\n\n\n\n\n\n\n\nWhat is the mean of \\(X\\) (the number of heads) in the previous example?\n\n\n\n\n\n\n\n Variance \n\nSuppose \\(X\\) takes values \\(x_1, \\dots , x_k\\) with probabilities \\(P(X = x_1), \\dots, P(X = x_k)\\) and expected value \\(\\mu = E(X)\\).\nThe variance of \\(X\\), denoted by \\(Var(X)\\) or \\(\\sigma^2\\), is \\[\\small Var(X) := (x_1 - \\mu)^2 \\times P(X = x_1) + \\dots + (x_k - \\mu)^2 \\times P(X = x_k) = \\sum_{i=1}^k(x_i - \\mu)^2P(X=x_i)\\]\n\nThe standard deviation of \\(X\\), \\(\\sigma\\), is the square root of the variance.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe variance of a discrete random variable \\(X\\) is also weighted.\nIt is the sum of squared deviation from the mean weighted by probability values.\n\n\n\n\n\n\n\n\n\nWhat is the variance of \\(X\\) (the number of heads) in the previous example?"
  },
  {
    "objectID": "prob-disc.html#binomial-distribution",
    "href": "prob-disc.html#binomial-distribution",
    "title": "9Â  Discrete Probability Distributions",
    "section": "\n9.2 Binomial Distribution",
    "text": "9.2 Binomial Distribution\n Binomial Experiment and Random Variable \n\nA binomial experiment is one that has the following properties:\n\nðŸ‘‰ The experiment consists of a fixed number of identical trials \\(n\\).\nðŸ‘‰ Each trial results in one of exactly two outcomes (success (S) and failure (F)).\nðŸ‘‰ Trials are independent, meaning that the outcome of one trial does not affect the outcome of any other trial.\nðŸ‘‰ The probability of success is constant for all trials.\n\n\nIf \\(X\\) is defined as  the number of successes observed in \\(n\\) trials , then \\(X\\) is a binomial random variable.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe word success just means one of the two outcomes and does not necessarily mean something good. \n\nðŸ˜² We can define Drug abuse as success and No drug abuse as failure.\n\n\n\n\n Distribution \n\nThe probability function \\(P(X = x)\\) of a binomial r.v. \\(X\\) can be fully determined by\n\nthe number of trials, \\(n\\)\n\nprobability of success, \\(\\pi\\)\n\n\n\nDifferent \\((n, \\pi)\\) pairs generate different binomial probability distributions.\n\n\\(X\\) is said to follow a binomial distribution with parameters \\(n\\) and \\(\\pi\\), written as \\(\\color{blue}{X \\sim binomial(n, \\pi)}\\).\nThe binomial probability function is \\[ \\color{blue}{P(X = x \\mid n, \\pi) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x}, \\quad x = 0, 1, 2, \\dots, n}\\]\n\nThis distribution has a mean \\(\\mu = E(X) = n\\pi\\) and variance \\(\\sigma^2 = Var(X) = n\\pi(1-\\pi)\\).\n\n\n\n\n\n\n\nIf we toss a fair coin two times independently and let \\(X =\\) # of heads, is \\(X\\) a binomial r.v.?\n\n\n\n\n\n\n Example \n\n\n\nAssume that 20% of all drivers have a blood alcohol level above the legal limit. For a random sample of 15 vehicles, compute the probability that:\n\nExactly 6 of the 15 drivers will exceed the legal limit.\nOf the 15 drivers, 6 or more will exceed the legal limit.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose itâ€™s a binomial experiment with \\(n = 15\\) and \\(\\pi = 0.2\\).\nLet \\(X\\) be the number of drivers exceeding limit.\n\n\\(X \\sim binomial(15, 0.2)\\).\n\n\\[ \\color{blue}{P(X = x \\mid n=15, \\pi=0.2) = \\frac{15!}{x!(15-x)!}(0.2)^x(1-0.2)^{15-x}, \\quad x = 0, 1, 2, \\dots, n}\\]\n\n\n\n\nFigureÂ 9.2: Binomial Distribution Example \\(X \\sim binomial(15, 0.2)\\)\n\n\n\n\n\n\\(\\small P(X = 6) = \\frac{n!}{x!(n-x)!}\\pi^x(1-\\pi)^{n-x} = \\frac{15!}{6!(15-6)!}(0.2)^6(1-0.2)^{15-6} = 0.043\\)\n\\(\\small P(X \\ge 6) = p(6) + \\dots + p(15) = 1 - P(X \\le 5) = 1 - (p(0) + p(1) + \\dots + p(5)) = 0.0611\\)\n\n\n\n\n\n\n\nTip\n\n\n\nNever do this by hand. We can compute them using R!\n\n\n\n Computation in R\n\n\nWith size being the number of trials and prob being the probability of success,\n\nuse dbinom(x, size, prob) to compute \\(P(X = x)\\)\n\nuse pbinom(q, size, prob) to compute \\(P(X \\le q)\\)\n\nuse pbinom(q, size, prob, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n\n\n## 1. P(X = 6)\ndbinom(x = 6, size = 15, prob = 0.2) \n\n[1] 0.04299262\n\n## 2. P(X >= 6) = 1 - P(X <= 5)\n1 - pbinom(q = 5, size = 15, prob = 0.2) \n\n[1] 0.06105143\n\n\n\n\n\n\n## 2. P(X >= 6) = P(X > 5)\npbinom(q = 5, size = 15, prob = 0.2, \n       lower.tail = FALSE)  \n\n[1] 0.06105143\n\n\n\n\n\nBelow is an example of how to generate the binomial probability distribution as a graph.\n\n\nplot(x = 0:15, y = dbinom(0:15, size = 15, prob = 0.2), \n     type = 'h', xlab = \"x\", ylab = \"P(X = x)\", \n     lwd = 5, main = \"Binomial(15, 0.2)\")"
  },
  {
    "objectID": "prob-disc.html#poisson-distribution",
    "href": "prob-disc.html#poisson-distribution",
    "title": "9Â  Discrete Probability Distributions",
    "section": "\n9.3 Poisson Distribution",
    "text": "9.3 Poisson Distribution\n Poisson Random Variables \n\nIf we want to count the number of occurrences of some event over a unit of time or space (region) and observe its associated probability, we could consider the Poisson distribution.\nFor example,\n\nThe number of COVID patients arriving at ICU in one hour\nThe number of Marquette students logging onto D2L in one day\nThe number of dandelions per square meter on Marquetteâ€™s campus\n\n\nLet \\(X\\) be a Poisson random variable. Then \\(\\color{blue}{X \\sim Poisson(\\lambda)}\\), where \\(\\lambda\\) is the parameter representing the mean number of occurrences of the event in the interval. \\[\\color{blue}{P(X = x \\mid \\lambda) = \\frac{\\lambda^x e^{-\\lambda}}{x!}, \\quad x = 0, 1, 2, \\dots}\\]\n\nBoth the mean and the variance are equal to \\(\\lambda\\).\n\n\n Assumptions and Properties of Poisson Variables \n\nðŸ‘‰ Events occur one at a time; two or more events do not occur at the same time or in the same space or spot.\nðŸ‘‰ The occurrence of an event in a given period of time or region of space is independent of the occurrence of the event in a nonoverlapping time period or region of space.\nðŸ‘‰ \\(\\lambda\\) is constant for any period or region.\n\n\n\n\n\n\n\nWhat are the differences between Binomial and Poisson distributions?\n\n\n\n\nThe Poisson distribution\n\nis determined by one single parameter \\(\\lambda\\)\n\nhas possible values \\(x = 0, 1, 2, \\dots\\) with no upper limit (countable), while a binomial variable has possible values \\(0, 1, 2, \\dots, n\\) (finite)\n\n\n\n\n\n\n Example \n\n\n\nLast year there were 4200 births at the University of Wisconsin Hospital. Assume \\(X\\) be the number of births in a given day at the center, and \\(X \\sim Poisson(\\lambda)\\). Find\n\n\n\\(\\lambda\\), the mean number of births per day.\nthe probability that on a randomly selected day, there are exactly 10 births.\n\n\\(P(X > 10)\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\small \\lambda = \\frac{\\text{Number of birth in a year}}{\\text{Number of days}} = \\frac{4200}{365} = 11.5\\)\n\\(\\small P(X = 10 \\mid \\lambda = 11.5) = \\frac{\\lambda^x e^{-\\lambda}}{x!} = \\frac{11.5^{10} e^{-11.5}}{10!} = 0.113\\)\n\\(\\small P(X > 10) = p(11) + p(12) + \\dots + p(20) + \\dots\\) (No end!) \\(\\small P(X > 10) = 1 - P(X \\le 10) = 1 - (p(1) + p(2) + \\dots + p(10))\\).\n\n\n Computation in R \n\n\n\nWith lambda being the mean of Poisson distribution,\n\nuse dpois(x, lambda) to compute \\(P(X = x)\\)\n\nuse ppois(q, lambda) to compute \\(P(X \\le q)\\)\n\nuse ppois(q, lambda, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n\n\n(lam <- 4200 / 365)\n\n[1] 11.50685\n\n## P(X = 10)\ndpois(x = 10, lambda = lam)  \n\n[1] 0.112834\n\n\n\n\n\n\n## P(X > 10) = 1 - P(X <= 10)\n1 - ppois(q = 10, lambda = lam)  \n\n[1] 0.5990436\n\n## P(X > 10)\nppois(q = 10, lambda = lam, \n      lower.tail = FALSE) \n\n[1] 0.5990436\n\n\n\n\n\nBelow is an example of how to generate the Poisson probability distribution as a graph.\n\n\nplot(0:24, dpois(0:24, lambda = lam), type = 'h', lwd = 5, \n     ylab = \"P(X = x)\", xlab = \"x\", main = \"Poisson(11.5)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(X\\) has no upper limit; the graph is truncated at \\(x = 24\\)."
  },
  {
    "objectID": "prob-disc.html#exercises",
    "href": "prob-disc.html#exercises",
    "title": "9Â  Discrete Probability Distributions",
    "section": "\n9.4 Exercises",
    "text": "9.4 Exercises\n\nData collected by the Substance Abuse and Mental Health Services Administration (SAMSHA) suggests that 65% of 18-20 year olds consumed alcoholic beverages in any given year.\n\nSuppose a random sample of twelve 18-20 year olds is taken. When does it make sense to use binomial distribution for calculating the probability that exactly five consumed alcoholic beverages?\nWhat is the probability that exactly five out of twelve 18-20 year olds have consumed an alcoholic beverage?\nWhat is the probability that at most 3 out of 7 randomly sampled 18-20 year olds have consumed alcoholic beverages?\n\n\nA Dunkinâ€™ Donuts in Milwaukee serves an average of 65 customers per hour during the morning rush.\n\nWhich distribution have we studied that is most appropriate for calculating the probability of a given number of customers arriving within one hour during this time of day?\nWhat are the mean and the standard deviation of the number of customers this Starbucks serves in one hour during this time of day?\nCalculate the probability that this Dunkinâ€™ Donuts serves 55 customers in one hour during this time of day."
  },
  {
    "objectID": "prob-cont.html#introduction",
    "href": "prob-cont.html#introduction",
    "title": "10Â  Continuous Probability Distributions",
    "section": "\n10.1 Introduction",
    "text": "10.1 Introduction\n\nA continuous random variable can take on any values from an interval of the real line.\nInstead of probability functions, a continuous random variable, \\(X\\), has the probability density function (pdf) \\(f(x)\\) such that for any real value \\(a < b\\), \\[P(a < X < b) = \\int_{a}^b f(x) dx\\]\nThe cumulative distribution function (cdf) of \\(X\\) is defined as \\[F(x) := P(X \\le x) = \\int_{-\\infty}^x f(t)dt\\]\nEvery probability density function must satisfy  (1) \\(f(x) \\ge 0\\) for all \\(x\\); (2) \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\) \nðŸ˜Ž Luckily, we donâ€™t deal with integrals in this course.\n\n\n Density Curve \n\nA probability distribution function generates a graph called a density curve that shows the likelihood of a random variable at all possible values.\n\nThe area under the density curve between \\(a\\) and \\(b\\): \\(P(a < X < b) = \\int_{a}^b f(x) dx\\).\n\nThe total area under any density curve is equal to 1: \\(\\int_{-\\infty}^{\\infty} f(x) dx = 1\\)\n\n\n\n\n\n\nFigureÂ 10.1: Density curve for a random variable\n\n\n\n\n\n Commonly Used Continuous Distributions \n\nR Shiny app is a Continuous Distribution calculator.\nIn this course, we will touch on normal (Gaussian), studentâ€™s t, chi-square, F\n\nSome other common distributions include uniform, exponential, gamma, beta, inverse gamma, Cauchy, etc. (MATH 4700)"
  },
  {
    "objectID": "prob-cont.html#normal-gaussian-distribution",
    "href": "prob-cont.html#normal-gaussian-distribution",
    "title": "10Â  Continuous Probability Distributions",
    "section": "\n10.2 Normal (Gaussian) Distribution",
    "text": "10.2 Normal (Gaussian) Distribution\n\nThe normal distribution, \\(N(\\mu, \\sigma^2\\)), has the probability distribution function given by \\[\\small f(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{\\frac{-(x-\\mu)^2}{2\\sigma^2}}, \\quad -\\infty < x < \\infty\\]\n\nThis distribution has two parameters: mean, \\(\\mu\\), and variance, \\(\\sigma^2\\) (standard deviation \\(\\sigma\\)).\nIt is always bell-shaped and symmetric about the mean, \\(\\mu\\).\nWhen \\(\\mu = 0\\) and \\(\\sigma = 1\\), \\(N(0, 1)\\) is called standard normal.\n\n\nBelow are examples of normal distribution curves and how they change with different means and standard deviations.\n\n\n\n\n\nFigureÂ 10.2: Normal density curve with mean 100 and standard deviation 15\n\n\n\n\n\n\n\n\nFigureÂ 10.3: Normal density curves with varying means and standard deviations"
  },
  {
    "objectID": "prob-cont.html#standardization-and-z-scores",
    "href": "prob-cont.html#standardization-and-z-scores",
    "title": "10Â  Continuous Probability Distributions",
    "section": "\n10.3 Standardization and Z-Scores",
    "text": "10.3 Standardization and Z-Scores\n\n\nStandardization allows us to convert \\(N(\\mu, \\sigma^2)\\) to \\(N(0, 1)\\).\n\nWhy do we perform standardization?\n\nWe want to put data on a standardized scale, because it helps us make comparisons more easily!\n\n\nIf \\(x\\) is an observation from a distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\), the standardized value of \\(x\\) is its so-called \\(z\\)-score: \\[z = \\frac{x - \\mu}{\\sigma}\\]\n\nThe \\(z\\)-score tells us how many standard deviations \\(x\\) falls away from the mean and in which direction.\n\nObservations larger than the mean have positive \\(z\\)-scores.\nObservations smaller than the mean have negative \\(z\\)-scores.\nA \\(z\\)-score -1.2 means that \\(x\\) is 1.2 standard deviations to the left of or below the mean.\nA \\(z\\)-score 1.8 means that \\(x\\) is 1.8 standard deviations to the right of or above the mean.\n\n\n If \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma}\\) follows the standard normal distribution \\(Z \\sim N(0, 1)\\). \n\n\n Illustration \n\n\n\\(X - \\mu\\) shifts the mean from \\(\\mu\\) to 0.\n\n\n\n\n\nFigureÂ 10.4: Standardization shifts mean from 3 to 0\n\n\n\n\n\n\n\\(\\frac{X - \\mu}{\\sigma}\\) scales the variation from 4 to 1.\n\n\n\n\n\nFigureÂ 10.5: Standardization scales variance from 4 to 1\n\n\n\n\n\nA value of \\(x\\) that is 2 standard deviation below the mean, \\(\\mu\\), corresponds to \\(z = -2\\).\n\n\n\\(z = \\frac{x -\\mu}{\\sigma} \\iff x = \\mu + z\\sigma\\). If \\(z = -2\\), \\(x = \\mu - 2\\sigma\\).\n\n\n\nFigureÂ 10.6 depicts how the values on the x-axis change when standardization is performed.\n\n\n\n\n\nFigureÂ 10.6: Standardized Normal Distribution\n\n\n\n\n\n SAT and ACT Example \n\nStandardization can help us compare the performance of students on the SAT and ACT, which both have nearly normal distributions.\n\nThe table below lists the parameters for each distribution.\n\n\n\n\n\nMeasure\nSAT\nACT\n\n\n\nMean\n1100\n21\n\n\nSD\n200\n6\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to determine whether Anna or Tommy performed better on their respective tests.\n\nAnna scored a 1300 on her SAT and Tommy scored a 24 on his ACT.\n\n\n\n Standardization \n\n\n\\(z_{A} = \\frac{x_{A} - \\mu_{SAT}}{\\sigma_{SAT}} = \\frac{1300-1100}{200} = 1\\); \\(z_{T} = \\frac{x_{T} - \\mu_{ACT}}{\\sigma_{ACT}} = \\frac{24-21}{6} = 0.5\\).\n\n\n\n\n\nFigureÂ 10.7: Standardization of Anna and Tommyâ€™s scores\n\n\n\n\n\nThis standardization tells us that Anna scored 1 standard deviation above the mean and Tommy scored 0.5 standard deviations above the mean.\nFrom this information, we can conclude that Anna performed better on teh SAT than Tommy performed on the ACT."
  },
  {
    "objectID": "prob-cont.html#tail-areas-and-normal-percentiles",
    "href": "prob-cont.html#tail-areas-and-normal-percentiles",
    "title": "10Â  Continuous Probability Distributions",
    "section": "\n10.4 Tail Areas and Normal Percentiles",
    "text": "10.4 Tail Areas and Normal Percentiles\n Finding Tail Areas \\(P(X < x)\\) \n\nFinding tail areas allows us to determine the percentage of cases that are above or below a certain score.\nGoing back to the SAT and ACT example, this can help us determine the fraction of students have an SAT score below Annaâ€™s score of 1300.\n\nThis is the same as determining what percentile Anna scored at, which is the percentage of cases that had lower scores than Anna.\n\n\nTherefore, we are looking for \\(P(X < 1300 \\mid \\mu = 1100, \\sigma = 200)\\) or \\(P(Z < 1 \\mid \\mu = 0, \\sigma = 1)\\).\n\nWe can calculate this value by using R.\n\n\n\n\n\n\n\nFigureÂ 10.8: Tail area for scores below 1300\n\n\n\n\n Calculation in R \n\nWith mean and sd representing the mean and standard deviation of a normal distribution,\n\nuse pnorm(q, mean, sd) to compute \\(P(X \\le q)\\)\n\nuse pnorm(q, mean, sd, lower.tail = FALSE) to compute \\(P(X > q)\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\npnorm(1, mean = 0, sd = 1)\n\n[1] 0.8413447\n\npnorm(1300, mean = 1100, sd = 200)\n\n[1] 0.8413447\n\n\n\nThe shaded area represents the 84.1% of SAT test takers who had z-score below 1.\n\n\n\n Second ACT and SAT Example \n\n\n\n\n\n\nWhat is the probability Shannon SAT scores at least 1190?\n\n\n\n\nSAT score follows \\(N(1100, 200^2)\\).\nShannon is an SAT taker, and nothing else is known about her SAT aptitude.\n\n\n\n\n\n Step 1: State the problem \n\n We want to compute \\(P(X \\ge 1190)\\). \n\n\n Step 2: Draw a picture\n\n\n\n\n\nFigureÂ 10.9: Tail area for scores greater than 1190\n\n\n\n\n\n\n\n\nFigureÂ 10.10: Method to determine right tail areas\n\n\n\n\n\n\n Step 3: Find \\(z\\)-score \n\n \\(z = \\frac{1190 - 1100}{200} = 0.45\\) and we want to compute \\(P(X > 1190) = P\\left( \\frac{X - \\mu}{\\sigma} > \\frac{1190 - 1000}{200} \\right) = P(Z > 0.45) = 1 - P(Z \\le 0.45)\\) \n\n\n Step 4: Find the area using pnorm() \n\n\n1 - pnorm(0.45)\n\n[1] 0.3263552\n\n\n\n Normal Percentiles in R \n\nTo get the \\(100p\\)-th percentile (or the \\(p\\) quantile \\(q\\) ), given probability \\(p\\),\n\nuse qnorm(p, mean, sd) to get a value of \\(X\\), \\(q\\), such that \\(P(X \\le q) = p\\)\n\nuse qnorm(p, mean, sd, lower.tail = FALSE) to get \\(q\\) such that \\(P(X \\ge q) = p\\)\n\n\n\n\n SAT and ACT Example \n\nWhat is the 95th percentile for SAT scores?\n\n Step 1: State the problem \n\n We want to find \\(x\\) s.t \\(P(X < x) = 0.95\\). \n\n\n Step 2: Draw a picture\n\n\n\n\n\nFigureÂ 10.11: Picture for the 95th percentile of SAT scores\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe want to find an \\(x\\) value of the normal distribution, which is greater than 95% of all other cases.\n\n\n\n\n Step 3: Find \\(z\\)-score s.t. \\(P(Z < z) = 0.95\\) using qnorm():\n\n\n(z_95 <- qnorm(0.95))\n\n[1] 1.644854\n\n\n\n\n Step 4: Find the \\(x\\) of the original scale \n\n \\(z_{0.95} = \\frac{x-\\mu}{\\sigma}\\). So \\(x = \\mu + z_{0.95}\\sigma\\). \n\n\n\n\n(x_95 <- 1100 + z_95 * 200)\n\n[1] 1428.971\n\n\n\nThe 95th percentile for SAT scores is 1429.\n\n\nqnorm(p = 0.95, mean = 1100, sd = 200)\n\n[1] 1428.971"
  },
  {
    "objectID": "prob-cont.html#finding-probabilties",
    "href": "prob-cont.html#finding-probabilties",
    "title": "10Â  Continuous Probability Distributions",
    "section": "\n10.5 Finding Probabilties",
    "text": "10.5 Finding Probabilties\nðŸ‘‰ ALWAYS draw and label the normal curve and shade the area of interest.\n\nðŸ‘‰ Less than\n\n\\(\\small P(X < x) = P(Z < z)\\)\npnorm(z, mean = 0, sd = 1)\npnorm(x, mean = mu, sd = sigma)\n\n\nðŸ‘‰ Greater than\n\n\\(\\small P(X > x) = P(Z > z) = 1 - P(Z \\le z)\\)\n1 - pnorm(z)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nStandardization is not a must.\nIf we donâ€™t standardize, we must specify the mean and SD of the original distribution of \\(X\\), like pnorm(x, mean = mu, sd = sigma).\n\n\n\n\n\nðŸ‘‰ Between two numbers\n\n\\(\\small P(a < X < b) = P(z_a < Z < z_b) = P(Z < z_b) - P(Z < z_a)\\)\npnorm(z_b) - pnorm(z_a)\n\n\n\nðŸ‘‰ Outside of two numbers \\((a < b)\\) \\[\\small \\begin{align} P(X < a \\text{ or } X > b) &= P(Z < z_a \\text{ or } Z > z_b) \\\\ &= P(Z < z_a) + P(Z > z_b) \\\\ &= P(Z < z_a) + 1 - P(Z < z_b) \\end{align}\\]\n\npnorm(z_a) + 1 - pnorm(z_b)\npnorm(z_a) + pnorm(z_b, lower.tail = FALSE)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nAny probability can be computed using the â€œless thanâ€ form (lower or left tail).\nIf the calculation involves the â€œgreater thanâ€ form, add lower.tail = FALSE in pnorm()."
  },
  {
    "objectID": "prob-cont.html#checking-normality-normal-quantile-plot",
    "href": "prob-cont.html#checking-normality-normal-quantile-plot",
    "title": "10Â  Continuous Probability Distributions",
    "section": "\n10.6 Checking Normality: Normal Quantile Plot",
    "text": "10.6 Checking Normality: Normal Quantile Plot\n\nMany statistical methods assume variables are normally distributed.\nTherefore, testing the appropriateness of the normal assumption is a key step.\nWe can check this normality assumption using a normal quantile plot (normal probability plot) or a Quantile-Quantile plot (QQ plot).\n\n\n\\(X\\)-axis: Quantiles of the ordered data if the data were normally distributed.\n\n\\(Y\\)-axis: Ordered data values\n\n\nIf the data are normally distributed, the points on the QQ plot will lie close to a straight line.\n\n\n QQ plot in R \n\nqqnorm(normal_sample, main = \"Normal data\", col = 4)\nqqline(normal_sample)\nqqnorm(right_skewed_sample, main = \"Right-skewed data\", col = 6)\nqqline(right_skewed_sample)\n\n\n\nFigureÂ 10.12: QQ plots for normal and right-skewed data samples"
  },
  {
    "objectID": "prob-cont.html#exercises",
    "href": "prob-cont.html#exercises",
    "title": "10Â  Continuous Probability Distributions",
    "section": "\n10.7 Exercises",
    "text": "10.7 Exercises\n\nWhat percentage of data that follow a standard normal distribution \\(N(\\mu=0, \\sigma=1)\\) is found in each region? Drawing a normal graph may help.\n\n\\(Z < -1.75\\)\n\\(-0.7 < Z < 1.3\\)\n\\(|Z| > 1\\)\n\n\nThe average daily high temperature in June in Chicago is 74\\(^{\\circ}\\)F with a standard deviation of 4\\(^{\\circ}\\)F. Suppose that the temperatures in June closely follows a normal distribution.\n\nWhat is the probability of observing an 81\\(^{\\circ}\\) F temperature or higher in Chigcago during a randomly chosen day in June?\nHow cool are the coldest 15% of the days (days with lowest average high temperature) during June in Chicago?\n\n\nHead lengths of Virginia opossums follow a normal distribution with mean 104 mm and standard deviation 6 mm.\n\nCompute the \\(z\\)-scores for opossums with head lengths of 97 mm and 108 mm.\nWhich observation (97 mm or 108 mm) is more unusual or less likely to happen than another observation? Why?"
  },
  {
    "objectID": "prob-samdist.html#introduction",
    "href": "prob-samdist.html#introduction",
    "title": "\n11Â  Sampling Distribution\n",
    "section": "\n11.1 Introduction",
    "text": "11.1 Introduction\n Parameter \n\nA parameter is a number that describes a probability distribution.\n\n Binomial: two parameters, \\(n\\) and \\(\\pi\\) \n Poisson: one parameter, \\(\\lambda\\) \n Normal: two parameters, \\(\\mu\\) and \\(\\sigma\\) \n\n\nIn statistics, we usually assume our target population follows some distribution, but its parameters are unknown to us.\n\n\n\n\n Human weight follows \\(N(\\mu, \\sigma^2)\\) \n\n\n\n\n\n\n\n\n\n\n\n # of snowstorms in one year follows \\(Poisson(\\lambda)\\) \n\n\n\n\n\n\n\n\n\n\n\n\n Treat Each Data Point as a Random Variable \n\n\n\\(n\\) random variables: \\(X_1, X_2, \\dots, X_n\\).\nAssume \\(X_1, X_2, \\dots, X_n\\) follow the same distribution.\nView \\(X_i\\) as a data point to be drawn from a population with some distribution, say \\(N(\\mu, \\sigma^2)\\).\n\n\n\n\n\nFigureÂ 11.1: Illustration of sampling from a population\n\n\n\n\n\nAssume that \\(X_1, X_2, \\dots, X_n\\) are independent, meaning the distribution/value of \\(X_i\\) is not affected by any other \\(X_j\\).\nWith the same distribution, \\(X_1, X_2, \\dots, X_n\\) are independent and identically distributed (iid).\n\n \\(X_1, X_2, \\dots, X_n \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) \n\n\n\n\\((X_1, X_2, \\dots, X_n)\\) is a random sample of size \\(n\\) from the population.\n\n Example: \\(X_1, X_2, \\dots, X_{50}\\) are randomly selected SAT scores from the SAT score population that follows \\(N(1100, 200^2)\\) \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\nBefore we actually collect the data, the data \\(X_1, X_2, \\dots, X_n\\) are random variables from the population distribution \\(N(\\mu, \\sigma^2)\\).\nOnce we collect the data, we know the realized value of these random variables: \\(x_1, x_2, \\dots, x_n\\)."
  },
  {
    "objectID": "prob-samdist.html#sampling-distribution",
    "href": "prob-samdist.html#sampling-distribution",
    "title": "\n11Â  Sampling Distribution\n",
    "section": "\n11.2 Sampling Distribution",
    "text": "11.2 Sampling Distribution\n\nAny value computed from a sample \\((X_1, X_2, \\dots, X_n)\\) is called a (sample) statistic.\n\n The sample mean \\(\\frac{1}{n}\\sum_{i=1}^n X_i\\) is a statistic. \n Sample variance \\(\\frac{\\sum_{i=1}^n \\left(X_i - \\overline{X}\\right)^2}{n-1}\\) is also a statistic. \n\n\nSince \\(X_1, X_2, \\dots, X_n\\) are random variables, any transformation or function of \\((X_1, X_2, \\dots, X_n)\\) or its statistics is also a random variable.\nThe probability distribution of a statistic is called the sampling distribution of that statistic.\n\nIt is the probability distribution of that statistic if we were to repeatedly draw samples of the same size from the population.\n\n\n\n\n\n\n\n\n\nDoes the sample mean \\(\\overline{X} = \\frac{1}{n}\\sum_{i=1}^n X_i\\) have a sampling distribution?\n\n\n\n\nYes!\n\n\n\n\n Sampling Distribution of Sample Mean \n\n\n\n\nFigureÂ 11.2: Sampling distribution of sample means (Biostatistics for the Biological and Health Sciences p.241)\n\n\n\n\n\nSampling Distribution Applet\n\n\n\n\n\n\n\nWhat are the differences between the sampling distribution of \\(\\overline{X}\\) and the population distribution each individual random variable, \\(X_i\\), is drawn from?\n\n\n\n\nSample means \\((\\overline{X})\\) are  less variable  than individual observations \\(X_i\\).\nSample means \\((\\overline{X})\\) are  more normal  than individual observations \\(X_i\\).\n\n\n\n\nSuppose \\((X_1, \\dots, X_n)\\) is the random sample from a population distribution with mean, \\(\\mu\\), and standard deviation, \\(\\sigma\\).\nThe mean of the sampling distribution of the sample mean, \\(\\overline{X} = \\frac{\\sum_{i=1}^nX_i}{n}\\), is  \\(\\mu_{\\overline{X}} = \\mu\\) .\nThe standard deviation of the sampling distribution of the sample mean, \\(\\overline{X}\\), is  \\(\\sigma_{\\overline{X}} = \\frac{\\sigma}{\\sqrt{n}}\\) .\n\n\n\\(\\sigma_{\\overline{X}}\\) is also known as the standard error of \\(\\overline{X}\\).\n\n\nIf the population distribution is  \\(N(\\mu, \\sigma^2)\\) , the sampling distribution of \\(\\overline{X}\\) is  \\(N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\) .\n\nFigureÂ 11.3 depicts that sampling distributions are less variable and more normal than the population distribution.\n\n\n\n\n\nFigureÂ 11.3: Comparison between sampling distributions and the population distribution\n\n\n\n\n Example: Rolling a Die \n\nRoll a fair die 3 times ðŸŽ²ðŸŽ² ðŸŽ² independently to obtain 3 values from the population \\(\\{1, 2, 3, 4, 5, 6\\}\\).\nRepeat the process 10,000 times and plot the histogram of the sampling mean.\n\nFigureÂ 11.4 shows that the population distribution is not normal, but the sampling distribution of the sample mean is normal.\n\n\n\n\n\n\n\nFigureÂ 11.4: Population distribution is Binomial\n\n\n\n\n\n\n\n\n\nFigureÂ 11.5: Sampling distribution of sample means is normal"
  },
  {
    "objectID": "prob-samdist.html#standardization-of-sample-mean",
    "href": "prob-samdist.html#standardization-of-sample-mean",
    "title": "\n11Â  Sampling Distribution\n",
    "section": "\n11.3 Standardization of Sample Mean",
    "text": "11.3 Standardization of Sample Mean\n\nFor a single random variable \\(X \\sim N(\\mu, \\sigma^2)\\), \\(Z = \\frac{X - \\mu}{\\sigma} \\sim N(0, 1)\\).\nFor the sample mean of \\(n\\) variables, \\(\\overline{X} \\sim N(\\mu_{\\overline{X}}, \\sigma^2_{\\overline{X}}) = N(\\mu, \\frac{\\sigma^2}{n})\\).\nHence,  \\[Z = \\frac{\\overline{X} - \\mu_{\\overline{X}}}{\\sigma_{\\overline{X}}} = \\frac{\\overline{X} - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0, 1)\\]\n\n\n Example: Psychomotor Retardation \n\n\n\nPsychomotor retardation scores for a group of patients have a normal distribution with a mean of 930 and a standard deviation of 130.\nWhat is the probability that the mean retardation score of a random sample of 20 patients was between 900 and 960?\n\n\\(X_1, \\dots, X_{20} \\stackrel{iid}{\\sim} N(930, 130^2)\\), then \\(\\overline{X} = \\frac{\\sum_{i=1}^{20}X_i}{20} \\sim N\\left(930, \\frac{130^2}{20} \\right)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\\small \\begin{align}\nP(900 < \\overline{X} < 960) &= P\\left( \\frac{900-930}{130/\\sqrt{20}} < \\frac{\\overline{X}-930}{130/\\sqrt{20}} < \\frac{960-930}{130/\\sqrt{20}}\\right)=P(-1.03 < Z < 1.03)\\\\\n&=P(Z < 1.03) - P(Z < -1.03)\n  \\end{align}\\]\n\npnorm(1.03) - pnorm(-1.03)\n\n[1] 0.69699\n\n\n\npnorm(960, mean = 930, sd = 130/sqrt(20)) - pnorm(900, mean = 930, sd = 130/sqrt(20))\n\n[1] 0.6979426\n\n\n\nThe probability that the mean psychomotor retardation score of a random sample of 20 patients is between 900 and 960 is about 70%."
  },
  {
    "objectID": "prob-samdist.html#exercises",
    "href": "prob-samdist.html#exercises",
    "title": "\n11Â  Sampling Distribution\n",
    "section": "\n11.4 Exercises",
    "text": "11.4 Exercises\n\nHead lengths of Virginia possums follow a normal distribution with mean 104 mm and standard deviation 6 mm.\n\nWhat is the sampling distribution of the sample mean of the head length when the sample size \\(n = 18\\)?\n\n\nAssume that females have pulse rates that are normally distributed with a mean of 76.0 beats per minute and a standard deviation of 11.5 beats per minute.\n\nIf 1 adult female is randomly selected, find the probability that her pulse rate is less than 81 beats per minute.\nIf 18 adult female are randomly selected, find the probability that their mean pulse rate is less than 81 beats per minute."
  },
  {
    "objectID": "prob-llnclt.html#central-limit-theorem",
    "href": "prob-llnclt.html#central-limit-theorem",
    "title": "12Â  Law of Large Numbers and Central Limit Theorem",
    "section": "\n12.1 Central Limit Theorem",
    "text": "12.1 Central Limit Theorem\n\nWe know if \\(X_i \\stackrel{iid}{\\sim} N(\\mu, \\sigma^2)\\) , then \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\).\n\n But what if the population distribution is NOT normal? \n\n\n\n\nThe central limit theorem (CLT) gives us the answer!\n\n\n\n\nCentral Limit Theorem (CLT):\n\nSuppose \\(\\overline{X}\\) is from a random sample of size \\(n\\) and from a population distribution having mean \\(\\mu\\) and standard deviation \\(\\sigma < \\infty\\).\nAs \\(n\\) increases, the sampling distribution of \\(\\overline{X}\\) looks more and more like \\(N(\\mu, \\sigma^2/n)\\) regardless of the distribution from which we are sampling!\n\n\n\n\n\n\n\n\nFigureÂ 12.1: Illustration of Central Limit Theorem (https://en.wikipedia.org/wiki/Central_limit_theorem#/media/File:IllustrationCentralTheorem.png)\n\n\n\n\n\n Illustration of the Central Limit Theorem \n\n\n\n\nFigureÂ 12.2: CLT Illustration: A Right-Skewed Distribution\n\n\n\n\n\n\n\n\nFigureÂ 12.3: CLT Illustration: A U-shaped Distribution\n\n\n\n\n\n Why is the Central Limit Theorem Important? \n\nMany well-developed statistical methods are based on the normal distribution assumption.\nWith the Central Limit Theorem, we can use these methods even if we are sampling from a non-normal distribution or if we have no idea what the population distribution is, provided that the sample size is large enough."
  },
  {
    "objectID": "prob-llnclt.html#central-limit-theorem-example",
    "href": "prob-llnclt.html#central-limit-theorem-example",
    "title": "12Â  Law of Large Numbers and Central Limit Theorem",
    "section": "\n12.2 Central Limit Theorem Example",
    "text": "12.2 Central Limit Theorem Example\n\n\n\nSuppose that the selling prices of houses in Milwaukee are known to have a mean of $382,000 and a standard deviation of $150,000.\nIn 100 randomly selected sales, what is the probability the average selling price is more than $400,000?\nSince the sample size is fairly large \\((n = 100)\\), by the Central Limit Theorem, the sampling distribution of the average selling price is approximately normal with a mean of $382,000 and a SD of \\(150,000 / \\sqrt{100}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(P(\\overline{X} > 400000) = P\\left(\\frac{\\overline{X} - 382000}{150000/\\sqrt{100}} > \\frac{400000 - 382000}{150000/\\sqrt{100}}\\right) \\approx P(Z > 1.2)\\) where \\(Z \\sim N(0, 1)\\).\n\n\npnorm(1.2, lower.tail = FALSE)\n\n[1] 0.1150697\n\npnorm(400000, mean = 382000, sd = 150000/sqrt(100), lower.tail = FALSE)\n\n[1] 0.1150697"
  },
  {
    "objectID": "infer-ci.html#foundations-for-inference",
    "href": "infer-ci.html#foundations-for-inference",
    "title": "13Â  Confidence Interval",
    "section": "\n13.1 Foundations for Inference",
    "text": "13.1 Foundations for Inference\n Inference Framework \n\n\nInferential statistics uses sample data to learn about an unknown population.\n\nIdea: Assume the target population follows some distribution but with unknown parameters.\n\n Assume the population is normally distributed but its mean and/or variance are unknown. \n\n\n\nGoal: Learn the unknown parameters of the assumed population distribution.\n\n\n\n\n\n\n\nFigureÂ 13.1: Sampling from a population\n\n\n\n\n\n\n\n\n\nFigureÂ 13.2: Relationship between probabiltity and statistical inference\n\n\n\n\n\n\n\nThere are two approaches in parameter learning.\n\nEstimation\nHypothesis Testing"
  },
  {
    "objectID": "infer-ci.html#point-estimator",
    "href": "infer-ci.html#point-estimator",
    "title": "13Â  Confidence Interval",
    "section": "\n13.2 Point Estimator",
    "text": "13.2 Point Estimator\n\n\n\n\n\n\nIf you could only use a single number to guess the unknown population mean, \\(\\mu\\), what number would you like to use?\n\n\n\n\n\n\n\nThe single point used to estimate the unknown parameter is known as a point estimator.\nA point estimator is any function of data \\((X_1, X_2, \\dots, X_n)\\).\n\n\nAny statistic is considered a point estimator (before actually being collected).\n\n\n\nA point estimate is a value of a point estimator used to estimate a population parameter.\n\nThis is a value calculated from the collected data.\n\n\nThe sample mean, \\((\\overline{X})\\), is a statistic and a point estimator for the population mean, \\(\\mu\\).\n\n\n Sample Mean as an Point Estimator \n\nDraw 5 values from the population that follows \\(N(2, 1)\\) as sample data \\((x_1, x_2, x_3, x_4, x_5)\\).\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n1.64\n1.84\n1.23\n0.83\n1.68\n1.45\n\n\n\n\n\n\n\\(\\mu = 2\\), and we use the point estimate \\(\\overline{x}=\\) 1.45 to estimate it.\n\n\n\n\n\n\n\nWhy is \\(\\overline{x}\\) not equal to \\(\\mu\\)?\n\n\n\n\nDue to its randomness nature\n\n\n\n\n\n\n\nFigureÂ 13.3: Sample mean has randomness associated with it\n\n\n\n\n\nIf another sample of size \\(5\\) is drawn from the same population,\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\nx5\nsample mean\n\n\n1.65\n1.41\n0.41\n3.69\n2.56\n1.95\n\n\n\n\n\nThe second sample mean, \\(\\overline{x} =\\) 1.95, is different from the first one.\n\n\n\n\n\n\n\nWhy do the first sample and the second sample give us different sample means?\n\n\n\n\nA point estimator has its own sampling distribution.\n\n\n\n\n\n\n\nFigureÂ 13.4: Sampling Distribution of Sampling Mean\n\n\n\n\n\n Why Point Estimates Are Not Enough \n\n\n\n\n\n\nIf you want to estimate \\(\\mu\\), would you prefer to report a range of values the parameter might be in or a single estimate like \\(\\overline{x}\\)?\n\n\n\n\n\n\n\n\n\n\n\n\nIf you want to catch a fish, would you prefer to use a spear or a net?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDue to the variation of \\(\\overline{X}\\), if we report a point estimate \\(\\overline{x}\\), we probably wonâ€™t hit the exact \\(\\mu\\).\nIf we report a range of plausible values, we have a better shot at capturing the parameter!"
  },
  {
    "objectID": "infer-ci.html#confidence-intervals",
    "href": "infer-ci.html#confidence-intervals",
    "title": "13Â  Confidence Interval",
    "section": "\n13.3 Confidence Intervals",
    "text": "13.3 Confidence Intervals\n\nA plausible range of values for \\(\\mu\\) is called a confidence interval (CI).\n\nThis range depends on how precise and reliable our statistic is as an estimate of the parameter.\n\n\nTo construct a CI for \\(\\mu\\), we first need to quantify the variability of our sample mean.\nQuantifying this uncertainty requires a measurement of how much we would expect the sample statistic to vary from sample to sample.\n\nThis is the variance of the sampling distribution of the sample mean!\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe larger the variation of \\(\\overline{X}\\) is, the wider the CI for \\(\\mu\\) will be given the same â€œlevel of confidenceâ€.\n\n\n\nDo we know the variance of \\(\\overline{X}\\)?\n\nBy CLT, \\(\\overline{X} \\sim N(\\mu, \\sigma^2/n)\\) regardless of what the population distribution is.\n\n\n\n\n Precision vs.Â Reliability \n\n\n\n\n\n\nIf we want to be very certain that we capture \\(\\mu\\), should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\n\n\n\n\nFigureÂ 13.5: Balance between precision and reliability\n\n\n\n\n\nWith a fixed sample size, precision and reliability have a trade-off relationship.\n\nNarrower intervals are more precise but less reliable, while wider intervals are more reliable but less precise.\n\n\n\n\n A Confidence Interval Is for a Parameter \n\nA confidence interval is for a parameter, NOT a statistic.\n\nFor example, we use the sample mean to form a confidence interval for the population mean.\n\n\nWe never say â€œThe confidence interval of the sample mean, \\(\\overline{X}\\), is â€¦â€\n\nWe say â€œThe confidence interval for the true population mean, \\(\\mu\\), is â€¦â€\n\nIn general, a confidence interval for \\(\\mu\\) has the form\n\n\n\\(\\large \\overline{x} \\pm m = (\\overline{x} - m, \\overline{x} + m)\\)\n\n\nThe \\(m\\) is called the margin of error.\n\n\\(\\overline{x} - m\\) is the lower bound and \\(\\overline{x} + m\\) is the upper bound of the confidence interval.\nThe point estimate, \\(\\overline{x}\\), and margin of error, \\(m\\), can be obtained from known quantities and our data once sampled.\n\n\n \\((1 - \\alpha)100\\%\\) Confidence Intervals \n\nThe confidence level \\(1-\\alpha\\): the proportion of times that the CI contains the population parameter, assuming that the estimation process is repeated a large number of times.\nCommon choices for the confidence level include\n\n90% \\((\\alpha = 0.10)\\)\n\n95% \\((\\alpha = 0.05)\\)\n\n99% \\((\\alpha = 0.01)\\)\n\n\n\n95% is the most common level because it has a good balance between precision (width of the CI) and reliability (confidence level).\n\n\n High reliability and Low precision: I am 100% confident that the mean height of Marquette students is between 3â€™0â€ and 8â€™0â€. \n\nDuhâ€¦ðŸ¤·\n\n\n\n Low reliability and High precision: I am 20% confident that mean height of Marquette students is between 5â€™6â€ and 5â€™7â€. \n\nThis is far from the truthâ€¦ ðŸ™…\n\n\n\n\n\n\n \\(95\\%\\) Confidence Intervals for \\(\\mu\\) \n Z-score \n\n\n\n\\(\\alpha = 0.05\\)\nStart with the sampling distribution of \\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n}\\right)\\)\n\n\n\\(\\overline{x}\\) will be within 1.96 SDs of the population mean, \\(\\mu\\), \\(95\\%\\) of the time.\nThe \\(z\\)-score of 1.96 is associated with 2.5% area to the right and is called a critical value denoted as \\(z_{0.025}\\) .\n\n\n\n\n\n\n\n\n\n Probability \n\n\n\\[P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} < \\overline{X} < \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\]\n\n\n\n\n\n\nIs the interval \\(\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}}, \\mu+1.96\\frac{\\sigma}{\\sqrt{n}} \\right)\\) our confidence interval?\n\n\n\n\nâŒ No! We donâ€™t know \\(\\mu\\), which is the quantity we want to estimate, but weâ€™re almost there!\n\n\n\n\n\n\n\n\n\n\n\n\n Formula\n\n\n\\[\\begin{align}\n&P\\left(\\mu-1.96\\frac{\\sigma}{\\sqrt{n}} < \\overline{X} < \\mu + 1.96\\frac{\\sigma}{\\sqrt{n}} \\right) = 0.95\\\\\n&P\\left( \\boxed{\\overline{X}-1.96\\frac{\\sigma}{\\sqrt{n}} < \\mu < \\overline{X} + 1.96\\frac{\\sigma}{\\sqrt{n}}} \\right) = 0.95\n\\end{align}\\]\n\n With sample data of size \\(n\\), \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\) is our \\(95\\%\\) CI for \\(\\mu\\) if \\(\\sigma\\) is known to us! \nThe margin of error \\(m = 1.96\\frac{\\sigma}{\\sqrt{n}}\\)."
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-known",
    "title": "13Â  Confidence Interval",
    "section": "\n13.4 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known",
    "text": "13.4 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Known\n\n\nRequirements for estimating \\(\\mu\\) when \\(\\sigma\\) is known:\n\nðŸ‘‰ The sample should be a random sample, such that all data \\(X_i\\) are drawn from the same population and \\(X_i\\) and \\(X_j\\) are independent.\n\n Any methods in this course are based on the assumption of a random sample \n\n\nðŸ‘‰ The population standard deviation, \\(\\sigma\\), is known.\nðŸ‘‰ The population is either normally distributed, \\(n > 30\\) or both, i.e., \\(X_i \\sim N(\\mu, \\sigma^2)\\).\n\n \\(n > 30\\) allows the Central Limit Theorem to be applied and hence normality is satisfied. \n\n\n\n\n\n\n \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\) \n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-1.96\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + 1.96\\frac{\\sigma}{\\sqrt{n}}\\right)\\)   \\(\\left(\\overline{x}-z_{0.025}\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + z_{0.025}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) \n\n\n\n\n\n\n\n\n \\(\\left(\\overline{x}-z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\overline{x} + z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}\\right)\\) \n\n\n\n\n\n\nProcedures for constructing a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is known:\n\nCheck that the requirements are satisfied.\nDecide \\(\\alpha\\) or the confidence level \\((1 - \\alpha)\\).\nFind the critical value, \\(z_{\\alpha/2}\\).\nEvaluate margin of error, \\(m = z_{\\alpha/2} \\cdot \\frac{\\sigma}{\\sqrt{n}}\\).\nConstruct the \\((1 - \\alpha)100\\%\\) CI for \\(\\mu\\) using the sample mean, \\(\\overline{x}\\), and margin of error, \\(m\\):\n\n\n\n\n \\[\\boxed{\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} \\text{  or  } \\left( \\overline{x} -z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}}, \\, \\overline{x} + z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\right)}\\]\n\n\n Example \n\n\n\nSuppose we want to know the mean systolic blood pressure (SBP) of a population.\nAssume that the population distribution is normal and has a standard deviation of 5 mmHg.\nWe have a random sample of 16 subjects from this population with a mean of 121.5 mmHg.\nEstimate the mean SBP with a 95% confidence interval.\n\n\n\n\n\n\n\n\n\n\nRequirements:\n\n Normality is assumed, \\(\\sigma = 5\\) is known and a random sample is collected.\n\n\nDecide \\(\\alpha\\):\n\n \\(\\alpha = 0.05\\) \n\n\nFind the critical value \\(z_{\\alpha/2}\\):\n\n \\(z_{\\alpha/2} = z_{0.025} = 1.96\\) \n\n\nEvaluate margin of error \\(m = z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\):\n\n \\(m = (1.96) \\frac{5}{\\sqrt{16}} = 2.45\\) \n\n\nConstruct the \\((1 - \\alpha)100\\%\\) CI:\n\n The 95% CI for the mean SBP is \\(\\overline{x} \\pm z_{\\alpha/2}\\frac{\\sigma}{\\sqrt{n}} = (121.5 -2.45, 121.5 + 2.45) = (119.05, 123.95)\\) \n\n\n\n Computation in R \n\nBelow is a demonstration of how to find the 95% CI for SBP using R.\n\n\n## save all information we have\nalpha <- 0.05\nn <- 16\nx_bar <- 121.5\nsig <- 5\n\n## 95% CI\n## z-critical value\n(cri_z <- qnorm(p = alpha / 2, lower.tail = FALSE))  \n\n[1] 1.959964\n\n## margin of error\n(m_z <- cri_z * (sig / sqrt(n)))  \n\n[1] 2.449955\n\n## 95% CI for mu when sigma is known\nx_bar + c(-1, 1) * m_z  \n\n[1] 119.05 123.95\n\n\n\n\n\n\n\n\nConstruct a 99% CI for the mean SBP. Do you expect it to have a wider or narrower interval than the 95% CI? Why?\n\n\n\n\n\n\n Interpreting the Confidence Interval\n\n\nWRONG âŒ â€œThere is a 95% chance/probability that the true population mean will fall between 119.1 mm and 123.9 mm.â€\n\n\nWRONG âŒ â€œThe probability that the true population mean falls between 119.1 mm and 123.9 mm is 95%.â€\n\n ðŸ‘‰ The sample mean is a random variable with a sampling distribution, so it makes sense to compute a probability of it being in some interval. \n\n ðŸ‘‰ The population mean is unknown and FIXED, so we cannot assign or compute any probability of it. \n\nIf we were using Bayesian inference, a different inference method, we could compute a probability associated with \\(\\mu\\) because \\(\\mu\\) is treated as a random variable.\n\n\nInstead we say,  â€œWe are 95% confident that the mean SBP lies between 119.1 mm and 123.9 mm.â€ \n\nThis means if we were able to collect our data many times and build the corresponding CIs, we would expect that about 95% of those intervals would contain the true population parameter, which, in this case, is the mean systolic blood pressure.\n\n Remember: \\(\\overline{x}\\) varies from sample to sample and so does its corresponding CI .\n\nThis idea is shown in FigureÂ 13.6.\n\n\n\n\n\n\n\nFigureÂ 13.6: Illustration of 100 95% confidence intervals\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe never know with certainty that 95% of the intervals, or any single interval for that matter, contains the true population parameter."
  },
  {
    "objectID": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "href": "infer-ci.html#confidence-intervals-for-mu-when-sigma-is-unknown",
    "title": "13Â  Confidence Interval",
    "section": "\n13.5 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown",
    "text": "13.5 Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown\n\n\n\\(\\sigma^2 = \\frac{\\sum_{i=1}^{N}(x_i - \\mu)^2}{N}\\), where \\(N\\) is the population size.\nItâ€™s rare that we donâ€™t know \\(\\mu\\) but know \\(\\sigma\\), so what do we do if \\(\\sigma\\) is unknown?\n\nWe use the Student t distribution to construct a confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown.\n\n\nTo construct these confidence intervals we still need\n\nA random sample\nA population that is normally distributed and/or \\(n > 30\\).\n\n\n\n\n\n\n\n\n\nWhat is a natural estimator for the unknown \\(\\sigma\\)?\n\n\n\n\n\n\n\nWhen \\(\\sigma\\) is unknown, we use the sample standard deviation, \\(S = \\sqrt{\\frac{\\sum_{i=1}^{n}(X_i - \\overline{X})^2}{n-1}}\\), instead when constructing the CI.\n\n\n Student t Distribution \n\nIf the population is normally distributed or \\(n > 30\\),\n\n\\(\\overline{X} \\sim N\\left(\\mu, \\frac{\\sigma^2}{n} \\right)\\)\n\\(Z = \\frac{\\overline{X} - \\mu}{\\color{red}\\sigma/\\sqrt{n}} \\sim N(0, 1)\\)\n \\(T = \\frac{\\overline{X} - \\mu}{\\color{red}S/\\sqrt{n}} \\sim t_{n-1}\\) \n\n\\(t_{n-1}\\) denotes the Student t distribution with degrees of freedom (df) \\(n-1\\).\n\n\n\n Properties \n\nIt is symmetric about the mean 0 and bell-shaped like \\(N(0, 1)\\).\nIt has more variability than \\(N(0, 1)\\) (heavier tails and lower peak).\nThe variability is different for different sample sizes (degrees of freedom).\n\nAs \\(n \\rightarrow \\infty\\) \\((df \\rightarrow \\infty)\\), the Student t distribution approaches \\(N(0, 1)\\).\n\n\n\n\n\n\n\nFigureÂ 13.7: Student t distributions with various degrees of freedom\n\n\n\n\n Critical Values of \\(t_{\\alpha/2, n-1}\\) \n\nWhen \\(\\sigma\\) is unknown, we use \\(t_{\\alpha/2, n-1}\\) as the critical value, instead of \\(z_{\\alpha/2}\\).\n\n\n\n\n\nFigureÂ 13.8: Illustration of critical value for Student t distribution\n\n\n\n\n\n\n\n\n\n\nWith the same \\(\\alpha\\), is \\(t_{\\alpha, n-1}\\) or \\(z_{\\alpha}\\) larger?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLevel\nt df = 5\nt df = 15\nt df = 30\nt df = 1000\nt df = inf\nz\n\n\n\n90%\n2.02\n1.75\n1.70\n1.65\n1.64\n1.64\n\n\n95%\n2.57\n2.13\n2.04\n1.96\n1.96\n1.96\n\n\n99%\n4.03\n2.95\n2.75\n2.58\n2.58\n2.58\n\n\n\n\n\n\n \\((1-\\alpha)100\\%\\) Confidence Intervals for \\(\\mu\\) When \\(\\sigma\\) is Unknown \n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\mu\\) when \\(\\sigma\\) is unknown is  \\[\\left(\\overline{x} - t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}, \\overline{x} + t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\right)\\] \n\nGiven the same confidence level \\(1-\\alpha\\), \\(t_{\\alpha/2, n-1} > z_{\\alpha/2}\\), which means the confidence interval for \\(\\mu\\) is wider when \\(\\sigma\\) is unknown.\n\n\n\n\n\n\n\nNote\n\n\n\n\nWe are more â€œuncertainâ€ when doing inference about \\(\\mu\\) because donâ€™t have information about \\(\\sigma\\) and replacing it with \\(s\\) adds additional uncertainty.\n\n\n\n Computation in R \n\nBack to the systolic blood pressure (SBP) example. We have \\(n=16\\) and \\(\\overline{x} = 121.5\\).\nEstimate the mean SBP with a 95% confidence interval with unknown \\(\\sigma\\) and \\(s = 5\\).\n\n\n\nalpha <- 0.05\nn <- 16\nx_bar <- 121.5\ns <- 5  ## sigma is unknown and s = 5\n\n## t-critical value\n(cri_t <- qt(p = alpha / 2, df = n - 1, lower.tail = FALSE)) \n\n[1] 2.13145\n\n## margin of error\n(m_t <- cri_t * (s / sqrt(n)))  \n\n[1] 2.664312\n\n## 95% CI for mu when sigma is unknown\nx_bar + c(-1, 1) * m_t  \n\n[1] 118.8357 124.1643"
  },
  {
    "objectID": "infer-ci.html#summary",
    "href": "infer-ci.html#summary",
    "title": "13Â  Confidence Interval",
    "section": "\n13.6 Summary",
    "text": "13.6 Summary\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\) unknown\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\n\nPopulation Mean \\(\\mu\\)\n\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\n\nRemember to check if the population is normally distributed and/or \\(n>30\\).\nWhat if the population is not normal and \\(n \\le 30\\)?\n\nUse a so-called nonparametric method, for example bootstrapping."
  },
  {
    "objectID": "infer-ci.html#exercises",
    "href": "infer-ci.html#exercises",
    "title": "13Â  Confidence Interval",
    "section": "\n13.7 Exercises",
    "text": "13.7 Exercises\n\n\nHere are summary statistics for randomly selected weights of newborn boys: \\(n =207\\), \\(\\bar{x} = 30.2\\)hg (1hg = 100 grams), \\(s = 7.3\\)hg.\n\nCompute a 95% confidence interval for \\(\\mu\\), the mean weight of newborn boys.\nIs the result in (a) very different from the 95% confidence interval if \\(\\sigma = 7.3\\)?\n\n\nA 95% confidence interval for a population mean \\(\\mu\\) is given as (18.635, 21.125). This confidence interval is based on a simple random sample of 32 observations. Calculate the sample mean and standard deviation. Assume that all conditions necessary for inference are satisfied. Use the t-distribution in any calculations.\nA market researcher wants to evaluate car insurance savings at a competing company. Based on past studies he is assuming that the standard deviation of savings is $95. He wants to collect data such that he can get a margin of error of no more than $12 at a 95% confidence level. How large of a sample should he collect?\n\nThe 95% confidence interval for the mean rent of one bedroom apartments in Chicago was calculated as ($2400, $3200).\n\nInterpret the meaning of the 95% interval.\nFind the sample mean rent from the interval."
  },
  {
    "objectID": "infer-bt.html#the-r-user-interface",
    "href": "infer-bt.html#the-r-user-interface",
    "title": "14Â  Bootstrapping",
    "section": "14.1 The R User Interface",
    "text": "14.1 The R User Interface"
  },
  {
    "objectID": "infer-ht.html#introduction",
    "href": "infer-ht.html#introduction",
    "title": "15Â  Hypothesis Testing",
    "section": "\n15.1 Introduction",
    "text": "15.1 Introduction\n What is Hypothesis Testing? \n\nA hypothesis is a claim or statement about a property of a population, often the value of a population parameter.\n\n The mean body temperature of humans is less than \\(98.6^{\\circ}\\) F, or \\(\\mu < 98.6\\). \n Marquette studentsâ€™ IQ scores has standard deviation equal to 15, or \\(\\sigma = 15\\). \n\n\n\n\nThe null hypothesis, \\((H_0)\\), is a statement that the value of a parameter is\n\n\nequal to some claim value\nthe negation of the alternative hypothesis\noften a skeptical perspective to be tested\n\n\nThe alternative hypothesis, \\((H_1\\) or \\(H_a)\\), is a claim that the parameter is less than, greater than or not equal to some value.\n\nIt is usually our research hypothesis of some new scientific theory or finding.\n\n\n\n\n\n\n\n\n\nAre these \\(H_0\\) or \\(H_1\\) claims?\n\n\n\n\n The percentage of Marquette female students loving Japanese food is equal to 80%.\n On average, Marquette students consume less than 3 drinks per week. \n\n\n\n\n\nHypothesis testing 1 is a procedure to decide whether or not to reject \\(H_0\\) based on how much evidence there is against \\(H_0\\).\n\nIf the evidence is strong enough, we reject \\(H_0\\) in favor of \\(H_1\\).\n\n\n\n[1] Null Hypothesis Statistical Testing (NHST), statistical testing or test of significance.\n\n Example \n\n\n\nA person is charged with a crime.\n\nA jury decides whether the person is guilty or not.\nThe accused is assumed to be innocent until the jury declares otherwise.\nIf overwhelming evidence of the personâ€™s guilt can be shown, then the jury is expected to declare the person guilty; otherwise, the person is considered not guilty.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat should \\(H_0\\) and \\(H_1\\) be for this example?\n\n\n\\(H_0:\\) The person is  not guilty  ðŸ™‚\n\n\\(H_1:\\) The person is  guilty  ðŸ˜Ÿ\n\n\nThe evidence is  photos, videos, witnesses, fingerprints, DNA, etc. \n\nThe decision rule is the  juryâ€™s voting .\nThe conclusion is the verdict  â€œguiltyâ€  or  â€œNOT enough evidence to convictâ€ ."
  },
  {
    "objectID": "infer-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "href": "infer-ht.html#how-to-formally-do-a-statistical-hypothesis-testing",
    "title": "15Â  Hypothesis Testing",
    "section": "\n15.2 How to Formally Do a Statistical Hypothesis Testing",
    "text": "15.2 How to Formally Do a Statistical Hypothesis Testing\n\nStep 0: Check Method Assumptions\nStep 1: Set the \\(H_0\\) and \\(H_a\\) in Symbolic Form from a Claim\nStep 2: Set the Significance Level \\(\\alpha\\)\n\nStep 3: Calculate the Test Statistic (Evidence)\n\n\n\nDecision Rule I: Critical Value Method\n\n Step 4-c: Find the Critical Value \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n\nDecision Rule II: P-Value Method\n\n Step 4-p: Find the P-Value \n Step 5-p: Draw a Conclusion Using P-Value Method \n\n\n\n\nStep 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim\nðŸ˜Ž We will learn this step by step!\n\n\n Step 0: Check Method Assumptions \n\nThe testing methods are based on normality or approximate normality by CLT.\n\nRandom sample\nNormally distributed and/or \\(n > 30\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \n\nðŸ§‘â€ðŸ« The mean IQ score of statistics professors is higher than 120.\n\n \\(\\begin{align}&H_0: \\mu \\le 120 \\\\ &H_1: \\mu > 120 \\end{align}\\) \n\n\nðŸ’µ The mean starting salary for Marquette graduates who didnâ€™t take MATH 4720 is less than $60,000.\n\n \\(\\begin{align} &H_0: \\mu \\ge 60000 \\\\ &H_1: \\mu < 60000 \\end{align}\\) \n\n\nðŸ“º The mean time between uses of a TV remote control by males during commercials equals 5 sec.Â \n\n \\(\\begin{align} &H_0: \\mu = 5 \\\\ &H_1: \\mu \\ne 5 \\end{align}\\) \n\n\n\n\n Step 2: Set the Significance Level \\(\\alpha\\) \n\nThe significance level, \\(\\alpha\\), determines how rare or unlikely our evidence must be in order to represent sufficient evidence against \\(H_0\\).\nAn \\(\\alpha\\) level of 0.05 implies that evidence occurring with probability lower than 5% will be considered sufficient evidence to reject \\(H_0\\).\n\\(\\alpha = P(\\text{Reject } H_0 \\mid H_0 \\text{ is true})\\)\n\n\\(\\alpha = 0.05\\) means that we incorrectly reject \\(H_0\\) 5 out of every 100 times we collect a sample and run the test.\n\n\n\n\n\n\n\nFigureÂ 15.1: Illustration of significance level, alpha\n\n\n\n\n\n\n\n\n\n\n\nRare Event Rule\n\n\n\nIf, under a given assumption, the probability of a particular observed event is exceptionally small, we conclude that the assumption is probably not correct.\n\n\n\n\n\n Step 3: Calculate the Test Statistic \n\nA test statistic is a statistical value used in making a decision about the \\(H_0\\).\nSuppose  \\(H_0: \\mu = \\mu_0\\) and \\(\\quad H_1: \\mu < \\mu_0\\) .\nWhen computing a test statistic, we assume \\(H_0\\) is true.\nWhen \\(\\sigma\\) is known, the test statistic for testings about \\(\\mu\\) is\n\n\\[\\boxed{ z_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{\\sigma/\\sqrt{n}} }\\] - When \\(\\sigma\\) is unknown, the test statistic for testings about \\(\\mu\\) is\n\\[\\boxed{ t_{test} = \\frac{\\overline{x} - \\color{blue}{\\mu_0}}{s/\\sqrt{n}} }\\]\n\n Step 4-c: Find the Critical Value \n\nThe critical value(s) separates the rejection region or critical region, where we reject \\(H_0\\), from the values of the test statistic that do not lead to the rejection of \\(H_0\\).\n\nThese depend on whether the test is a right-tailed, left-tailed or two-tailed.\n\n\n\n\n\n\n\nFigureÂ 15.2: Rejection regions for the different types of hypothesis tests\n\n\n\n\n\n\n\\(z_{\\alpha}\\) is such that \\(P(Z > z_{\\alpha}) = \\alpha\\) and \\(Z \\sim N(0, 1)\\).\n\n\\(t_{\\alpha, n-1}\\) is such that \\(P(T > t_{\\alpha, n-1}) = \\alpha\\) and \\(T \\sim t_{n-1}\\).\n\n\n\n\n\n\n\n\n\nCondition Â  Â \n\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{\\alpha}\\)\n\\(-z_{\\alpha}\\)\n\n\\(-z_{\\alpha/2}\\) and \\(z_{\\alpha/2}\\)\n\n\n\n\n\\(\\sigma\\) unknown\n\\(t_{\\alpha, n-1}\\)\n\\(-t_{\\alpha, n-1}\\)\n\n\\(-t_{\\alpha/2, n-1}\\) and \\(t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\n\\(z_{0.025} =\\) 1.96, \\(z_{0.05} =\\) 1.64\n\n\\(z_{\\alpha}\\) and \\(t_{\\alpha, n-1}\\) are always positive.\n\n\n Step 5-c: Draw a Conclusion Using Critical Value \n\nIf the test statistic is\n\nin the rejection region, we reject \\(H_0\\).\n\nnot in the rejection region, we do not or fail to reject \\(H_0\\).\n\n\n\nReject \\(H_0\\) if\n\n\n\n\n\n\n\n\nCondition Â  Â \n\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(z_{test} > z_{\\alpha}\\)\n\\(z_{test} < -z_{\\alpha}\\)\n\\(\\mid z_{test}\\mid \\, > z_{\\alpha/2}\\)\n\n\n\n\\(\\sigma\\) unknown\n\\(t_{test} > t_{\\alpha, n-1}\\)\n\\(t_{test} < -t_{\\alpha, n-1}\\)\n\\(\\mid t_{test}\\mid \\, > t_{\\alpha/2, n-1}\\)\n\n\n\n\n\n\n\nFigureÂ 15.3: Test statistic inside of critical region\n\n\n\n\n\n Step 4-p: Find the P-Value \n\nThe \\(p\\)-value measures the strength of the evidence against \\(H_0\\) provided by the data.\n\n\nThe smaller the \\(p\\)-value, the greater the evidence against \\(H_0\\).\n\n\nThe \\(p\\)-value is the probability of getting a test statistic value that is at least as extreme as the one obtained from the data, assuming that \\(H_0\\) is true. \\((\\mu = \\mu_0)\\)\n\nFor example, \\(p\\)-value \\(= P(Z \\ge z_{test} \\mid H_0)\\) for a right-tailed test.\n\n\nWe are more likely to get a \\(p\\)-value near 0 when \\(H_0\\) is false than when \\(H_0\\) is true.\n\n P-Value Illustration \n\n\n\n\nFigureÂ 15.4: Illustration of p-values for different types of hypothesis tests\n\n\n\n\n\n Step 5-p: Draw a Conclusion Using P-Value Method \n\nIf the \\(p\\)-value \\(\\le \\alpha\\) , we reject \\(H_0\\).\nIf the \\(p\\)-value \\(> \\alpha\\), we do not reject or fail to reject \\(H_0\\).\n\n\n\n\n\n\n\n\n\nCondition Â  Â \n\nRight-tailed \\((H_1: \\mu > \\mu_0)\\)\n\n\nLeft-tailed \\((H_1: \\mu < \\mu_0)\\)\n\n\nTwo-tailed \\((H_1: \\mu \\ne \\mu_0)\\)\n\n\n\n\n\n\\(\\sigma\\) known\n\\(P(Z > z_{test} \\mid H_0)\\)\n\\(P(Z < z_{test} \\mid H_0)\\)\n\\(2P(Z > \\,\\mid z_{test} \\mid \\, \\mid H_0)\\)\n\n\n\n\\(\\sigma\\) unknown\n\\(P(T > t_{test} \\mid H_0)\\)\n\\(P(T < t_{test} \\mid H_0)\\)\n\\(2P(T > \\, \\mid t_{test} \\mid \\, \\mid H_0)\\)\n\n\n\n\n Both Methods Lead to the Same Conclusion \n\n\n\n\nFigureÂ 15.5: The conclusion is the same regardless of the method used (Critical Value or P-Value).\n\n\n\n\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n\n\n\nFigureÂ 15.6: Conclusions based on testing results (https://www.drdawnwright.com/category/statistics/)\n\n\n\n\n\n\nReminderâ€¦\n\n\n\n\n\nFigureÂ 15.7: Meme about hypothesis testing conclusions (https://www.pinterest.com/pin/287878601159173631/)"
  },
  {
    "objectID": "infer-ht.html#example-is-the-new-treatment-effective",
    "href": "infer-ht.html#example-is-the-new-treatment-effective",
    "title": "15Â  Hypothesis Testing",
    "section": "\n15.3 Example: Is the New Treatment Effective?",
    "text": "15.3 Example: Is the New Treatment Effective?\n\n\n\nA population of patients with hypertension is normal and has mean blood pressure (BP) of 150.\nAfter 6 months of treatment, the BP of 25 patients from this population was recorded.\n\n\n\\(\\overline{x} = 147.2\\) and \\(s = 5.5\\).\n\n\n\nGoal: Determine whether a new treatment is effective in reducing BP.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Step-by-Step \n Step 0: Check Method Assumptions \n\n\n A population of hypertension group is normal .\n\n Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \n\nThe claim that the new treatment is effective in reducing BP means the mean BP is less than 150, which is an \\(H_1\\) claim.\n\n \\(\\small \\begin{align} &H_0: \\mu = 150 \\\\ &H_1: \\mu < 150 \\end{align}\\) \n\n\n\n Step 2: Set the Significance Level \\(\\alpha\\) \n\nLetâ€™s set \\(\\alpha= 0.05\\).\nThis means we are asking, â€œIs there a sufficient evidence at \\(\\alpha= 0.05\\) that the new treatment is effective?â€\n\n Step 3: Calculate the Test Statistic \n\n The test statistic is \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\)\n\n Step 4-c: Find the Critical Value \n\n The critical value is \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) \n\n Step 5-c: Draw a Conclusion Using Critical Value \n\n \\(\\small t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}} = \\frac{147.2 - 150}{5.5/\\sqrt{25}} = -2.55\\) \n \\(\\small -t_{0.05, 25-1} = -t_{0.05, 24} = -1.711\\) \n We reject \\(H_0\\) if \\(t_{test} < -t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = -2.55 < -1.711 = -t_{\\alpha, n-1}\\), we reject \\(H_0\\).\n\n Step 4-p: Find the P-Value \n\nThis is a left-tailed test, so the \\(p\\)-value is \\(P(T < t_{test})=P(T < -2.55) =\\) 0.01 \n\n Step 5-p: Draw a Conclusion Using P-Value Method \n\n We reject \\(H_0\\) if the \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(= 0.01 < 0.05 = \\alpha\\), we reject \\(H_0\\).\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n There is sufficient evidence to support the claim that the new treatment is effective. \n\n\n Example Calculation in R \n\nBelow is a demonstration of how to work through this example using R.\n\n\n## create objects for any information we have\nalpha <- 0.05; mu_0 <- 150; \nx_bar <- 147.2; s <- 5.5; n <- 25\n\n## Test statistic\n(t_test <- (x_bar - mu_0) / (s / sqrt(n))) \n\n[1] -2.545455\n\n## Critical value\n(t_cri <- qt(alpha, df = n - 1, lower.tail = TRUE)) \n\n[1] -1.710882\n\n## p-value\n(p_val <- pt(t_test, df = n - 1, lower.tail = TRUE)) \n\n[1] 0.008878158"
  },
  {
    "objectID": "infer-ht.html#example-two-tailed-z-test",
    "href": "infer-ht.html#example-two-tailed-z-test",
    "title": "15Â  Hypothesis Testing",
    "section": "\n15.4 Example: Two-tailed z-test",
    "text": "15.4 Example: Two-tailed z-test\n\n\n\nThe milk price of a gallon of 2% milk is normally distributed with standard deviation of $0.10.\nLast week the mean price of a gallon of milk was 2.78. This week, based on a sample of size 25, the sample mean price of a gallon of milk was \\(\\overline{x} = 2.80\\).\nUnder \\(\\alpha = 0.05\\), determine if the mean price is different this week.\n\n\n\n\n\n\n\n\n\n\n\n\n\n Step-by-Step \n Step 1: Set the \\(H_0\\) and \\(H_1\\) from a Claim \n\nThis is an \\(H_1\\) claim:  \\(\\small \\begin{align}&H_0: \\mu = 2.78 \\\\ &H_1: \\mu \\ne 2.78 \\end{align}\\) \n\n\n Step 2: Set the Significance Level \\(\\alpha\\) \n\n \\(\\small \\alpha = 0.05\\) \n\n Step 3: Calculate the Test Statistic \n\n \\(\\small z_{test} = \\frac{\\overline{x} - \\mu_0}{\\sigma/\\sqrt{n}} = \\frac{2.8 - 2.78}{0.1/\\sqrt{25}} = 1.00\\) \n\n Step 4-c: Find the Critical Value \n\n \\(\\small z_{0.05/2} = 1.96\\). \n\n Step 5-c: Draw a Conclusion Using Critical Value \n\nThis is a two-tailed test, and we reject \\(H_0\\) if \\(|z_{test}| > z_{\\alpha/2}\\). Since \\(\\small |z_{test}| = 1 < 1.96 = z_{\\alpha/2}\\), we DO NOT reject \\(H_0\\).\n\n Step 4-p: Find the P-Value \n\n\nThis is a two-tailed test, and the test statistic is on the right \\((> 0)\\), so the \\(p\\)-value is \\(2P(Z > z_{test})=\\) 0.317 .\n\n Step 5-p: Draw a Conclusion Using P-Value Method \n\n We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(= 0.317 > 0.05 = \\alpha\\), we DO NOT reject \\(H_0\\).\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n There is insufficient evidence to support the claim that this week the mean price of milk is different from the price last week. \n\n\n\n\n\nFigureÂ 15.8: Illustration of Critical Value and P-Value methods\n\n\n\n\n\n Calculation in R \n\nBelow is an example of how to perform the two-tailed Z-test in R.\n\n\n## create objects to be used\nalpha <- 0.05; mu_0 <- 2.78; \nx_bar <- 2.8; sigma <- 0.1; n <- 25\n\n## Test statistic\n(z_test <- (x_bar - mu_0) / (sigma / sqrt(n))) \n\n[1] 1\n\n## Critical value\n(z_crit <- qnorm(alpha/2, lower.tail = FALSE)) \n\n[1] 1.959964\n\n## p-value\n(p_val <- 2 * pnorm(z_test, lower.tail = FALSE)) \n\n[1] 0.3173105"
  },
  {
    "objectID": "infer-ht.html#testing-summary",
    "href": "infer-ht.html#testing-summary",
    "title": "15Â  Hypothesis Testing",
    "section": "\n15.5 Testing Summary",
    "text": "15.5 Testing Summary\n\nBelow is a table that summarizes what we have learned about Hypothesis Testing in this chapter.\n\n\n\n\n\n\n\n\n\nNumerical Data, \\(\\sigma\\) known\nNumerical Data, \\(\\sigma\\)  unknown \n\n\n\n\nParameter of Interest\nPopulation Mean \\(\\mu\\)\n\nPopulation Mean \\(\\mu\\)\n\n\n\nTest Type\nOne sample \\(\\color{blue}{z}\\) test \\(H_0: \\mu = \\mu_0\\)\n\nOne sample \\(\\color{blue}{t}\\) test \\(H_0: \\mu = \\mu_0\\)\n\n\n\nConfidence Interval\n\\(\\bar{x} \\pm z_{\\alpha/2} \\frac{\\sigma}{\\sqrt{n}}\\)\n\\(\\bar{x} \\pm t_{\\alpha/2, n-1} \\frac{\\color{blue}{s}}{\\sqrt{n}}\\)\n\n\nTest Stat under \\(H_0\\) \n\\(z_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\sigma}{\\sqrt{n}}}\\)\n\\(t_{test} = \\frac{\\overline{x} - \\mu_0}{\\frac{\\color{blue}{s}}{\\sqrt{n}}}\\)\n\n\n\\(p\\)-value under \\(H_0\\)\n\n\\(H_1: \\mu < \\mu_0\\) \\(p\\)-value \\(=P(Z \\le z_{test})\\)\n\n\n\\(H_1: \\mu < \\mu_0\\) \\(p\\)-value \\(=P(T_{n-1} \\le t_{test})\\)\n\n\n\n\n\n\\(H_1: \\mu > \\mu_0\\) \\(p\\)-value \\(=P(Z \\ge z_{test})\\)\n\n\n\\(H_1: \\mu < \\mu_0\\) \\(p\\)-value \\(=P(T_{n-1} \\ge t_{test})\\)\n\n\n\n\n\n\\(H_1: \\mu \\ne \\mu_0\\) \\(p\\)-value \\(=2P(Z \\ge \\, \\mid z_{test}\\mid)\\)\n\n\n\\(H_1: \\mu \\ne \\mu_0\\) \\(p\\)-value \\(=2P(T_{n-1} \\ge \\, \\mid t_{test} \\mid)\\)"
  },
  {
    "objectID": "infer-ht.html#type-i-and-type-ii-errors",
    "href": "infer-ht.html#type-i-and-type-ii-errors",
    "title": "15Â  Hypothesis Testing",
    "section": "\n15.6 Type I and Type II Errors",
    "text": "15.6 Type I and Type II Errors\n\nIt is important to remember that hypothesis testing is not perfect.\nBecause of this, Type I and Type II errors are important to understand.\n\n\n\nDecision\n\n\\(H_0\\) is true\n\n\\(H_0\\) is false\n\n\n\nReject \\(H_0\\)\n\nType I error\nCorrect decision\n\n\nDo not reject \\(H_0\\)\n\nCorrect decision\nType II error\n\n\n\n\nBack to the crime example:\n\n\n\\(H_0:\\) The person is  not guilty  v.s. \\(H_1:\\) The person is  guilty \n\n\n\n\n\n\n\n\n\n\n\nDecision\nTruth is the person innocent\nTruth is the person guilty\n\n\n\nJury decides the person guilty\nType I error\nCorrect decision\n\n\nJury decides the person innocent\nCorrect decision\nType II error\n\n\n\n\n\\(\\alpha = P(\\text{type I error}) = P(\\text{rejecting } H_0 \\text{ when } H_0 \\text{ is true})\\)\n\\(\\beta = P(\\text{type II error}) = P(\\text{failing to reject } H_0 \\text{ when } H_0 \\text{ is false})\\)\n\n\n\n\n\nFigureÂ 15.9: Example of Type I and Type II errors (https://www.statisticssolutions.com/wp-content/uploads/2017/12/rachnovblog.jpg)"
  },
  {
    "objectID": "infer-ht.html#exercises",
    "href": "infer-ht.html#exercises",
    "title": "15Â  Hypothesis Testing",
    "section": "\n15.7 Exercises",
    "text": "15.7 Exercises\n\n\nHere are summary statistics for randomly selected weights of newborn boys: \\(n =207\\), \\(\\bar{x} = 30.2\\)hg (1hg = 100 grams), \\(s = 7.3\\)hg.\n\nWith significance level 0.01, use the critical value method to test the claim that the population mean of birth weights of females is greater than 30hg.\nDo the test in (c) by using the p-value method.\n\n\nYou are given the following hypotheses: \\[\\begin{align*}\nH_0&: \\mu = 45 \\\\\nH_A&: \\mu \\neq 45\n\\end{align*}\\] We know that the sample standard deviation is 5 and the sample size is 24. For what sample mean would the p-value be equal to 0.05? Assume that all conditions necessary for inference are satisfied.\n\nOur one sample \\(z\\) test is \\(H_0: \\mu = \\mu_0 \\quad H_1: \\mu < \\mu_0\\) with a significance level \\(\\alpha\\).\n\nDescribe how we reject \\(H_0\\) using the critical-value method and the \\(p\\)-value method.\nWhy do the two methods lead to the same conclusion?"
  },
  {
    "objectID": "infer-twomean.html#introduction",
    "href": "infer-twomean.html#introduction",
    "title": "16Â  Comparing Two Population Means",
    "section": "\n16.1 Introduction",
    "text": "16.1 Introduction\n Why Compare Two Populations? \n\nOften we are faced with a comparison of parameters from different populations.\n\n Comparing the mean annual income for Male and Female groups. \n Testing if a diet used for losing weight is effective from Placebo samples and New Diet samples. \n\n\nIf these two samples are drawn from populations with means, \\(\\mu_1\\) and \\(\\mu_2\\) respectively, the testing problem can be formulated as  \\[\\begin{align}\n&H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 > \\mu_2\n\\end{align}\\] \n\n\n\\(\\mu_1\\): male mean annual income; \\(\\mu_2\\): female mean annual income\n\n\\(\\mu_1\\): mean weight loss from the New Diet group; \\(\\mu_2\\): mean weight loss from the Placebo group\n\n\n\n\n Dependent and Independent Samples \n\nThe two samples collected can be independent or dependent.\n\n\n\n\nTwo samples are dependent or matched pairs if the sample values are matched, where the matching is based on some inherent relationship.\n\n Height data of fathers and daughters, where the height of each dad is matched with the height of his daughter. \n Weights of subjects measure before and after some diet treatment, where the subjects are the same both before and after measurements. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Dependent Samples (Matched Pairs) \n\nSubject 1 may refer to\n\nthe same person with two measurements (before and after)\nthe first matched pair (dad-daughter)\n\n\n\n\n\n\n\nSubject\n(Dad) Before\n(Daughter) After\n\n\n\n1\n\\(x_{b1}\\)\n\\(x_{a1}\\)\n\n\n2\n\\(x_{b2}\\)\n\\(x_{a2}\\)\n\n\n3\n\\(x_{b3}\\)\n\\(x_{a3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\n\\(x_{bn}\\)\n\\(x_{an}\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Independent Samples \n\n\n\nTwo samples are independent if the sample values from one population are not related to the sample values from the other.\n\n Salary samples of men and women, where the two samples are drawn independently from the male and female groups. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject 1 of the Group 1 has nothing to do with the Subject 1 of the Group 2.\n\n\n\n\n\n\n\n\n\nSubject of Group 1 (Male)\nMeasurement of Group 1\nSubject of Group 2 (Female)\nMeasurement of Group 2\n\n\n\n1\n\\(x_{11}\\)\n1\n\\(x_{21}\\)\n\n\n2\n\\(x_{12}\\)\n2\n\\(x_{22}\\)\n\n\n3\n\\(x_{13}\\)\n3\n\\(x_{23}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n_1\\)\n\\(x_{1n_1}\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\n\n\\(n_2\\)\n\\(x_{2n_2}\\)\n\n\n\n\n Inference from Two Samples \n\nThe statistical methods are different for these two types of samples.\nThe good news is the concepts of confidence intervals and hypothesis testing for one population can be applied to two-population cases.\n\n\\(\\text{CI = point estimate} \\pm \\text{margin of error (E)}\\)\n\ne.g., \\(\\overline{x} \\pm t_{\\alpha/2, n-1} \\frac{s}{\\sqrt{n}}\\)\n\n\n\nMargin of error = critical value \\(\\times\\) standard error of the point estimator\nThe 6 testing steps are the same, and both critical value and \\(p\\)-value method can be applied too\n\ne.g., \\(t_{test} = \\frac{\\overline{x} - \\mu_0}{s/\\sqrt{n}}\\)"
  },
  {
    "objectID": "infer-twomean.html#inferences-about-two-means-dependent-samples-matched-pairs",
    "href": "infer-twomean.html#inferences-about-two-means-dependent-samples-matched-pairs",
    "title": "16Â  Comparing Two Population Means",
    "section": "\n16.2 Inferences About Two Means: Dependent Samples (Matched Pairs)",
    "text": "16.2 Inferences About Two Means: Dependent Samples (Matched Pairs)\n Hypothesis Testing for Dependent Samples \n\n\n\n\n\n\nTo analyze a paired data set, we can simply analyze the differences!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSubject\n\\(x_1\\)\n\\(x_2\\)\nDifference \\(d = x_1 - x_2\\)\n\n\n\n1\n\\(x_{11}\\)\n\\(x_{21}\\)\n\\(\\color{red}{d_1}\\)\n\n\n2\n\\(x_{12}\\)\n\\(x_{22}\\)\n\\(\\color{red}{d_2}\\)\n\n\n3\n\\(x_{13}\\)\n\\(x_{23}\\)\n\\(\\color{red}{d_3}\\)\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\color{red}{\\vdots}\\)\n\n\n\\(n\\)\n\\(x_{1n}\\)\n\\(x_{2n}\\)\n\\(\\color{red}{d_n}\\)\n\n\n\n\n\n\\(\\mu_d = \\mu_1 - \\mu_2\\)\n \\(\\begin{align} & H_0: \\mu_1 - \\mu_2 = 0 \\iff \\mu_d = 0 \\\\ & H_1: \\mu_1 - \\mu_2 > 0 \\iff \\mu_d > 0 \\\\ & H_1: \\mu_1 - \\mu_2 < 0 \\iff \\mu_d < 0 \\\\ & H_1: \\mu_1 - \\mu_2 \\ne 0 \\iff \\mu_d \\ne 0 \\end{align}\\) \n\n\n\n\n\n\n\n\n\nThe point estimate of \\(\\mu_1 - \\mu_2\\) is \\(\\overline{x}_1 - \\overline{x}_2 = \\overline{d}\\).\n\n\n\n\n\n\n\n Inference for Paired Data \n\nRequirements: the sample differences \\(\\color{blue}{d_i}\\)s are\n\nfrom a random sample\nfrom a normal distribution and/or \\(n > 30\\)\n\nThis can be confirmed using the QQ-plot of \\(d_i\\)s.\n\n\n\n\nFollow the same procedure as the one-sample \\(t\\)-test!\nThe test statistic is \\(\\color{blue}{t_{test} = \\frac{\\overline{d}-0}{s_d/\\sqrt{n}}} \\sim T_{n-1}\\) under \\(H_0\\) where \\(\\overline{d}\\) and \\(s_d\\) are the mean and SD of the difference samples \\((d_1, d_2, \\dots, d_n)\\).\nThe critical value is either \\(t_{\\alpha, n-1}\\) and \\(t_{\\alpha/2, n-1}\\) depending on if it is a one-tailed or two-tailed test.\nBelow is a table that summarizes information necessary to make inferences about paired data.\n\n\n\n\n\n\n\n\nPaired \\(t\\)-test\nTest Statistic\nConfidence Interval for \\(\\mu_d = \\mu_1 - \\mu_2\\)\n\n\n\n\n\\(\\sigma_d\\) is unknown\n\\(\\large t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}}\\)\n\\(\\large \\overline{d} \\pm t_{\\alpha/2, n-1} \\frac{s_d}{\\sqrt{n}}\\)\n\n\n\nThe test for matched pairs is called a paired \\(t\\)-test.\n\n\n Example \n\nConsider a capsule used to reduce blood pressure (BP) for individuals with hypertension.\nA sample of 10 individuals with hypertension takes the medicine for 4 weeks.\nDoes the data provide sufficient evidence that the treatment is effective in reducing BP?\n\n\n\n\n\n\n\n\n\nSubject\nBefore \\((x_b)\\)\n\nAfter \\((x_a)\\)\n\nDifference \\(d = x_b - x_a\\)\n\n\n\n1\n143\n124\n19\n\n\n2\n153\n129\n24\n\n\n3\n142\n131\n11\n\n\n4\n139\n145\n-6\n\n\n5\n172\n152\n20\n\n\n6\n176\n150\n26\n\n\n7\n155\n125\n30\n\n\n8\n149\n142\n7\n\n\n9\n140\n145\n-5\n\n\n10\n169\n160\n9\n\n\n\n\n\n\\(\\overline{d} = 13.5\\), \\(s_d= 12.48\\).\n\n\\(\\mu_1 =\\) Mean Before, \\(\\mu_2 =\\) Mean After, and \\(\\mu_d = \\mu_1 - \\mu_2\\).\n\n Step 1 \n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\iff \\mu_d = 0\\\\ &H_1: \\mu_1 > \\mu_2 \\iff \\mu_d > 0 \\end{align}\\) \n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(t_{test} = \\frac{\\overline{d}}{s_d/\\sqrt{n}} = \\frac{13.5}{12.48/\\sqrt{10}} = 3.42\\) \n\n Step 4-c \n\n \\(t_{\\alpha, n-1} = t_{0.05, 9} = 1.833\\).\n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} > t_{\\alpha, n-1}\\). Since \\(\\small t_{test} = 3.42 > 1.833 = t_{\\alpha, n-1}\\), we reject \\(H_0\\).\n\n Step 6 \n\n There is sufficient evidence to support the claim that the drug is effective in reducing blood pressure. \n\n\n\n\n\nFigureÂ 16.1: Illustration of right-tailed test for blood pressure example\n\n\n\n\n\nThe 95% CI for \\(\\mu_d = \\mu_1 - \\mu_2\\) is \\[\\begin{align}\\overline{d} \\pm t_{\\alpha/2, df} \\frac{s_d}{\\sqrt{n}} &= 13.5 \\pm t_{0.025, 9}\\frac{12.48}{\\sqrt{10}}\\\\ &= 13.5 \\pm 8.927 \\\\ &= (4.573, 22.427).\\end{align}\\]\n\nWe are 95% confident that the mean difference in blood pressure is between 4.57 and 22.43.\nSince the interval does NOT include 0, it leads to the same conclusion as rejection of \\(H_0\\).\n\n\n Two-Sample Paired T-Test in R \n\nBelow is the same data as in the previous hypertension example.\nThese figures illustrate how to perform the hypothesis testing for paired data in R.\n\n\n\n\npair_data\n\n   before after\n1     143   124\n2     153   129\n3     142   131\n4     139   145\n5     172   152\n6     176   150\n7     155   125\n8     149   142\n9     140   145\n10    169   160\n\n(d <- pair_data$before - pair_data$after)\n\n [1] 19 24 11 -6 20 26 30  7 -5  9\n\n(d_bar <- mean(d))\n\n[1] 13.5\n\n\n\n\n\n\n(s_d <- sd(d))\n\n[1] 12.48332\n\n## t_test\n(t_test <- d_bar/(s_d/sqrt(length(d))))\n\n[1] 3.419823\n\n## t_cv\nqt(p = 0.95, df = length(d) - 1)\n\n[1] 1.833113\n\n## p_value\npt(q = t_test, df = length(d) - 1, \n   lower.tail = FALSE)\n\n[1] 0.003815036\n\n\n\n\n\nBelow is an example of how to calculate the confidence interval for the change in blood pressure.\n\n\nd_bar + c(-1, 1) * qt(p = 0.975, df = length(d) - 1) * (s_d / sqrt(length(d))) ## CI\n\n[1]  4.569969 22.430031\n\n\n\nWe can see that performing these calculations in R leads us to the same conclusions we previously made.\nThere is also a t-test function in R.\n\n\n## t.test() function\nt.test(x = pair_data$before, y = pair_data$after,\n       alternative = \"greater\", mu = 0, paired = TRUE)\n\n\n    Paired t-test\n\ndata:  pair_data$before and pair_data$after\nt = 3.4198, df = 9, p-value = 0.003815\nalternative hypothesis: true mean difference is greater than 0\n95 percent confidence interval:\n 6.263653      Inf\nsample estimates:\nmean difference \n           13.5 \n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nBe careful about the one-sided CI listed in the table above! We should use the two-sided CI!"
  },
  {
    "objectID": "infer-twomean.html#inference-about-two-means-independent-samples",
    "href": "infer-twomean.html#inference-about-two-means-independent-samples",
    "title": "16Â  Comparing Two Population Means",
    "section": "\n16.3 Inference About Two Means: Independent Samples",
    "text": "16.3 Inference About Two Means: Independent Samples\n Compare Population Means: Independent Samples \n\nExamples of independent data:\n\nWhether stem cells can improve heart function.\nThe relationship between pregnant womenâ€™s smoking habits and newbornsâ€™ weights.\nWhether one variation of an exam is harder than another variation.\n\n\n\n\n\n\n\n\n\nFigureÂ 16.2: Boxplots for two variations of an exam\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Testing for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \n\nRequirements:\n\nThe two samples are independent.\nBoth samples are random samples.\n\n\\(n_1 > 30\\), \\(n_2 > 30\\) and/or both samples are from a normally distributed population.\n\n\nWe are interested in whether the two population means, \\(\\mu_1\\) and \\(\\mu_2\\), are equal or if one is larger than the other.\n\n\\(H_0: \\mu_1 = \\mu_2\\)\n\nThis is equivalent to testing if their difference is zero.\n\n\n\\(H_0: \\mu_1 - \\mu_2 = 0\\)\n\n\n\n\n\n\n\nWe start by finding a point estimate for \\(\\mu_1 - \\mu_2\\). What is the best point estimator for \\(\\mu_1 - \\mu_2\\)?\n\n\n\n\n\n\\(\\overline{X}_1 - \\overline{X}_2\\) is the best point estimator for \\(\\mu_1 - \\mu_2\\)!\n\n\n\n Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) \n\nIf the two samples are from independent normally distributed populations or \\(n_1 > 30\\) and \\(n_2 > 30\\), \\[\\small \\overline{X}_1 \\sim N\\left(\\mu_1, \\frac{\\sigma_1^2}{n_1} \\right), \\quad \\overline{X}_2 \\sim N\\left(\\mu_2,\n\\frac{\\sigma_2^2}{n_2} \\right)\\]\n\\(\\overline{X}_1 - \\overline{X}_2\\) has the sampling distribution \\[\\small \\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} \\color{red}{+} \\color{black}{\\frac{\\sigma_2^2}{n_2}} \\right) \\]\n\n\\[\\small Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\sim N(0, 1)\\]\n Test Statistic for Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \n\nWith \\(D_0\\) being a hypothesized value (often 0), our testing problem is\n\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\le D_0\\\\ &H_1: \\mu_1 - \\mu_2 > D_0 \\end{align}\\)  (right-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 \\ge D_0\\\\ &H_1: \\mu_1 - \\mu_2 < D_0 \\end{align}\\)  (left-tailed)\n \\(\\small \\begin{align} &H_0: \\mu_1 - \\mu_2 = D_0\\\\ &H_1: \\mu_1 - \\mu_2 \\ne D_0 \\end{align}\\)  (two-tailed)\n\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\small \\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}} \\]\n\nThen we find \\(z_{\\alpha}\\) or \\(z_{\\alpha/2}\\) and follow our testing steps!\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, the test statistic becomes \\(t_{test}\\):\n\n\\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} \\]\n\nThe critical value is either \\(t_{\\alpha, df}\\) (one-tailed) pr \\(t_{\\alpha/2, df}\\) (two-tailed), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\] where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\).\nIf the \\(df\\) is not an integer, we round it down to the nearest integer.\n\n Inference About Independent Samples \\((\\sigma_1 \\ne \\sigma_2)\\) \n\nBelow is a table that summarizes ways to make inferences about independent samples when \\((\\sigma_1 \\ne \\sigma_2)\\).\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 \\ne \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sqrt{\\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}\\)\n\n\n\n\nUse \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) where \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\) to get the \\(p\\)-value, critical value and confidence interval.\nThe unequal-variance t-test is called Welchâ€™s t-test.\n\n Example: Two-Sample t-Test \n\n\n\nDoes an over-sized tennis racket exert less stress/force on the elbow?\n\n\nOver-sized: \\(n_1 = 33\\), \\(\\overline{x}_1 = 25.2\\), \\(s_1 = 8.6\\)\n\n\nConventional: \\(n_2 = 12\\), \\(\\overline{x}_2 = 33.9\\), \\(s_2 = 17.4\\)\n\nThe two populations are nearly normal.\nThe large difference in the sample SD suggests \\(\\sigma_1 \\ne \\sigma_2\\).\n\n\nForm a hypothesis test with \\(\\alpha = 0.05\\), and construct a 95% CI for the mean difference of force on the elbow.\n\n\n\n\n\n\n\n\n\n\n\n\n Step 1 \n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}\\) \n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1-\\mu_2)}{\\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}}} = \\frac{(25.2 - 33.9) - 0}{\\sqrt{\\frac{\\color{red}{8.6^2}}{33} + \\frac{\\color{red}{17.4^2}}{12}}} = -1.66\\)\n\n\\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{n_1-1}+ \\dfrac{B^2}{n_2-1}},\\) \\(\\small A = \\dfrac{s_1^2}{n_1}\\) and \\(\\small B = \\dfrac{s_2^2}{n_2}\\)\n\n\n\\(\\small A = \\dfrac{8.6^2}{33}\\), \\(\\small B = \\dfrac{17.4^2}{12}\\), \\(\\small df = \\dfrac{(A+B)^2}{\\dfrac{A^2}{33-1}+ \\dfrac{B^2}{12-1}} = 13.01\\)\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf the computed value of \\(df\\) is not an integer, always round down to the nearest integer.\n\n\n Step 4-c \n\n \\(-t_{0.05, 13} = -1.771\\). \n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} < -t_{\\alpha, df}\\). \\(\\small t_{test} = -1.66 > -1.771 = -t_{\\alpha, df}\\), we fail to reject \\(H_0\\). \n\n Step 6 \n\n There is insufficient evidence to support the claim that the the oversized racket delivers less stress to the elbow. \nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is\n\n\\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} \\sqrt{\\frac{\\color{red}{s_1^2}}{n_1} + \\frac{\\color{red}{s_2^2}}{n_2}} &= (25.2 - 33.9) \\pm t_{0.025,13}\\sqrt{\\frac{8.6^2}{33} + \\frac{17.4^2}{12}}\\\\&= -8.7 \\pm 11.32 = (-20.02, 2.62).\\end{align}\\]\n\nWe are 95% confident that the difference in the mean forces is between -20.02 and 2.62.\nSince the interval includes 0, it leads to the same conclusion as failing to reject \\(H_0\\).\n\n Two-Sample t-Test in R \n\nn1 = 33; x1_bar = 25.2; s1 = 8.6\nn2 = 12; x2_bar = 33.9; s2 = 17.4\nA <- s1^2 / n1; B <- s2^2 / n2\ndf <- (A + B)^2 / (A^2/(n1-1) + B^2/(n2-1))\n(df <- floor(df))\n\n[1] 13\n\n## t_test\n(t_test <- (x1_bar - x2_bar) / sqrt(s1^2/n1 + s2^2/n2))\n\n[1] -1.659894\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.770933\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 0.06042575\n\n\n\n Testing for Independent Samples (\\(\\sigma_1 = \\sigma_2 = \\sigma\\)) \n Sampling Distribution of \\(\\overline{X}_1 - \\overline{X}_2\\) \n\\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\frac{\\sigma_1^2}{n_1} + \\frac{\\sigma_2^2}{n_2} \\right) \\] If \\(\\sigma_1 = \\sigma_2 = \\sigma\\), \\[\\overline{X}_1 - \\overline{X}_2 \\sim N\\left(\\mu_1 - \\mu_2, \\sigma^2\\left(\\frac{1}{n_1} + \\frac{1}{n_2} \\right) \\right) \\] \\[ Z = \\frac{(\\overline{X}_1 - \\overline{X}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\sim N(0, 1)\\]\n Test Statistic for Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\) \n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are known, the test statistic is the z-score of \\(\\overline{X}_1 - \\overline{X}_2\\) under \\(H_0\\): \\[z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} \\]\n\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, we use \\(t_{test}\\) just like we would for the one-sample case.\nAs \\(\\sigma_1 = \\sigma_2 = \\sigma\\), we just need one sample SD to replace the population SD, \\(\\sigma\\).\nUse the pooled sample variance to estimate the common population variance, \\(\\sigma^2\\): \\[ s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2} \\] which is the weighted average of \\(s_1^2\\) and \\(s_2^2\\).\nIf \\(\\sigma_1\\) and \\(\\sigma_2\\) are unknown, \\[t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - (\\mu_1 - \\mu_2)}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\n\nHere, the critical value is either \\(t_{\\alpha, df}\\) (for one-tailed tests) or \\(t_{\\alpha/2, df}\\) (for two-tailed tests), and the \\(t\\) distribution used to compute the \\(p\\)-value has the degrees of freedom \\[df = n_1 + n_2 - 2\\]\n\n\n Inference from Independent Samples \\((\\sigma_1 = \\sigma_2 = \\sigma)\\) \n\nBelow is a table that summarizes ways to make inferences about independent samples when \\((\\sigma_1 = \\sigma_2)\\).\n\n\n\n\n\n\n\n\n\\(\\large \\color{red}{\\sigma_1 = \\sigma_2}\\)\nTest Statistic\nConfidence Interval for \\(\\mu_1 - \\mu_2\\)\n\n\n\n\nknown\n\\(\\large z_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{\\sigma\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm z_{\\alpha/2} \\sigma \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\nunknown\n\\(\\large t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\n\\(\\large (\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}\\)\n\n\n\n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\\)\nUse \\(df = n_1+n_2-2\\) to get the \\(p\\)-value, critical value and confidence interval.\nThe test from two independent samples with \\(\\sigma_1 = \\sigma_2 = \\sigma\\) is usually called two-sample pooled \\(z\\)-test or two-sample pooled \\(t\\)-test.\n\n Example: Weight Loss \n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months\n\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\n\n\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\n\n\nIs there a sufficient evidence at \\(\\alpha = 0.05\\) to conclude that the program is effective?\n\nIf yes, construct a 95% CI for \\(\\mu_1 - \\mu_2\\) to show how much effective it is.\n\n\n\n Step 1 \n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 \\\\ &H_1: \\mu_1 < \\mu_2 \\end{align}\\) \n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n\n \\(t_{test} = \\frac{(\\overline{x}_1 - \\overline{x}_2) - \\color{blue}{D_0}}{{\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\). \n\n\\(s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}} = \\sqrt{\\frac{(10-1)0.5^2 + (10-1)0.7^2}{10+10-2}}=0.6083\\)\n \\(t_{test} = \\frac{(2.1 - 4.2) - 0}{0.6083\\sqrt{\\frac{1}{10} + \\frac{1}{10}}} = -7.72\\)\n\n\n\n Step 4-c \n\n \\(df = n_1 + n_2 - 2 = 10 + 10 - 2 = 18\\). So \\(-t_{0.05, df = 18} = -1.734\\). \n\n Step 5-c \n\n We reject \\(H_0\\) if \\(\\small t_{test} < -t_{\\alpha, df}\\). Since \\(\\small t_{test} = -7.72 < -1.734 = -t_{\\alpha, df}\\), we reject \\(H_0\\).\n\n Step 4-p \n\n The \\(p\\)-value is \\(P(T_{df=18} < t_{test}) \\approx 0\\) \n\n Step 5-p \n\n We reject \\(H_0\\) if \\(p\\)-value < \\(\\alpha\\). Since \\(p\\)-value \\(\\approx 0 < 0.05 = \\alpha\\), we reject \\(H_0\\).\n\n Step 6 \n\n There is sufficient evidence to support the claim that the weight loss program is effective. \nThe 95% CI for \\(\\mu_1 - \\mu_2\\) is \\[\\begin{align}(\\overline{x}_1 - \\overline{x}_2) \\pm t_{\\alpha/2, df} {\\color{red}{s_p}}\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}} &= (2.1 - 4.2) \\pm t_{0.025, 18} (0.6083)\\sqrt{\\frac{1}{10} + \\frac{1}{10}}\\\\ &= -2.1 \\pm 0.572 = (-2.672, -1.528) \\end{align}\\]\nWe are 95% confident that the difference in the mean weight is between -2.672 and -1.528.\nSince the interval does not include 0, it leads to the same conclusion as rejection of \\(H_0\\).\n\n Two-Sample Pooled t-Test in R \n\nn1 = 10; x1_bar = 2.1; s1 = 0.5\nn2 = 10; x2_bar = 4.2; s2 = 0.7\nsp <- sqrt(((n1 - 1) * s1 ^ 2 + (n2 - 1) * s2 ^ 2) / (n1 + n2 - 2))\nsp\n\n[1] 0.6082763\n\ndf <- n1 + n2 - 2\n## t_test\n(t_test <- (x1_bar - x2_bar) / (sp * sqrt(1 / n1 + 1 / n2)))\n\n[1] -7.719754\n\n## t_cv\nqt(p = 0.05, df = df)\n\n[1] -1.734064\n\n## p_value\npt(q = t_test, df = df)\n\n[1] 2.028505e-07"
  },
  {
    "objectID": "infer-twomean.html#exercises",
    "href": "infer-twomean.html#exercises",
    "title": "16Â  Comparing Two Population Means",
    "section": "\n16.4 Exercises",
    "text": "16.4 Exercises\n\nA study was conducted to assess the effects that occur when children are expected to cocaine before birth. Children were tested at age 4 for object assembly skill, which was described as â€œa task requiring visual-spatial skills related to mathematical competence.â€ The 187 children born to cocaine users had a mean of 7.1 and a standard deviation of 2.5. The 183 children not exposed to cocaine had a mean score of 8.4 and a standard deviation of 2.5.\n\nWith \\(\\alpha = 0.05\\), use the critical-value method and p-value method to perform a 2-sample t-test on the claim that prenatal cocaine exposure is associated with lower scores of 4-year-old children on the test of object assembly.\nTest the claim in part (a) by using a confidence interval.\n\n\nListed below are heights (in.) of mothers and their first daughters.\n\nUse \\(\\alpha = 0.05\\) to test the claim that there is no difference in heights between mothers and their first daughters.\nTest the claim in part (a) by using a confidence interval.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHeight of Mother\n66\n62\n62\n63.5\n67\n64\n69\n65\n62.5\n67\n\n\nHeight of Daughter\n67.5\n60\n63.5\n66.5\n68\n65.5\n69\n68\n65.5\n64"
  },
  {
    "objectID": "infer-var.html#inference-for-one-population-variance",
    "href": "infer-var.html#inference-for-one-population-variance",
    "title": "17Â  Inference About Variances",
    "section": "\n17.1 Inference for One Population Variance",
    "text": "17.1 Inference for One Population Variance\n Why Inference for Population Variances? \n\nWe want to know if \\(\\sigma_1 = \\sigma_2\\), because the method we use depends on whether or not this is true.\n\n\n\n\n\n\n\nWhich test did we learn that needs \\(\\sigma_1 = \\sigma_2\\)?\n\n\n\n\nTwo-sample pooled test\n\n\n\n\nIn some situations, we care about variation!\n\n\n\n\n\n\n The variation in potency of drugs: affects patientsâ€™ health\n\n\n\n\n\n\n\n\n\n\n\n\n\n The variance of stock prices : the higher the variance, the riskier the investment\n\n\n\n\n\n\n\n\n\n\n\n\n\n Inference for Population Variances \n\nThe sample variance \\(S^2 = \\frac{\\sum_{i=1}^n(X_i - \\overline{X})^2}{n-1}\\) is our point estimator for the population variance, \\(\\sigma^2\\).\n\n\\(S^2\\) is an unbiased estimator for \\(\\sigma^2\\).\n\nThis means \\(E(S^2) = \\sigma^2\\).\n\n\nThe inference methods for \\(\\sigma^2\\) require the population to be normal.\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe inference methods can work poorly if normality is violated, even if the sample is large.\n\n\n\n\n\n\n\n\n\n\n\n\n Chi-Square \\(\\chi^2\\) Distribution \n\nThe inference for \\(\\sigma^2\\) involves the so called \\(\\chi^2\\) distribution.\n\n\n\n\n\nParameter: degrees of freedom \\(df\\)\n\n\nRight-skewed distribution\nDefined over positive numbers\nMore symmetric as \\(df\\) gets larger\nChi-Square Distribution\n\n\n\n\n\n\nFigureÂ 17.1: Illustration of \\(\\chi^2\\) distributions with varying degrees of freedom\n\n\n\n\n\n\n Upper Tail and Lower Tail of Chi-Square \n\n\n\\(\\chi^2_{\\frac{\\alpha}{2},\\, df}\\) has area to the right of \\(\\alpha/2\\).\n\n\\(\\chi^2_{1-\\frac{\\alpha}{2},\\, df}\\) has area to the left of \\(\\alpha/2\\).\nIn \\(N(0, 1)\\), \\(z_{1-\\frac{\\alpha}{2}} = -z_{\\frac{\\alpha}{2}}\\), but \\(\\chi^2_{1-\\frac{\\alpha}{2},\\,df} \\ne -\\chi^2_{\\frac{\\alpha}{2},\\,df}\\).\n\n\n\n\n\nFigureÂ 17.2: Illustration of \\(\\alpha/2\\) significance levels for \\(\\chi^2_{df}\\) distribution\n\n\n\n\n Sampling Distribution \n\nWhen a random sample of size \\(n\\) is from \\(\\color{red}{N(\\mu, \\sigma^2)}\\), \\[ \\frac{(n-1)S^2}{\\sigma^2} \\sim \\chi^2_{n-1} \\]\nReminder: The inference method for \\(\\sigma^2\\) introduced here can work poorly if the normality assumption is violated, even for large samples.\n\n \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma^2\\) \n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{(n-1)S^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}, \\frac{(n-1)S^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}} \\right)}}\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe CI for \\(\\sigma^2\\) cannot be expressed as \\((S^2-m, S^2+m)\\) anymore!\n\n\n\n\n Example: Supermodel Heights \n\n\n\nListed below are heights (cm) for the simple random sample of 16 female supermodels.\n\n\nheights <- c(178, 177, 176, 174, 175, 178, 175, 178, \n             178, 177, 180, 176, 180, 178, 180, 176)\n\n\nThe supermodelsâ€™ heights are normally distributed.\nConstruct a \\(95\\%\\) confidence interval for population standard deviation \\(\\sigma\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 16\\), \\(s^2 = 3.4\\), \\(\\alpha = 0.05\\).\n\\(\\chi^2_{\\alpha/2, n-1} = \\chi^2_{0.025, 15} = 27.49\\)\n\\(\\chi^2_{1-\\alpha/2, n-1} = \\chi^2_{0.975, 15} = 6.26\\)\nThe \\(95\\%\\) CI for \\(\\sigma\\) is \\(\\small \\left( \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{\\frac{\\alpha}{2}, \\, n-1}}}, \\sqrt{\\frac{(n-1)s^2}{\\chi^2_{1-\\frac{\\alpha}{2}, \\, n-1}}} \\right) = \\left( \\sqrt{\\frac{(16-1)(3.4)}{27.49}}, \\sqrt{\\frac{(16-1)(3.4)}{6.26}}\\right) = (1.36, 2.85)\\)\n\n\n Computation in R \n\nn <- 16\ns2 <- var(heights)\nalpha <- 0.05\n\n## two chi-square critical values\nchi2_right <- qchisq(alpha/2, n - 1, lower.tail = FALSE)\nchi2_left <- qchisq(alpha/2, n - 1, lower.tail = TRUE)\n\n## two bounds of CI for sigma2\nci_sig2_lower <- (n - 1) * s2 / chi2_right\nci_sig2_upper <- (n - 1) * s2 / chi2_left\n\n## two bounds of CI for sigma\n(ci_sig_lower <- sqrt(ci_sig2_lower))\n\n[1] 1.362104\n\n(ci_sig_upper <- sqrt(ci_sig2_upper))\n\n[1] 2.853802\n\n\n Testing \n\nUse \\(\\alpha = 0.05\\) to test the claim that â€œsupermodels have heights with a standard deviation that is less than the standard deviation, \\(\\sigma = 7.5\\) cm, for the population of womenâ€.\n\n Step 1 \n\n\n\\(H_0: \\sigma = \\sigma_0\\) vs.Â \\(H_1: \\sigma < \\sigma_0\\), where \\(\\sigma_0 = 7.5\\) cm.\n\n Step 2 \n\n\\(\\alpha = 0.05\\)\n\n Step 3 \n\nUnder \\(H_0\\), \\(\\chi_{test}^2 = \\frac{(n-1)s^2}{\\sigma_0^2} = \\frac{(16-1)(3.4)}{7.5^2} = 0.91\\), drawn from \\(\\chi^2_{n-1}\\).\n\n\n\n Step 4-c \n\nThis is a left-tailed test.\nThe critical value is \\(\\chi_{1-\\alpha, df}^2 = \\chi_{0.95, 15}^2 = 7.26\\)\n\n\n Step 5-c \n\nReject \\(H_0\\) in favor of \\(H_1\\) if \\(\\chi_{test}^2 < \\chi_{1-\\alpha, df}^2\\).\nSince \\(0.91 < 7.26\\), we reject \\(H_0\\).\n\n Step 6 \n\nThere is sufficient evidence to support the claim that supermodels have heights with a SD that is less than the SD for the population of all women.\n\n\n\n\n\n\n\nFigureÂ 17.3: \\(\\chi_{0.95,15}^2\\) distribution for supermodel heights\n\n\n\n\n\n\n\nWe conclude that the heights of supermodels vary less than heights of women in the general population.\n\n\n\n Back to Pooled t-Test \n\nIn a pooled t-test, we assume\n\n \\(n_1 \\ge 30\\) and \\(n_2 \\ge 30\\) or that both samples are drawn from normal populations. \n \\(\\sigma_1 = \\sigma_2\\) \n\n\nWe can use a QQ-plot (and normality tests, Anderson, Shapiro, etc.) to check the assumption of a normal distribution.\nWe will now learn how to check the assumption \\(\\sigma_1 = \\sigma_2\\)."
  },
  {
    "objectID": "infer-var.html#inference-for-comparing-two-population-variances",
    "href": "infer-var.html#inference-for-comparing-two-population-variances",
    "title": "17Â  Inference About Variances",
    "section": "\n17.2 Inference for Comparing Two Population Variances",
    "text": "17.2 Inference for Comparing Two Population Variances\n F Distribution \n\nWe use the \\(F\\) distribution for inference about two population variances.\n\n\n\n\n\nTwo parameters: \\(df_1\\), \\(df_2\\)\n\n\nRight-skewed distribution\nDefined over positive numbers\nR Shiny app: F Distribution\n\n\n\n\n\n\n\nFigureÂ 17.4: F distributions with different parameters\n\n\n\n\n\n\n Upper and Lower Tail of F Distribution \n\nWe denote \\(F_{\\alpha, \\, df_1, \\, df_2}\\) as the \\(F\\) quantile such that \\(P(F_{df_1, df_2} > F_{\\alpha, \\, df_1, \\, df_2}) = \\alpha\\).\n\n\n\n\n\nFigureÂ 17.5: Illustration of \\(\\alpha/2\\) significance levels for \\(F_{df_1, df_2}\\) distribution\n\n\n\n\nSampling Distribution \n\nWhen random samples of sizes \\(n_1\\) and \\(n_2\\) have been independently drawn from two normally distributed populations, \\(N(\\mu_1, \\sigma_1^2)\\) and \\(N(\\mu_2, \\sigma_2^2)\\), the ratio \\[\\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\n\n\n \\((1-\\alpha)100\\%\\) Confidence Interval for \\(\\sigma_1^2 / \\sigma_2^2\\) \n\nThe \\((1-\\alpha)100\\%\\) confidence interval for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\color{blue}{\\boxed{\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, n_1 - 1, \\, n_2 - 1}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, \\, n_1 - 1, \\, n_2 - 1}} \\right)}}\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nThe CI for \\(\\sigma_1^2 / \\sigma_2^2\\) cannot be expressed as \\(\\left(\\frac{s_1^2}{s_2^2}-m, \\frac{s_1^2}{s_2^2} + m\\right)\\) anymore!\n\n\n\n\n F test for comparing \\(\\sigma_1^2\\) and \\(\\sigma_2^2\\) \n Step 1 \n\nRight-tailed:  \\(\\small \\begin{align} &H_0: \\sigma_1 \\le \\sigma_2 \\\\ &H_1: \\sigma_1 > \\sigma_2 \\end{align}\\) \n\nTwo-tailed:  \\(\\small \\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\) \n\n\n Step 2 \n\n\\(\\alpha = 0.05\\)\n\n Step 3 \n\nUnder \\(H_0\\), \\(\\sigma_1 = \\sigma_2\\), and the test statistic is \\[\\small F_{test} = \\frac{s_1^2/s_2^2}{\\sigma_1^2/\\sigma_2^2} = \\frac{s_1^2}{s_2^2} \\sim F_{n_1-1, \\, n_2-1}\\]\n\n\n Step 4-c \n\nRight-tailed:  \\(F_{\\alpha, \\, n_1-1, \\, n_2-1}\\) .\nTwo-tailed:  \\(F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\) \n\n\n Step 5-c \n\nRight-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha, \\, n_1-1, \\, n_2-1}\\).\nTwo-tailed: reject \\(H_0\\) if  \\(F_{test} \\ge F_{\\alpha/2, \\, n_1-1, \\, n_2-1}\\) or \\(F_{test} \\le F_{1-\\alpha/2, \\, n_1-1, \\, n_2-1}\\)\n\n\n\n Example: Weight Loss \n\n\n\nA study was conducted to see the effectiveness of a weight loss program.\n\nTwo groups (Control and Experimental) of 10 subjects were selected.\nThe two populations are normally distributed and have the same SD.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe data on weight loss was collected at the end of six months.\n\n\nControl: \\(n_1 = 10\\), \\(\\overline{x}_1 = 2.1\\, lb\\), \\(s_1 = 0.5\\, lb\\)\n\n\nExperimental: \\(n_2 = 10\\), \\(\\overline{x}_2 = 4.2\\, lb\\), \\(s_2 = 0.7\\, lb\\)\n\n\n\nAssumptions:\n\n \\(\\sigma_1 = \\sigma_2\\) \nThe weight loss for both groups are normally distributed.\n\n\n\n Check if \\(\\sigma_1 = \\sigma_2\\) \n\n\n\\(n_1 = 10\\), \\(s_1 = 0.5 \\, lb\\)\n\n\n\\(n_2 = 10\\), \\(s_2 = 0.7 \\, lb\\)\n\n\n Step 1 \n\n\\(\\begin{align} &H_0: \\sigma_1 = \\sigma_2 \\\\ &H_1: \\sigma_1 \\ne \\sigma_2 \\end{align}\\)\n\n Step 2 \n\n\\(\\alpha = 0.05\\)\n\n Step 3 \n\nThe test statistic is \\(F_{test} = \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\).\n\n Step 4-c \n\nThis is a two-tailed test.\nThe critical value is \\(F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\) or \\(F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\).\n\n\n\n\n\nFigureÂ 17.6: F Distribution for Weight Loss Example\n\n\n\n\n Step 5-c \n\nIs \\(F_{test} > 4.03\\) or \\(F_{test} < 0.25\\)?\n\nNo.Â \n\n\n\n Step 6 \n\nThe evidence is not sufficient to reject the claim that \\(\\sigma_1 = \\sigma_2\\).\n\n 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) \n\n\n\n\\(\\small F_{\\alpha/2, \\, df_1, \\, df_2} = F_{0.05/2, \\, 10-1, \\, 10-1} = 4.03\\)\n\\(\\small F_{1-\\alpha/2, \\, df_1, \\, df_2} = F_{1-0.05/2, \\, 10-1, \\, 10-1} = 0.25\\)\n\\(\\small \\frac{s_1^2}{s_2^2} = \\frac{0.5^2}{0.7^2} = 0.51\\)\nThe 95% CI for \\(\\sigma_1^2 / \\sigma_2^2\\) is \\[\\small \\begin{align} &\\left( \\frac{s_1^2/s_2^2}{F_{\\alpha/2, \\, df_1, \\, df_2}}, \\frac{s_1^2/s_2^2}{F_{1-\\alpha/2, \\, df_1, \\, df_2}} \\right) \\\\ &= \\left( \\frac{0.51}{4.03}, \\frac{0.51}{0.25} \\right) = \\left(0.127, 2.04\\right)\\end{align}\\]\n\n\n\n\n\n\n\nFigureÂ 17.7: F Distribution for significance level 0.05\n\n\n\n\n\n\n\nWe are 95% confident that the ratio \\(\\sigma_1^2 / \\sigma_2^2\\) is between 0.127 and 2.04.\nBecause 1 is included in this interval, it leads to the same conclusion as the F test.\n\n Implementing F-test in R \n\n\n\n\nn1 <- 10; n2 <- 10\ns1 <- 0.5; s2 <- 0.7\nalpha <- 0.05\n\n## 95% CI for sigma_1^2 / sigma_2^2\nf_small <- qf(p = alpha / 2, \n              df1 = n1 - 1, df2 = n2 - 1, \n              lower.tail = TRUE)\nf_big <- qf(p = alpha / 2, \n            df1 = n1 - 1, df2 = n2 - 1, \n            lower.tail = FALSE)\n\n## lower bound\n(s1 ^ 2 / s2 ^ 2) / f_big\n\n[1] 0.1267275\n\n## upper bound\n(s1 ^ 2 / s2 ^ 2) / f_small\n\n[1] 2.054079\n\n\n\n\n\n\n## Testing sigma_1 = sigma_2\n(test_stats <- s1 ^ 2 / s2 ^ 2)\n\n[1] 0.5102041\n\n(cri_val_big <- qf(p = alpha/2, \n                   df1 = n1 - 1, \n                   df2 = n2 - 1, \n                   lower.tail = FALSE))\n\n[1] 4.025994\n\n(cri_val_small <- qf(p = alpha/2, \n                     df1 = n1 - 1, \n                     df2 = n2 - 1, \n                     lower.tail = TRUE))\n\n[1] 0.2483859\n\n# var.test(x, y, alternative = \"two.sided\")"
  },
  {
    "objectID": "infer-var.html#exercises",
    "href": "infer-var.html#exercises",
    "title": "17Â  Inference About Variances",
    "section": "\n17.3 Exercises",
    "text": "17.3 Exercises\n\nThe data about male and female pulse rates are summarized below.\n\nConstruct a 95% CI for \\(\\sigma_{male}\\) of pulse rates for males.\nConstruct a 95% CI for \\(\\sigma_{male}/\\sigma_{female}\\).\nDoes it appear that the population standard deviations for males and females are different? Why or why not?\n\n\n\n\n\n\n\nMale\nFemale\n\n\n\n\\(\\overline{x}\\)\n71\n75\n\n\n\\(s\\)\n9\n12\n\n\n\\(n\\)\n14\n12"
  },
  {
    "objectID": "infer-prop.html#introduction",
    "href": "infer-prop.html#introduction",
    "title": "18Â  Inference About Proportions",
    "section": "\n18.1 Introduction",
    "text": "18.1 Introduction\n One Categorical Variable with Two Categories \n\nLet \\(X\\) be the categorical variable Gender with 2 categories, Male and Female.\n\n\n\n\n\nSubject\nMale\nFemale\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n\\(n\\)\nx\n\n\n\n\n\nOne-way frequency/count table\n\n\n\n\\(X\\)\nCount\n\n\n\nMale\n\\(y\\)\n\n\nFemale\n\\(n-y\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe number of males can be viewed as a random variable because the count, \\(y\\), varies from sample to sample.\n\n\n\n\n\n\n\nWhat probability distribution might be appropriate for the count, \\(Y\\)?\n\n\n\n\n\n\n\n\n\n Probability Distribution for Count Data: Two Levels \n\n\n\\(binomial(n, \\pi)\\) could be a good option for count data with 2 categories.\n\nFixed number of trials.  (Fixed \\(n\\) subjects) \n\nEach trial results in one of two outcomes.  (Either \\(M\\) or \\(F\\)) \n\nTrials are independent.  (If the subjects are randomly sampled) \n\n\n\nIf the proportion of being in category, \\(M\\), is \\(\\pi\\), the count, \\(Y\\), has \\[P(Y = y \\mid n, \\pi) = \\frac{n!}{y!(n-y)!}\\pi^{y}(1-\\pi)^{n-y}\\]\n\nGoal: Estimate or test the population proportion, \\(\\pi\\), of the category, \\(M\\)."
  },
  {
    "objectID": "infer-prop.html#inference-for-a-single-proportion",
    "href": "infer-prop.html#inference-for-a-single-proportion",
    "title": "18Â  Inference About Proportions",
    "section": "\n18.2 Inference for a Single Proportion",
    "text": "18.2 Inference for a Single Proportion\n Hypothesis Testing for \\(\\pi\\) (Exact Binom Test) \n Step 0: Method Assumptions \n\n\n \\(n\\pi_0 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) \n\nThe larger, the better\n\n\n\n Step 1: Set the Null and Alternative Hypothesis \n\n \\(\\begin{align} &H_0: \\pi = \\pi_0 \\\\ &H_1: \\pi > \\pi_0 \\text{ or } \\pi < \\pi_0 \\text{ or } \\pi \\ne \\pi_0 \\end{align}\\) \n\n Step 2: Set the Significance Level, \\(\\alpha\\) \n Step 3: Calculate the Test Statistic \n\n Under \\(H_0\\), \\(z_{test} = \\dfrac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}}\\) where \\(\\hat{\\pi} = \\frac{y}{n} =\\) sample proportion \n\n\n\n\n\n\n\nNote\n\n\n\n\nThe sampling distribution of \\(\\hat{\\pi}\\) is approximately normal with mean, \\(\\pi\\), and standard error, \\(\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\), if \\(y_i\\) are independent and the assumptions are satisfied.\n\n\n\n Step 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed) \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n \\(H_1: \\pi > \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z > z_{\\alpha}\\) \n \\(H_1: \\pi < \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z < -z_{\\alpha}\\) \n \\(H_1: \\pi \\ne \\pi_0\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| > z_{\\alpha/2}\\) \n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n Confidence Interval for \\(\\pi\\) \n\nAssumptions:\n\n\n\\(n\\hat{\\pi} \\ge 5\\) and \\(n(1-\\hat{\\pi}) \\ge 5\\)\n\n\n\nThe \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi\\) is \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\pi(1-\\pi)}{n}}\\] where \\(\\hat{\\pi} = y/n\\).\n\n\\(\\pi\\) is unknown, so we use the estimate, \\(\\hat{\\pi}\\), instead: \\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}}\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\n\nNo hypothesized value, \\(\\pi_0\\), is involved in the confidence interval.\n\n\n\n\n Example: Exit Poll \n\nSuppose we collect data on 1,000 voters in an election with only two candidates, R and D.\n\n\n\n\n\nVoter\nR\nD\n\n\n\n1\nx\n\n\n\n2\n\nx\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\n1000\nx\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the data, we want to predict who won the election.\nLet \\(Y\\) be the number of voters that voted for R.\nAssume the count, \\(Y\\), is sampled from \\(binomial(n = 1000, \\pi)\\).\n\n\n\\(\\pi = P(\\text{a voter voted for R}) =\\) (population) proportion of all voters that for R\n\nThis is the unknown parameter to be estimated or tested.\n\n\n\n\nPredict whether or not R won the election.\n\n\n\n\n\n\n\nWhat are \\(H_0\\) and \\(H_1\\)?\n\n\n\n\n \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi > 1/2 \\text{ (more than half voted for R)} \\end{align}\\) \n\n\n\n Hypothesis Testing \n\nIn an exit poll of 1,000 voters, 520 voted for R, one of the two candidates.\n\n Step 0 \n\n \\(n\\pi_0 = 1000(1/2) = 500 \\ge 5\\) and \\(n(1-\\pi_0) \\ge 5\\) \n\n Step 1 \n\n \\(\\begin{align} &H_0: \\pi \\le 1/2 \\\\ &H_1: \\pi > 1/2 \\end{align}\\) \n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(z_{test} = \\frac{\\hat{\\pi} - \\pi_0}{\\sqrt{\\frac{\\pi_0(1-\\pi_0)}{n}}} = \\frac{\\frac{520}{1000} - 0.5}{\\sqrt{\\frac{0.5(1-0.5)}{1000}}} = 1.26\\) \n\n Step 4-c \n\n \\(z_{\\alpha} = z_{0.05} = 1.645\\) \n\n Step 5-c \n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} > z_{\\alpha}\\). \n Since \\(z_{test} < z_{\\alpha}\\), we do not reject \\(H_0\\). \n\n Step 6 \n\n We do not have sufficient evidence to conclude that R won. \nWe make the same conclusion using the \\(p\\)-value method.\n\n\\[ p\\text{-value} = P(Z > 1.26) = 0.1 > 0.05\\]\n Confidence Interval \n\nAssumptions:\n\n\n\\(n\\hat{\\pi} = 1000(0.52) = 520 \\ge 5\\) and \\(n(1-\\hat{\\pi}) = 480 \\ge 5\\).\n\n\nEstimate the proportion of all voters that voted for R using a 95% confidence interval.\n\n\\[\\hat{\\pi} \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}(1-\\hat{\\pi})}{n}} = 0.52 \\pm z_{0.025}\\sqrt{\\frac{0.52(1-0.52)}{1000}} = (0.49, 0.55).\\]\n\nBelow is a demonstration of how to perform a binomial test in R.\n\n\n# Use alternative = \"two.sided\" to get CI\n# binom.test()\nprop.test(x = 520, n = 1000, p = 0.5, alternative = \"greater\", correct = FALSE)\n\n\n    1-sample proportions test without continuity correction\n\ndata:  520 out of 1000, null probability 0.5\nX-squared = 1.6, df = 1, p-value = 0.103\nalternative hypothesis: true p is greater than 0.5\n95 percent confidence interval:\n 0.4939945 1.0000000\nsample estimates:\n   p \n0.52"
  },
  {
    "objectID": "infer-prop.html#inference-for-two-proportions",
    "href": "infer-prop.html#inference-for-two-proportions",
    "title": "18Â  Inference About Proportions",
    "section": "\n18.3 Inference for Two Proportions",
    "text": "18.3 Inference for Two Proportions\n\n\n\n\nGroup 1\nGroup 2\n\n\n\n\n\\(n_1\\) trials\n\n\\(n_2\\) trials\n\n\n\n\\(Y_1\\) number of successes\n\n\\(Y_2\\) number of successes\n\n\n\\(Y_1 \\sim binomial(n_1, \\pi_1)\\)\n\\(Y_2 \\sim binomial(n_2, \\pi_2)\\)\n\n\n\n\n\n\\(\\pi_1\\): Population proportion of success of Group 1\n\n\\(\\pi_2\\): Population proportion of success of Group 2\n\n\n\n\n\nIs the male presidential approval rate, \\(\\pi_1\\), higher than the female approval rate, \\(\\pi_2\\)?\n\n\n\n\n\n\n\n\n\n\n\n\n Hypothesis Testing for \\(\\pi_1\\) and \\(\\pi_2\\) \n Step 0: Check Method Assumptions \n\n \\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\) \n\n Step 1: Set the Null and Alternative Hypothesis \n\n \\(\\begin{align} &H_0: \\pi_1 = \\pi_2 \\\\ &H_1: \\pi_1 > \\pi_2 \\text{ or } \\pi_1 < \\pi_2 \\text{ or } \\pi_1 \\ne \\pi_2 \\end{align}\\) \n\n Step 2: Set the Significance Level, \\(\\alpha\\) \n Step 3: Calculate the Test Statistic \n\n \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})}\\), \\(\\bar{\\pi} = \\frac{y_1+y_2}{n_1+n_2}\\) is the pooled sample proportion estimating \\(\\pi\\) \n\n Step 4-c: Find the Critical Value \\(z_{\\alpha}\\) (one-tailed) or \\(z_{\\alpha/2}\\) (two-tailed) \n Step 5-c: Draw a Conclusion Using Critical Value Method \n\n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \n\n \\(H_1: \\pi_1 > \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z > z_{\\alpha}\\) \n \\(H_1: \\pi_1 < \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z < -z_{\\alpha}\\) \n \\(H_1: \\pi_1 \\ne \\pi_2\\): Reject \\(H_0\\) in favor of \\(H_1\\) if \\(|z| > z_{\\alpha/2}\\) \n\n\n\n Step 6: Restate the Conclusion in Nontechnical Terms, and Address the Original Claim \n\n Confidence Interval for \\(\\pi_1 - \\pi_2\\)\n\nThe \\(100(1 - \\alpha)\\%\\) confidence interval for \\(\\pi_1 - \\pi_2\\) is \\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\n\nRequirements:\n\n\n\\(n_1\\hat{\\pi}_1 \\ge 5\\), \\(n_1(1-\\hat{\\pi}_1) \\ge 5\\) and \\(n_2\\hat{\\pi}_2 \\ge 5\\), \\(n_2(1-\\hat{\\pi}_2) \\ge 5\\)\n\n\n\n\n\n Example: Effectiveness of Learning \n\n\n\nSuppose we do a study on 300 students to compare the effectiveness of learning statistics in online vs.Â in-person programs.\nRandomly assign\n\n125 students to the online program\nthe remaining 175 to the in-person program\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExam Results\nOnline Instruction\nIn-Person Instruction\n\n\n\nPass\n94\n113\n\n\nFail\n31\n62\n\n\nTotal\n125\n175\n\n\n\n\n\n\n\n\n\nIs there sufficient evidence to conclude that the online program is more effective than the traditional in-person program at \\(\\alpha=0.05\\)?\n\n\n\n\n\n\n Hypothesis Testing \n Step 0 \n\n \\(\\hat{\\pi}_1 = 94/125 = 0.75\\) and \\(\\hat{\\pi}_2 = 113/175 = 0.65\\). \n \\(n_1\\hat{\\pi}_1 = 94 > 5\\), \\(n_1(1-\\hat{\\pi}_1) = 31 > 5\\), and \\(n_2\\hat{\\pi}_2 = 113 > 5\\), \\(n_2(1-\\hat{\\pi}_2) = 62 > 5\\) \n\n Step 1 \n\n\n \\(H_0: \\pi_1 = \\pi_2\\) vs.Â \\(H_1: \\pi_1 > \\pi_2\\) \n\n\n\\(\\pi_1\\) \\((\\pi_2)\\) is the population proportion of students passing the exam in the online (in-person) program.\n\n Step 2 \n\n \\(\\alpha = 0.05\\) \n\n Step 3 \n\n \\(\\bar{\\pi} = \\frac{94+113}{125+175} = 0.69\\) \n \\(z_{test} = \\dfrac{\\hat{\\pi}_1 - \\hat{\\pi}_2}{\\sqrt{\\bar{\\pi}(1-\\bar{\\pi})(\\frac{1}{n_1} + \\frac{1}{n_2}})} = \\frac{0.75 - 0.65}{\\sqrt{0.69(1-0.69)(\\frac{1}{125} + \\frac{1}{175})}} = 1.96\\) \n\n Step 4-c \n\n \\(z_{\\alpha} = z_{0.05} = 1.645\\) \n\n Step 5-c \n\n Reject \\(H_0\\) in favor of \\(H_1\\) if \\(z_{test} > z_{\\alpha}\\). \n Since \\(z_{test} > z_{\\alpha}\\), we reject \\(H_0\\). \n\n Step 6 \n\n We have sufficient evidence to conclude that the online program is more effective. \n\n Confidence Interval \n\nWe want to know how effective the online program is.\nEstimate \\(\\pi_1 - \\pi_2\\) using a \\(95\\%\\) confidence interval. \\[\\hat{\\pi}_1 - \\hat{\\pi}_2 \\pm z_{\\alpha/2}\\sqrt{\\frac{\\hat{\\pi}_1(1-\\hat{\\pi}_1)}{n_1}+\\frac{\\hat{\\pi}_2(1-\\hat{\\pi}_2)}{n_2}}\\]\n\n\\(z_{0.05/2} = 1.96\\)\nThe 95% confidence interval is \\[0.75 - 0.65 \\pm 1.96\\sqrt{\\frac{(0.75)(1-0.75)}{125} + \\frac{(0.65)(1-0.65)}{175}}\\\\\n= (0.002, 0.210)\\]\n\nBecause 0 is not included in this interval, we reach the same conclusion as the hypothesis testing.\n\n Implementation in R \n\nBelow is a demonstration of how to make inferences about two proportions in R.\n\n\n# Use alternative = \"two.sided\" to get CI\nprop.test(x = c(94, 113), n = c(125, 175), alternative = \"greater\", correct = FALSE)\n\n\n    2-sample test for equality of proportions without continuity correction\n\ndata:  c(94, 113) out of c(125, 175)\nX-squared = 3.8509, df = 1, p-value = 0.02486\nalternative hypothesis: greater\n95 percent confidence interval:\n 0.01926052 1.00000000\nsample estimates:\n   prop 1    prop 2 \n0.7520000 0.6457143 \n\nprop_ci <- prop.test(x = c(94, 113), n = c(125, 175), alternative = \"two.sided\", correct = FALSE)\nprop_ci$conf.int\n\n[1] 0.002588801 0.209982628\nattr(,\"conf.level\")\n[1] 0.95"
  },
  {
    "objectID": "infer-prop.html#exercises",
    "href": "infer-prop.html#exercises",
    "title": "18Â  Inference About Proportions",
    "section": "\n18.4 Exercises",
    "text": "18.4 Exercises\n\nLipitor (atorvastatin) is a drug used to control cholesterol. In clinical trials of Lipitor, 98 subjects were treated with Lipitor and 245 subjects were given a placebo. Among those treated with Lipitor, 6 developed infections. Among those given a placebo, 24 developed infections. Use a 0.05 significance level to test the claim that the rate of inflections was the same for those treated with Lipitor and those given a placebo.\n\nTest the claim using the critical-value and p-value methods.\nTest the claim by constructing a confidence interval."
  },
  {
    "objectID": "infer-goodnessfit.html#sec-infer-goodnessfit",
    "href": "infer-goodnessfit.html#sec-infer-goodnessfit",
    "title": "\n19Â  Inference about Categorical Data\n",
    "section": "\n19.1 Test of Goodness of Fit",
    "text": "19.1 Test of Goodness of Fit\n\n\n\n Categorical Variable with More Than 2 Categories \n\n\n\nA categorical variable has \\(k\\) categories \\(A_1, \\dots, A_k\\).\n\n\n\nSubject\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(A_k\\)\n\n\n\n1\nx\n\n\n\n\n\n\n2\n\nx\n\n\n\n\n\n3\n\n\n\n\nx\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\\(n\\)\n\n\n\nx\n\n\n\n\n\n\n\n\nWith the size \\(n\\), for categories \\(A_1, \\dots , A_k\\), their observed count is \\(O_1, \\dots, O_k\\), and \\(\\sum_{i=1}^kO_i = n\\).\nOne-way count table:\n\n\n\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(A_k\\)\nTotal\n\n\n\\(O_1\\)\n\\(O_2\\)\n\\(\\cdots\\)\n\\(O_k\\)\n\\(n\\)\n\n\n\n\n Example \n\n\n\nAre the selected jurors racially representative of the population?\nIf the jury is representative of the population, the proportions in the sample should reflect the proportions of the population of eligible jurors (i.e.Â registered voters).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nRepresentation in juries\n205\n26\n25\n19\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nRepresentation in juries\n0.745\n0.095\n0.091\n0.069\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\n\n\nAre the proportions of juries close enough to the proportions of registered voters, so that we are confident saying that the jurors really were randomly sampled from the registered voters?\n\n\n\n\n\n\n\n Goodness-of-Fit Test \n\nA goodness-of-fit test tests the hypothesis that the observed frequency distribution fits or conforms to some claim distribution.\n\n\n\n\n\n\n\nIn the jury example, what is our observed frequency distribution, and what is our claim distribution?\n\n\n\n\n\n\n\n\n\n\n\n\nIf the individuals are randomly selected to serve on a jury, about how many of the 275 people would we expect to be white? How about black?\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\nAbout \\(72\\%\\) of the population is white, so we would expect about \\(72\\%\\) of the jurors to be white.\n\n\n\\(0.72 \\times 275 = 198\\).\n\n\nWe expect about \\(7\\%\\) of the jurors to be black.\n\nThis corresponds to about \\(0.07 \\times 275 = 19.25\\) black jurors.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n205\n26\n25\n19\n\n\n\nExpected Count\n198\n19.25\n33\n24.75\n\n\nPopulation Proportion \\((H_0)\\)\n\n0.72\n0.07\n0.12\n0.09\n\n\n\n\nThe observed count and expected count will be similar if there was no bias in selecting the members of the jury.\nWe want to test whether the differences are strong enough to provide convincing evidence that the jurors were not selected from a random sample of all registered voters.\n\n Example \n\n \\(\\begin{align} &H_0: \\text{No racial bias in who serves on a jury, and } \\\\ &H_1: \\text{There is racial bias in juror selection} \\end{align}\\) \n \\(\\begin{align} &H_0: \\pi_1 = \\pi_1^0, \\pi_2 = \\pi_2^0, \\dots, \\pi_k = \\pi_k^0\\\\ &H_1: \\pi_i \\ne \\pi_i^0 \\text{ for some } i \\end{align}\\) \n Under \\(H_0\\), \\(\\chi^2_{test} = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} + \\cdots + \\frac{(O_k - E_k)^2}{E_k}\\), \\(E_i = n\\pi_i^0, i = 1, \\dots, k\\) \nReject \\(H_0\\) if  \\(\\chi^2_{test} > \\chi^2_{\\alpha, df}\\), \\(df = k-1\\) \nRequire each \\(E_i \\ge 5\\), \\(i = 1, \\dots, k\\).\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\n\n\n\nUnder \\(H_0\\), \\(\\chi^2_{test} = \\frac{(205 - 198)^2}{198} + \\frac{(26 - 19.25)^2}{19.25} + \\frac{(25 - 33)^2}{33} + \\frac{(19 - 24.75)^2}{24.75} = 5.89\\)\n\n\n\\(\\chi^2_{0.05, 3} = 7.81\\).\nBecause \\(5.89 < 7.81\\), we fail to reject \\(H_0\\) in favor of \\(H_1\\).\nWe do not have convincing evidence of racial bias in the juror selection process.\n\n Goodness-of-Fit Test in R \n\nBelow is an example of how to perform a Goodness-of-Fit test in R.\n\n\nobs <- c(205, 26, 25, 19)\npi_0 <- c(0.72, 0.07, 0.12, 0.09)\n\n## Use chisq.test() function\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 5.8896, df = 3, p-value = 0.1171"
  },
  {
    "objectID": "infer-goodnessfit.html#test-of-independence",
    "href": "infer-goodnessfit.html#test-of-independence",
    "title": "\n19Â  Inference about Categorical Data\n",
    "section": "\n19.2 Test of Independence",
    "text": "19.2 Test of Independence\n Contingency Table and Expected Count\n Contingency Table \n\nWe have TWO categorical variables, and we want to test whether or not the two variables are independent.\n Does the opinion of the Presidentâ€™s job performance depend on gender? \n\n\n\n\n\nJob performance: approve, disapprove, no opinion\n\nGender: male, female\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\nMale\n18\n22\n10\n\n\nFemale\n23\n20\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Expected Count \n\nCompute the expected count of each cell in the two-way table under the condition that the two variables are independent of each other.\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\nThe expected count for the \\(i\\)-th row and \\(j\\)-th column, which is listed in parentheses in the table above, is: \\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\]\n\n\n\n Test of Independence Procedure \n\nRequirements:\n\nEvery \\(E_{ij} \\ge 5\\) in the contingency table.\n\n\n \\(\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\)\n\n\\(\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the table.\nReject \\(H_0\\) if \\(\\chi^2_{test} > \\chi^2_{\\alpha, \\, df}\\), \\(df = (r-1)(c-1)\\).\n\n Example \n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\n \\(\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\)\n\\(\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\)\n\n\\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\).\nSince \\(\\chi_{test}^2 < \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\).\nWe fail to conclude that the opinion of the Presidentâ€™s job performance depends on gender.\n\n Test of Independence in R \n\nBelow is an example of how to perform the test of independence using R.\n\n\n(contingency_table <- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(indept_test <- chisq.test(contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.65019, df = 2, p-value = 0.7225\n\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  ## critical value\n\n[1] 5.991465"
  },
  {
    "objectID": "infer-goodnessfit.html#exercises",
    "href": "infer-goodnessfit.html#exercises",
    "title": "\n19Â  Inference about Categorical Data\n",
    "section": "\n19.3 Exercises",
    "text": "19.3 Exercises\n\nA researcher has developed a model for predicting eye color. After examining a random sample of parents, she predicts the eye color of the first child. The table below lists the eye colors of offspring. On the basis of her theory, she predicted that 87% of the offspring would have brown eyes, 8% would have blue eyes, and 5% would have green eyes. Use 0.05 significance level to test the claim that the actual frequencies correspond to her predicted distribution.\n\n\n\nEye Color\nBrown\nBlue\nGreen\n\n\nFrequency\n127\n21\n5\n\n\n\nIn a study of high school students at least 16 years of age, researchers obtained survey results summarized in the accompanying table. Use a 0.05 significance level to test the claim of independence between texting while driving and driving when drinking alcohol. Are these two risky behaviors independent of one another?\n\n\n\n\nDrove after drinking alcohol?\n\n\n\n\n\nYes\nNo\n\n\nTexted while driving\n720\n3027\n\n\nDid not text while driving\n145\n4472"
  },
  {
    "objectID": "infer-bayes.html#the-r-user-interface",
    "href": "infer-bayes.html#the-r-user-interface",
    "title": "21Â  Bayesian Inference",
    "section": "21.1 The R User Interface",
    "text": "21.1 The R User Interface"
  },
  {
    "objectID": "infer-nonpar.html#the-r-user-interface",
    "href": "infer-nonpar.html#the-r-user-interface",
    "title": "22Â  Nonparametric Inference",
    "section": "22.1 The R User Interface",
    "text": "22.1 The R User Interface"
  },
  {
    "objectID": "model.html#the-r-user-interface",
    "href": "model.html#the-r-user-interface",
    "title": "Statistical Models",
    "section": "The R User Interface",
    "text": "The R User Interface"
  },
  {
    "objectID": "model-anova.html#anova-rationale",
    "href": "model-anova.html#anova-rationale",
    "title": "23Â  Analysis of Variance",
    "section": "\n23.1 ANOVA Rationale",
    "text": "23.1 ANOVA Rationale\n Comparing More Than Two Population Means \n\nIn many research settings, we want to compare 3 or more population means.\n\n\n\n\n Are there differences in the mean readings of 4 types of devices used to determine the pH of soil samples? \n\n\n\n\n\n\n\n\n\n\n\n Do different treatments (None, Fertilizer, Irrigation, Fertilizer and Irrigation) affect the mean weights of poplar trees? \n\n\n\n\n\n\n\n\n\n\n\n\n One-Way Analysis of Variance \n\nA factor is a property or characteristic (categorical variable) that allows us to distinguish the different populations from one another.\n\n\nType of device and treatment of trees are factors from the examples previously provided.\n\n\nOne-way ANOVA examines the effect of a categorical variable on the mean of a numerical variable (response).\n\nWe use analysis of  variance  to test the equality of 3 or more population  means. ðŸ¤”\nThe method is one-way because we use one single property (categorical variable) for categorizing the populations.\n\n\n\n Requirements \n\nThe populations of each category are normally distributed.\nThe populations have the same variance \\(\\sigma^2\\) (two sample pooled \\(t\\)-test).\nThe samples are random samples.\nThe samples are independent of each other (not matched or paired in any way).\n\n Rationale \n\nData 1 and Data 2 have the same group sample means \\(\\bar{y}_1\\), \\(\\bar{y}_2\\) and \\(\\bar{y}_3\\) denoted as red dots.\nHowever, they differ with regards to the variance within each group.\n\n\n\n\n\nFigureÂ 23.1: Boxplots illustrating the variance within samples\n\n\n\n\n\n\n\n\n\n\nFor which data do you feel more confident in saying the population means \\(\\mu_1\\), \\(\\mu_2\\) and \\(\\mu_3\\) are not all the same?\n\n\n\n\n\n\n\n Variation Between Samples & Variation Within Samples \n\nData 1: Variability between samples is large in comparison to the variation within samples.\nData 2: Variation between samples is small relatively to the variation within samples.\n\n\n\n\n\n\n\nWe are more confident concluding there is a difference in population means when variation between samples is larger than variation within samples.\n\n\n\n\n\n\n\n\n\n\nFigureÂ 23.2: Illustration of small and large variance within samples"
  },
  {
    "objectID": "model-anova.html#anova-procedures",
    "href": "model-anova.html#anova-procedures",
    "title": "23Â  Analysis of Variance",
    "section": "\n23.2 ANOVA Procedures",
    "text": "23.2 ANOVA Procedures\n\n \\(\\begin{align} &H_0: \\mu_1 = \\mu_2 = \\cdots = \\mu_k\\\\ &H_1: \\text{Population means are not all equal} \\end{align}\\) \n\n\nStatistician Ronald Fisher found a way to define a variable that follows the \\(F\\) distribution: \\[\\frac{\\text{variance between samples}}{\\text{variance within samples}} \\sim F_{df_B,\\, df_W}\\]\n\nIf the variance between samples is larger than the variance within samples (\\(F_{test}\\) is much greater than 1), as in Data 1, we reject \\(H_0\\).\n\n\n\n\n\n\n\nKey\n\n\n\n\nDefine variance between samples and variance within samples so that the ratio is \\(F\\) distributed.\n\n\n\n\n Variance Within Samples \n\nBack to the two-sample pooled \\(t\\)-test with equal variance, \\(\\sigma^2\\). We have \\[s_p^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}\\]\n\n\n\n\n\n\n\n\nHow about the pooled sample variance for \\(k\\) samples?\n\n\n\n\n\n\n\nANOVA assumes the populations have the same variance such that \\(\\sigma_1^2 = \\sigma_2^2 = \\cdots = \\sigma_k^2 = \\sigma^2\\). \\[\\boxed{s_W^2 = \\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2 + \\cdots + (n_k-1)s_k^2}{n_1 + n_2 + \\cdots + n_k - k}}\\] where \\(s_i^2\\), \\(i = 1, \\dots ,k\\), is the sample variance of group \\(i\\).\n\n\\(s_W^2\\) represents a combined estimate of the common variance, \\(\\sigma^2\\).\n\nIt measures variability of the observations within the \\(k\\) populations.\n\n\n\n\n Variance Between Samples \n\\[\\boxed{s^2_{B} = \\frac{\\sum_{i=1}^k n_i (\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot})^2}{k-1}}\\]\n\n\n\\(\\bar{y}_{i\\cdot}\\) is the \\(i\\)-th sample mean.\n\n\\(\\bar{y}_{\\cdot\\cdot}\\) is the grand sample mean with all data points in all groups combined.\n\n\\(s^2_{B}\\) is also an estimate of \\(\\sigma^2\\) and measures variability among sample means for the \\(k\\) groups.\nIf \\(H_0\\) is true \\((\\mu_1 = \\cdots = \\mu_k = \\mu)\\), any variation in the sample means is due to chance and randomness, so it shouldnâ€™t be too large.\n\n\n\\(\\bar{y}_{1\\cdot}, \\cdots, \\bar{y}_{k\\cdot}\\) should be close each other and should be close to \\(\\bar{y}_{\\cdot \\cdot}\\).\n\n\n\n\n ANOVA Table: Sum of Squares \n\nTotal Sum of Squares (SST) measures the total variation around \\(\\bar{y}_{\\cdot\\cdot}\\) in all of the sample data combined (ignoring the groups): \\[\\scriptsize{\\color{blue}{SST = \\sum_{j=1}^{n_i}\\sum_{i=1}^{k} \\left(y_{ij} - \\bar{y}_{\\cdot\\cdot}\\right)^2}}\\] where \\(y_{ij}\\) is the \\(j\\)-th data point in the \\(i\\)-th group.\nSum of Squares Between Samples (SSB) measures the variation between sample means: \\[\\scriptsize{ \\color{blue}{SSB = \\sum_{i=1}^{k}n_i \\left(\\bar{y}_{i\\cdot} - \\bar{y}_{\\cdot\\cdot}\\right)^2}}\\]\nSum of Squares Within Samples (SSW) measures the variation of any value, \\(y_{ij}\\), about its sample mean, \\(\\bar{y}_{i\\cdot}\\): \\[\\scriptsize{ \\color{blue}{SSW = \\sum_{i=1}^{k} \\sum_{j=1}^{n_i} \\left(y_{ij} - \\bar{y}_{i\\cdot}\\right)^2 = \\sum_{i=1}^{k} (n_i - 1)s_i^2}}\\]\n\n Sum of Squares Identity \n\n\\(SST = SSB + SSW\\)\n\\(df_{T} = df_{B} + df_{W} \\implies N - 1 = (k-1) + (N - k)\\) \n\\(\\text{Mean Square (MS)} = \\frac{\\text{sum of squares}}{\\text{degrees of freedom}}\\)\n\\(MSB = \\frac{SSB}{k-1} = s^2_{B}\\)\n\\(MSW = \\frac{SSW}{N-k} = s^2_{W}\\)\n\\(F_{test} = \\frac{MSB}{MSW}\\)\nUnder \\(H_0\\), \\(\\frac{S^2_{B}}{S_W^2} \\sim F_{k-1, \\, N-k}\\)\n\nReject \\(H_0\\) if\n\n\\(F_{test} > F_{\\alpha, \\, k - 1,\\, N-k}\\)\n\n\\(p\\)-value \\(P(F_{k - 1,\\, N-k} > F_{test}) < \\alpha\\)\n\n\n\n\n ANOVA Table"
  },
  {
    "objectID": "model-anova.html#anova-example",
    "href": "model-anova.html#anova-example",
    "title": "23Â  Analysis of Variance",
    "section": "\n23.3 ANOVA Example",
    "text": "23.3 ANOVA Example\n\nWe hypothesize that a nutrient called â€œisoflavonesâ€ varies among three types of food: (1) cereals and snacks, (2) energy bars and (3) veggie burgers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA sample of 5 is taken from each type of food and the amount of isoflavones is measured.\nIs there a sufficient evidence to conclude that the mean isoflavone levels vary among these food items at \\(\\alpha = 0.05\\)?\n\n\n Data \n\n\n\n\nWe prefer a data format like the one shown on the right.\n\n\n\n\ndata\n\n   1  2  3\n1  3 19 25\n2 17 10 15\n3 12  9 12\n4 10  7  9\n5  4  5  8\n\n\n\n\n\n\n\n\nSo tell me what is the value of \\(y_{23}\\)!\n\n\n\n\n\n\n\n\n\n\ndata_anova\n\n    y    food\n1   3 cereals\n2  17 cereals\n3  12 cereals\n4  10 cereals\n5   4 cereals\n6  19  energy\n7  10  energy\n8   9  energy\n9   7  energy\n10  5  energy\n11 25  veggie\n12 15  veggie\n13 12  veggie\n14  9  veggie\n15  8  veggie\n\n\n\n\n\n\n\n\n\nFigureÂ 23.3: Boxplot of the Isoflavone Content in 3 Types of Food\n\n\n\n\n\n Test Assumptions \n\nAssumptions:\n\n\n\\(\\sigma_1 = \\sigma_2 = \\sigma_3\\) (I tested it).\nData are generated from a normal distribution for each type of food (QQ plots confirm this).\n\n\n\n\n\n\n\nFigureÂ 23.4: QQ plots for each type of food\n\n\n\n\n ANOVA Testing \n\n \\(\\begin{align}&H_0: \\mu_1 = \\mu_2 = \\mu_3\\\\&H_1: \\mu_is \\text{ not all equal} \\end{align}\\) \n\n\n\n\n\n\n\n\n\n\nWe can do all the calculations and generate an ANOVA table using just one line of code, as shown below.\n\n\nanova(lm(y ~ food, data = data_anova))\n\nAnalysis of Variance Table\n\nResponse: y\n          Df Sum Sq Mean Sq F value Pr(>F)\nfood       2   60.4   30.20   0.828   0.46\nResiduals 12  437.6   36.47"
  },
  {
    "objectID": "model-reg.html#correlation",
    "href": "model-reg.html#correlation",
    "title": "24Â  Linear Regression",
    "section": "\n24.1 Correlation",
    "text": "24.1 Correlation\n Relationship Between 2 Numerical Variables \n\nDepending on the situation, the two variables can be classified as the explanatory variable and the response variable. (Discussed in Regression)\nHowever, there is not always an explanatory-response relationship.\nExamples:\n\n height and weight \n income and age \n SAT/ACT math score and verbal score \n amount of time spent studying for an exam and exam grade \n\n\n\n\n\n\n\n\n\nCan you provide an example that 2 variables are associated?\n\n\n\n\n\n\n\n Scatterplots \n\n\n\n\nFigureÂ 24.1: Examples of scatterplots\n\n\n\n\n\nThe overall pattern can be described in several ways.\n\nForm: linear or clusters\nDirection: positively associated or negatively associated\nStrength: how close the points lie to a line/curve\n\n\n\n\n Linear Correlation Coefficient \n\nThe sample correlation coefficient, denoted by \\(r\\), measures the direction and strength of the linear relationship between two numerical variables: \\[\\small r :=\\frac{1}{n-1}\\sum_{i=1}^n\\left(\\frac{x_i-\\overline{x}}{s_x}\\right)\\left(\\frac{y_i-\\overline{y}}{s_y}\\right) = \\frac{\\sum_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y})}{\\sqrt{\\sum_{i=1}^n(x_i-\\overline{x})^2\\sum_{i=1}^n(y_i-\\overline{y})^2}}\\]\n\n\n\n\n\n\n\\(-1 \\le r\\le 1\\)\n\n\\(r > 0\\): The larger value of \\(X\\) is, the larger value of \\(Y\\) tends to be.\n\n\\(r = 1\\): Perfect positive linear relationship.\n\n\n\n\n\n\nFigureÂ 24.2: Positive correlation between two variables\n\n\n\n\n\n\n\n\n\n\n\n\\(r < 0\\): The larger value of \\(X\\) is, the smaller value of \\(Y\\) tends to be.\n\n\\(r = -1\\): Perfect negative linear relationship\n\n\n\n\n\n\nFigureÂ 24.3: Negative correlation between two variables\n\n\n\n\n\n\n\n\n\n\n\n\\(r = 0\\): No linear relationship.\nIf the explanatory and response variables are switched, \\(r\\) remains the same.\n\n\\(r\\) has no units of measurement, so scale changes do not affect \\(r\\).\n\n\n\n\n\n\nFigureÂ 24.4: No correlation between two variables\n\n\n\n\n\n\n Example \n\nIt is possible that there is a strong relationship between two variables, but they still have \\(r = 0\\).\n\n\n\n\n\nFigureÂ 24.5: Examples of relationships between two variables and their correlation coefficients (https://upload.wikimedia.org/wikipedia/commons/d/d4/Correlation_examples2.svg)\n\n\n\n\n Example in R \n\n\n\nplot(x = mtcars$wt, y = mtcars$mpg, \n     main = \"MPG vs. Weight\", \n     xlab = \"Car Weight\", \n     ylab = \"Miles Per Gallon\", \n     pch = 16, col = 4, las = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\ncor(x = mtcars$wt,\n    y = mtcars$mpg)\n\n[1] -0.8676594"
  },
  {
    "objectID": "model-reg.html#introduction-to-regression",
    "href": "model-reg.html#introduction-to-regression",
    "title": "24Â  Linear Regression",
    "section": "\n24.2 Introduction to Regression",
    "text": "24.2 Introduction to Regression\n What is Regression? \n\n\nRegression models the relationship between one or more numerical/categorical response variables \\((Y)\\) and one or more numerical/categorical explanatory variables \\((X)\\).\nA regression function, \\(f(X)\\), describes how a response variable, \\(Y\\), on average, changes as an explanatory variable, \\(X\\), changes.\n\n\n\n\nExamples:\n\n College GPA \\((Y)\\) vs.Â ACT/SAT score \\((X)\\)\n Sales \\((Y)\\) vs.Â Advertising Expenditure \\((X)\\)\n Crime Rate \\((Y)\\) vs.Â Median Income Level \\((X)\\) \n\n\n\n\n\n\n\n\nFigureÂ 24.6: Example of a Regression Function\n\n\n\n\n\n\n\n Unknown Regression Function \n\nThe true relationship between \\(X\\) and the mean of \\(Y\\), the regression function \\(f(X)\\), is unknown.\nThe collected data \\((x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\) are all we know and have.\n\n\n\n\n\nFigureÂ 24.7: Data with an unknown regression function\n\n\n\n\n\nGoal: Estimate \\(f(X)\\) from our data, and use it to predict the value of \\(Y\\) given a value of \\(X\\).\n\n\n Simple Linear Regression \n\nStart with simple linear regression.\n\nThere is only one predictor, \\(X\\) (known and constant), and one response variable, \\(Y\\).\nThe regression function used for predicting \\(Y\\) is a linear function.\nUse a regression line in an X-Y plane to predict the value of \\(Y\\) for a given value of \\(X = x\\).\n\n\n\n\n\n\n\nMath review: A linear function \\(y = f(x) = \\beta_0 + \\beta_1 x\\) represents a straight line.\n\n\n\\(\\beta_1\\): slope, the amount by which \\(y\\) changes when \\(x\\) increases by one unit\n\n\\(\\beta_0\\): intercept, the value of \\(y\\) when \\(x = 0\\)\n\nThe linearity assumption: \\(\\beta_1\\) does not change as \\(x\\) changes.\n\n\n\n\n\n\n\n\nFigureÂ 24.8: Example of a regression line\n\n\n\n\n\n\n Sample Data: Relationship Between X and Y \n\nReal data \\((x_i, y_i), i = 1, 2, \\dots, n\\) do not form a perfect straight line!\n\\(y_i = \\beta_0+\\beta_1x_i + \\color{red}{\\epsilon_i}\\)\n\n\n\n\n\nFigureÂ 24.9: Actual relationship between X and Y isnâ€™t perfectly linear\n\n\n\n\n\nWhen we collect our data, at any given level of \\(X = x\\), \\(y\\) is assumed to be drawn from a normal distribution (for inference purpose).\nIts value varies and will not be exactly equal to its mean, \\(\\mu_y\\).\n\n\n\n\n\nFigureÂ 24.10: Illustration that the responses, y, follow a normal distribution\n\n\n\n\n\nThe mean of \\(Y\\) and \\(X\\) form a straight line.\n\n\n\n\n\nFigureÂ 24.11: Illustration that the regression line is formed from the mean of Y and X\n\n\n\n\n\n Simple Linear Regression Model (Population) \n\nFor the \\(i\\)-th measurement in the target population, \\[Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\]\n\n\n\\(Y_i\\): the \\(i\\)-th value of the response (random) variable.\n\n\\(X_i\\): the \\(i\\)-th fixed known value of the predictor.\n\n\\(\\epsilon_i\\): the \\(i\\)-th random error with the assumption \\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\).\n\n\\(\\beta_0\\) and \\(\\beta_1\\) are model coefficients.\n\n\\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\) are fixed unknown parameters to be estimated from the sample data once we collect them.\n\n\n\n Important Features of Model \\(Y_i = \\beta_0 + \\beta_1X_i + \\epsilon_i\\)\n\n\n\\(\\epsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) \\[\\begin{align*}\n\\mu_{Y_i \\mid X_i} &= E(\\beta_0 + \\beta_1X_i + \\epsilon_i) \\\\\n&= \\beta_0 + \\beta_1X_i\n\\end{align*}\\]\n\nThe mean response, \\(\\mu_{Y\\mid X}\\), has a straight-line relationship with \\(X\\) given by a population regression line \\[\\mu_{Y\\mid X} = \\beta_0 + \\beta_1X\\]\n\n\n\n\n\n\n\nFigureÂ 24.12: Illustration of the straight line relationship between \\(\\mu_{Y\\mid X}\\) and \\(X\\)\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\nVar(Y_i \\mid X_i) &= Var(\\epsilon_i) = \\sigma^2\n\\end{align*}\\]\n\nThe variance of \\(Y\\) does not depend on \\(X\\).\n\n\n\n\n\n\nFigureÂ 24.13: \\(Y\\) has a constant variance regardless of \\(X\\).\n\n\n\n\n\n\n\n\n\n\\[\\begin{align*}\nY_i \\mid X_i \\stackrel{indep}{\\sim} N(\\beta_0 + \\beta_1X_i, \\sigma^2)\n\\end{align*}\\]\n\nFor any fixed value of \\(X_i = x_i\\), the response, \\(Y_i\\), varies with \\(N(\\mu_{Y_i\\mid x_i}, \\sigma^2)\\).\n\n\n\n\n\n\nFigureÂ 24.14: The response \\(Y_i\\) follows a normal distribution.\n\n\n\n\n\n\n\n\nJob: Collect data and estimate the unknown parameters \\(\\beta_0\\), \\(\\beta_1\\) and \\(\\sigma^2\\)!"
  },
  {
    "objectID": "model-reg.html#fitting-a-regression-line-haty-b_0-b_1x",
    "href": "model-reg.html#fitting-a-regression-line-haty-b_0-b_1x",
    "title": "24Â  Linear Regression",
    "section": "\n24.3 Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n",
    "text": "24.3 Fitting a Regression Line \\(\\hat{Y} = b_0 + b_1X\\)\n\n Idea of Fitting \n\nGiven the sample data \\(\\{ (x_1, y_1), (x_2, y_2), \\dots, (x_n, y_n)\\},\\)\n\nWhich sample regression line is the best?\nWhat are the best estimators, \\(b_0\\) and \\(b_1\\), for \\(\\beta_0\\) and \\(\\beta_1\\)?\n\n\n\n\n\n\n\nFigureÂ 24.15: One data set can have multiple fitted regression lines\n\n\n\n\n\nWe are interested in \\(\\beta_0\\) and \\(\\beta_1\\) in the following sample regression model: \\[\\begin{align*}\ny_i = \\beta_0 + \\beta_1~x_{i} + \\epsilon_i,\n\\end{align*}\\] or \\[E({y}_{i}) = \\mu_{y|x_i} = \\beta_0 + \\beta_1~x_{i}\\]\nWe use the sample statistics \\(b_0\\) and \\(b_1\\), which are computed from our sample data, to estimate \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\hat{y}_{i} = b_0 + b_1~x_{i}\\) is called the fitted value of \\(y_i\\) and is a point estimate of the mean, \\(\\mu_{y|x_i}\\), and \\(y_i\\) itself.\n\n\n Ordinary Least Squares (OLS) \n\nWhat does best mean?\n\nWe want to choose \\(b_0\\) and \\(b_1\\) or the sample regression line \\(b_0 + b_1x\\) that minimizes the sum of squared residuals \\(SS_{res}\\).\n\n\nThe residual, \\(e_i = y_i - \\hat{y}_i = y_i - (b_0 + b_1x_i)\\), is a point estimate of \\(\\epsilon_i\\).\nThe sample regression line minimizes \\(SS_{res} = e_1^2 + e_2^2 + \\dots + e_n^2 = \\sum_{i = 1}^n e_i^2\\). \\[\\small{\\begin{align} SS_{res} &= (y_1 - b_0 - b_1x_1)^2 + (y_2 - b_0 - b_1x_2)^2 + \\dots + (y_n - b_0 - b_1x_n)^2\\\\ &= \\sum_{i=1}^n(y_i - b_0 - b_1x_i)^2 \\end{align}}\\]\n\n\n Visualizing Residuals \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Least Squares Estimates (LSE) \n\nIn the least squares approach, we choose the \\(b_0\\) and \\(b_1\\) that minimize the \\(SS_{res}\\). \\[(b_0, b_1) = \\arg \\min_{\\beta_0, \\beta_1} \\sum_{i=1}^n(y_i - \\beta_0 - \\beta_1x_i)^2\\]\nMATH 1450 â€¦\n\n\\[\\color{red}{b_0 = \\overline{y} - b_1\\overline{x}}\\]\n\\[\\color{red}{b_1 = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})}{\\sum_{i=1}^n(x_i - \\overline{x})^2} = \\frac{S_{xy}}{S_{xx}} = r \\frac{\\sqrt{S_{yy}}}{\\sqrt{S_{xx}}}},\\] where \\(S_{xx} = \\sum_{i=1}^n(x_i - \\overline{x})^2\\), \\(S_{yy} = \\sum_{i=1}^n(y_i - \\overline{y})^2\\), \\(S_{xy} = \\sum_{i=1}^n(x_i - \\overline{x})(y_i - \\overline{y})\\)\n\n\n\n\n\n\nWhat can we learn from the formula of \\(b_0\\) and \\(b_1\\)?\n\n\n\n\n\n\n\n Estimation for \\(\\sigma^2\\) \n\nWe can think of \\(\\sigma^2\\) as variance around the line or the mean square (prediction) error.\nThe estimate of \\(\\sigma^2\\) is the mean square residual, \\(MS_{res}\\): \\[\\hat{\\sigma}^2 = MS_{res} = \\frac{SS_{res}}{n-2} = \\frac{\\sum_{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}\\]\n\n\n\\(MS_{res}\\) is often shown in computer output as \\(\\texttt{MS(Error)}\\) or \\(\\texttt{MS(Residual)}\\).\n\n\\(E(MS_{res}) = \\sigma^2\\)\n\nTherefore, \\(\\hat{\\sigma}^2\\) is an unbiased estimator for \\(\\sigma^2\\) ðŸ‘."
  },
  {
    "objectID": "model-reg.html#confidence-intervals-and-hypothesis-testing-for-beta_0-and-beta_1",
    "href": "model-reg.html#confidence-intervals-and-hypothesis-testing-for-beta_0-and-beta_1",
    "title": "24Â  Linear Regression",
    "section": "\n24.4 Confidence Intervals and Hypothesis Testing for \\(\\beta_0\\) and \\(\\beta_1\\)\n",
    "text": "24.4 Confidence Intervals and Hypothesis Testing for \\(\\beta_0\\) and \\(\\beta_1\\)\n\n Confidence Intervals for \\(\\beta_0\\) and \\(\\beta_1\\) \n\n\n\\(\\frac{b_1 - \\beta_1}{\\sqrt{\\hat{\\sigma}^2/S_{xx}}} \\sim t_{n-2}\\); \\(\\quad \\frac{b_0 - \\beta_0}{\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}\\)\n\nThe \\((1-\\alpha)100\\%\\) CI for \\(\\beta_1\\) is \\(b_1 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2/S_{xx}}\\)\n\nThe \\((1-\\alpha)100\\%\\) CI for \\(\\beta_0\\) is \\(b_0 \\pm t_{\\alpha/2, n-2}\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}\\)\n\n\n\n Hypothesis Testing \n \\(\\beta_1\\) \n\n\n \\(H_0: \\beta_1 = \\beta_1^0 \\quad H_1: \\beta_1 \\ne \\beta_1^0\\)  \n\nTest statistic: Under \\(H_0\\), \\[t_{test} = \\frac{b_1 - \\color{red}{\\beta_1^0}}{\\sqrt{\\frac{\\hat{\\sigma}^2}{S_{xx}}}} \\sim t_{n-2}\\]\n\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{$p$-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\)\n\n\n\n \\(\\beta_0\\) \n\n\n \\(H_0: \\beta_0 = \\beta_0^0 \\quad H_1: \\beta_0 \\ne \\beta_0^0\\)  \n\nTest statistic: Under \\(H_0\\), \\[t_{test} = \\frac{b_0 - \\color{red}{\\beta_0^0}}{\\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n} + \\frac{\\overline{x}^2}{S_{xx}}\\right)}} \\sim t_{n-2}\\]\n\nReject \\(H_0\\) in favor of \\(H_1\\) if\n\n\\(|t_{test}| > t_{\\alpha/2, \\, n-2}\\)\n\\(\\text{$p$-value} = 2P(t_{n-2} > |t_{test}|) < \\alpha\\)\n\n\n\n Interpretation of Testing Results \n\n \\(H_0: \\beta_1 = 0 \\quad H_1: \\beta_1 \\ne 0\\) \n\nFailing to reject \\(H_0: \\beta_1 = 0\\) implies there is no linear relationship between \\(Y\\) and \\(X\\).\n\n\n\n\n\nFigureÂ 24.16: Failing to reject \\(H_0\\) means there is no linear relationship between X and Y, but they could have some other type of relationship.\n\n\n\n\n\n\n\n\n\n\nIf we reject \\(H_0: \\beta_1 = 0\\), does it mean \\(X\\) and \\(Y\\) are linearly related?\n\n\n\n\n\n\n Test of Significance of Regression \n\n\nRejecting \\(H_0: \\beta_1 = 0\\) could mean\n\nthe straight-line model is adequate.\nbetter results could be obtained with a more complicated model.\n\n\n\n\n\n\n\nFigureÂ 24.17: Rejecting \\(H_0\\) doesnâ€™t necessarily mean that a linear model is the best model, just that it is adequate."
  },
  {
    "objectID": "model-reg.html#analysis-of-variance-anova-approach",
    "href": "model-reg.html#analysis-of-variance-anova-approach",
    "title": "24Â  Linear Regression",
    "section": "\n24.5 Analysis of Variance (ANOVA) Approach",
    "text": "24.5 Analysis of Variance (ANOVA) Approach\n \\(X\\) - \\(Y\\) Relationship Explains Some Deviation \n\n\n\n\n\n\nSuppose we only have data for \\(Y\\) and have no information about \\(X\\) or the relationship between \\(X\\) and \\(Y\\). How do we predict a value of \\(Y\\)?\n\n\n\n\n\n\n\nIf the data have no pattern, our best guess for \\(Y\\) would be \\(\\overline{y}\\) (i.e., \\(\\hat{y}_i = \\overline{y}\\)).\n\nWe would treat \\(X\\) and \\(Y\\) as uncorrelated.\n\n\nThe (total) deviation from the mean is \\((y_i - \\overline{y})\\)\n\nIf \\(X\\) and \\(Y\\) are linearly related, fitting a linear regression model helps us predict the value of \\(Y\\) when the value of \\(X\\) is provided.\n\n\\(\\hat{y}_i = b_0 + b_1x_i\\) is closer to \\(y_i\\) than \\(\\overline{y}\\).\nThe regression model explains some of the deviation of \\(y\\).\n\n\n Partition of Deviation \n\nTotal deviation = Deviation explained by regression + unexplained deviation\n\\((y_i - \\overline{y}) = (\\hat{y}_i - \\overline{y}) + (y_i - \\hat{y}_i)\\)\n\\((19 - 9) = (13 - 9) + (19 - 13)\\)\n\n\n\n\n\nFigureÂ 24.18: Explained vs.Â unexplained deviation\n\n\n\n\n Sum of Squares (SS) \n\n\\(\\sum_{i=1}^n(y_i - \\overline{y})^2 = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 + \\sum_{i=1}^n(y_i - \\hat{y}_i)^2\\)\nTotal SS \\((SS_T)\\) = Regression SS \\((SS_R)\\) + Residual SS \\((SS_{res})\\)\n\\(df_T = df_R + df_{res}\\)\n\\(\\color{blue}{(n-1) = 1 +(n-2)}\\)\n\n\n ANOVA for Testing Significance of Regression \n\n\n\n\nFigureÂ 24.19: Example of an ANOVA table\n\n\n\n\n\nA larger value of \\(F_{test}\\) indicates that the regression is significant.\nReject \\(H_0\\) if\n\n\\(F_{test} > F_{\\alpha, 1, n-2}\\)\n\n\\(\\text{$p$-value} = P(F_{1, n-2} > F_{test}) < \\alpha\\).\n\n\nANOVA is designed to test the \\(H_0\\) that all predictors have no value in predicting \\(y\\).\nIn SLR, the \\(F\\)-test of ANOVA gives the same result as a two-sided \\(t\\)-test of \\(H_0: \\beta_1=0\\).\n\n\n Coefficient of Determination \n\nThe coefficient of determination \\((R^2)\\) is the proportion of the variation in \\(y\\) that is explained by the regression model.\nIt is computed as \\[R^2 = \\frac{SS_R}{SS_T} =\\frac{SS_T - SS_{res}}{SS_T} = 1 - \\frac{SS_{res}}{SS_T}\\]\n\n\n\\(R^2\\) is the proportionate reduction of total variation associated with the use of \\(X\\).\n\n(a) \\(\\hat{y}_i = y_i\\) and \\(\\small SS_{res} = \\sum_{i=1}^n(y_i - \\hat{y}_i)^2 = 0\\). (b) \\(\\hat{y}_i = \\overline{y}\\) and \\(\\small SS_R = \\sum_{i=1}^n(\\hat{y}_i - \\overline{y})^2 = 0\\).\n\n\n\n\n\nFigureÂ 24.20: Examples of \\(R^2\\) being equal to 1 and 0"
  },
  {
    "objectID": "model-reg.html#prediction",
    "href": "model-reg.html#prediction",
    "title": "24Â  Linear Regression",
    "section": "\n24.6 Prediction",
    "text": "24.6 Prediction\n Predicting the Mean Response \n\nWith the predictor value \\(x = x_0\\), we want to estimate the mean response \\(E(y\\mid x_0) = \\mu_{y|x_0} = \\beta_0 + \\beta_1 x_0\\).\n\n Example: The mean highway MPG \\(E(y \\mid x_0)\\) when displacement is \\(x = x_0 = 5.5\\). \n\n\nIf \\(x_0\\) is within the range of \\(x\\), an unbiased point estimator for \\(E(y\\mid x_0)\\) is \\[\\widehat{E(y\\mid x_0)} = \\hat{\\mu}_{y | x_0} = b_0 + b_1 x_0\\]\n\nThe \\((1-\\alpha)100\\%\\) CI for \\(E(y\\mid x_0)\\) is \\(\\boxed{\\hat{\\mu}_{y | x_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{\\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\).\n\n\n\n\n\n\n\nDoes the length of the CI for \\(E(y\\mid x_0)\\) stay the same at any location of \\(x_0\\)?\n\n\n\n\n\n\n\n Predicting New Observations \n\nPredict the value of a new observation, \\(y_0\\), with \\(x = x_0\\).\n\n Example: The highway MPG of a car \\(y_0(x_0)\\) when its displacement is \\(x = x_0 = 5.5\\). \n\n\nAn unbiased point estimator for \\(y_0(x_0)\\) is \\[\\hat{y}_0(x_0) = b_0 + b_1 x_0\\]\n\nThe \\((1-\\alpha)100\\%\\) prediction interval (PI) for \\(y_0(x_0)\\) is \\(\\small \\boxed{\\hat{y_0} \\pm t_{\\alpha/2, n-2} \\hat{\\sigma}\\sqrt{1+ \\frac{1}{n} + \\frac{(x_0 - \\overline{x})^2}{S_{xx}}}}\\)\n\n\n\n\n\n\n\n\nWhat is the difference between CI for \\(E(y\\mid x_0)\\) and PI for \\(y_0(x_0)\\)?\n\n\n\n\n\n\n\n\nThe PI is wider as it includes the uncertainty about \\(b_0\\), \\(b_1\\) as well as \\(y_0\\) due to error, \\(\\epsilon\\)."
  },
  {
    "objectID": "model-reg.html#r-lab",
    "href": "model-reg.html#r-lab",
    "title": "24Â  Linear Regression",
    "section": "\n24.7 R Lab",
    "text": "24.7 R Lab\n mpg Data \n\nlibrary(ggplot2)  ## use data mpg in ggplot2 package\nmpg\n\n# A tibble: 234 Ã— 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   <chr>        <chr>      <dbl> <int> <int> <chr> <chr> <int> <int> <chr> <chr>\n 1 audi         a4           1.8  1999     4 autoâ€¦ f        18    29 p     compâ€¦\n 2 audi         a4           1.8  1999     4 manuâ€¦ f        21    29 p     compâ€¦\n 3 audi         a4           2    2008     4 manuâ€¦ f        20    31 p     compâ€¦\n 4 audi         a4           2    2008     4 autoâ€¦ f        21    30 p     compâ€¦\n 5 audi         a4           2.8  1999     6 autoâ€¦ f        16    26 p     compâ€¦\n 6 audi         a4           2.8  1999     6 manuâ€¦ f        18    26 p     compâ€¦\n 7 audi         a4           3.1  2008     6 autoâ€¦ f        18    27 p     compâ€¦\n 8 audi         a4 quattro   1.8  1999     4 manuâ€¦ 4        18    26 p     compâ€¦\n 9 audi         a4 quattro   1.8  1999     4 autoâ€¦ 4        16    25 p     compâ€¦\n10 audi         a4 quattro   2    2008     4 manuâ€¦ 4        20    28 p     compâ€¦\n# â„¹ 224 more rows\n\n\n\n Highway MPG hwy vs.Â Displacement displ \n\nplot(x = mpg$displ, y = mpg$hwy,\n     las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\n\n\n\n\n\n\n\n\n Fit Simple Linear Regression \n\n\n\nreg_fit <- lm(formula = hwy ~ displ, \n              data = mpg)\nreg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nCoefficients:\n(Intercept)        displ  \n     35.698       -3.531  \n\ntypeof(reg_fit)\n\n[1] \"list\"\n\n\n\n\n\n\n## all elements in reg_fit\nnames(reg_fit)\n\n [1] \"coefficients\"  \"residuals\"     \"effects\"       \"rank\"         \n [5] \"fitted.values\" \"assign\"        \"qr\"            \"df.residual\"  \n [9] \"xlevels\"       \"call\"          \"terms\"         \"model\"        \n\n## use $ to extract an element of a list\nreg_fit$coefficients\n\n(Intercept)       displ \n  35.697651   -3.530589 \n\n\n\n\n\n\\(\\widehat{hwy}_{i} = b_0 + b_1 \\times displ_{i} = 35.7 - 3.5 \\times displ_{i}\\)\n\n\\(b_1\\): For a one unit (liter) increase of the displacement, we expect the highway MPG to be decreased, on average, by 3.5.\n\n\n Fitted Values of \\(y\\) \n\n## the first 5 observed response value y\nmpg$hwy[1:5]\n\n[1] 29 29 31 30 26\n\n## the first 5 fitted value y_hat\nhead(reg_fit$fitted.values, 5)\n\n       1        2        3        4        5 \n29.34259 29.34259 28.63647 28.63647 25.81200 \n\n## the first 5 predictor value x\nmpg$displ[1:5]\n\n[1] 1.8 1.8 2.0 2.0 2.8\n\nlength(reg_fit$fitted.values)\n\n[1] 234\n\n\n\n Add a Regression Line \n\nplot(x = mpg$displ, y = mpg$hwy, las = 1, pch = 19, col = \"navy\", cex = 0.5,\n     xlab = \"Displacement (litres)\", ylab = \"Highway MPG\",\n     main = \"Highway MPG vs. Engine Displacement (litres)\")\nabline(reg_fit, col = \"#FFCC00\", lwd = 3)\n\n\n\n\n\n\n\n\n Standard Error of Regression \n\n(summ_reg_fit <- summary(reg_fit))\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\n\n# lots of fitted information saved in summary(reg_fit)!\nnames(summ_reg_fit)\n\n [1] \"call\"          \"terms\"         \"residuals\"     \"coefficients\" \n [5] \"aliased\"       \"sigma\"         \"df\"            \"r.squared\"    \n [9] \"adj.r.squared\" \"fstatistic\"    \"cov.unscaled\" \n\n# residual standard error (sigma_hat)\nsumm_reg_fit$sigma\n\n[1] 3.835985\n\n# from reg_fit\nsqrt(sum(reg_fit$residuals^2) / reg_fit$df.residual)\n\n[1] 3.835985\n\n\n\n Confidence Intervals and Testing for \\(\\beta_0\\) and \\(\\beta_1\\) \n\nconfint(reg_fit, level = 0.95)\n\n                2.5 %   97.5 %\n(Intercept) 34.278353 37.11695\ndispl       -3.913828 -3.14735\n\n\n\nsumm_reg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\n\nsumm_reg_fit$coefficients\n\n             Estimate Std. Error   t value      Pr(>|t|)\n(Intercept) 35.697651  0.7203676  49.55477 2.123519e-125\ndispl       -3.530589  0.1945137 -18.15085  2.038974e-46\n\n\n\n ANOVA Table \n\nFor \\(H_0: \\beta_1 = 0\\) in SLR, \\(t_{test}^2 = F_{test}\\).\n\n\nanova(reg_fit)\n\nAnalysis of Variance Table\n\nResponse: hwy\n           Df Sum Sq Mean Sq F value    Pr(>F)    \ndispl       1 4847.8  4847.8  329.45 < 2.2e-16 ***\nResiduals 232 3413.8    14.7                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsumm_reg_fit$coefficients\n\n             Estimate Std. Error   t value      Pr(>|t|)\n(Intercept) 35.697651  0.7203676  49.55477 2.123519e-125\ndispl       -3.530589  0.1945137 -18.15085  2.038974e-46\n\nsumm_reg_fit$coefficients[2, 3] ^ 2\n\n[1] 329.4533\n\n\n\n \\(R^2\\) \n\nsumm_reg_fit\n\n\nCall:\nlm(formula = hwy ~ displ, data = mpg)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.1039 -2.1646 -0.2242  2.0589 15.0105 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  35.6977     0.7204   49.55   <2e-16 ***\ndispl        -3.5306     0.1945  -18.15   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 3.836 on 232 degrees of freedom\nMultiple R-squared:  0.5868,    Adjusted R-squared:  0.585 \nF-statistic: 329.5 on 1 and 232 DF,  p-value: < 2.2e-16\n\n\n\nsumm_reg_fit$r.squared\n\n[1] 0.5867867\n\n\n\n Prediction \n\n## CI for the mean response\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"confidence\", level = 0.95)\n\n       fit      lwr      upr\n1 16.27941 15.35839 17.20043\n\n## PI for the new observation\npredict(reg_fit, newdata = data.frame(displ = 5.5), interval = \"predict\", level = 0.95)\n\n       fit      lwr      upr\n1 16.27941 8.665682 23.89314"
  },
  {
    "objectID": "model-reg.html#exercises",
    "href": "model-reg.html#exercises",
    "title": "24Â  Linear Regression",
    "section": "\n24.8 Exercises",
    "text": "24.8 Exercises\nUse the data in the table below to answer questions 1-7.\n\n\nTar\n24\n28\n21\n23\n20\n22\n20\n25\n\n\n\n\nNicotine\n1.6\n1.7\n1.2\n1.4\n1.1\n1.0\n1.3\n1.2\n\n\n\n\n\nConstruct a scatterplot using tar for the \\(x\\) axis and nicotine for the \\(y\\) axis. Does the scatterplot suggest a linear relationship between the two variables? Are they positively or negatively related?\nLet \\(y\\) be the amount of nicotine and let \\(x\\) be the amount of tar. Fit a simple linear regression to the data and identify the sample regression equation.\nWhat percentage of the variation in nicotine can be explained by the linear correlation between nicotine and tar?\nThe Raleigh brand king size cigarette is not included in the table, and it has 21 mg of tar. What is the best predicted amount of nicotine? How does the predicted amount compare to the actual amount of 1.2 mg of nicotine? What is the value of residual?\nPerform the test \\(H_0: \\beta_1 = 0\\) vs.Â \\(H_1: \\beta_1 \\ne 0\\).\nProvide 95% confidence interval for \\(\\beta_1\\).\nGenerate the ANOVA table for the linear regression.\n\n\n\n\nCorrelation (30 points): Match each correlation to the corresponding scatterplot.\n\n\\(R = -0.65\\)\n\\(R = 0.47\\)\n\\(R = 0.03\\)\n\\(R = 0.93\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRegression (30 points): The following regression output is for predicting the heart weight (in g) of cats from their body weight (in kg).\n\nWrite out the simple linear regression equation.\nWhich one is the response variable and which one is the predictor (explanatory variable)?\nInterpret the slope and intercept.\n\n\n\n\n\n(Intercept)\n-0.346\n\n\nbody wt\n3.953"
  },
  {
    "objectID": "model-logistic.html#regression-vs.-classification",
    "href": "model-logistic.html#regression-vs.-classification",
    "title": "25Â  Logistic Regression",
    "section": "\n25.1 Regression vs.Â Classification",
    "text": "25.1 Regression vs.Â Classification\n\nLinear regression assumes that the response \\(Y\\) is numerical.\nIn many situations, \\(Y\\) is categorical.\n\n\n\nNormal vs.Â COVID vs.Â Smokerâ€™s Lungs\n\n\n\n\n\n\n\n\n\n\n\nFake vs.Â Fact\n\n\n\n\n\n\n\n\n\n\n\nThe process of predicting a categorical response is known as classification.\n\n\n Regression Function \\(f(x)\\) vs.Â Classifier \\(C(x)\\) \n\n\n\n\n\n\n\n\n\nFigureÂ 25.1: Difference between classification and regression (https://daviddalpiaz.github.io/r4sl/classification-overview.html)\n\n\n\n\n\n Classification Example \n\nPredict whether people will default on their credit card payment, where \\((Y)\\) is yes or no, based on their monthly credit card balance, \\((X)\\).\nWe use the sample data \\(\\{(x_1, y_1), \\dots, (x_n, y_n)\\}\\) to build a classifier.\n\n\n\n\n\n\n\nFigureÂ 25.2: Boxplot of Default vs.Â Balance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Why Not Linear Regression? \n\\[Y =\\begin{cases}\n    0  & \\quad \\text{if not default}\\\\\n    1  & \\quad \\text{if default}\n     \\end{cases}\\]\n\n\n\\(Y = \\beta_0 + \\beta_1X + \\epsilon\\), \\(\\, X =\\) credit card balance\n\n\n\n\n\n\n\nWhat is the problem with this dummy variable approach?\n\n\n\n\n\n\n\n\n\\(\\hat{Y} = b_0 + b_1X\\) estimates \\(P(Y = 1 \\mid X) = P(default = yes \\mid balance)\\)\n\n\n\n\n\n\nFigureÂ 25.3: Graphical illustration of why a simple linear regression model wonâ€™t work for Default ~ Balance\n\n\n\n\n\n\nSome estimates might be outside \\([0, 1]\\), which doesnâ€™t make sense given that \\(Y\\) is a probability.\n\n\n Why Logistic Regression? \n\nWe first predict the probability of each category of \\(Y\\).\nThen, we predict the probability of default using an S-shaped curve.\n\n\n\n\n\nFigureÂ 25.4: Graphical illustration of why a logistic regression model works better for Default ~ Balance"
  },
  {
    "objectID": "model-logistic.html#introduction-to-logistic-regression",
    "href": "model-logistic.html#introduction-to-logistic-regression",
    "title": "25Â  Logistic Regression",
    "section": "\n25.2 Introduction to Logistic Regression",
    "text": "25.2 Introduction to Logistic Regression\n Binary Responses \n\nTreat each outcome, default \\((y = 1)\\) and not default \\((y = 0)\\), as success and failure arising from separate Bernoulli trials.\n\n\n\n\n\n\n\nWhat is a Bernoulli trial?\n\n\n\n\nA Bernoulli trial is a special case of a binomial trial when the number of trials is \\(m = 1\\).\n\nThere are exactly two possible outcomes, â€œsuccessâ€ and â€œfailureâ€.\nThe probability of success, \\(\\pi\\), is constant.\n\n\n\n\n\n\n\n\n\n\n\nIn the default credit card example,\n\n\n\n\nDo we have exactly two outcomes?\nDo we have constant probability? \\(P(y_1 = 1) = P(y_2 = 1) = \\cdots = P(y_n = 1) = \\pi?\\)\n\n\n\n\n Nonconstant Probability\n\n\n\nTwo outcomes: Default \\((y = 1)\\) and Not Default \\((y = 0)\\)\n\nThe probability of success, \\(\\pi\\), changes with the value of predictor, \\(X\\)!\nWith a different value of \\(x_i\\), each Bernoulli trial outcome, \\(y_i\\), has a different probability of success, \\(\\pi_i\\).\n\n\n\n\n\n\n\n\n\n\n\n\\[ y_i \\mid x_i \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i)) = binomial(m=1,\\pi = \\pi(x_i)) \\]\n\n\n\\(X =\\) balance. \\(x_1 = 2000\\) has a larger \\(\\pi_1 = \\pi(2000)\\) than \\(\\pi_2 = \\pi(500)\\) with \\(x_2 = 500\\) because credit cards with a higher balance are more likely to default.\n\n\n Logistic Regression \n\n\nLogistic regression models a binary response \\((Y)\\) using predictors \\(X_1, \\dots, X_k\\).\n\n\n\\(k = 1\\): simple logistic regression\n\n\\(k > 1\\): multiple logistic regression\n\n\nInstead of predicting \\(y_i\\) directly, we use the predictors to model its probability of success, \\(\\pi_i\\).\n\n\nBut how?\n\n\n Logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) \n\nTransform \\(\\pi \\in (0, 1)\\) into another variable \\(\\eta \\in (-\\infty, \\infty)\\). Then fit a linear regression on \\(\\eta\\).\n\nLogit function: For \\(0 < \\pi < 1\\)\n\n\n\\[\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\]\n\n\n\n\nFigureÂ 25.5: Graphical illustration of the logit function\n\n\n\n\n Logistic Function \\(\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)}\\) \n\nThe logit function \\(\\eta = logit(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right)\\) takes a value \\(\\pi \\in (0, 1)\\) and maps it to a value \\(\\eta \\in (-\\infty, \\infty)\\).\n\nLogistic function: \\[\\pi = logistic(\\eta) = \\frac{\\exp(\\eta)}{1+\\exp(\\eta)} = \\frac{1}{1+\\exp(-\\eta)} \\in (0, 1)\\]\n\nThe logistic function takes a value \\(\\eta \\in (-\\infty, \\infty)\\) and maps it to a value \\(\\pi \\in (0, 1)\\).\nSo once \\(\\eta\\) is estimated by the linear regression, we use the logistic function to transform \\(\\eta\\) back to the probability.\n\n\n\n\n\nFigureÂ 25.6: Graphical illustration of the logistic function"
  },
  {
    "objectID": "model-logistic.html#simple-logistic-regression-model",
    "href": "model-logistic.html#simple-logistic-regression-model",
    "title": "25Â  Logistic Regression",
    "section": "\n25.3 Simple Logistic Regression Model",
    "text": "25.3 Simple Logistic Regression Model\n\nFor \\(i = 1, \\dots, n\\) and with one predictor \\(X\\): \\[(Y_i \\mid X = x_i) \\stackrel{indep}{\\sim} \\text{Bernoulli}(\\pi(x_i))\\] \\[\\text{logit}(\\pi_i) = \\ln \\left( \\frac{\\pi(x_i)}{1 - \\pi(x_i)} \\right) = \\eta_i = \\beta_0+\\beta_1 x_{i}\\]\n\n\n\\[\\small \\pi_i = \\frac{\\exp(\\beta_0+\\beta_1 x_{i})}{1+\\exp(\\beta_0+\\beta_1 x_{i})} = \\frac{\\exp(\\eta_i)}{1 + \\exp(\\eta_i)}\\]\n\\[\\small \\hat{\\pi}_i = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i} )}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x_{i})}\\]\n\n Probability Curve \n\n\n\nThe relationship between \\(\\pi(x)\\) and \\(x\\) is not linear! \\[\\pi(x) = \\frac{\\exp(\\beta_0+\\beta_1 x)}{1+\\exp(\\beta_0+\\beta_1 x)}\\]\n\nThe amount that \\(\\pi(x)\\) changes due to a one-unit change in \\(x\\) depends on the current value of \\(x\\).\nRegardless of the value of \\(x\\), if \\(\\beta_1 > 0\\), increasing \\(x\\) will increase \\(\\pi(x)\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Interpretation of Coefficients \n\nThe ratio \\(\\frac{\\pi}{1-\\pi} \\in (0, \\infty)\\) is called the odds of some event.\nExample: If 1 in 5 people will default, the odds is 1/4 since \\(\\pi = 0.2\\) implies an odds of \\(0.2/(1âˆ’0.2) = 1/4\\).\n\n\\[\\ln \\left( \\frac{\\pi(x)}{1 - \\pi(x)} \\right)= \\beta_0 + \\beta_1x\\]\n-Increasing \\(x\\) by one unit changes the log-odds by \\(\\beta_1\\), or it multiplies the odds by \\(e^{\\beta_1}\\).\n\n\n\n\n\n\nNote\n\n\n\n\n\n\\(\\beta_1\\) does not correspond to the change in \\(\\pi(x)\\) associated with a one-unit increase in \\(x\\).\n\n\\(\\beta_1\\) is the change in log odds associated with one-unit increase in \\(x\\)."
  },
  {
    "objectID": "model-logistic.html#logistic-regression-in-r",
    "href": "model-logistic.html#logistic-regression-in-r",
    "title": "25Â  Logistic Regression",
    "section": "\n25.4 Logistic Regression in R",
    "text": "25.4 Logistic Regression in R\n\n\n\n\nGENDER = 1 if male\n\nGENDER = 0 if female\nUse HEIGHT (centimeter, 1 cm = 0.3937 in) to predict/classify GENDER: whether the person is male or female.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbody <- read.table(\"./data/01 - Body Data.txt\", header = TRUE)\nhead(body)\n\n  AGE GENDER PULSE SYSTOLIC DIASTOLIC HDL LDL WHITE  RED PLATE WEIGHT HEIGHT\n1  43      0    80      100        70  73  68   8.7 4.80   319   98.6  172.0\n2  57      1    84      112        70  35 116   4.9 4.73   187   96.9  186.0\n3  38      0    94      134        94  36 223   6.9 4.47   297  108.2  154.4\n4  80      1    74      126        64  37  83   7.5 4.32   170   73.1  160.5\n5  34      1    50      114        68  50 104   6.1 4.95   140   83.1  179.0\n6  77      1    60      134        60  55  75   5.7 3.95   192   86.5  166.7\n  WAIST ARM_CIRC  BMI\n1 120.4     40.7 33.3\n2 107.8     37.0 28.0\n3 120.3     44.3 45.4\n4  97.2     30.3 28.4\n5  95.1     34.0 25.9\n6 112.0     31.4 31.1\n\n\n\n Data Summary \n\n\n\ntable(body$GENDER)\n\n\n  0   1 \n147 153 \n\nsummary(body[body$GENDER == 1, ]$HEIGHT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  155.0   169.1   173.8   174.1   179.4   193.3 \n\nsummary(body[body$GENDER == 0, ]$HEIGHT)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  134.5   156.5   162.2   161.7   166.8   181.4 \n\n\n\n\n\n\nboxplot(body$HEIGHT ~ body$GENDER)\n\n\n\n\n\n\n\n Model Fitting \n\nlogit_fit <- glm(GENDER ~ HEIGHT, data = body, family = \"binomial\")\n(summ_logit_fit <- summary(logit_fit))\n\n\nCall:\nglm(formula = GENDER ~ HEIGHT, family = \"binomial\", data = body)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -40.54809    4.63084  -8.756   <2e-16 ***\nHEIGHT        0.24173    0.02758   8.764   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 415.77  on 299  degrees of freedom\nResidual deviance: 251.50  on 298  degrees of freedom\nAIC: 255.5\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n\nsumm_logit_fit$coefficients\n\n               Estimate Std. Error   z value     Pr(>|z|)\n(Intercept) -40.5480864 4.63083742 -8.756102 2.021182e-18\nHEIGHT        0.2417325 0.02758399  8.763507 1.892674e-18\n\n\n\n\\(\\hat{\\eta} = \\ln \\left( \\frac{\\hat{\\pi}}{1 - \\hat{\\pi}}\\right) = -40.55 + 0.24 \\times \\text{HEIGHT}\\)\n\\(\\hat{\\eta}(x) = \\hat{\\beta}_0 + \\hat{\\beta}_1x\\)\n\\(\\hat{\\eta}(x+1) = \\hat{\\beta}_0 + \\hat{\\beta}_1(x+1)\\)\n\\(\\hat{\\eta}(x+1) - \\hat{\\eta}(x) = \\hat{\\beta}_1 = \\ln(\\text{odds}_{x+1}) - \\ln(\\text{odds}_{x}) = \\ln \\left( \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} \\right)\\)\nA one centimeter increase in HEIGHT increases the log odds of being male by 0.24 units.\nThe odds ratio, \\(\\widehat{OR} = \\frac{\\text{odds}_{x+1}}{\\text{odds}_{x}} = e^{\\hat{\\beta}_1} = e^{0.24} = 1.273\\).\nThe odds of being male increases by 27.3% with an additional one centimeter of HEIGHT.\n\n\n Prediction \n Pr(GENDER = 1) When HEIGHT is 170 cm \n\n\\[ \\hat{\\pi}(x = 170) = \\frac{\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x)}{1+\\exp(\\hat{\\beta}_0+\\hat{\\beta}_1 x)} = \\frac{\\exp(-40.55+0.24 \\times 170)}{1+\\exp(-40.55+0.24 \\times 170)} = 0.633 = 63.3\\%\\]\n\npi_hat <- predict(logit_fit, type = \"response\")\neta_hat <- predict(logit_fit, type = \"link\")  ## default gives us b0 + b1*x\npredict(logit_fit, newdata = data.frame(HEIGHT = 170), type = \"response\")\n\n        1 \n0.6333105 \n\n\n Probability Curve \n\n\n\n\n\n\nWhat is the probability of being male when the HEIGHT is 160 cm? What about when the HEIGHTis 180 cm?\n\n\n\n\n\n\n\npredict(logit_fit, newdata = data.frame(HEIGHT = c(160, 170, 180)), type = \"response\")\n\n        1         2         3 \n0.1334399 0.6333105 0.9509103 \n\n\n\n\n\n\n\n\n\n\n\n 160 cm, Pr(male) = 0.13\n 170 cm, Pr(male) = 0.63\n 180 cm, Pr(male) = 0.95"
  },
  {
    "objectID": "model-logistic.html#evaluation-metrics",
    "href": "model-logistic.html#evaluation-metrics",
    "title": "25Â  Logistic Regression",
    "section": "\n25.5 Evaluation Metrics",
    "text": "25.5 Evaluation Metrics\n Sensitivity and Specificity \n\n\n\n\n\n\n\n\n1\n0\n\n\n\nLabeled 1\nTrue Positive (TP)\nFalse Positive (FP)\n\n\nLabeled 0\nFalse Negative (FN)\nTrue Negative (TN)\n\n\n\n\n\nSensitivity (True Positive Rate) \\(= P( \\text{Labeled 1} \\mid \\text{1}) = \\frac{TP}{TP+FN}\\)\n\n\nSpecificity (True Negative Rate) \\(= P( \\text{Labeled 0} \\mid \\text{0}) = \\frac{TN}{FP+TN}\\)\n\n\nAccuracy \\(= \\frac{TP + TN}{TP+FN+FP+TN}\\) \n\nMore on Wiki page\n\n\n\n Confusion Matrix \n\nprob <- predict(logit_fit, type = \"response\")\n\n## true observations\ngender_true <- body$GENDER\n\n## predicted labels\ngender_predict <- (prob > 0.5) * 1\n\n## confusion matrix\ntable(gender_predict, gender_true)\n\n              gender_true\ngender_predict   0   1\n             0 118  29\n             1  29 124\n\n\n\n Receiver Operating Characteristic (ROC) Curve \n\n\nReceiver operating characteristic (ROC) curve\n\nPlots True Positive Rate (Sensitivity) vs.Â False Positive Rate (1 - Specificity)\n\n\nR packages for ROC curves: ROCR and pROC, yardstick of Tidymodels\n\n\n\n\n\n\nFigureÂ 25.7: ROC curve for Gender ~ Height"
  },
  {
    "objectID": "model-logistic.html#exercises",
    "href": "model-logistic.html#exercises",
    "title": "25Â  Logistic Regression",
    "section": "\n25.6 Exercises",
    "text": "25.6 Exercises\n\nThe following logistic regression equation is used for predicting whether a bear is male or female. The value of \\(\\pi\\) is the probability that the bear is male. \\[\\log\\left(\\frac{\\pi}{1-\\pi}\\right) = 2.3 - 0.0573 (\\text{Length}) + 0.00842(\\text{Weight})\\]\n\nIdentify the predictor and response variables. Which of these are dummy variables?\nGiven that the variable Lengthis in the model, does a heavier weight increase or decrease the probability that the bear is a male? Please explain.\nThe given regression equation has an overall p-value of 0.218. What does that suggest about the quality of predictions made using the regression equation?\nUse a length of 60 in. and a weight of 300 lb to find the probability that the bear is a male. Also, what is the probability that the bear is a female?"
  },
  {
    "objectID": "model-bayes.html#the-r-user-interface",
    "href": "model-bayes.html#the-r-user-interface",
    "title": "26Â  Bayesian Linear Regression",
    "section": "26.1 The R User Interface",
    "text": "26.1 The R User Interface"
  },
  {
    "objectID": "model-survival.html#life-table",
    "href": "model-survival.html#life-table",
    "title": "27Â  Survival Analysis",
    "section": "\n27.1 Life Table",
    "text": "27.1 Life Table\n\nA period life table describes mortality and longevity data for a hypothetical cohort.\nThe data is computed with the assumption that the conditions affecting mortality in a particular year remain the same throughout the lives of everyone in the hypothetical cohort.\n\nFor example, a 1-year-old toddler and an elderly 70-year-old live their entire life in a world with the same constant death rates that were present in a given year.\n\n\nAn example of a life table is shown in FigureÂ 27.1.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 27.1: Life table for total population in the United States in 2018\n\n\n\n\n\nThe entire report can be downloaded at CDC Publications and Information Products.\nMortality experiences are different for various gender and race groups, so it is common to have tables for specific groups.\n\nFor example, FigureÂ 27.2 below is a table for females in the United States.\n\n\n\n\n\n\n\nFigureÂ 27.2: Life table for females in the United States in 2018\n\n\n\n\n\nThe basis year for the mortality rate in this table is 2018, as is highlighted in FigureÂ 27.3.\nThis life table has data for a cohort of 100,000 hypothetical people.\n\n\n\n\n\nFigureÂ 27.3: The life table lists its basis year and number of hypothetical individuals\n\n\n\n\n\nThe age ranges chosen for this life table include the following classes: \\([0, 1)\\), \\([1, 2)\\), \\([2, 3)\\), â€¦ \\([99, 100)\\), \\([100, \\infty)\\).\n\n\n\n\n\nFigureÂ 27.4: The first column lists the age intervals of the individuals\n\n\n\n\n\nThe probabilities of dying during the age interval are listed in the 1st column of the life table.\nFor example, in FigureÂ 27.5, there is a 0.000367 probability of someone dying between their 1st birthday and their 2nd birthday.\n\n\n\n\n\nFigureÂ 27.5: The second column lists the probability of dying between two ages\n\n\n\n\n\nThe number of people alive at the beginning of the age interval is listed in column 2.\nAs FigureÂ 27.6 displays, among the 100,000 hypothetical people who were born, 99,435 of them are alive on their 1st birthday.\n\n\n\n\n\nFigureÂ 27.6: The third column lists the number of individuals alive at the beginning of the age interval\n\n\n\n\n\nThe number of people who died during the age interval is listed in column 3.\n\n\n\n\n\n\n\nHow is this column related to the previous two columns?\n\n\n\n\n\n\n\n\n\n\nFigureÂ 27.7: The fourth column lists the number of individuals who die during a given age interval\n\n\n\n\n\nThe total number of years lived during the age interval by those who were alive at the beginning of the age interval is listed in the fourth column.\nFor example, the 100,000 people who were present at age 0 lived a total of 99,505 years (FigureÂ 27.8).\nIf none of those people had died, this entry would have been 100,000 years.\n\n\n\n\n\nFigureÂ 27.8: The fifth column lists the total number of person-years lived within a given age interval\n\n\n\n\n\nThe sixth column is similar to the fifth, but lists the total number of years lived during the age interval and all of the following age intervals as well.\n\n\n\n\n\nFigureÂ 27.9: The fifth column lists the total number of person-years lived above a given age\n\n\n\n\n\nThe final column lists the expected remaining lifetime in years, measured from the beginning of the age interval (FigureÂ 27.10).\n\n\n\n\n\n\n\nWhy does the age interval of 1-2 have an expected remaining lifetime of 78.2 years?\n\n\n\n\n\n\n\n\n\n\nFigureÂ 27.10: The final column lists the expectation of life at a given age\n\n\n\n\n\n Example: Probability of Dying \n\nUse FigureÂ 27.1 to find the probability of a person dying between age of 15 and 20.\n\n\\[\\begin{align*} Pr(\\text{die in } [15, 20)) &= Pr([15, 16) \\cup [16, 17) \\cup \\cdots \\cup [19, 20)) \\\\ &= Pr([15, 16) + Pr([16, 17)) + \\cdots + Pr([19, 20)) \\\\ &= 0.000214 + 0.000253 + 0.000292 + 0.000329 + 0.000365 = 0.001453 \\end{align*}\\]\n\\[\\begin{align*} Pr(\\text{surviving between 15th and 20th birthdays}) &= \\frac{\\text{Number of people alive on their 20th birthday}}{\\text{Number of people alive on their 15th birthday}} \\\\ &= \\frac{99,151}{99,296} \\\\ &= 0.99854 \\end{align*}\\]\n\\[Pr(\\text{die in } [15, 20)) = 1-Pr(\\text{survive in } [15, 20)) = 1 - 0.99854 = 0.00146\\]"
  },
  {
    "objectID": "model-survival.html#applications-of-life-tables",
    "href": "model-survival.html#applications-of-life-tables",
    "title": "27Â  Survival Analysis",
    "section": "\n27.2 Applications of Life Tables",
    "text": "27.2 Applications of Life Tables\n Social Security \n\n\n\nThere were 3,600,000 births in the U.S. in 2020.\nIf the age for receiving full Social Security payment is 67, how many of those born in 2020 are expected to be alive on their 67th birthday? Check the report!\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAmong 100,000 people born, we expect 81,637 of them will survive to their 67th birthday.\nTherefore, we expect that \\(3,600,000 \\times 0.81637 = 2,938,932\\) people born in 2020 will receive their full Social Security payment.\n\n\n Hypothesis Testing \n\n\n\nFor one city, there are 5000 people who reach their 16th birthday.\n25 of them die before their 17th birthday.\nDo we have sufficient evidence to conclude that this number of deaths is significantly high?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe probability of dying for the age interval of 16-17 is 0.000405.\nThis is a \\(H_1\\) claim.  \\(\\small \\begin{align} &H_0: \\pi = 0.000405 \\\\ &H_1: \\pi > 0.000405\\end{align}\\) \n\n\n\\(\\hat{\\pi} = 25/5000 = 0.005\\).\n\\(z = \\frac{0.005 - 0.000405}{\\sqrt{\\frac{(0.000405)(0.999595)}{5000}}} = 16.15\\)\n\n\\(P\\)-value \\(\\approx 0\\).\nThere is sufficient evidence to conclude that the proportion of deaths is significantly higher than the proportion that is usually expected for this age interval."
  },
  {
    "objectID": "model-survival.html#kaplan-meier-survival-analysis",
    "href": "model-survival.html#kaplan-meier-survival-analysis",
    "title": "27Â  Survival Analysis",
    "section": "\n27.3 Kaplan-Meier Survival Analysis",
    "text": "27.3 Kaplan-Meier Survival Analysis\n Survival Analysis \n\n\n\nThe life table method is based on fixed time intervals.\nThe Kaplan-Meier method\n\nis based on intervals that vary according to the times of survival to some particular terminating event.\nis used to describe the probability of surviving for a specific period of time.\n\n\n What is the probability of surviving for 5 more years after cancer chemotherapy? \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Survival Time \n\nThe time lapse from the beginning of observation to the time of terminating event is considered the survival time (FigureÂ 27.11).\n\n\n\n\n\nFigureÂ 27.11: Graph of survival time\n\n\n\n\n\n Survivor \n\nA survivor is a subject that successfully lasted throughout a particular time period.\n\n\n\n\n\n\n\nNote\n\n\n\n\nA survivor does not necessarily mean living.\n\nA patient trying to stop smoking is a survivor if smoking has not resumed.\nYour iPhone that worked for some particular length of time can be considered a survivor.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCensored Data \n\nSurvival times are censored data if the subjects\n\nsurvive past the end of the study\n\nare dropped from the study for reasons not related to the terminating event being studied.\n\n\n\n\n\n\n\nFigureÂ 27.12: Illustration of censored data (https://unc.live/3K1ph8f)\n\n\n\n\n\n Example: Medication Treatment for Quitting Smoking \n\n\n\n\n\n\n\n\n\n\nDay\nStatus (0 = censored, 1 = Smoke Again)\nNumber of Patients\nPatients Not Smoking\nProportion Not Smoking\nCumulative Proportion Not Smoking\n\n\n\n1\n0\n\n\n\n\n\n\n3\n1\n4\n3\n3/4 = 0.75\n0.75\n\n\n4\n1\n3\n2\n2/3 = 0.67\n0.5\n\n\n7\n1\n2\n1\n1/2 = 0.5\n0.25\n\n\n21\n1\n1\n0\n0\n0\n\n\n\n\n\n\nâ€œSurvivingâ€ means the patient has NOT resumed smoking.\nAs shown in FigureÂ 27.13, the Subject 1 disliked the medication and dropped out of the study on day one.\nThe table above also provides information regarding the study.\n\n2nd row: Subject 2 resumed smoking 3 days after the start of the program.\n3rd row: \\(0.5 = (3/4)(2/3)\\)\n\n4th row: \\(0.25 = (3/4)(2/3)(1/2)\\)\n\n5th row: \\(0 = (3/4)(2/3)(1/2)(0)\\)\n\n\n\n\n\n\n\n\n\n\n\nFigureÂ 27.13: Survival time for five subjects receiving the medication treatment\n\n\n\n\n\n\n\n Example: Counseling Treatment for Quitting Smoking \n\n\n\n\n\n\n\n\n\n\nDay\nStatus  (0 = censored, 1 = Smoke Again)\nNumber of Patients\nPatients Not Smoking\nProportion Not Smoking\nCumulative Proportion Not Smoking\n\n\n\n2\n1\n10\n9\n9/10\n0.9\n\n\n4\n1\n9\n8\n8/9\n0.8\n\n\n5\n0\n\n\n\n\n\n\n8\n1\n7\n6\n6/7\n0.686\n\n\n9\n1\n6\n5\n5/6\n0.571\n\n\n12\n0\n\n\n\n\n\n\n14\n1\n4\n3\n3/4\n0.429\n\n\n22\n1\n3\n2\n2/3\n0.286\n\n\n24\n0\n\n\n\n\n\n\n28\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy is the cumulative proportion on Day 8 0.686?\n\n\n\n\\[0.686 = (9/10)(8/9)(6/7)\\]\n\nOn Day 5 a patient dropped out, so we donâ€™t know whether he resumed smoking on Day 8 or not.\n\n\n\n\n\n\n\n\n\n\nFigureÂ 27.14: Survival time for ten subjects receiving the counseling treatment\n\n\n\n\n\n\n\n Kaplan-Meier Analysis \n\n\n\n\n\n\nWhich treatment is better for quitting smoking?\n\n\n\n\n\n\n\n\n\n\nFigureÂ 27.15: Data from the medication treatment group and the counseling treatment group are compared using a Kaplan-Meier plot"
  },
  {
    "objectID": "infer-cat.html#sec-infer-goodnessfit",
    "href": "infer-cat.html#sec-infer-goodnessfit",
    "title": "\n19Â  Inference about Categorical Data\n",
    "section": "\n19.1 Test of Goodness of Fit",
    "text": "19.1 Test of Goodness of Fit\n\n\n\n Categorical Variable with More Than 2 Categories \n\n\n\nA categorical variable has \\(k\\) categories \\(A_1, \\dots, A_k\\).\n\n\n\nSubject\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(\\cdots\\)\n\\(A_k\\)\n\n\n\n1\nx\n\n\n\n\n\n\n2\n\nx\n\n\n\n\n\n3\n\n\n\n\nx\n\n\n\\(\\vdots\\)\n\n\n\n\n\n\n\n\\(n\\)\n\n\n\nx\n\n\n\n\n\n\n\n\nWith the size \\(n\\), for categories \\(A_1, \\dots , A_k\\), their observed count is \\(O_1, \\dots, O_k\\), and \\(\\sum_{i=1}^kO_i = n\\).\nOne-way count table:\n\n\n\n\\(A_1\\)\n\\(A_2\\)\n\\(\\cdots\\)\n\\(A_k\\)\nTotal\n\n\n\\(O_1\\)\n\\(O_2\\)\n\\(\\cdots\\)\n\\(O_k\\)\n\\(n\\)\n\n\n\n\n Example \n\n\n\nAre the selected jurors racially representative of the population?\nIf the jury is representative of the population, the proportions in the sample should reflect the proportions of the population of eligible jurors (i.e.Â registered voters).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nRepresentation in juries\n205\n26\n25\n19\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nRepresentation in juries\n0.745\n0.095\n0.091\n0.069\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\n\n\n\n\n\n\nAre the proportions of juries close enough to the proportions of registered voters, so that we are confident saying that the jurors really were randomly sampled from the registered voters?\n\n\n\n\n\n\n\n Goodness-of-Fit Test \n\nA goodness-of-fit test tests the hypothesis that the observed frequency distribution fits or conforms to some claim distribution.\n\n\n\n\n\n\n\nIn the jury example, what is our observed frequency distribution, and what is our claim distribution?\n\n\n\n\n\n\n\n\n\n\n\n\nIf the individuals are randomly selected to serve on a jury, about how many of the 275 people would we expect to be white? How about black?\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\nRegistered voters\n0.72\n0.07\n0.12\n0.09\n\n\n\nAbout \\(72\\%\\) of the population is white, so we would expect about \\(72\\%\\) of the jurors to be white.\n\n\n\\(0.72 \\times 275 = 198\\).\n\n\nWe expect about \\(7\\%\\) of the jurors to be black.\n\nThis corresponds to about \\(0.07 \\times 275 = 19.25\\) black jurors.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\n\nObserved Count\n205\n26\n25\n19\n\n\n\nExpected Count\n198\n19.25\n33\n24.75\n\n\nPopulation Proportion \\((H_0)\\)\n\n0.72\n0.07\n0.12\n0.09\n\n\n\n\nThe observed count and expected count will be similar if there was no bias in selecting the members of the jury.\nWe want to test whether the differences are strong enough to provide convincing evidence that the jurors were not selected from a random sample of all registered voters.\n\n Example \n\n \\(\\begin{align} &H_0: \\text{No racial bias in who serves on a jury, and } \\\\ &H_1: \\text{There is racial bias in juror selection} \\end{align}\\) \n \\(\\begin{align} &H_0: \\pi_1 = \\pi_1^0, \\pi_2 = \\pi_2^0, \\dots, \\pi_k = \\pi_k^0\\\\ &H_1: \\pi_i \\ne \\pi_i^0 \\text{ for some } i \\end{align}\\) \n Under \\(H_0\\), \\(\\chi^2_{test} = \\frac{(O_1 - E_1)^2}{E_1} + \\frac{(O_2 - E_2)^2}{E_2} + \\cdots + \\frac{(O_k - E_k)^2}{E_k}\\), \\(E_i = n\\pi_i^0, i = 1, \\dots, k\\) \nReject \\(H_0\\) if  \\(\\chi^2_{test} > \\chi^2_{\\alpha, df}\\), \\(df = k-1\\) \nRequire each \\(E_i \\ge 5\\), \\(i = 1, \\dots, k\\).\n\n\n\n\n\n\n\n\n\n\n\nWhite\nBlack\nHispanic\nAsian\n\n\n\nObserved Count\n\\(O_1 = 205\\)\n\\(O_2 = 26\\)\n\\(O_3 = 25\\)\n\\(O_4 = 19\\)\n\n\nExpected Count\n\\(E_1 = 198\\)\n\\(E_2 = 19.25\\)\n\\(E_3 = 33\\)\n\\(E_4 = 24.75\\)\n\n\nProportion under \\(H_0\\)\n\n\\(\\pi_1^0 = 0.72\\)\n\\(\\pi_2^0 = 0.07\\)\n\\(\\pi_3^0 = 0.12\\)\n\\(\\pi_4^0 = 0.09\\)\n\n\n\n\nUnder \\(H_0\\), \\(\\chi^2_{test} = \\frac{(205 - 198)^2}{198} + \\frac{(26 - 19.25)^2}{19.25} + \\frac{(25 - 33)^2}{33} + \\frac{(19 - 24.75)^2}{24.75} = 5.89\\)\n\n\n\\(\\chi^2_{0.05, 3} = 7.81\\).\nBecause \\(5.89 < 7.81\\), we fail to reject \\(H_0\\) in favor of \\(H_1\\).\nWe do not have convincing evidence of racial bias in the juror selection process.\n\n Goodness-of-Fit Test in R \n\nBelow is an example of how to perform a Goodness-of-Fit test in R.\n\n\nobs <- c(205, 26, 25, 19)\npi_0 <- c(0.72, 0.07, 0.12, 0.09)\n\n## Use chisq.test() function\nchisq.test(x = obs, p = pi_0)\n\n\n    Chi-squared test for given probabilities\n\ndata:  obs\nX-squared = 5.8896, df = 3, p-value = 0.1171"
  },
  {
    "objectID": "infer-cat.html#test-of-independence",
    "href": "infer-cat.html#test-of-independence",
    "title": "\n19Â  Inference about Categorical Data\n",
    "section": "\n19.2 Test of Independence",
    "text": "19.2 Test of Independence\n Contingency Table and Expected Count\n Contingency Table \n\nWe have TWO categorical variables, and we want to test whether or not the two variables are independent.\n Does the opinion of the Presidentâ€™s job performance depend on gender? \n\n\n\n\n\nJob performance: approve, disapprove, no opinion\n\nGender: male, female\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\nMale\n18\n22\n10\n\n\nFemale\n23\n20\n12\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n Expected Count \n\nCompute the expected count of each cell in the two-way table under the condition that the two variables are independent of each other.\n\n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\nThe expected count for the \\(i\\)-th row and \\(j\\)-th column, which is listed in parentheses in the table above, is: \\[\\text{Expected Count}_{\\text{row i; col j}} = \\frac{\\text{(row i total}) \\times (\\text{col j total})}{\\text{table total}}\\]\n\n\n\n Test of Independence Procedure \n\nRequirements:\n\nEvery \\(E_{ij} \\ge 5\\) in the contingency table.\n\n\n \\(\\begin{align} &H_0: \\text{Two variables are independent }\\\\ &H_1: \\text{The two are dependent (associated) } \\end{align}\\)\n\n\\(\\chi^2_{test} = \\sum_{i=1}^r\\sum_{j=1}^c\\frac{(O_{ij} - E_{ij})^2}{E_{ij}}\\), where \\(r\\) is the number of rows and \\(c\\) is the number of columns in the table.\nReject \\(H_0\\) if \\(\\chi^2_{test} > \\chi^2_{\\alpha, \\, df}\\), \\(df = (r-1)(c-1)\\).\n\n Example \n\n\n\nApprove\nDisapprove\nNo Opinion\n\n\n\nMale\n18 (19.52)\n22 (20)\n10 (10.48)\n\n\nFemale\n23 (21.48)\n20 (22)\n12 (11.52)\n\n\nTotal\n41\n42\n22\n\n\n\n\n \\(\\begin{align} &H_0: \\text{ Opinion does not depend on gender } \\\\ &H_1: \\text{ Opinion and gender are dependent } \\end{align}\\)\n\\(\\small \\chi^2_{test} = \\frac{(18 - 19.52)^2}{19.52} + \\frac{(22 - 20)^2}{20} + \\frac{(10 - 10.48)^2}{10.48} + \\frac{(23 - 21.48)^2}{21.48} + \\frac{(20 - 22)^2}{22} + \\frac{(12 - 11.52)^2}{11.52}= 0.65\\)\n\n\\(\\chi^2_{\\alpha, df} =\\chi^2_{0.05, (2-1)(3-1)} = 5.991\\).\nSince \\(\\chi_{test}^2 < \\chi^2_{\\alpha, df}\\), we do not reject \\(H_0\\).\nWe fail to conclude that the opinion of the Presidentâ€™s job performance depends on gender.\n\n Test of Independence in R \n\nBelow is an example of how to perform the test of independence using R.\n\n\n(contingency_table <- matrix(c(18, 23, 22, 20, 10, 12), nrow = 2, ncol = 3))\n\n     [,1] [,2] [,3]\n[1,]   18   22   10\n[2,]   23   20   12\n\n## Using chisq.test() function\n(indept_test <- chisq.test(contingency_table))\n\n\n    Pearson's Chi-squared test\n\ndata:  contingency_table\nX-squared = 0.65019, df = 2, p-value = 0.7225\n\nqchisq(0.05, df = (2 - 1) * (3 - 1), lower.tail = FALSE)  ## critical value\n\n[1] 5.991465"
  },
  {
    "objectID": "infer-cat.html#exercises",
    "href": "infer-cat.html#exercises",
    "title": "\n19Â  Inference about Categorical Data\n",
    "section": "\n19.3 Exercises",
    "text": "19.3 Exercises\n\nA researcher has developed a model for predicting eye color. After examining a random sample of parents, she predicts the eye color of the first child. The table below lists the eye colors of offspring. On the basis of her theory, she predicted that 87% of the offspring would have brown eyes, 8% would have blue eyes, and 5% would have green eyes. Use 0.05 significance level to test the claim that the actual frequencies correspond to her predicted distribution.\n\n\n\nEye Color\nBrown\nBlue\nGreen\n\n\nFrequency\n127\n21\n5\n\n\n\nIn a study of high school students at least 16 years of age, researchers obtained survey results summarized in the accompanying table. Use a 0.05 significance level to test the claim of independence between texting while driving and driving when drinking alcohol. Are these two risky behaviors independent of one another?\n\n\n\n\nDrove after drinking alcohol?\n\n\n\n\n\nYes\nNo\n\n\nTexted while driving\n720\n3027\n\n\nDid not text while driving\n145\n4472"
  },
  {
    "objectID": "prob.html",
    "href": "prob.html",
    "title": "Probability",
    "section": "",
    "text": "If we want to describe or quantify the uncertainty about something happening or not, the most formal and rigorous way of doing it is to use probability. Therefore, we can view the probability as the language of uncertainty.\nMost of the time, we cannot do statistical inference or prediction using machine learning without probability because we usually assume our data at hand are random realizations from some targeted population that are described by a probability distribution. In other words, we are doing data analysis in the world of uncertainty. As a result, before jumping into statistical inference or data analysis, we need to have basic understanding of probability definitions, rules, and operations that are frequently used in the data science community.\nIn this part, we are going to learn the following topics:\n\nProbability definitions\nProbability rules\nDiscrete probability distributions\nContinuous probability distributions\nSampling distributions\nLaw of large numbers and central limit theorem"
  }
]