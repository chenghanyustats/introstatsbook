<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.555">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="description" content="A comprehensive introductory statistical data science book.">
<title>Introduction to Statistics - 21&nbsp; Bayesian Thinking and Inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>

<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./infer-nonpar.html" rel="next">
<link href="./infer-p.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark"><script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script><script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script><script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script><script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script><link rel="stylesheet" href="custom.css">
</head>
<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top"><nav class="quarto-secondary-nav"><div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./infer.html">Statistical Inference</a></li><li class="breadcrumb-item"><a href="./infer-bayes.html"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Bayesian Thinking and Inference</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav></header><!-- content --><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Introduction to Statistics</a> 
        <div class="sidebar-tools-main tools-wide">
    <a href="https://github.com/chenghanyustats/introstatsbook" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="./Introduction-to-Statistics.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
    <div class="dropdown">
      <a href="" title="Share" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Share"><i class="bi bi-share"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
<li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://twitter.com/intent/tweet?url=%7Curl%7C">
              <i class="bi bi-bi-twitter pe-1"></i>
            Twitter
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.facebook.com/sharer/sharer.php?u=%7Curl%7C">
              <i class="bi bi-bi-facebook pe-1"></i>
            Facebook
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-main-item" href="https://www.linkedin.com/sharing/share-offsite/?url=%7Curl%7C">
              <i class="bi bi-bi-linkedin pe-1"></i>
            LinkedIn
            </a>
          </li>
      </ul>
</div>
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Welcome</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistics and Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Science of Data and Data Science</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Data Collection and Data Type</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro-r.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Tool foR Data</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./datasummary.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Summarizing Data</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-graphics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Data Visualization</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data-numerics.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Numerical Measures of Data</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./prob.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Probability</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prob-define.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Definition of Probability</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prob-rule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Probability Rules</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prob-rv.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Random Variables</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prob-disc.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Discrete Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prob-cont.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Continuous Probability Distributions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prob-sampling.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Random Sampling and Sampling Distribution</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./prob-llnclt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Law of Large Numbers and Central Limit Theorem</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./infer.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Inference</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-ci.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">Point and Interval Estimation</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-bt.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Bootstrapping</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-ht.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Hypothesis Testing</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-twomean.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Comparing Two Population Means</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-var.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Inference About Variances</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-prop.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Inference About Proportions</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-cat.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Inference about Categorical Data</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-p.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">The Issues of <span class="math inline">\(p\)</span>-value and Statistical Significance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-bayes.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Bayesian Thinking and Inference</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./infer-nonpar.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Nonparametric Methods</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a href="./model.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Statistical Models</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model-anova.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">23</span>&nbsp; <span class="chapter-title">Analysis of Variance</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model-reg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">24</span>&nbsp; <span class="chapter-title">Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model-logistic.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">25</span>&nbsp; <span class="chapter-title">Logistic Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model-bayes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">26</span>&nbsp; <span class="chapter-title">Bayesian Linear Regression</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./model-survival.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">27</span>&nbsp; <span class="chapter-title">Survival Analysis</span></span></a>
  </div>
</li>
      </ul>
</li>
        <li class="sidebar-item sidebar-item-section">
      <span class="sidebar-item-text sidebar-link text-start">
 <span class="menu-text">References</span></span>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-7" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-7" class="collapse list-unstyled sidebar-section depth1 show">
<li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a-r_prog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">R programming</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./a-py_prog.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">Python programming</span></span></a>
  </div>
</li>
      </ul>
</li>
    </ul>
</div>
</nav><div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><h2 id="toc-title">Table of contents</h2>
   
  <ul>
<li><a href="#bayesian-thinking" id="toc-bayesian-thinking" class="nav-link active" data-scroll-target="#bayesian-thinking"><span class="header-section-number">21.1</span> Bayesian Thinking</a></li>
  <li><a href="#the-meaning-of-probability-relative-frequency-vs.-relative-plausibility" id="toc-the-meaning-of-probability-relative-frequency-vs.-relative-plausibility" class="nav-link" data-scroll-target="#the-meaning-of-probability-relative-frequency-vs.-relative-plausibility"><span class="header-section-number">21.2</span> The Meaning of Probability: <strong>Relative Frequency</strong> vs.&nbsp;<strong>Relative Plausibility</strong></a></li>
  <li><a href="#prior-information-and-empirical-evidence" id="toc-prior-information-and-empirical-evidence" class="nav-link" data-scroll-target="#prior-information-and-empirical-evidence"><span class="header-section-number">21.3</span> Prior Information and Empirical Evidence</a></li>
  <li><a href="#asking-different-questions" id="toc-asking-different-questions" class="nav-link" data-scroll-target="#asking-different-questions"><span class="header-section-number">21.4</span> Asking Different Questions</a></li>
  <li>
<a href="#bayesian-inference" id="toc-bayesian-inference" class="nav-link" data-scroll-target="#bayesian-inference"><span class="header-section-number">21.5</span> Bayesian Inference</a>
  <ul class="collapse">
<li><a href="#bayesian-updating-rule" id="toc-bayesian-updating-rule" class="nav-link" data-scroll-target="#bayesian-updating-rule"><span class="header-section-number">21.5.1</span> Bayesian Updating Rule</a></li>
  <li><a href="#likelihood-function" id="toc-likelihood-function" class="nav-link" data-scroll-target="#likelihood-function"><span class="header-section-number">21.5.2</span> Likelihood Function</a></li>
  <li><a href="#bayes-rule-for-posterior" id="toc-bayes-rule-for-posterior" class="nav-link" data-scroll-target="#bayes-rule-for-posterior"><span class="header-section-number">21.5.3</span> Bayes’ Rule for Posterior</a></li>
  </ul>
</li>
  <li>
<a href="#bayesian-inference-for-random-variables" id="toc-bayesian-inference-for-random-variables" class="nav-link" data-scroll-target="#bayesian-inference-for-random-variables"><span class="header-section-number">21.6</span> Bayesian Inference for Random Variables</a>
  <ul class="collapse">
<li><a href="#motivation-example" id="toc-motivation-example" class="nav-link" data-scroll-target="#motivation-example"><span class="header-section-number">21.6.1</span> Motivation Example</a></li>
  </ul>
</li>
  <li><a href="#further-reading-and-references" id="toc-further-reading-and-references" class="nav-link" data-scroll-target="#further-reading-and-references"><span class="header-section-number">21.7</span> Further Reading and References</a></li>
  </ul><div class="toc-actions"><ul><li><a href="https://github.com/chenghanyustats/introstatsbook/edit/main/infer-bayes.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/chenghanyustats/introstatsbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./infer.html">Statistical Inference</a></li><li class="breadcrumb-item"><a href="./infer-bayes.html"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Bayesian Thinking and Inference</span></a></li></ol></nav><div class="quarto-title">
<h1 class="title"><span id="sec-infer-bayes" class="quarto-section-identifier"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Bayesian Thinking and Inference</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header><div class="hidden">
<p><span class="math display">\[
\def\cD{{\cal D}}
\def\cL{{\cal L}}
\def\cX{{\cal X}}
\def\cF{{\cal F}}
\def\cH{{\cal H}}
\def\bA{\mathbf{A}}
\def\bB{\mathbf{B}}
\def\bX{\mathbf{X}}
\def\bH{\mathbf{H}}
\def\bI{\mathbf{I}}
\def\bU{\mathbf{U}}
\def\bD{\mathbf{D}}
\def\bV{\mathbf{V}}
\def\bS{\mathbf{S}}
\def\bW{\mathbf{W}}
\def\bY{\mathbf{Y}}
\def\bZ{\mathbf{Z}}
\def\bK{\mathbf{K}}
\def\bx{\mathbf{x}}
\def\by{\mathbf{y}}
\def\bs{\mathbf{s}}
\def\br{\mathbf{r}}
\def\bu{\mathbf{u}}
\def\be{\mathbf{e}}
\def\bv{\mathbf{v}}
\def\bp{\mathbf{p}}
\def\bw{\mathbf{w}}
\def\bz{\mathbf{z}}
\def\bzero{\mathbf{0}}
\def\balpha{\boldsymbol \alpha}
\def\bbeta{\boldsymbol \beta}
\def\btheta{\boldsymbol \theta}
\def\bSigma{\boldsymbol \Sigma}
\def\bxi{\boldsymbol \xi}
\def\bmu{\boldsymbol \mu}
\def\bep{\boldsymbol \epsilon}
\def\T{\text{T}}
\def\Trace{\text{Trace}}
\def\Cov{\text{Cov}}
\def\Corr{\text{Corr}}
\def\Var{\text{Var}}
\def\E{\text{E}}
\def\pr{\text{pr}}
\def\Prob{\text{P}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\]</span></p>
</div>
<blockquote class="blockquote">
<p><em>All models are wrong, but some are useful.</em> – George E. P. Box (1919 - 2013)</p>
</blockquote>
<blockquote class="blockquote">
<p><em>It is possible to do prediction without data, but one cannot do prediction without a priori information.</em> – Ray Solomonoff (1926 - 2009) in <a href="https://raysolomonoff.com/publications/alp-theory-and-applications.pdf">Algorithmic Probability — Theory and Applications</a></p>
</blockquote>
<p>The inference methods we have learned so far assume that unknown (population) parameters to be estimated or tested are constants. They are fixed and their values do not change. In other words, the parameters are not random variables, and the uncertainty that arises when estimating them is entirely due to sampling variation, not the variability of the parameters themselves. Additionally, when dealing with uncertainty, we use relative frequency to describe probabilities. Lots of times the answer to our questions relies on repetitions of experiments or trials. These methods adopt the so-called <strong>frequentist</strong> philosophy (<strong>frequentism</strong>). The frequentist approach, also known as the classical approach, has dominated the statistics community since the early 20th century. This dominance followed the development of a comprehensive inference framework by several great statisticians, such as Ronald Fisher, Jerzy Neyman, and Egon Pearson. This framework, which includes confidence intervals, hypothesis testing, p-values, and statistical significance, is taught in every introductory statistics course.</p>
<p>In fact, there is another way of thinking about statistical inference: the <strong>Bayesian</strong> philosophy (<strong>Bayesianism</strong>). The Bayesian approach did not emerge after the frequentist approach; rather, it has historical roots that trace back to Bayes’ theorem (Bayes’ rule), named after Thomas Bayes. People in the late 18th or early 19th century already used the Bayesian philosophy to tackle problems. For instance, Pierre-Simon Laplace used the Bayesian theorem to calculate the probability that the sun would rise the next day. However, the Bayesian approach lost popularity later on because as problems became more complex, solving them within the Bayesian framework became very tedious and time-consuming, especially given the lack of powerful computing resources at the time. With the advancement of computing power starting in the late 20th century, more statisticians and scientists began to embrace the Bayesian approach.</p>
<p>It is important to understand the frequentist and Bayesian philosophies, particularly their differences, advantages, and disadvantages. Depending on the research question and available resources, one method may be more suitable than the other. Alternatively, it may be beneficial to combine these two paradigms to leverage the advantages of both. Currently, introductory statistics courses focus heavily on the frequentist approach. Given the ease of computation and the increasing use of Bayesian methods in scientific research, I believe it is beneficial to introduce more Bayesian thinking to STAT 101 students, not just one lecture of Bayes theorem.</p>
<section id="bayesian-thinking" class="level2" data-number="21.1"><h2 data-number="21.1" class="anchored" data-anchor-id="bayesian-thinking">
<span class="header-section-number">21.1</span> Bayesian Thinking</h2>
<blockquote class="blockquote">
<p>All knowledge degenerates into probability; and this probability is greater or less, according to our expereience of veracity or deceitfulness of our understanding, and according to the simplicity or intricacy of the question. – David Hume (1711 - 1776)</p>
</blockquote>
<p>Before we dive into Bayesian statistics, let’s take a quiz to determine whether you currently lean more towards a Bayesian or frequentist mindset. This quiz is adapted from <a href="https://www.bayesrulesbook.com/">Bayes Rules</a>, a great new book on Bayesian statistics. I encourage you to spend some time on this if you would like to delve deeper.</p>
<ol type="1">
<li>
<p><span class="green">When flipping a fair coin, we say that “the probability of flipping Heads is 0.5.” How do you interpret this probability?</span> <em>(a = 1 pt, b = 3 pts, c = 2 pts)</em></p>
<ol type="a">
<li>If I flip this coin over and over, roughly 50% will be Heads.</li>
<li>Heads and Tails are equally plausible.</li>
<li>Both a. and b. make sense.</li>
</ol>
</li>
<li>
<p><span class="green">An election is coming up and a pollster claims that candidate Yu has a 0.9 probability of winning. How do you interpret this probability?</span> <em>(a = 1 pt, b = 3 pts, c = 1 pt)</em></p>
<ol type="a">
<li>If we observe the election over and over, candidate Yu will win roughly 90% of the time.</li>
<li>Candidate Yu is much more likely to win than to lose.</li>
<li>The pollster’s calculation is wrong. Candidate Yu will either win or lose, thus their probability of winning can only be 0 or 1.</li>
</ol>
</li>
<li>
<p><span class="green">Two claims.</span> <em>(a = 3 pts, b = 1 pt)</em></p>
<p><span class="green">(1) Ben claims he can predict the coin flip outcome. To test his claim, you flip a fair coin 8 times and he correctly predicts all.</span></p>
<p><span class="green">(2) Emma claims she can distinguish natural and artificial sweeteners. To test her claim, you give her 8 samples and she correctly identifies each.</span></p>
<p><span class="green">In light of these experiments, what do you conclude?</span></p>
<ol type="a">
<li>You’re more confident in Emma’s claim than Ben’s claim.</li>
<li>The evidence supporting Ben’s claim is just as strong as the evidence supporting Emma’s claim.</li>
</ol>
</li>
<li>
<p><span class="green">Suppose that during a doctor’s visit, you tested positive for COVID. If you only get to ask the doctor one question, which would it be?</span> <em>(a = 3 pts, b = 1 pt)</em></p>
<ol type="a">
<li>What’s the chance that I actually have COVID?</li>
<li>If in fact I don’t have COVID, what’s the chance that I would’ve gotten this positive test result?</li>
</ol>
</li>
</ol>
<p>Time tally up your quiz score. Are you frequentist or Bayesian? Totals from 4–5 indicate that your current thinking is fairly frequentist, whereas totals from 9–12 indicate alignment with the Bayesian philosophy. In between these extremes, totals from 6–8 indicate that you see strengths in both philosophies.</p>
<div class="columns">
<div class="column" style="width:50%;">
<ul>
<li>Totals <strong>4-5</strong>: your thinking is <strong>frequentist</strong>
</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://upload.wikimedia.org/wikipedia/commons/a/aa/Youngronaldfisher2.JPG" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Young Fisher from Wiki</figcaption></figure>
</div>
</div>
</div>
</div><div class="column" style="width:50%;">
<ul>
<li>Totals <strong>9-12</strong>: your thinking is <strong>Bayesian</strong>
</li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://upload.wikimedia.org/wikipedia/commons/d/d4/Thomas_Bayes.gif" class="img-fluid figure-img"></p>
<figcaption>Thomas Bayes from Wiki</figcaption></figure>
</div>
</div>
</div>
</div>
</div>
<ul>
<li>Totals <strong>6-8</strong>: you see strengths in both philosophies</li>
</ul>
<p>You are a frequentist or Bayesian? Are those confidence interval or hypothesis testing thing all Greek to you? Maybe it is because</p>
</section><section id="the-meaning-of-probability-relative-frequency-vs.-relative-plausibility" class="level2" data-number="21.2"><h2 data-number="21.2" class="anchored" data-anchor-id="the-meaning-of-probability-relative-frequency-vs.-relative-plausibility">
<span class="header-section-number">21.2</span> The Meaning of Probability: <strong>Relative Frequency</strong> vs.&nbsp;<strong>Relative Plausibility</strong>
</h2>
<p>The <em>frequentist</em> interprets probability as the <span class="green"><em>long-run</em> relative frequency of a <em>repeatable</em> experiment</span>. We’ve seen this interpretation before in <a href="prob-define.html" class="quarto-xref"><span>Chapter 6</span></a>. We know that using relative frequencies as probability has several issues:</p>
<ul>
<li><p>😕 How large of a number is large enough?</p></li>
<li><p>😕 Meaning of “under similar conditions”</p></li>
<li><p>😕 The relative frequency is reliable under identical conditions?</p></li>
<li><p>👉 We only obtain an approximation instead of exact value.</p></li>
<li><p>😂 How do you compute the probability that Chicago Cubs wins the World Series next year?</p></li>
</ul>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://media.giphy.com/media/EKURBxKKkw0uY/giphy.gif" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
<p>In statistical inference, our research questions or events of interest are often not repeatable or very difficult to replicate. For example, what is the probability that Donald Trump will win the 2024 presidential election? What is the probability that the Milwaukee Bucks will win the 2025 NBA championship? What would the mean income level for males be if every man was required to serve in the military for two years? Still many political scientists, sport analysts, or economists are answering those questions, right?! To rationalize their arguments, and to better answer these types of research questions, we must interpret probability in a different way.</p>
<hr>
<p>In the <em>Bayesian</em> philosophy, a probability measures the <span class="green"><strong>relative plausibility</strong></span> of an event. In the Bayesian paradigm, we can still compute the probability of some event whose frequency is 0! Here, <em>probability</em> is a quantitative measure that quantifies the relative plausibility of the event <em>according to some mathematical/statistical model</em>. This probability is <em>subjective</em>, and it logically describes our <em>epistemic</em> uncertainty about some thing. Because “Milwaukee Bucks wins the 2025 NBA championship” cannot be repeatable, we are not able to observe its “<em>objective</em>” frequencies. However, anybody, from a 5th grade girl to an ESPN NBA analyst, can express their opinion about how likely this event occurs, based on information they currently have and their own judgmental rules. The Bayesian probability is a unified way of expressing everybody’s judgement and uncertainty about something happening, and such relative plausibility is computed through a statistical model including the information or beliefs one carries. In Bayesian eyes, no probability is purely objective. Probability is <em>model-dependent</em>. Think about it. When you, or even an artificial intelligence (AI) machine choose which method to use to calculate the desired probability, whether or not the event is repeatable and relative frequencies can be collected, you (AI) implicitly express your preference and judgement on how the probability, or more generally the knowledge you make inference to, is computed.</p>
<p>Let’s conclude the interpretation of probability by some examples. We usually see the chance of winning presidency when we watch TV news waiting for the election result. Apparently, the chance of winning is a (subjective) relative plausibility measure calculated by some political scientists using some statistical models.</p>
<div class="columns">
<div class="column" style="width:50%;">
<p>For the statement “<em>candidate A has a 0.9 probability of winning</em>”, a frequentist might say</p>
<ul>
<li>the conclusion is wrong or</li>
<li>weirdly say in long-run <em>hypothetical</em> repetitions of the election, candidate A would win roughly 90% of the time.</li>
</ul>
<p>A Bayesian would say based on analysis the candidate A is 9 times more likely to win than to lose.</p>
<p>Back to our Quiz 1. For the statement “the probability of flipping Heads is 0.5”, a frequentist would conclude that if we flip the coin over and over, roughly 1/2 of these flips will be Heads. A Bayesian would conclude that Heads and Tails are equally likely.</p>
</div><div class="column" style="width:50%;">
<div class="small">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="./images/img-infer/winning.jpeg" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Source: https://x.com/nytgraphics/status/796195155158171648</figcaption></figure>
</div>
</div>
</div>
</div>
</div>
</div>
</section><section id="prior-information-and-empirical-evidence" class="level2" data-number="21.3"><h2 data-number="21.3" class="anchored" data-anchor-id="prior-information-and-empirical-evidence">
<span class="header-section-number">21.3</span> Prior Information and Empirical Evidence</h2>
<!-- - Examples of adding prior makes more sense -->
<!-- - Prejudices are subjective, but definitely not arbitrary. Personal life experience. (purely objective?) -->
<!-- - Cumulative learning -->
<!-- - Being difficulty to specify a good or reasonable prior does not mean the Bayesian paradigm does not make sense. In fact, it is priors that make the Bayesian paradigm makes more sense than frequentism. -->
<!-- - Our opinion is not Bayesian. -->
<!-- - Being objective does not mean being rational. -->
<!-- - According to Bayesianism, only those who uses their opinions can claim to be rational. -->
<!-- - Probability deduction; Statistical inference induction -->
<!-- - More and more Bayesian papers (data), someday conclude that Bayesian is the *correct* method using frequentist $p$-value. -->
<!-- - Uncertainty is a personal matter; it is not the uncertainty but your uncertainty. Dennis Lindley, Understanding Uncertainty (2006) -->
<blockquote class="blockquote">
<p>How can we live if we don’t change? – Beyoncé. Lyric from “Satellites.”</p>
</blockquote>
<blockquote class="blockquote">
<p>Uncertainty is a personal matter; it is not the uncertainty but your uncertainty. – Dennis Lindley, Understanding Uncertainty (2006)</p>
</blockquote>
<blockquote class="blockquote">
<p>A people without the knowledge of their past history, origin and culture is like a tree without roots. – Marcus Garvey (1887 - 1940)</p>
</blockquote>
<blockquote class="blockquote">
<p>Opinion is the medium between knowledge and ignorance. – Plato (427 – 348 BC)</p>
</blockquote>
<p>Everybody changes their mind. You likely even changed your mind in the last minute. For example, suppose there’s a new Italian restaurant in your town. It has a 5-star online rating and you love Italian food! Thus, prior to ever stepping foot in the restaurant, you anticipate that it will be quite delicious. On your first visit, you collect some edible data: your pasta dish arrives a soggy mess. Weighing the stellar online rating against your own terrible meal (which might have just been a fluke), you update your knowledge: this is a 3-star not 5-star restaurant. Willing to give the restaurant another chance, you make a second trip. On this visit, you’re pleased with your Alfredo and increase the restaurant’s rating to 4 stars. You continue to visit the restaurant, collecting edible data and updating your knowledge each time.</p>
<div class="cell fig-cap-location-bottom" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-restaurant" class="quarto-figure quarto-figure-center quarto-float anchored" data-cap-location="bottom" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-restaurant-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/img-infer/restaurant_diagram.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2000">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-restaurant-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.1: Fig 1.1 of Bayes Rules. The figures not being sourced come from this book too.
</figcaption></figure>
</div>
</div>
</div>
<ul>
<li><p>We use <strong>data</strong> and <strong>prior beliefs</strong> to update our knowledge (<strong>posterior</strong>), and repeating. So <em>today’s prior is yesterday’s posterior!</em></p></li>
<li><p>We continuously update our knowledge about the world as we accumulate lived experiences, or <em>collect data</em>.</p></li>
</ul>
<p><a href="#fig-bayes" class="quarto-xref">Figure&nbsp;<span>21.2</span></a> shows Bayesian Knowledge-building Process. If you’re an environmental scientist, yours might be an analysis of the human role in climate change. You don’t walk into such an inquiry without context – you carry a degree of incoming or prior information based on previous research and experience. Naturally, it’s in light of this information that you interpret new data, weighing both in developing your updated or posterior information.</p>
<div class="cell fig-cap-location-bottom" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-bayes" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center" width="50%">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-bayes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="./images/img-infer/bayes_diagram.png" id="fig-bayes" class="img-fluid quarto-figure quarto-figure-center anchored figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-bayes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.2
</figcaption></figure>
</div>
</div>
</div>
<p>Frequentist relies on (limited) data only. In Question 3, in a frequentist analysis, “8 out of 8” is “8 out of 8” no matter if it’s in the context of Ben’s coins or Emma’s sweeteners. Thus frequentists have <em>equally confident</em> conclusions that Ben can predict coin flips and Emma can distinguish between natural and artificial sweeteners.</p>
<div class="cell fig-cap-location-bottom" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="./images/img-infer/freq_diagram.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:30.0%"></p>
</figure>
</div>
</div>
</div>
<p>However, do you really believe Ben’s claim 100%? 🤔 😕. Let me guess. In fact, we judge their claim before evidence are collected, don’t we? 🤔 You probably think Ben overstates his ability but Emma’s claim sounds relatively reasonable, right?</p>
<p>Frequentist throws out all prior knowledge in favor of a mere 8 data points. Bayesian analyses <em>balance</em> and <em>weight</em> our prior experience/knowledge/belief and new data/evidence to judge a claim or make a conclusion.</p>
<div class="cell fig-cap-location-bottom" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="./images/img-infer/bayes-balance-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:56.0%"></p>
</figure>
</div>
</div>
</div>
<div class="columns">
<div class="column" style="width:80%;">
<p>However, <span class="green"><em>we are not stubborn!</em></span> If Ben had correctly predicted the outcome of <em>1 million</em> coin flips, the strength of this data would far surpass that of our prior judgement, leading to a posterior conclusion that perhaps Ben is psychic!</p>
</div><div class="column" style="width:20%;">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="./images/img-infer/psychic.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>The concept, whether referred to as prior information or prior belief, essentially represents our <em>current state of knowledge</em> about something <em>before</em> we gather additional evidence, data, or observations. This prior knowledge serves as the foundation upon which we build our understanding or enhance our knowledge of the subject of interest.</p>
<p>Personally, I prefer referring to <em>the current state of knowledge</em> as our <strong>opinion</strong> — or, to be more modest, our <strong>two cents</strong>! As Bayesians, we are individuals with opinions. We learn and grow from our life experiences, gradually aligning our opinions closer to the <em>truth</em> as we become more knowledgeable. I believe (and this is my opinion) that this process mirrors how we learn anything throughout our lives, from birth until our last day. A person without opinions is akin to someone without a brain—if you ask them a question, their response might always be, <em>“I have no idea,”</em> <em>“I have no judgment,”</em> or <em>“we should gather evidence to answer the question.”</em></p>
<p>However, as we grow up, we watch TV, read papers, attend classes, and continuously build our knowledge and judgments about the world around us. Our opinions, values, and the behavior shaped by these values and judgments define who we are. In essence, <em>a Bayesian holds an opinion on everything!</em></p>
<p><em>Opinions</em> lie at the core of the debate between Bayesians and frequentists. The reliance on <em>opinions</em> is a key reason why Bayes’ rule faced rejection over the past two centuries. Scientists have long sought to ensure that their work remains objective, but <em>opinions</em> are often viewed as inherently subjective. Consequently, frequentists and the broader scientific community have considered the subjectivity of <em>opinions</em> to be a fundamental flaw of Bayesianism.</p>
<p>However, while <em>opinions</em> are indeed subjective, they are by no means arbitrary, particularly when they are derived using Bayes’ rule and adhere to the laws of probability. In fact, Bayesians view <em>opinions</em> as fundamental strengths of Bayesian reasoning—provided these <em>opinions</em> are shaped by Bayes’ rule. <em>Opinions are considered a cornerstone of rationality</em>. This is the most contentious assertion of Bayesianism.</p>
</section><section id="asking-different-questions" class="level2" data-number="21.4"><h2 data-number="21.4" class="anchored" data-anchor-id="asking-different-questions">
<span class="header-section-number">21.4</span> Asking Different Questions</h2>
<blockquote class="blockquote">
<p>Our brain has this annoying habit to think that […], if under some hypothesis, results are unlikely, then the hypothesis is unlikely. This is false. – Christophe Michel (1974 - )</p>
</blockquote>
<blockquote class="blockquote">
<p>An approximate answer to the right problem is worth a good deal more than an exact answer to an approximate problem. – John Tukey (1915 - 2000)</p>
</blockquote>
<div class="columns">
<div class="column" style="width:80%;">
<p>Interestingly, Bayesians usually answer the question we care about.</p>
<p>In Question 4,</p>
<ul>
<li>Bayesians answer (a) <em>what’s the chance that I actually have COVID?</em>
</li>
<li>Frequentists answer (b) <em>if in fact I do not have COVID, what’s the chance that I would’ve gotten this positive test result?</em>
</li>
</ul>
</div><div class="column" style="width:20%;">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="./images/img-infer/covid.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="2484"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<table class="caption-top table">
<colgroup>
<col style="width: 22%">
<col style="width: 22%">
<col style="width: 31%">
<col style="width: 19%">
</colgroup>
<thead><tr class="header">
<th></th>
<th>Test Positive</th>
<th>Test Negative</th>
<th>Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>COVID</td>
<td>3</td>
<td>1</td>
<td>4</td>
</tr>
<tr class="even">
<td>No COVID</td>
<td>9</td>
<td>87</td>
<td>96</td>
</tr>
<tr class="odd">
<td>Total</td>
<td>12</td>
<td>88</td>
<td>100</td>
</tr>
</tbody>
</table>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Do not have COVID vs.&nbsp;<span class="math inline">\(H_1\)</span>: Have COVID</p></li>
<li><p>A frequestist assesses <em>the uncertainty of the observed data in light of an assumed hypothesis</em> <span class="math inline">\(P(Data \mid H_0) = 9/96\)</span></p></li>
<li><p>A Bayesian assesses <em>the uncertainty of the hypothesis in light of the observed data</em> <span class="math inline">\(P(H_0 \mid Data) = 9/12\)</span></p></li>
</ul>
<p>A Bayesian analysis would ask: Given my positive test result, what’s the chance that I actually have the disease? Since only 3 of the 12 people that tested positive have the disease, there’s only a 25% chance that you have the disease. Thus, when we take into account the disease’s rarity and the relatively high false positive rate, it’s relatively unlikely that you actually have the disease. What a relief.</p>
<p>Since disease status isn’t repeatable, the probability you have the disease is either 1 or 0 – you have it or you don’t. To the contrary, medical testing (and data collection in general) is repeatable. You can get tested for the disease over and over and over. Thus, a frequentist analysis would ask: If I don’t actually have the disease, what’s the chance that I would’ve tested positive? Since only 9 of the 96 people without the disease tested positive, there’s a roughly 10% (9/96) chance that you would’ve tested positive even if you didn’t have the disease.</p>
</section><section id="bayesian-inference" class="level2" data-number="21.5"><h2 data-number="21.5" class="anchored" data-anchor-id="bayesian-inference">
<span class="header-section-number">21.5</span> Bayesian Inference</h2>
<p>Here we use the Fake News example in the <a href="https://www.bayesrulesbook.com/chapter-2#building-a-bayesian-model-for-events">Bayes Rules! book</a> to illustrate a Bayesian model.</p>
<div class="columns">
<div class="column" style="width:80%;">
<ul>
<li>Tell if an incoming article is fake. The usage of an ! might seem odd for a real article. The exclamation point data is more consistent with fake news.</li>
</ul>
<ul>
<li>
<span class="green"><strong>Prior info</strong>:</span> 40% of the articles are fake</li>
</ul>
<div class="cell" data-layout-align="center">
<pre><code>#   type   n percent
#   fake  60     0.4
#   real  90     0.6
#  Total 150     1.0</code></pre>
</div>
</div><div class="column" style="width:20%;">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="./images/img-infer/fake_news.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="750"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<ul>
<li>
<span class="green"><strong>Data come in</strong>:</span> Check several fake and real articles, and found <code>!</code> is more consistent with fake news.</li>
</ul>
<div class="cell" data-layout-align="center">
<pre><code>#  title_has_excl fake real
#           FALSE   44   88
#            TRUE   16    2
#           Total   60   90</code></pre>
</div>
<hr>
<section id="bayesian-updating-rule" class="level3" data-number="21.5.1"><h3 data-number="21.5.1" class="anchored" data-anchor-id="bayesian-updating-rule">
<span class="header-section-number">21.5.1</span> Bayesian Updating Rule</h3>
<div class="cell fig-cap-location-bottom" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="./images/img-infer/fake_news_diagram.png" class="img-fluid quarto-figure quarto-figure-center figure-img" style="width:60.0%"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p><span class="math inline">\(F\)</span>: <em>an article is fake</em>.</p></li>
<li><p>The <strong>prior probability model</strong></p></li>
</ul>
<table class="caption-top table">
<colgroup>
<col style="width: 32%">
<col style="width: 19%">
<col style="width: 28%">
<col style="width: 17%">
</colgroup>
<thead><tr class="header">
<th>Event</th>
<th><span class="math inline">\(F\)</span></th>
<th><span class="math inline">\(F^c\)</span></th>
<th>Total</th>
</tr></thead>
<tbody><tr class="odd">
<td>Probability <span class="math inline">\(P(\cdot)\)</span>
</td>
<td>0.4</td>
<td>0.6</td>
<td>1</td>
</tr></tbody>
</table>
<div class="cell" data-layout-align="center">
<pre><code>#  title_has_excl fake real
#           FALSE   44   88
#            TRUE   16    2
#           Total   60   90</code></pre>
</div>
<ul>
<li><p><span class="math inline">\(D\)</span>: <em>an article title has exclamation mark</em>.</p></li>
<li><p>Conditional probability: <span class="math inline">\(P(D \mid F) = 16/60 = 0.27\)</span>; <span class="math inline">\(P(D \mid F^c) = 2/90 = 0.02\)</span>.</p></li>
<li>
<p><span class="green"><em>Opposite position</em></span>:</p>
<ul>
<li>Know the incoming article used <code>!</code> (observed data)</li>
<li>Don’t know whether or not the article is fake (what we want to decide).</li>
</ul>
</li>
<li><p>Compare <span class="math inline">\(P(D \mid F)\)</span> and <span class="math inline">\(P(D \mid F^c)\)</span> to ascertain the relative <strong>likelihoods</strong> of <em>observed data</em> <span class="math inline">\(D\)</span> under different scenarios of the <em>uncertain</em> article status.</p></li>
</ul>
<p>Since exclamation point usage is so much more likely among fake news than real news, this data provides some evidence that the article is fake To help distinguish this application of conditional probability calculations from that when <span class="math inline">\(D\)</span> is uncertain and <span class="math inline">\(F\)</span> is known, we’ll utilize the following likelihood function notation.</p>
<hr></section><section id="likelihood-function" class="level3" data-number="21.5.2"><h3 data-number="21.5.2" class="anchored" data-anchor-id="likelihood-function">
<span class="header-section-number">21.5.2</span> Likelihood Function</h3>
<div class="midi">
<ul>
<li>
<strong>Likelihood function</strong> <span class="math inline">\(L(\cdot\mid D)\)</span>:</li>
</ul>
<p><span class="math display">\[L(F \mid D) = P(D \mid F) \text{ and } L(F^c \mid D) = P(D \mid F^c)\]</span></p>
</div>
<div class="midi">
<ul>
<li>When <span class="math inline">\(F\)</span> is known, the <em>conditional probability</em> function <span class="math inline">\(P(\cdot \mid F)\)</span> compares the probabilities of an unknown event <span class="math inline">\(D\)</span>, <span class="math inline">\(D^c\)</span>, occurring with <span class="math inline">\(F\)</span>: <span class="math display">\[P(D \mid F) \text{  vs. } P(D^c \mid F)\]</span>
</li>
</ul>
</div>
<div class="midi">
<ul>
<li>When <span class="math inline">\(D\)</span> is known, the <em>likelihood function</em> <span class="math inline">\(L(\cdot \mid D) = P(D \mid \cdot)\)</span> evaluates the <em>relative compatibility</em> of data <span class="math inline">\(D\)</span> with <span class="math inline">\(F\)</span> or <span class="math inline">\(F^c\)</span>: <span class="math display">\[L(F \mid D) \text{  vs. } L(F^c \mid D)\]</span>
</li>
</ul>
</div>
<div class="small">
<table class="caption-top table">
<colgroup>
<col style="width: 36%">
<col style="width: 18%">
<col style="width: 26%">
<col style="width: 16%">
</colgroup>
<thead><tr class="header">
<th>Event</th>
<th><span class="math inline">\(F\)</span></th>
<th><span class="math inline">\(F^c\)</span></th>
<th>Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Probability <span class="math inline">\(P(\cdot)\)</span>
</td>
<td>0.4</td>
<td>0.6</td>
<td>1</td>
</tr>
<tr class="even">
<td>Likelihood <span class="math inline">\(L(\cdot \mid D)\)</span>
</td>
<td>0.27</td>
<td>0.02</td>
<td>0.29</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><span class="green"><em>The likelihood function is not a probability function!</em></span></li>
</ul>
<p>The likelihood function is essential to Bayesianism. When we want to update our belief or knowledge about the chance that a book is fake or not, given the information collected from the data, we need the likelihood function. Sometimes it may require a little imagination. It is like a <a href="https://en.wikipedia.org/wiki/Thought_experiment"><em>thought experiment</em></a>. If we were in the world that all books/articles are fake, what is the chance that we get the data like the one at hand? Similarly, what is that chance if we now lived in the world without any fake articles? Here the two hypotheses (fake vs.&nbsp;not fake), alternative theories, or proposed mechanisms postulate how data are generated and what data will be like under such mechanisms. And the likelihood function tells us which mechanism the current data fit in better.</p>
<p>The likelihood function is not a probability function! Although the word probability is avoided, the word likelihood is still misleading or hard to understand sometimes. <em>The likelihood of the data is not the credence of the theory/mechanism</em> which is <span class="math inline">\(P(F \mid D)\)</span> obtained by the Bayes’ rule.</p>
<hr></section><section id="bayes-rule-for-posterior" class="level3" data-number="21.5.3"><h3 data-number="21.5.3" class="anchored" data-anchor-id="bayes-rule-for-posterior">
<span class="header-section-number">21.5.3</span> Bayes’ Rule for Posterior</h3>
<p>To be Bayesian is to rest all knowledge upon the language of conditional probabilities. We obtain the credence of an article being fake given the fact that it has an exclamation mark in its title <span class="math inline">\(P(F \mid D)\)</span> using the conditional probability formula, and the law of total probabilities.</p>
<p><span class="math display">\[\begin{align*} P(F \mid D) &amp;= \frac{P(F \cap D)}{P(D)}\\ &amp;= \frac{P(D \mid F)P(F)}{P(D)} \\ &amp;= \frac{P(D \mid F)P(F)}{P(D \mid F)P(F) + P(D \mid F^c)P(F^c)}\\ &amp;= \frac{L(F \mid D)P(F)}{L(F \mid D)P(F) + L(F^c \mid D)P(F^c)}\end{align*}\]</span></p>
<p>In general, the update rule has the form</p>
<p><span class="math display">\[\text{posterior = } \frac{\text{likelihood} \cdot \text{prior }}{ \text{normalizing constant}} \]</span></p>
<p>The normalizing constant <span class="math inline">\(P(D)\)</span> is known as <strong>marginal likelihood</strong> or <strong>evidence</strong>. Here, the thing we care about, whether it is fake or not, only has two possible values, <span class="math inline">\(F\)</span> or <span class="math inline">\(F^c\)</span>. The marginal likelihood is then a sum of two terms. It combines different reasonings in mutually incompatible versions of reality. Even two terms summed up together could make Bayes’ rule hard to apply and understand, not to mention that there could be several even infinitely many possible values of what we care about. The computation of this normalizing constant can be extremely hard, and that’s why some people do not want to use Bayesian methods, even Bayesianism makes more sense to them. Fortunately, many statistical software packages for implementing Bayesian methods have been developed, doing all the necessary computation for us. If you love the Bayesian philosophy, don’t hesitate to use Bayes’ rule!</p>
<!-- -------------------------------------------------------------------------------- -->
<!-- ### Posterior -->
<p>Started with a prior understanding that there’s a 40% chance that the incoming article would be fake. Yet upon observing the use of an exclamation point in the title</p>
<div class="center">
<blockquote class="blockquote">
<p><em>“The president has a funny secret!”</em></p>
</blockquote>
</div>
<p>a feature that’s more common to fake news. Our posterior understanding evolved quite a bit – the chance that the article is fake jumped to 89%.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 40%">
<col style="width: 17%">
<col style="width: 24%">
<col style="width: 15%">
</colgroup>
<thead><tr class="header">
<th>Event</th>
<th><span class="math inline">\(F\)</span></th>
<th><span class="math inline">\(F^c\)</span></th>
<th>Total</th>
</tr></thead>
<tbody>
<tr class="odd">
<td>Prior prob <span class="math inline">\(P(\cdot)\)</span>
</td>
<td>0.4</td>
<td>0.6</td>
<td>1</td>
</tr>
<tr class="even">
<td>Posterior prob <span class="math inline">\(P(\cdot \mid D)\)</span>
</td>
<td>0.89</td>
<td>0.11</td>
<td>1</td>
</tr>
</tbody>
</table></section></section><section id="bayesian-inference-for-random-variables" class="level2" data-number="21.6"><h2 data-number="21.6" class="anchored" data-anchor-id="bayesian-inference-for-random-variables">
<span class="header-section-number">21.6</span> Bayesian Inference for Random Variables</h2>
<blockquote class="blockquote">
<p>If people do not believe that mathematics is simple, it is only because they do not realize how complicated life is. – John von Neumann (1903 - 1957)</p>
</blockquote>
<blockquote class="blockquote">
<p>Truth is much too complicated to allow anything but approximations. – John von Neumann (1903 - 1957)</p>
</blockquote>
<p>In this section, we use the example in <a href="https://www.bayesrulesbook.com/chapter-3">Chapther 3 of Bayes Rules!</a> to illustrate how to do the inference about the unknown parameter, for example the population mean <span class="math inline">\(\mu\)</span>, using Bayes’ rule. Instead of either simply providing a point estimate or calculating the confidence interval for the parameter, the Bayesian inference estimates the whole probability distribution of the parameter given the data. The distribution is the <strong>posterior distribution</strong> of the parameter that shows our credence of the parameter value.</p>
<p>In frequentist approaches, the parameter is considered unknown but fixed and constant. In contrast, Bayesian philosophy treats the parameter as variable, often assuming it to be <em>a random variable following some probability distribution</em>. This perspective acknowledges that the parameter may frequently change over time. For instance, do you believe the mean GPA of Marquette students remains unchanged? Or that the mean height or weight of Marquette students stays constant over time? These examples illustrate that many parameters are not static but evolve, reflecting the Bayesian view of a dynamic and probabilistic world.</p>
<p>Even if the parameter remains fixed at an unknown value, Bayesians still consider all the values they believe the <em>true</em> value could possibly be when making inferences. When Bayesians say the parameter value varies, they are referring to the idea that the <em>true</em> value has many possible values, and their belief about the <em>true</em> value is not quite certain or changes. The posterior distribution, or posterior probability, represents the plausibility of each possible value of the parameter, reflecting how their understanding of the <em>true</em> value changes with new evidence.</p>
<p>In the Bayesian framework, one never places all their confidence in a single model or parameter because, as the saying goes, “All models are wrong.” In parametric statistical inference, each parameter value typically corresponds to a specific model. Even if there were a <em>true</em> model defined by a <em>true</em> parameter value, Bayesians avoid putting all their eggs in one basket. Why is that? Consider this analogy: if you believe one of five company stocks will rise in value tomorrow, would you invest all your money in just one stock, or would you diversify your investments across all five stocks, allocating different shares based on your belief in the likelihood of each stock’s price increase? Moreover, what is considered “<em>truth</em>” can change over time. For instance, people once believed the world was flat—a “<em>truth</em>” that later proved to be incorrect. Therefore, Bayesians spread their credence across a range of possibilities, acknowledging that even widely accepted truths can evolve.</p>
<p>Yes, I know you want to pursue the truth. But unfortunately, <em>“truth is much too complicated to allow anything but approximations.”</em> Also <em>“all models are wrong.”</em> However, <em>some models are more believable and useful than others</em>. The universe is too complicated to be fully described by a model, whether it is mathematical, statistical, or physical. Since we never be able to capture the <em>truth</em>, it would benefit us if we switch our focus to the useful models that better approximates the <em>truth</em>. In estimation and prediction, almost all questions have no simple, unambiguous answer because we are always uncertain about something we don’t know. Therefore, the uncertainty quantification for the unknowns is important because good uncertainty quantification better describes what the truth could possibly be. As a result, what we need to pursue or develop is a less wrong but quite <em>useful</em> model that well qualifies the uncertainty and our credence of the unknown parameter. In other words, the model gives us a pretty well approximated posterior distribution.</p>
<p>Yes, the pursuit of truth is a noble endeavor, but as the saying goes, <em>“truth is much too complicated to allow anything but approximations.”</em> Furthermore, <em>“all models are wrong, but some models are more believable and useful than others.”</em> The complexity of the universe is such that no model—whether mathematical, statistical, or physical—can fully capture it. Since we can never truly grasp the truth, it is more beneficial to focus on developing models that better approximate the truth and are <em>useful</em> in practice.</p>
<p>In the realms of estimation and prediction, most questions lack simple, unambiguous answers because there is always some uncertainty surrounding the unknowns. This is why uncertainty quantification is crucial; it provides a better description of what the truth could potentially be. Consequently, our goal should be to develop models that are less wrong but highly <em>useful</em>, effectively quantifying uncertainty and our confidence in the unknown parameter. In other words, we should strive to create models that provide a well-approximated posterior distribution, offering a more accurate representation of the possible <em>truths</em>.</p>
<p>Done with the concepts. Let’s see the mathematical formulation of Bayesian inference. Suppose the unknown parameter to be estimated is <span class="math inline">\(\theta\)</span> and data are denoted as <span class="math inline">\({\bf Y} = (Y_1, \dots, Y_n)\)</span>. Both are assumed random variables before the data are collected. Our job is to do inference about <span class="math inline">\(\theta\)</span>, or obtain the posterior distribution of <span class="math inline">\(\theta\)</span>. Unlike frequentists, who develop a variety of tools for different estimation problems, Bayesians rely on just one fundamental principle: Bayes’ rule.</p>
<p>Let <span class="math inline">\(\pi(\theta)\)</span> be the prior pmf/pdf of <span class="math inline">\(\theta\)</span>. Let <span class="math inline">\(L(\theta \mid y_1,\dots, y_n)\)</span> be the likelihood of <span class="math inline">\(\theta\)</span> given observed data <span class="math inline">\(\by = \{y_i \}_{i = 1}^n\)</span>. Then with the Bayes’ rule, the posterior distribution of <span class="math inline">\(\theta\)</span> given <span class="math inline">\(\by\)</span> is</p>
<p><span class="math display">\[\pi(\theta \mid \by) = \frac{L(\theta \mid \by)\pi(\theta)}{p(\by)}\]</span> where <span class="math display">\[p(\by) = \begin{cases} \int_{\Theta} L(\theta \mid \by)\pi(\theta) ~ d\theta &amp; \text{if } \theta \text{ is continuous }\\
\sum_{\theta \in \Theta} L(\theta \mid \by)\pi(\theta) &amp; \text{if } \theta \text{ is discrete }
\end{cases}\]</span></p>
<p>Here, <span class="math inline">\(\Theta\)</span> is the collection of all possible values of <span class="math inline">\(\theta\)</span>. To obtain the posterior <span class="math inline">\(\pi(\theta \mid \by)\)</span>, we need to compute the marginal likelihood <span class="math inline">\(p(\by)\)</span> that is the core of Bayesian computation. As you can see, when <span class="math inline">\(\Theta\)</span> contains plenty of or infinitely many possible values of <span class="math inline">\(\theta\)</span>, or when several, say <span class="math inline">\(K\)</span>, unknown parameters are estimated simultaneously in one single model, i.e., <span class="math inline">\(\theta = (\theta_1, \dots, \theta_K)\)</span>, this normalizing constant is getting hard to compute. One of the Bayesian mainstream research is to develop well-approximated and computationally efficient algorithms to compute the marginal likelihood, and hence the posterior distribution.</p>
<p>We don’t need to worry about the computation issue at this moment. We focus on the ideas and concepts of applying Bayes’ formula. Notice that the marginal likelihood <span class="math inline">\(p(\by)\)</span> has nothing to do with <span class="math inline">\(\theta\)</span> because <span class="math inline">\(\theta\)</span> has been integrated out. Therefore, the posterior is in fact proportional to the numerator of the formula:</p>
<p><span class="math display">\[\pi(\theta \mid \by ) = \frac{L(\theta \mid \by)\pi(\theta)}{p(\by)} \propto_{\theta} L(\theta \mid \by)\pi(\theta)\]</span></p>
<p><span class="math display">\[\text{posterior } \propto \text{ likelihood } \cdot \text{ prior } \]</span></p>
<p>If, from our model, we determine that our posterior distribution is proportional to a known probability distribution, then our work is essentially complete. We have captured the entire shape of the posterior distribution, and the missing constant simply serves to scale it so that it becomes a valid probability distribution, with the integral under its density curve equal to one.</p>
<hr>
<section id="motivation-example" class="level3" data-number="21.6.1"><h3 data-number="21.6.1" class="anchored" data-anchor-id="motivation-example">
<span class="header-section-number">21.6.1</span> Motivation Example</h3>
<div class="columns">
<div class="column" style="width:70%;">
<ul>
<li>Michelle has decided to run for governor of Wisconsin.</li>
</ul>
<!-- - You've conducted 30 different polls throughout the election season. --><ul>
<li>According to <em>previous</em> 30 polls,
<ul>
<li>Michelle’s support is centered round 45%</li>
<li>she polled at around 35% in the dreariest days and around 55% in the best days</li>
</ul>
</li>
<li>With this prior information, we’d like to estimate/update Michelle’s support by <em>conducting a new poll</em>.</li>
</ul>
</div><div class="column" style="width:30%;">
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="./images/img-infer/vote.jpeg" class="img-fluid quarto-figure quarto-figure-center figure-img" width="3008"></p>
</figure>
</div>
</div>
</div>
</div>
</div>
<p>Key: Describe prior and data information using probabilistic models.</p>
<ul>
<li>The parameter to be estimated is <span class="math inline">\(\theta\)</span>, the Michelle’s support, which is between 0 and 1.</li>
</ul>
<section id="prior-distribution" class="level4" data-number="21.6.1.1"><h4 data-number="21.6.1.1" class="anchored" data-anchor-id="prior-distribution">
<span class="header-section-number">21.6.1.1</span> Prior Distribution</h4>
<ul>
<li>A popular probability distribution for probability is <strong>beta distribution</strong>, <span class="math inline">\(\text{beta}(\alpha, \beta)\)</span>, where <span class="math inline">\(\alpha &gt; 0\)</span> and <span class="math inline">\(\beta &gt; 0\)</span> are shape parameters.</li>
</ul>
<p><span class="math display">\[\pi(\theta \mid \alpha, \beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha - 1}(1-\theta)^{\beta-1}\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="https://upload.wikimedia.org/wikipedia/commons/f/f3/Beta_distribution_pdf.svg" class="img-fluid quarto-figure quarto-figure-center figure-img"></p>
</figure>
</div>
</div>
</div>
<ul>
<li><p>In a Bayesian model, we write <span class="math inline">\(\theta \sim \text{beta}(\alpha, \beta)\)</span> or <span class="math inline">\(\pi(\theta) = \text{beta}(\alpha, \beta)\)</span>.</p></li>
<li><p>In the prior model, <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are <strong>hyperparameters</strong> to be chosen to reflect our prior information.</p></li>
</ul>
<p>Here is what we know before we conduct a new survey.</p>
<blockquote class="blockquote">
<p>Michelle’s support is centered round 45%, and she polled at around 35% in the dreariest days and around 55% in the best days.</p>
</blockquote>
<ul>
<li>
<p>We can choose <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> so that the <em>prior mean is about 0.45</em> and <em>the range is from 0.35 to 0.55</em> using the fact that</p>
<ul>
<li><span class="math inline">\(\E(\theta) = \frac{\alpha}{\alpha + \beta}\)</span></li>
<li><span class="math inline">\(\Var(\theta) = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\)</span></li>
</ul>
</li>
</ul>
<p>We decide to use <span class="math inline">\(\text{beta}(45, 55)\)</span> as our prior distribution illustrating our prior belief and uncertainty about the Michelle’s support rate. The distribution is shown below.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="infer-bayes_files/figure-html/unnamed-chunk-20-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The process of choosing the optimal or reasonable hyperparameter values is called <strong>tuning</strong> or <strong>hyperparameter optimization</strong>.</p>
</section><section id="likelihood" class="level4" data-number="21.6.1.2"><h4 data-number="21.6.1.2" class="anchored" data-anchor-id="likelihood">
<span class="header-section-number">21.6.1.2</span> Likelihood</h4>
<blockquote class="blockquote">
<p>You plan to conduct a new poll of <span class="math inline">\(n = 50\)</span> Cheeseheads and record <span class="math inline">\(Y\)</span>, the number that support Michelle.</p>
</blockquote>
<div class="question">
<p>What distribution can be used for modeling likelihood connecting the data <span class="math inline">\(y\)</span> and the parameter we are interested, <span class="math inline">\(\theta\)</span>?</p>
</div>
<ul>
<li>If voters answer the poll <em>independently</em>, and the probability that any polled voter supports Michelle is <span class="math inline">\(\theta\)</span>, we could consider</li>
</ul>
<p><span class="math display">\[Y \mid \theta \sim \text{binomial}(n=50, \theta)\]</span></p>
<ul>
<li>The poll result is <span class="math inline">\(y = 30\)</span>, the likelihood is</li>
</ul>
<p><span class="math display">\[L(\theta \mid y = 30) = {50 \choose 30}\theta^{30}(1-\theta)^{20}, \quad \theta \in (0, 1)\]</span></p>
<p>The likelihood function is shown below.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure"><p><img src="infer-bayes_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576"></p>
</figure>
</div>
</div>
</div>
<p>The likelihood tells us that if <span class="math inline">\(\text{binomial}(n=50, \theta)\)</span> is the data generating mechanism, the data <span class="math inline">\(y = 30\)</span> is most compatible with the binomial mechanism when <span class="math inline">\(\theta\)</span> is 0.6. In other words, <span class="math inline">\(\text{binomial}(n=50, 0.6)\)</span> is the model that most likely generate the data <span class="math inline">\(y = 30\)</span>.</p>
<p>At this point, you might be wondering why we use the beta distribution to describe our prior belief and why we consider the thought experiment as a binomial experiment. These are indeed valid questions. Typically, we choose well-known and popular distributions or functions for our prior and likelihood because they allow us to derive the posterior distribution with minimal computational effort. You could argue that more sophisticated distributions or functions might better capture our belief and the specifics of the thought experiment. However, it’s important to remember that <em>“truth is much too complicated to allow anything but approximations,”</em> and <em>“all models are wrong, but some are useful.”</em> In this context, using the beta distribution and binomial likelihood proves to be particularly useful. It simplifies the tuning of hyperparameters in the prior distribution, and the posterior can be easily obtained. Moreover, as we will see, the posterior distribution remains within the beta distribution family, which greatly aids in interpreting how we update our knowledge.</p>
</section><section id="bayesian-model-and-posterior-distribution" class="level4" data-number="21.6.1.3"><h4 data-number="21.6.1.3" class="anchored" data-anchor-id="bayesian-model-and-posterior-distribution">
<span class="header-section-number">21.6.1.3</span> Bayesian model and posterior distribution</h4>
<p>Once we decide the prior distribution and likelihood, a Bayesian model is specified:</p>
<p><span class="math display">\[\begin{align}Y \mid \theta &amp;\sim \text{binomial}(n=50, \theta)\\ \theta &amp;\sim \text{beta}(45, 55)
\end{align}\]</span></p>
<p>Then our goal is to obtain the posterior distribution <span class="math inline">\(\pi(\theta \mid y)\)</span>. Here we don’t need any statistical algorithm, and we can derive the posterior distribution exactly through mathematical calculation.</p>
<p><span class="math display">\[
\begin{align} \pi(\theta \mid y) &amp;\propto_{\theta} L(\theta \mid y)\pi(\theta) \\
&amp;= {50 \choose 30}\theta^{30}(1-\theta)^{20} \times \frac{\Gamma(100)}{\Gamma(45)\Gamma(55)}\theta^{44}(1-\theta)^{54}\\
&amp;\propto_{\theta} \theta^{74}(1-\theta)^{74}\\
&amp;= \frac{\Gamma(150)}{\Gamma(75)\Gamma(75)} \theta^{74}(1-\theta)^{74} \\
&amp;= \text{beta}(75, 75)\end{align}
\]</span></p>
<p>using the fact that <span class="math inline">\(\int_{\mathcal{X}} f(x) dx = 1\)</span> for any pdf <span class="math inline">\(f(x)\)</span>.</p>
<p>In the calculation, we first write down the specific expression of the prior distribution and likelihood. Then we drop all constant terms with respect to <span class="math inline">\(\theta\)</span>, for example, <span class="math inline">\({50 \choose 30}\)</span> and <span class="math inline">\(\frac{\Gamma(100)}{\Gamma(45)\Gamma(55)}\)</span>. These terms have nothing to do with the shape of the posterior distribution.</p>
<p>The interesting part is that the terms related to <span class="math inline">\(\theta\)</span> from the prior and likelihood can be combined together, ending up with a single term <span class="math inline">\(\theta^{74}(1-\theta)^{74}\)</span>. Take a closer look at this term. First, it can be written as <span class="math inline">\(\theta^{75 - 1}(1-\theta)^{75-1}\)</span>. Then this term is actually the term related to <span class="math inline">\(\theta\)</span> in the beta distribution when <span class="math inline">\(\alpha = 75\)</span> and <span class="math inline">\(\beta = 75\)</span>, which is called the <strong>kernel</strong> of the probability distribution. Therefore, through the calculation, we actually capture the kernel of the distribution <span class="math inline">\(\text{beta}(75, 75)\)</span>. We are done. When we get the kernel of some probability distribution in the calculation of the posterior distribution, the posterior distribution will be that probability distribution. Therefore, the posterior distribution <span class="math inline">\(\pi(\theta \mid y)\)</span> is <span class="math inline">\(\frac{\Gamma(150)}{\Gamma(75)\Gamma(75)} \theta^{74}(1-\theta)^{74}\)</span>, the <span class="math inline">\(\text{beta}(75, 75)\)</span> distribution. We don’t need to worry about how to calculate the normalizing constant <span class="math inline">\(\frac{\Gamma(150)}{\Gamma(75)\Gamma(75)}\)</span>. Once we get <span class="math inline">\(\theta^{74}(1-\theta)^{74}\)</span>, the only way we make the kernel become a valid probability distribution is to multiply by the constant (w.r.t <span class="math inline">\(\theta\)</span>) <span class="math inline">\(\frac{\Gamma(150)}{\Gamma(75)\Gamma(75)}\)</span> that has been known from the beta distribution.</p>
<!-- #### Posterior Distribution -->
<p>Did you find that the prior and the posterior belong to the same family of probability distribution? They are both beta distributions. The prior is <span class="math inline">\(\pi(\theta) = \text{beta}(45, 55)\)</span> and the posterior is <span class="math inline">\(\pi(\theta \mid y) = \text{beta}(75, 75)\)</span>. When the prior and the posterior belong to the same family with respect to some likelihood, such prior is called a <strong>conjugate prior</strong>.</p>
</section><section id="relationship-between-prior-likelihood-and-posterior" class="level4" data-number="21.6.1.4"><h4 data-number="21.6.1.4" class="anchored" data-anchor-id="relationship-between-prior-likelihood-and-posterior">
<span class="header-section-number">21.6.1.4</span> Relationship between prior, likelihood and posterior</h4>
<p><a href="#fig-prior-lik-post" class="quarto-xref">Figure&nbsp;<span>21.3</span></a> illustrates the prior, likelihood, and posterior distributions in Michelle’s example. First, it’s important to note that the likelihood is not a probability distribution, so it has been scaled to resemble a density function for comparison. Let’s examine how our belief about Michelle’s support is updated. Initially, we believed her support was likely around 45%. However, after incorporating the new poll results, the model suggests that support around 60% offers the highest likelihood. This new evidence leads us to reconsider, suggesting that Michelle’s support may have increased.</p>
<p>Bayesians are not rigid in their thinking; we continuously update our knowledge and beliefs as new evidence emerges. The updated belief, reflected in the posterior distribution, effectively combines both prior information and new evidence. As a result, the posterior distribution typically falls between the prior and the likelihood. In this case, our belief about Michelle’s support is now highest within the range of 45% to 60%.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div id="fig-prior-lik-post" class="quarto-figure quarto-figure-center quarto-float anchored" data-fig-align="center">
<figure class="quarto-float quarto-float-fig figure"><div aria-describedby="fig-prior-lik-post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="infer-bayes_files/figure-html/fig-prior-lik-post-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="576">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-prior-lik-post-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;21.3: Prior, likelihood and posterior of the motivation beta-binomial example.
</figcaption></figure>
</div>
</div>
</div>
<p>Additionally, the variation in the posterior distribution is always smaller than that in the prior distribution. This occurs because the posterior distribution incorporates more information about Michelle’s support than the prior distribution does. The posterior distribution not only reflects the original prior information but also integrates the new data provided by the recent evidence. As a result, with more information about the parameter, we gain greater confidence in its likely value, leading to reduced uncertainty. Essentially, our belief becomes more refined and precise as we gather more evidence.</p>
<p>This process of belief updating can be understood more deeply through the mathematical relationship between the prior and posterior distributions. The parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> in the beta distribution are not just arbitrary numbers; they carry significant meaning. Specifically, the variance of the beta distribution decreases as<span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> increase. This explains why the posterior distribution <span class="math inline">\(\pi(\theta \mid y) = \text{beta}(75, 75)\)</span> has a smaller variance than the prior distribution <span class="math inline">\(\pi(\theta \mid y) = \text{beta}(45, 55)\)</span>.</p>
<p>In this beta-binomial example, <span class="math inline">\(\alpha + \beta = 45 + 55 = 100\)</span> an be interpreted as the <em>“prior sample size.”</em> Out of these 100 hypothetical prior samples, 45 support Michelle. The new data consists of a sample size of 50, with 30 supporting Michelle. Now, observe what happens with the posterior. When we combine the <em>prior sample</em> with the <em>new data sample</em>, we have a total of 150 samples in the posterior analysis. Out of these 150 data points, <span class="math inline">\(45 + 30 = 75\)</span> support Michelle, leading to the posterior distribution <span class="math inline">\(\pi(\theta \mid y) = \text{beta}(75, 75)\)</span>. The <em>posterior sample size</em> is now <span class="math inline">\(75 + 75 = 150\)</span>, with exactly 75 supporting Michelle.</p>
<p>Through this beta distribution, we can see that the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> reflect the amount of information we have about the parameter <span class="math inline">\(\theta\)</span>. This is how we systematically update our knowledge, with the posterior distribution representing a refined belief that incorporates both the prior information and the new data.</p>
<p>If each data point represents one piece of information, it’s clear that the posterior distribution now holds 150 pieces of information - 50 more than the prior distribution. Where do these additional 50 pieces come from? They come from the newly collected data. As we gather more information, the variability and uncertainty about the parameter decrease, leading to greater knowledge and confidence in the potential values of the parameter. The process of updating the posterior reflects this accumulation of information, resulting in a more precise and reliable estimate.</p>
<p>our posterior belief can be viewed as a weighted average of the prior and the likelihood, with the weights determined by the amount of information each contributes. In this example, the prior carries 100 pieces of information, while the new data contributes 50 pieces. Consequently, when we update our belief, the posterior belief is influenced two-thirds by the prior and one-third by the new data, since <span class="math inline">\(100/(100+50) = 2/3\)</span> and <span class="math inline">\(50/(100+50) =1/3\)</span>. This explains why the posterior mean support rate is <span class="math inline">\(0.45 \times (2/3) + 0.6 \times 1/3 = 0.5\)</span>. The posterior reflects a balanced combination of our prior knowledge and the new evidence, weighted by the information each provides. The Bayesians are logically consistent.</p>
<!-- #### Take-home Message -->
<!-- - The parameter is a random variable in the Bayesian world. -->
<!-- - It is fixed and constant in the frequentist world. -->
</section></section></section><section id="further-reading-and-references" class="level2" data-number="21.7"><h2 data-number="21.7" class="anchored" data-anchor-id="further-reading-and-references">
<span class="header-section-number">21.7</span> Further Reading and References</h2>
<ul>
<li><p><a href="https://www.bayesrulesbook.com/">Bayes Rules!</a></p></li>
<li><p><a href="https://www.youtube.com/watch?v=eDMGDhyDxuY">All About that Bayes: Probability, Statistics, and the Quest to Quantify Uncertainty</a> by Dr.&nbsp;Kristin Lennox</p></li>
<li><p><a href="https://www.amazon.com/Equation-Knowledge-Unified-Philosophy-Science/dp/0367428156">The Equation of Knowledge</a> by Dr.&nbsp;Le Nguyen Hoang</p></li>
</ul>


</section></main><!-- /main --><script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleGiscusIfUsed = (isAlternate, darkModeDefault) => {
    const baseTheme = document.querySelector('#giscus-base-theme')?.value ?? 'light';
    const alternateTheme = document.querySelector('#giscus-alt-theme')?.value ?? 'dark';
    let newTheme = '';
    if(darkModeDefault) {
      newTheme = isAlternate ? baseTheme : alternateTheme;
    } else {
      newTheme = isAlternate ? alternateTheme : baseTheme;
    }
    const changeGiscusTheme = () => {
      // From: https://github.com/giscus/giscus/issues/336
      const sendMessage = (message) => {
        const iframe = document.querySelector('iframe.giscus-frame');
        if (!iframe) return;
        iframe.contentWindow.postMessage({ giscus: message }, 'https://giscus.app');
      }
      sendMessage({
        setConfig: {
          theme: newTheme
        }
      });
    }
    const isGiscussLoaded = window.document.querySelector('iframe.giscus-frame') !== null;
    if (isGiscussLoaded) {
      changeGiscusTheme();
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  const darkModeDefault = false;
  let localAlternateSentinel = darkModeDefault ? 'alternate' : 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
    toggleGiscusIfUsed(toAlternate, darkModeDefault);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/chenghanyu-introstatsbook\.netlify\.app\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><nav class="page-navigation"><div class="nav-page nav-page-previous">
      <a href="./infer-p.html" class="pagination-link" aria-label="The Issues of $p$-value and Statistical Significance">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">The Issues of <span class="math inline">\(p\)</span>-value and Statistical Significance</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./infer-nonpar.html" class="pagination-link" aria-label="Nonparametric Methods">
        <span class="nav-page-text"><span class="chapter-number">22</span>&nbsp; <span class="chapter-title">Nonparametric Methods</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer">
    <div class="nav-footer-left">
<p>Copyright 2024, Dr.&nbsp;Cheng-Han Yu</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/chenghanyustats/introstatsbook/edit/main/infer-bayes.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/chenghanyustats/introstatsbook/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
<li class="nav-item compact">
    <a class="nav-link active" href="https://github.com/chenghanyustats/introstatsbook" aria-current="page">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
</div>
  </div>
</footer>


</body></html>